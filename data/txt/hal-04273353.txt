Fair Without Leveling Down: A New Intersectional
Fairness Definition
Gaurav Maheshwari, Aurélien Bellet, Pascal Denis, Mikaela Keller

To cite this version:

Gaurav Maheshwari, Aurélien Bellet, Pascal Denis, Mikaela Keller. Fair Without Leveling Down:
A New Intersectional Fairness Definition. EMNLP 2023 - The 2023 Conference on Empirical Meth-
ods in Natural Language Processing, Empirical Methods in Natural Language Processing, Dec 2023,
Singapore (SG), Singapore. ￿hal-04273353￿

HAL Id: hal-04273353

https://hal.science/hal-04273353

Submitted on 7 Nov 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Fair Without Leveling Down: A New Intersectional Fairness Definition

Gaurav Maheshwari, Aurélien Bellet, Pascal Denis, Mikaela Keller
Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France
first_name.last_name@inria.fr

Abstract

In this work, we consider the problem of in-
tersectional group fairness in the classifica-
tion setting, where the objective is to learn
discrimination-free models in the presence of
several intersecting sensitive groups. First, we
illustrate various shortcomings of existing fair-
ness measures commonly used to capture in-
tersectional fairness. Then, we propose a new
definition called the α-Intersectional Fairness,
which combines the absolute and the relative
performance across sensitive groups and can
be seen as a generalization of the notion of
differential fairness. We highlight several de-
sirable properties of the proposed definition
and analyze its relation to other fairness mea-
sures. Finally, we benchmark multiple popular
in-processing fair machine learning approaches
using our new fairness definition and show that
they do not achieve any improvement over a
simple baseline. Our results reveal that the in-
crease in fairness measured by previous def-
initions hides a “leveling down” effect, i.e.,
degrading the best performance over groups
rather than improving the worst one.

1

Introduction

The aim of fair machine learning is to develop
models that are devoid of discriminatory behav-
ior towards sensitive subgroups of the population.
A broad range of approaches have been devel-
oped (see e.g., Zafar et al., 2017; Denis et al.,
2021; Maheshwari and Perrot, 2022; Mehrabi et al.,
2022, and references therein), with most of them
focusing on a single sensitive axis (Lohaus et al.,
2020; Agarwal et al., 2018) such as gender (e.g.,
Male vs. Female) or race (e.g., African-Americans
vs. European-Americans). However, recent stud-
ies (Yang et al., 2020; Kirk et al., 2021) have
demonstrated that even when fairness can be en-
sured at the level of each individual sensitive axis,
significant unfairness can still exist at the inter-
section levels (e.g., Male European-Americans vs.

Female African-Americans). For example, Buo-
lamwini and Gebru (2018) showed that commer-
cially available face recognition tools exhibit sig-
nificantly higher error rates for darker-skinned fe-
males than for lighter-skinned males. Similar obser-
vations have been made by several studies in NLP
including contextual word representation (Tan and
Celis, 2019), and generative models (Kirk et al.,
2021). These findings resonate with the analytical
framework of intersectionality (Crenshaw, 1989),
which argues that systems of inequality based on
various attributes (like gender and race) may “inter-
sect” to create unique effects.

To capture these effects in the context of machine
learning, several intersectional fairness measures
have been proposed (Kearns et al., 2018; Hébert-
Johnson et al., 2018; Foulds et al., 2020). Amongst
them the most commonly used (Lalor et al., 2022;
Zhao et al., 2022; Subramanian et al., 2021) is Dif-
ferential Fairness (DF) (Foulds et al., 2020), which
is the log-ratio of the best-performing group to the
worst-performing group for a given performance
measure (such as the True Positive Rate). While
DF has many desirable properties, in this work
we emphasize that DF implements a “strictly egal-
itarian” view, i.e., it only considers the relative
performance between the group and ignores their
absolute performance. In particular, a trivial way to
improve fairness as measured by DF is by harming
the best-off group without improving the worst-off
group. This phenomenon, known as leveling down,
does not fit the desired fairness requirements in
many practical use-cases (Mittelstadt et al., 2023;
Zietlow et al., 2022). Yet, we empirically observe
that (i) popular fairness-promoting approaches tend
to level down more in intersectional fairness, and
(ii) this often goes unnoticed in the overall perfor-
mance of the model due to the large number of
groups induced by intersectional fairness.

To address these issues and explicitly capture
the leveling down phenomena, we propose a gener-

alization of DF, called α-Intersectional Fairness
(IFα), which takes into account both the relative
performance between the groups and the absolute
performance of the groups. More precisely, IFα is a
weighted average between the relative and absolute
performance of the groups, and allows the explo-
ration of the whole trade-off between these two
quantities by changing their relative importance via
a weight α ∈ [0, 1]. Our extensive benchmarks
across various datasets show that many existing
fairness-inducing methods aim for a different point
in the aforementioned trade-off and generally show
no consistent improvement over a simple uncon-
strained approach.

In summary, our primary contributions are as

follows:

• We showcase the shortcomings of the existing
intersectional fairness definition and propose
a generalization called α-Intersectional Fair-
ness. We analyze the properties and behavior
of the proposed fairness measure, and contrast
them with DF.

• We benchmark existing fairness approaches
on multiple datasets and evaluate their perfor-
mance with several fairness measures, includ-
ing ours. On the one hand, we find that many
fairness approaches optimize for existing fair-
ness measures by harming both the worst-off
and best-off groups or only the best-off group.
On the other hand, our measure is more care-
ful in showing improvements over a simple
baseline than previous metrics, allowing the
emphasis on cases of leveling down.

2 Setting

non-binary}, race: {European American, African
American}, and age: {under 45, above 45}. We de-
fine a sensitive group g as any p-dimensional vector
in the Cartesian product set G = A1 × · · · × Ap of
these sensitive axes. A sensitive group g ∈ G can
then be written as (a1, . . . , ap) with aj ∈ Aj.

2.2 Problem Statement

Consider a feature space X , a finite discrete label
space Y, and a set G representing all possible inter-
sections of p sensitive axes as defined above. Let
D be an unknown distribution over X × Y × G
through which we sample i.i.d a finite dataset
T = {(xi, yi, gi)}n
i=1 consisting of n examples.
This sample can be rewritten as T = (cid:83)
g∈G Tg
where Tg represents the subset of examples from
group g. The goal of fair machine learning is then
to learn an accurate model hθ ∈ H, with learnable
parameters θ ∈ RD, such that hθ : X → Y is fair
with respect to a given group fairness definition
like Equal Opportunity (Hardt et al., 2016), Equal
Odds (Hardt et al., 2016), Accuracy Parity (Zafar
et al., 2017), etc.

Existing group fairness definitions generally con-
sist of comparing a certain performance measure,
such as True Positive Rate (TPR), False Positive
In the
Rate (FPR) or accuracy, across groups.
following, for the sake of generality, we abstract
away from the particular measure and denote by
m(hθ, Tg) ∈ [0, 1] the group-wise performance for
model hθ on the group of examples Tg, with the
convention that higher values of m correspond
to better performance. For instance, in the case
of TPR (used to define Equal Opportunity) we de-
fine m(hθ, Tg) = TPR(hθ, Tg), while for FPR, we
define it as m(hθ, Tg) = 1 − FPR(hθ, Tg).

In this section, we begin by introducing our nota-
tions and then formally define problem statement.

3 Existing Intersectional Framework

2.1 Notations

In this study, we adopt and extend the notations
proposed by Morina et al. (2019). Let p denote
the number of distinct sensitive axes of interest,
which generally correspond to socio-demographic
features of a population. We refer to these sensi-
tive axes as A1, . . . , Ap, each of which is a set of
discrete-valued sensitive attributes. For instance,
a dataset may be composed of gender, race, and
age as the three sensitive axes, and each of these
sensitive axes may be encoded by a set of sen-
sitive attributes, such as gender: {male, female,

While the literature on group fairness in machine
learning initially considered a single sensitive axis,
several works have recently proposed fairness defi-
nitions for the intersectional setting (Gohar and
Cheng, 2023). Kearns et al. (2018) proposed
subgroup-fairness, which is based on the difference
in performance of a particular group weighted by
the size of the group. Several calibration and metric
fairness-based variants were considered by Hébert-
Johnson et al. (2018) and Yona and Rothblum
(2018). A shortcoming of these notions is that
they weight each group by its size, hence small
groups may not be protected even though they are

often the disadvantaged ones.

3.1 Differential Fairness

To circumvent the above issue, Foulds et al. (2020)
proposed Differential Fairness (DF), which puts a
constraint on the relative performance between all
pairs of groups. DF was originally proposed for
statistical parity (Foulds et al., 2020), and was then
extended by (Morina et al., 2019) to generalize
other fairness definitions such as parity in False
Positive Rates and Equal Odds. Below, we provide
a general definition of DF based on an arbitrary
group-wise performance measure m as defined in
Section 2.2.1

Definition 1 (Differential Fairness). A model hθ is
ϵ-differentially fair (DF ) with respect to a group-
wise performance measure m, if

DF(hθ, m) ≡ max
g,g′∈G

log

m(hθ, Tg)
m(hθ, Tg′)

≤ ϵ.

It is important to note that DF only depends
on the relative performance between the best-
performing group and the worst-performing group.

3.2 Shortcomings of Differential Fairness

We now highlight what we believe to be a key
shortcoming of DF in the context of intersectional
fairness: DF can be improved by leveling down,
i.e., harming the best-off and/or worst-off group,
without significantly affecting the overall perfor-
mance of the model. This problem is caused by
the combination of two factors.

First, DF is a strictly egalitarian measure that
only considers the relative performance between
groups. This can lead to situations where a model
that improves the performance across all groups is
deemed more unfair by DF. To illustrate this, let
the group-wise performance measure m to be the
TPR and consider two models hθ and h˜θ. Let the
worst-off and best-off group-wise performance of
hθ be 0.50 and 0.60, respectively. For h˜θ, let it be
0.65 and 0.95. According to DF, hθ is more fair
than h˜θ as the two groups are closer, while h˜θ has
better performance for both groups. In other words,
hθ is leveling down compared to h˜θ, but is deemed
more fair. This exhibits the tension between the

1We note that, in their extension to parity in False Positive
Rates, Morina et al. (2019) did not account for the fact that
higher FPR means lower performance, hence harming all
groups always leads to better fairness. Our general formulation
in Definition 1 fixes this problem through the convention that
higher m corresponds to better performance.

relative performance between groups, and the ab-
solute performance of the groups.

The second factor is that in intersectional fair-
ness, leveling down can have a negligible effect
on the overall performance of a model on the full
dataset. This is because the number of groups in
intersectional fairness is typically quite large (ex-
ponential in the number of sensitive axes p). There-
fore, the bulk of examples generally do not belong
to either the worst-off or best-off group, leading to
a situation where the performance of other groups
accounts for most of the model’s overall perfor-
mance. This issue may be further exacerbated if
the class proportions are imbalanced across groups.

4 α-Intersectional Fairness

In order to circumvent the above issue and effec-
tively capture intersectional fairness while taking
into account the leveling down phenomena, we
propose α-Intersectional Fairness (IFα). Our defi-
nition is essentially a convex combination of two
components, namely (i) ∆rel, which takes into ac-
count the relative performance between the two
groups, such as the ratio of their performance, and
(ii) ∆abs, which captures the leveling down effect
by accounting for the absolute performance of the
worst-off group.

More precisely, given a model hθ and a group-
wise performance measure m, let us first define a
measure of fairness for a pair of groups g and g′:

Iα(g, g′, hθ, m) = α∆abs + (1 − α)∆rel,

(1)

where α ∈ [0, 1] and
∆abs = max (cid:0)1 − m(hθ, Tg), 1 − m(hθ, Tg′)(cid:1) ,
1 − max (cid:0)m(hθ, Tg), m(hθ, Tg′)(cid:1)
1 − min (cid:0)m(hθ, Tg), m(hθ, Tg′)(cid:1) .

∆rel =

Now taking the maximum value of Iα over all
pairs of groups, we get our proposed notion of α-
Intersectional Fairness.

Definition 2 (α-Intersectional Fairness). A model
hθ is (α, γ)-intersectionally fair (IFα) with respect
to a group-wise performance measure m, if

IFα(hθ, m) ≡ max
g,g′∈G

Iα(g, g′, hθ, m) ≤ γ.

Note that IFα(hθ, m) can be equivalently ob-
tained as the the value of Iα over the pair of
worst performing and the best performing group,
as shown by the following proposition.

(α, γ)-
Proposition 1.
intersectionally fair with respect to a group-wise
performance measure m, then

If a model hθ

is

IFα(hθ, m) = Iα(gw, gb, hθ, m) ≤ γ,

where gw = arg ming∈G m(hθ, Tg) and gb =
arg maxg∈G m(hθ, Tg).

In the following, we compare and contrast our
fairness definition with DF when evaluating the
fairness of the two models. We then investigate
various properties of our proposed definition and
discuss the impact of α. In the interest of space, we
delegate our discussion around the design choices
for ∆rel and ∆abs to Appendix A.

Comparing DF and IFα. The primary differ-
ence between DF and IFα when comparing two
models arises when one model adversely affects
the worst-off group (∆abs) more than the other,
despite having better relative performance (∆rel).
In this case, DF would consistently consider one
model more fair than the other, whereas IFα en-
ables the exploration of this tension by varying the
relative importance of both criteria through α.

We formally capture this intuition as follows.
Consider two models hθ and h˜θ. Let the value of
the worst-off and the best-off group’s performance
for the model hθ be w and b, respectively. Similarly,
for model h˜θ let the worst and the best group’s
performance be ˜w and ˜b, respectively. Without
the loss of generality, ˜w and ˜b can be written as
˜w = w + x and ˜b = b + y. Note that x and y can
be either positive or negative as long as ˜w ≤ ˜b. We
visualize this setup in Figure 1. Based on this setup,
we have following cases:

• x ≥ y ≥ 0: In this case, hθ harms the worst-
off group (absolute performance) more, and
its relative performance is worse than h˜θ. In
Figure 1, this corresponds to ˜w ≥ w and
the blue region is smaller than the red region.
Here, IFα(h˜θ, m) ≤ IFα(hθ, m) ∀ α ∈ [0, 1],
and DF(h˜θ, m) ≤ DF(hθ, m).

• x ≤ y ≤ 0: This is similar to the case above,
but with h˜θ harming the groups more than hθ.
In Figure 1, this corresponds to ˜w ≤ w and
the blue region is larger than the red region.
Here, IFα(h˜θ, m) ≤ IFα(hθ, m) ∀ α ∈ [0, 1],
and DF(h˜θ, m) ≤ DF(hθ, m).

• All other cases: In this setting, one of the
model has better ∆abs performance, while

w

x

|

+

˜w

0

y

b

|

+
˜b

1

Figure 1: Group-wise performance range comparison.
The range of group-wise performances of models hθ
and model h˜θ are respectively [w, b] and [ ˜w, ˜b]. Note
that the difference x (resp. y) between the best (resp.
worst) group-wise performances of hθ and h˜θ can be
positive or negative.

the other model has better ∆rel performance.
The fairness in this setting depends on the
relative importance of absolute and relative
performance for IFα, while for DF it exclu-
sively depends on absolute performance. In
Figure 1, this corresponds to ˜w ≤ w and
the blue region is smaller than the red region
or vice-versa. Here, ∃α ∈ [0, 1] for which
IFα(h˜θ, m) ≥ IFα(hθ, m) and vice versa. On
the other hand, DF(h˜θ, m) ≤ DF(hθ, m) if
y ×m(hθ, Tgw ) ≤ x×m(hθ, Tgb), otherwise
DF(h˜θ, m) > DF(hθ, m).

To summarize, in the first two cases, one model
harms the worst-off group (absolute performance),
and the relative performance of that model is worse
than the other. Thus, a good fairness measure
should assign a higher unfairness to that model,
which both DF and IFα do.
In the third case,
one model performs better on the worst-off group,
while the other model has a closer relative perfor-
mance. The fairness in this setting depends on the
relative importance of absolute and relative perfor-
mance. Here, DF consistently assigns one model
a higher fairness than the other, while IFα enables
to explore this tension and tune the relative impor-
tance of both criteria through α. For instance, the
previous example in Section 2 falls in the third case.
On the one hand, DF will assign higher ϵ for hθ1
in comparison to hθ2. On the other hand, IFα will
assign higher γ for hθ1 for α ∈ (0.0, 0.81), while
for all other α, the γ would be higher for hθ2. We
illustrate the effect of α in more details below.

Impact of α: The parameter α allows to tune the
relative importance of ∆abs and ∆rel. On the one
end of the spectrum, α = 0 corresponds to con-
sidering only the relative performance ∆rel, while
α = 1 corresponds to considering only the abso-
lute performance. At α = 0.0 we recover the same
relative ranking of unfairness as DF, and thus DF
can be seen as a special case of IFα. In other words,

for any three models hθ1, hθ2, and hθ3 such that
DF(hθ1, m) ≥ DF(hθ2, m) ≥ DF(hθ3, m), then
IF0(hθ1, m) ≥ IF0(hθ2, m) ≥ IF0(hθ3, m). On
the other end, α = 1 only considers the absolute
performance ∆abs), and α = 0.5 corresponds to
giving ∆abs and ∆rel an equal importance. In prac-
tice, it is useful to visualize the complete trade-off
by plotting α (cid:55)→ IFα (see Section 5).

Intersectional Property: We have the following
intersectional property.

the model hθ be (α, γ)-
Proposition 2. Let
intersectionally fair over the set of groups defined
by G = A1 × · · · Ap. Let 1 ≤ s1 ≤ · · · ≤ sk ≤ p,
and P = As1 × · · · Ask be the Cartesian product
of the sensitive axes where sj ∈ N+. Then, hθ is
(α, γ)-intersectionally fair over P.

In other words, the fairness value calculated
over the intersectional groups also holds over in-
dependent and “gerrymandering” intersectional
groups (Yang et al., 2020). For instance, if a model
is (α, γ)-intersectionally fair in a space defined
by gender, race, and age, then it is also (α, γ)-
intersectionally fair in the space defined by gender
and race, or just gender. We delegate the proof to
Appendix B.

Generalization Guarantees: α-Intersectional
Fairness enjoys the same generalization guaran-
tees as the ones shown for DF in (Foulds et al.,
2020). Indeed, the result of Foulds et al. (2020) re-
lies on a generalization analysis of the group-wise
performance measure m, which directly translates
into generalization guarantees for IFα.

Guidelines for setting α: α-Intersectional Fair-
ness enables exploring the tradeoff between worst-
case performance and relative performance across
groups. Indeed, at alpha=0.0, only relative perfor-
mance is considered, aligning with strictly egalitar-
ian measures. On the other extreme, at alpha=1.0,
solely the worst-off group performance is consid-
ered. Based on this, we recommend:
Setting α = 0.75 (more focus towards worst case
performance) in:

• Situations where the cost of misclassification
is not similar for each group. In these cases,
leveling down would disproportionately affect
those subgroups for whom the cost is higher.
One example can be seen in education system,
where the cost of denying financial assistance

has higher impact on minority (Nora and Hor-
vath, 1989; Hinojosa, 2023).

• Cases where data for disadvantaged groups is
unreliable due to historical underrepresenta-
tion and lack of opportunities. For instance,
certain facial recognition systems exhibit a
higher likelihood of error when analyzing im-
ages of dark-skinned female individuals (Buo-
lamwini and Gebru, 2018). Similarly, Sap
et al. (2019) found that the hate speech detec-
tion systems are biased against black people.

In such contexts, emphasizing improvement for
these disadvantaged groups is more pivotal than
uniform performance over all subgroups, in line
with the ideas of affirmative action. These scenar-
ios best align with strategies seeking Demographic
Parity or Equalized Odds.

Setting α = 0.25 (more focus on relative perfor-

mance) in:

• Scenarios where no group is significantly
worse off, but to make sure that the algo-
rithm behaves similarly for all the groups in-
volved. This is related to algorithmic bias, as
presented by Mehrabi et al. (2021). Moreover,
the misclassification costs are similar in this
setting.

• Legal or regulatory requirements may man-
date similar outcomes across groups, like the
4/5th employment rule2. However, one must
exercise caution when extrapolating this to
other contexts, as it can lead to the "portabil-
ity trap" as discussed by (Selbst et al., 2019).

In such a context, the emphasis is on equality
among the groups. In practice, these scenarios in-
dicate places where the partitioner would advocate
for Accuracy Parity.

Otherwise, we recommend setting α = 0.50 (a
neutral default) when no domain or context-specific
insights are available. This is what we used in our
experiments. Ultimately, the choice of alpha re-
flects an understanding of the domain, the inherent
biases in the data, and the real-world consequences
of misclassifications.

2https://www.law.cornell.edu/cfr/text/29/1607.4

5 Experiments

In this section, we present experiments3 that show-
case (i) the model’s performance over the worst-off
group as the number of sensitive axes increases,
and (ii) the “leveling down" phenomenon observed
in various fairness-promoting mechanisms, along
with the effectiveness of α-Intersectional Fairness
in uncovering it. However, before describing these
experiments, we begin with an overview of the
datasets, baselines, and fairness measures used.

Datasets: We benchmark over four datasets cov-
ering both text and images, with varying numbers
of examples and sensitive groups:

• Twitter Hate Speech: The dataset is derived
from multilingual Twitter Hate speech cor-
pus (Huang et al., 2020) consisting of tweets
annotated with 4 demographic factors (sensi-
tive axes), namely age, race, gender, and coun-
try. The primary objective is to classify indi-
vidual tweets as either hate speech or non-hate
speech. In this work, we focus on the English
subset and binarize all the demographic fac-
tors resulting in a total of 63 sensitive groups.
Moreover, we only choose tweets where all
the demographic factors are present. Conse-
quently, our train, valid and test sets consists
of 22, 818, 4, 512, and 5, 032 tweets.

• CelebA (Liu et al., 2015): The dataset consists
of 202, 599 images of human faces, alongside
40 binary attributes for each image. We set
‘sex’, ‘Young’, ‘Attractive’, and ‘Pale Skin’
attributes as the sensitive axis for the images
and ‘Smiling’ as the class label. We split the
dataset into 80% training and 20% test split.
Furthermore, we set aside 20% of the training
set as the validation split.

• Psychometric dataset (Abbasi et al., 2021):
The dataset is a collection of 8, 502 free text
responses alongside numerical scores over
multiple psychometric dimensions.
In this
work, we focus on two dimensions:

– Numeracy reflects the numerical compre-
hension capability of the individual.
– Anxiety reflects the anxiety level as de-

scribed by the patient.

3source code is available here: https://github.com/

saist1993/BenchmarkingIntersectionalBias

Both these datasets consists of free text re-
sponses and binarized scores by the medical
expert. Moreover, each response is associated
with gender, race, age, and income. We use
same pre-processing as (Lalor et al., 2022)
and follow the same procedure to split the
dataset as described above.

For improved readability, we present a subset
of experiments in the main article. The remaining
experiments are included in the Appendix.

Methods. We evaluate the fairness performance
(i) Unconstrained
of the following methods:
which is oblivious to any fairness measure
and solely optimizes the model’s accuracy; (ii)
Adversarial implements standard adversarial
learning approach (Li et al., 2018), where an ad-
versary is added to the Unconstrained with the
objective to predict the sensitive attributes; (iii)
FairGrad (Maheshwari and Perrot, 2022), is an in-
processing approach that iteratively learns group-
specific weights based on the fairness level of the
model; (iv) INLP (Ravfogel et al., 2020), is a post-
processing approach that iteratively trains a clas-
sifier to predict the sensitive attributes and then
projects the representation on the classifier’s null
space. To enforce fairness across multiple sensi-
tive axes in this work, we follow the extension
proposed by Subramanian et al. (2021); (v) Fair
MixUp (Chuang and Mroueh, 2021) is a data aug-
mentation mechanism that enforces fairness by reg-
ularizing the model on the paths of interpolated
samples between the sensitive groups.

In all our experiments, we employ the same
model architecture for all the approaches to have
a fair comparison. Specifically, we use a three-
hidden layer fully connected neural network with
128, 64, and 32 corresponding sizes. Furthermore,
we use ReLU as the activation with dropout fixed
to 0.5. We optimize cross-entropy loss in all cases
with Adam (Kingma and Ba, 2015) as the opti-
mizer using default parameters. Moreover, for
Twitter Hate Speech and Numeracy datasets, we
encode the text using bert-base-uncased (Devlin
et al., 2019) text encoder. For CelebA, an image
dataset, we employ ResNet18 (He et al., 2016)
as the encoder. In all cases, we do not fine-tune
the pre-trained encoders. Lastly, several previous
studies have shown the effectiveness of equal sam-
pling in improving fairness (Kamiran and Calders,
2009; Chawla et al., 2003; Kamiran and Calders,
2010; González-Zelaya et al., 2021). That is, to

(a) CelebA TPR

(b) Numeracy TPR

(c) Twitter Hate Speech TPR

(d) CelebA FPR

(e) Numeracy FPR

(f) Twitter Hate Speech FPR

Figure 2: Test results over the worst-off group on CelebA, Twitter Hate Speech, and (b) Numeracy by varying the
number of sensitive axes. For p binary sensitive axis in the dataset, the total number of sensitive groups are p3 − 1.
Note that in FPR, lower the value better it is, while for TPR opposite is true.

counter the imbalance in the training data, the data
is resampled so that there is an equal number of
examples from each group and class in the final
training set. Through preliminary experiments, we
determine that equal sampling improves the worst-
case performance of several approaches, including
Unconstrained in various settings. We thus incor-
porate it as a hyperparameter indicating a contin-
uous scale between undersampling and oversam-
pling. Note that we also incorporate a setting where
no equal sampling is performed, and we take the
distribution as it is.

Fairness performance measure.
In this work
we focus on True Positive Rate parity and False
Positive Rate parity as the fairness measure. The
corresponding group wise performance measure
m for these fairness measures are TPR and FPR.
Formally, m in case of TPR for a group g is:

m(hθ, Tg) = P (hθ(x) = 1|y = 1) ∀x, y ∈ Tg,

while the FPR for a group g is:

m(hθ, Tg) = 1 − P (hθ(x) = 0|y = 1) ∀x, y ∈ Tg

In order to estimate the empirical probabilities,
we employ the bootstrap estimation procedure as
proposed by Morina et al. (2019).
In total, we
generate 1000 datasets by sampling from the orig-
inal dataset with replacement. We then estimate
the probabilities on this dataset using smoothed
empirical estimation mechanism and then average

the results over all the sampled datasets. In order
to evaluate the utility of various methods, we em-
ploy balanced accuracy. Note that the choice of
TPR Parity, and FPR Parity allows the derivation
of several other fairness measures including Equal
Opportunities, and Equalized Odds.

5.1 Worst-off performance and number of

sensitive axis

In this experiment, we empirically evaluate the in-
terplay between the number of sensitive groups and
the harm towards the worst-off group. To this end,
we iteratively increase the number of sensitive axes
in the dataset and report the performance of the
worst-off group for each approach. For instance,
with CelebA we first randomly added gender (ran-
domly chosen) when considering 1 sensitive axis.
In the next iteration, we added race (randomly cho-
sen) to the set with gender (previously added). Sim-
ilarly, we then added age, and finally country. Note
that for all the datasets, we start with a random
choice of sensitive axis hoping to remove any form
of selection bias. To select the optimal hyperpa-
rameters for this experiment, we follow the same
procedure described in (Maheshwari et al., 2022)
with the objective to select the hyperparameters
with the best performance over the worst-off group.
We plot the results of this experiment in Figure 2.
The results over the Anxiety dataset, which fol-
low similar trend, can be found in the Appendix C.
Based on these results, we observe that as the num-
ber of subgroups increases, the performance of the

1234sensitive axis0.50.60.70.8Worst TPRUnconstrainedAdversarialFairGradINLPFair MixUp123sensitive axis0.600.650.700.750.800.85Worst TPRUnconstrainedAdversarialFairGradINLPFair MixUp1234sensitive axis0.40.50.60.70.80.9Worst TPRUnconstrainedAdversarialFairGradINLPFair MixUp1234sensitive axis0.150.200.250.300.350.40Worst FPRUnconstrainedAdversarialFairGradINLPFair MixUp123sensitive axis0.250.300.350.400.450.50Worst FPRUnconstrainedAdversarialFairGradINLPFair MixUp1234sensitive axis0.250.300.350.400.450.50Worst FPRUnconstrainedAdversarialFairGradINLPFair MixUp(a) CelebA FPR

(b) Anxiety FPR

Figure 3: Value of IFα on the test set of CelebA, and Numeracy datasets for varying α ∈ [0, 1].

Method

BA ↑

Best Off ↓ Worst Off ↓

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.81 + 0.0
0.8 + 0.0
0.77 + 0.01
0.8 + 0.0
0.8 + 0.0

0.08 + 0.01
0.07 + 0.02
0.14 + 0.01
0.09 + 0.01
0.08 + 0.01

0.36 + 0.04
0.32 + 0.02
0.39 + 0.01
0.34 + 0.04
0.37 + 0.02

0.36 +/- 0.06
0.31 +/- 0.12
0.34 +/- 0.03
0.32 +/- 0.03
0.38 +/- 0.04

(a) Results on CelebA

Method

BA ↑

Best Off ↓ Worst Off ↓

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.63 + 0.01
0.62 + 0.01
0.63 + 0.01
0.63 + 0.01
0.61 + 0.02

0.27 + 0.04
0.28 + 0.05
0.33 + 0.04
0.27 + 0.04
0.3 + 0.03

0.5 + 0.03
0.53 + 0.09
0.59 + 0.06
0.49 + 0.03
0.61 + 0.07

0.38 +/- 0.05
0.43 +/- 0.04
0.49 +/- 0.05
0.36 +/- 0.03
0.58 +/- 0.03

(b) Results on Anxiety

IFα=0.5 ↓

0.31 +/- 0.02
0.28 +/- 0.04
0.4 +/- 0.02
0.32 +/- 0.01
0.3 +/- 0.01

IFα=0.5 ↓

0.55 +/- 0.06
0.55 +/- 0.06
0.61 +/- 0.03
0.56 +/- 0.05
0.56 +/- 0.03

Table 1: Test results on (a) CelebA, and (b) Anxiety using False Positive Rate while optimizing for DF. The utility
of various approaches is measured by balanced accuracy (BA), whereas fairness is measured by differential fairness
DF and intersectional fairness IFα=0.5. For both fairness definition, lower is better, while for balanced accuracy,
higher is better. The Best Off and Worst Off, in both cases lower is better, represents the min FPR and max FPR.
Results have been averaged over 5 different runs. We deem a method to exhibit leveling down if its performance
on either the worst-off or best-off group is inferior to the performance of an unconstrained model which we have
highlighted using cyan ( ).

worst-off group becomes worse for all approaches
in all settings. This can be attributed to the fact
that the number of training examples available
for each group decreases as the number of sen-
sitive axis in the dataset increases. In terms of the
performance of other approaches in comparison
to Unconstrained, we find that fairness-inducing
approaches generally perform better or similar to
Unconstrained when 1 or 2 sensitive axes are con-
sidered. However, when 3 or more sensitive axis
are considered, the performance of all approaches
tends to converge to that of Unconstrained. For
instance, in CelebA, on the one hand, with 1 sensi-

tive axis, all approaches significantly outperform
Unconstrained with the difference between the
best-performing method and Unconstrained be-
ing 0.26. On the other hand, when 4 sensitive axes
are considered, the difference between the best-
performing method and Unconstrained is 0.03,
with only Adversarial outperforming it.

In a similar fashion, when considering TPR over
Numeracy dataset, Unconstrained performs sig-
nificantly worse than FairGrad and Adversarial
with 1 sensitive axis while outperforming all ap-
proaches apart from INLP when 3 sensitive axis are
considered. Similar observations can be made for

0.00.10.20.30.40.50.60.70.80.91.00.250.300.350.40IFUnconstrainedAdversarialFairGradINLPFair MixUp0.00.10.20.30.40.50.60.70.80.91.00.500.550.600.65IFUnconstrainedAdversarialFairGradINLPFair MixUpNumeracy and Twitter Hate Speech datasets in the
FPR setting, with some minor exceptions. Overall
we find that most fairness approaches start harming
or do not improve the worst-off group as the num-
ber of sensitive axes grows in the dataset. Thus it
is pivotal for an intersectional fairness measure to
consider the harm induced by an approach while
calculating its fairness.

5.2 Benchmarking Intersectional Fairness

In this experiment, we showcase the leveling down
phenomena shown by various existing approaches.
We also compare and contrast IFα and DF. The
results of this comparison over FPR parity can be
found in Table 1a and 1b for CelebA and Anxiety
respectively. The results of remaining two datasets
over FPR parity, and all datasets over TPR Parity
can be found in Appendix C. In these experiment,
we deem a method to exhibit leveling down if its per-
formance on either the worst-off or best-off group
is inferior to the performance of an unconstrained
model. In the results table, we highlight the meth-
ods that show leveling down in cyan ( ).

We find that most of the methods have similar
balanced accuracy across all the datasets, even if
the fairness levels are different. This observation
aligns with the arguments presented in Section 3
about the relationship between group fairness mea-
sure and the overall performance. In terms of fair-
ness, most methods showcase leveling down. For
instance, over the CelebA dataset, all methods apart
from Adversarial shows leveling down. While in
the case of Anxiety, all methods apart from INLP
shows leveling down.

While comparing DF and IFα=0.5, we find that
IFα=0.5 is more conservative in assigning fairness
value, with most approaches performing similarly
to Unconstrained. Moreover, leveling down cases
may go unnoticed in DF. For instance, over the
CelebA dataset, even though FairGrad and INLP
showcases leveling down, the fairness value as-
signed by DF is lower for them than the one as-
signed to Unconstrained. Similar observation can
be seen over Numeracy in case of INLP.

A particular advantage of IFα over DF is that it
equips the practitioner with a more nuanced view
of the results. In Figure 3, we plot the complete
trade-off between the relative and the absolute per-
formance of groups by varying α. For instance, in
CelebA FPR, Fair MixUp shows the lowest level
of unfairness at α = 0.0. However, as soon as the

worst-off group’s performance is considered, i.e.,
α > 0.0, it rapidly becomes unfair with it being
one of the most unfair method at α = 1.0. Interest-
ingly, in Anxiety, INLP starts as one of the worst-
performing mechanisms. However, with α > 0.0,
it quickly outperforms most approaches.

These findings shed light on the trade-offs and
complexities inherent in optimizing fairness while
maintaining worst-off group performance. It high-
lights the need for comprehensive evaluation met-
rics and the importance of considering the per-
formance of both advantaged and disadvantaged
groups in the fairness analysis. Finally, we em-
phasize that methods do not always exhibit level-
ing down. In settings without leveling down, DF
adequately captures unfairness, producing values
similar to α-Intersectional Fairness. However, ev-
ery method displays some degree of leveling down
for some combinations of datasets and metrics. A
robust fairness measure should expose unfairness
universally, which our experiments demonstrate
IFα achieves.

6 Conclusion

We propose a new definition for measuring inter-
sectional fairness of statistical models, in the group
classification setting. We provide various compar-
ative analyses of our proposed measure, and con-
trast it with existing ones. Through them, we show
that our fairness definition can uncover various no-
tions of harm, including notably, the leveling down
phenomenon. We further show that many fairness-
inducing methods show no significant improvement
over a simple unconstrained approach. Through
this work, we provide tools to the community to
better uncover latent vectors of harm. Further, our
findings chart a path for developing new fairness-
inducing approaches which optimizes for fairness
without harming the groups involved.

7 Acknowledgement

The authors would like to thank the Agence Na-
tionale de la Recherche for funding this work under
grant number ANR-19-CE23-0022, as well as the
reviewers for their feedback and suggestions.

8 Limitations

While appealing, α-Intersectional Fairness also has
limitations. One of the primary ones is that it as-
sumes a minimum number of examples for each
subgroup to estimate the fairness level of the model

correctly. Moreover, it does not consider the data
drift over time, as it assumes a static view of the
problem. Thus we recommend checking the fair-
ness level over time to account for it. Further, in
this definition, setting up α is left to the practitioner
and thus can be abused. In the future, we aim to
develop mechanisms to validate α without access
to the dataset or model.

Finally, we want to emphasize that a hypothetical
perfectly fair model might not be devoid of social
harm. Firstly, vectors of harm of using statistical
models are not restricted to existing definitions of
group fairness. Further, if some socio-economic
groups are not present in a given dataset, exist-
ing fairness-inducing approaches are likely to not
have any positive impact towards them when en-
countered upon deployment. Such is the case with
commonly used datasets in the community, which
over-simplify gender and race as binary features,
ignoring people of mixed heritage, or non-binary
gender, for example. In our experiments, we too
have used these datasets, owing to their prevalence,
and we urge the community to create dataset with
non-binary attributes. That said, our measure works
with non-binary sensitive attributes, with no modi-
fications.

References

Ahmed Abbasi, David G. Dobolyi, John P. Lalor,
Richard G. Netemeyer, Kendall Smith, and Yi Yang.
2021. Constructing a psychometric testbed for fair
natural language processing. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11 November,
2021, pages 3748–3758. Association for Computa-
tional Linguistics.

Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík,
John Langford, and Hanna Wallach. 2018. A re-
In Inter-
ductions approach to fair classification.
national Conference on Machine Learning, pages
60–69. PMLR.

Joy Buolamwini and Timnit Gebru. 2018. Gender
shades: Intersectional accuracy disparities in com-
mercial gender classification. In Conference on Fair-
ness, Accountability and Transparency, FAT 2018,
23-24 February 2018, New York, NY, USA, volume 81
of Proceedings of Machine Learning Research, pages
77–91. PMLR.

Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O.
Hall, and Kevin W. Bowyer. 2003. Smoteboost: Im-
proving prediction of the minority class in boosting.
In Knowledge Discovery in Databases: PKDD 2003,

7th European Conference on Principles and Prac-
tice of Knowledge Discovery in Databases, Cavtat-
Dubrovnik, Croatia, September 22-26, 2003, Pro-
ceedings, volume 2838 of Lecture Notes in Computer
Science, pages 107–119. Springer.

Ching-Yao Chuang and Youssef Mroueh. 2021. Fair
In 9th Inter-
mixup: Fairness via interpolation.
national Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net.

Equal Employment Opportunity Commission et al.
1990. Uniform guidelines on employee selection
procedures. Fed Register, 1:216–243.

Kimberle Crenshaw. 1989. Demarginalizing the inter-
section of race and sex: A black feminist critique
of antidiscrimination doctrine, feminist theory and
antiracist politics. The University of Chicago Legal
Forum, 140:139–167.

Christophe Denis, Romuald Elie, Mohamed Hebiri, and
François Hu. 2021. Fairness guarantee in multi-class
classification. arXiv preprint arXiv:2109.13642.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers),
pages 4171–4186. Association for Computational
Linguistics.

James R. Foulds, Rashidul Islam, Kamrun Naher Keya,
and Shimei Pan. 2020. An intersectional definition
of fairness. In 36th IEEE International Conference
on Data Engineering, ICDE 2020, Dallas, TX, USA,
April 20-24, 2020, pages 1918–1921. IEEE.

Usman Gohar and Lu Cheng. 2023. A survey on in-
tersectional fairness in machine learning: Notions,
mitigation, and challenges. CoRR, abs/2305.06969.

Vladimiro González-Zelaya, Julián Salas, Dennis Pran-
gle, and Paolo Missier. 2021. Optimising fairness
through parametrised data sampling. In EDBT, pages
445–450.

Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equal-
In Ad-
ity of opportunity in supervised learning.
vances in Neural Information Processing Systems 29:
Annual Conference on Neural Information Process-
ing Systems 2016, December 5-10, 2016, Barcelona,
Spain, pages 3315–3323.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Úrsula Hébert-Johnson, Michael P. Kim, Omer Rein-
gold, and Guy N. Rothblum. 2018. Multicalibration:
Calibration for the (computationally-identifiable)
In Proceedings of the 35th International
masses.
Conference on Machine Learning, ICML 2018, Stock-
holmsmässan, Stockholm, Sweden, July 10-15, 2018,
volume 80 of Proceedings of Machine Learning Re-
search, pages 1944–1953. PMLR.

Elisa Reyes Hinojosa. 2023. Unequal access to higher
education: Student loan debt disproportionately im-
pacts minority students. Scholar, 25:63.

Xiaolei Huang, Linzi Xing, Franck Dernoncourt, and
Michael J. Paul. 2020. Multilingual Twitter cor-
pus and baselines for evaluating demographic bias
in hate speech recognition. In Proceedings of the
Twelfth Language Resources and Evaluation Confer-
ence, pages 1440–1448, Marseille, France. European
Language Resources Association.

Faisal Kamiran and Toon Calders. 2009. Classifying
without discriminating. In 2009 2nd international
conference on computer, control and communication,
pages 1–6. IEEE.

Faisal Kamiran and Toon Calders. 2010. Classification
with no discrimination by preferential sampling. In
Proc. 19th Machine Learning Conf. Belgium and The
Netherlands. Citeseer.

Michael J. Kearns, Seth Neel, Aaron Roth, and Zhi-
wei Steven Wu. 2018. Preventing fairness gerryman-
dering: Auditing and learning for subgroup fairness.
In Proceedings of the 35th International Conference
on Machine Learning, ICML 2018, Stockholmsmäs-
san, Stockholm, Sweden, July 10-15, 2018, volume 80
of Proceedings of Machine Learning Research, pages
2569–2577. PMLR.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In 3rd Inter-
method for stochastic optimization.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider
Iqbal, Elias Benussi, Frédéric A. Dreyer, Aleksandar
Shtedritski, and Yuki M. Asano. 2021. Bias out-
of-the-box: An empirical analysis of intersectional
occupational biases in popular generative language
In Advances in Neural Information Pro-
models.
cessing Systems 34: Annual Conference on Neural
Information Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual, pages 2611–2624.

John Lalor, Yi Yang, Kendall Smith, Nicole Forsgren,
and Ahmed Abbasi. 2022. Benchmarking intersec-
tional biases in NLP. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL 2022, Seattle, WA,
United States, July 10-15, 2022, pages 3598–3609.
Association for Computational Linguistics.

Yitong Li, Timothy Baldwin, and Trevor Cohn. 2018.
Towards robust and privacy-preserving text represen-
In Proceedings of the 56th Annual Meet-
tations.
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 25–30, Melbourne,
Australia. Association for Computational Linguistics.

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
2015. Deep learning face attributes in the wild. In
2015 IEEE International Conference on Computer
Vision, ICCV 2015, Santiago, Chile, December 7-13,
2015, pages 3730–3738. IEEE Computer Society.

Michael Lohaus, Michaël Perrot,

and Ulrike
Too relaxed to be fair.
Von Luxburg. 2020.
In International Conference on Machine Learning,
pages 6360–6369. PMLR.

Gaurav Maheshwari, Pascal Denis, Mikaela Keller, and
Aurélien Bellet. 2022. Fair NLP models with dif-
ferentially private text encoders. In Findings of the
Association for Computational Linguistics: EMNLP
2022, Abu Dhabi, United Arab Emirates, December
7-11, 2022, pages 6913–6930. Association for Com-
putational Linguistics.

Gaurav Maheshwari and Michaël Perrot. 2022. Fair-
grad: Fairness aware gradient descent. CoRR,
abs/2206.10923.

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena,
Kristina Lerman, and Aram Galstyan. 2021. A sur-
vey on bias and fairness in machine learning. ACM
computing surveys (CSUR), 54(6):1–35.

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena,
Kristina Lerman, and Aram Galstyan. 2022. A sur-
vey on bias and fairness in machine learning. ACM
Comput. Surv., 54(6):115:1–115:35.

Brent D. Mittelstadt, Sandra Wachter, and Chris Rus-
sell. 2023. The unfairness of fair machine learning:
Levelling down and strict egalitarianism by default.
CoRR, abs/2302.02404.

Giulio Morina, Viktoriia Oliinyk, Julian Waton, Ines
Marusic, and Konstantinos Georgatzis. 2019. Audit-
ing and achieving intersectional fairness in classifica-
tion problems. CoRR, abs/1911.01468.

Amaury Nora and Fran Horvath. 1989. Financial assis-
tance: Minority enrollments and persistence. Educa-
tion and Urban Society, 21(3):299–311.

Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael
Twiton, and Yoav Goldberg. 2020. Null it out: Guard-
ing protected attributes by iterative nullspace projec-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020, pages 7237–7256.
Association for Computational Linguistics.

Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,
and Noah A Smith. 2019. The risk of racial bias in
hate speech detection. In Proceedings of the 57th
annual meeting of the association for computational
linguistics, pages 1668–1678.

Andrew D Selbst, Danah Boyd, Sorelle A Friedler,
Suresh Venkatasubramanian, and Janet Vertesi. 2019.
Fairness and abstraction in sociotechnical systems.
In Proceedings of the conference on fairness, ac-
countability, and transparency, pages 59–68.

Shivashankar Subramanian, Xudong Han, Timothy
Baldwin, Trevor Cohn, and Lea Frermann. 2021.
Evaluating debiasing techniques for intersectional
biases. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Domini-
can Republic, 7-11 November, 2021, pages 2492–
2498. Association for Computational Linguistics.

Yi Chern Tan and L. Elisa Celis. 2019. Assessing so-
cial and intersectional biases in contextualized word
representations. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neu-
ral Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 13209–13220.

Forest Yang, Mouhamadou Cisse, and Oluwasanmi
Koyejo. 2020. Fairness with overlapping groups;
a probabilistic perspective. In Advances in Neural
Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual.

Gal Yona and Guy N. Rothblum. 2018. Probably ap-
proximately metric-fair learning. In Proceedings of
the 35th International Conference on Machine Learn-
ing, ICML 2018, Stockholmsmässan, Stockholm, Swe-
den, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pages 5666–5674.
PMLR.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez
Rogriguez, and Krishna P Gummadi. 2017. Fairness
constraints: Mechanisms for fair classification. In
Artificial Intelligence and Statistics, pages 962–970.
PMLR.

Eric Zhao, De-An Huang, Hao Liu, Zhiding Yu, Anqi
Liu, Olga Russakovsky, and Anima Anandkumar.
2022. Scaling fair learning to hundreds of intersec-
tional groups.

Dominik Zietlow, Michael Lohaus, Guha Balakrishnan,
Matthäus Kleindessner, Francesco Locatello, Bern-
hard Schölkopf, and Chris Russell. 2022. Leveling
down in computer vision: Pareto inefficiencies in fair
deep classifiers. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2022,
New Orleans, LA, USA, June 18-24, 2022, pages
10400–10411. IEEE.

A Design Choices

In this section, we discuss our design choices for
∆abs and ∆rel.

Choice of ∆rel An alternate choice of ∆rel is
to utilize the performance difference between the
groups instead of the above mentioned ratio. How-
ever, we advocate for the ratio as a superior choice
for the following reasons:

• Scale-Invariant Comparison: The ratio en-
ables comparing two models without the influ-
ence of the scale by normalizing the relative
performance of a model. For instance, assume
two models hθ and hθ′ with the worst and the
best group’s performance for hθ as 0.01 and
0.02 respectively, and 0.1 and 0.2 for hθ′. In
this setting, the ∆rel as the difference would
always assign hθ as fairer, even though both
models are twice worse for the worst group
compared to the best group. Note that our
overall fairness measure accounts for the ef-
fect of scale through the inclusion of ∆abs.
This is in-contrast to DFwhich does not take
scale into account.

• Alignment with the 80% rule: The ratio aligns
with the well-known 80% rule (Commission
et al., 1990), which states that there exists le-
gal evidence of discrimination if the ratio of
the probabilities for a favorable outcome be-
tween the disadvantaged sensitive group and
the advantaged sensitive group is less than
0.8. By adopting the ratio as ∆rel, our metric
adheres to this established criterion.

• Influence of worst-case group: If ∆rel repre-
sents the difference in performance, then at
α = 0.5 the model with better worst-case per-
formance will always have a lower γ than the
one with worse worst-case performance. In
other words, at α = 0.5, ∆abs would always
dominate ∆rel. However, this contradicts the
intuitive understanding that, at α = 0.5, both
∆rel and ∆abs should exert an equal influence.

Choice of ∆abs An alternate choice we explored
for ∆abs was the average performance of the
two groups involved instead of just the worst-
performing one. However, Proposition 1 does not
hold in the average case. This implies that a pair
of groups can exist for which Iα is larger than the
pair of groups consisting of the worst and best-
performing groups. Moreover, Proposition 1 is an
essential building block for intersectional property
which is described later.

Method

BA ↑

Best Off ↑ Worst Off ↑

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.8 + 0.01
0.8 + 0.01
0.78 + 0.01
0.8 + 0.0
0.79 + 0.01

0.84 + 0.01
0.84 + 0.01
0.85 + 0.02
0.85 + 0.03
0.85 + 0.03

0.45 + 0.04
0.46 + 0.04
0.44 + 0.04
0.52 + 0.05
0.48 + 0.05

0.62 +/- 0.03
0.6 +/- 0.04
0.66 +/- 0.02
0.49 +/- 0.04
0.57 +/- 0.05

(a) Results on CelebA

Method

BA ↑

Best Off ↑ Worst Off ↑

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.68 + 0.02
0.7 + 0.01
0.68 + 0.02
0.68 + 0.01
0.7 + 0.01

0.87 + 0.05
0.81 + 0.05
0.88 + 0.04
0.84 + 0.05
0.81 + 0.05

0.61 + 0.04
0.55 + 0.08
0.64 + 0.09
0.66 + 0.1
0.54 + 0.05

0.36 +/- 0.01
0.39 +/- 0.03
0.32 +/- 0.03
0.24 +/- 0.03
0.41 +/- 0.02

(b) Results on Numeracy

Method

BA ↑

Best Off ↑ Worst Off ↑

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.79 + 0.01
0.76 + 0.0
0.76 + 0.02
0.67 + 0.01
0.76 + 0.01

0.96 + 0.01
0.97 + 0.01
0.95 + 0.01
0.73 + 0.03
0.98 + 0.0

0.77 + 0.03
0.81 + 0.04
0.78 + 0.03
0.38 + 0.03
0.84 + 0.02

0.22 +/- 0.03
0.18 +/- 0.04
0.2 +/- 0.04
0.65 +/- 0.05
0.15 +/- 0.02

(c) Results on Twitter Hate Speech

Method

BA ↑

Best Off ↑ Worst Off ↑

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.63 + 0.01
0.63 + 0.01
0.63 + 0.01
0.63 + 0.01
0.62 + 0.01

0.77 + 0.02
0.82 + 0.05
0.76 + 0.01
0.76 + 0.02
0.75 + 0.07

0.47 + 0.07
0.51 + 0.1
0.47 + 0.06
0.51 + 0.04
0.45 + 0.07

0.49 +/- 0.05
0.47 +/- 0.06
0.48 +/- 0.04
0.4 +/- 0.01
0.51 +/- 0.03

(d) Results on Anxiety

IFα=0.5 ↓

0.43 +/- 0.01
0.44 +/- 0.01
0.43 +/- 0.03
0.41 +/- 0.02
0.43 +/- 0.04

IFα=0.5 ↓

0.38 +/- 0.09
0.45 +/- 0.07
0.35 +/- 0.07
0.44 +/- 0.08
0.44 +/- 0.07

IFα=0.5 ↓

0.2 +/- 0.03
0.21 +/- 0.03
0.25 +/- 0.03
0.56 +/- 0.03
0.16 +/- 0.01

IFα=0.5 ↓

0.5 +/- 0.02
0.45 +/- 0.05
0.52 +/- 0.02
0.51 +/- 0.03
0.52 +/- 0.06

Table 2: Test results on (a) CelebA, (b) Numeracy, and (c) Twitter Hate Speech using True Positive Rate while
optimizing for DF. The utility of various approaches is measured by balanced accuracy (BA), whereas fairness is
measured by differential fairness DF and intersectional fairness IFα=0.5. For both fairness definition, lower is better,
while for balanced accuracy, higher is better. The Best Off and Worst Off, in both cases higher is better, represents
the min TPR and max TPR. Results have been averaged over 5 different runs. We have also highlighted methods
which showcase leveling down using cyan ( ).

B Intersectional Property

In this section, we prove the intersectional property
stated in Section 4. The proof follows the same
procedure as described by Foulds et al. (2020). The
intersectional property states that:
the model hθ be (α, γ)-
Proposition. Let
intersectionally fair over the set of groups defined
by G = A1 × · · · Ap. Let 1 ≤ s1 ≤ · · · ≤ sk ≤ p,

and P = As1 × · · · Ask be the Cartesian product
of the sensitive axes where sj ∈ N+. Then, hθ is
(α, γ)-intersectionally fair over P.

The essential idea of the proof is to show that
the maximum and the minimum group wise per-
formance in P is bounded by the maximum and
the minimum group wise performance in G. After
proving the above, then using Proposition 1, we

Method

BA ↑

Best Off ↓ Worst Off ↓

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.7 + 0.01
0.71 + 0.01
0.7 + 0.02
0.68 + 0.01
0.7 + 0.01

0.22 + 0.03
0.14 + 0.03
0.19 + 0.06
0.27 + 0.08
0.22 + 0.05

0.5 + 0.04
0.38 + 0.02
0.51 + 0.07
0.52 + 0.08
0.48 + 0.03

0.44 +/- 0.1
0.33 +/- 0.22
0.5 +/- 0.22
0.42 +/- 0.13
0.41 +/- 0.17

(a) Results on Numeracy

Method

BA ↑

Best Off ↓ Worst Off ↓

DF ↓

Unconstrained
Adversarial
FairGrad
INLP
Fair MixUp

0.81 + 0.01
0.8 + 0.01
0.79 + 0.01
0.67 + 0.01
0.81 + 0.01

0.18 + 0.02
0.18 + 0.02
0.19 + 0.03
0.18 + 0.1
0.18 + 0.02

0.47 + 0.02
0.46 + 0.02
0.51 + 0.04
0.38 + 0.18
0.49 + 0.04

0.44 +/- 0.04
0.42 +/- 0.03
0.5 +/- 0.03
0.28 +/- 0.02
0.47 +/- 0.02

(b) Results on Twitter Hate Speech

IFα=0.5 ↓

0.51 +/- 0.04
0.42 +/- 0.08
0.45 +/- 0.07
0.58 +/- 0.06
0.52 +/- 0.06

IFα=0.5 ↓

0.46 +/- 0.03
0.47 +/- 0.04
0.47 +/- 0.04
0.47 +/- 0.1
0.46 +/- 0.03

Table 3: Test results on (a) Numeracy, and (b) Twitter Hate Speech using False Positive Rate while optimizing for
DF. The utility of various approaches is measured by balanced accuracy (BA), whereas fairness is measured by
differential fairness DF and intersectional fairness IFα=0.5. For both fairness definition, lower is better, while for
balanced accuracy, higher is better. The Best Off and Worst Off, in both cases lower is better, represents the min
FPR and max FPR. Results have been averaged over 5 different runs. We have also highlighted methods which
showcase leveling down using cyan ( ).

C Extended Experiments

In this section, we detail the additional results. Ta-
ble 2 provides results for the True Positive Rate
(TPR) fairness measure, as outlined in the Experi-
ment Section 5.2. In Figure 6, we vary the number
of sensitive axes and plot the worst-case perfor-
mance for Anxiety in FPR and TPR settings. Fi-
nally, Table 3 displays results related to the FPR
parity fairness measure, focusing on the Twitter
Hate Speech and Numeracy datasets. Notably, for
TPR, each method exhibits leveling down in at least
one dataset. For example, Adversarial shows lev-
eling down in the Numeracy dataset, whereas INLP
does so in both the Twitter Hate Speech and Anxi-
ety datasets. Similarly, as with FPR, DF does not
consistently identify leveling down. As evidence,
while both FairGrad and INLP demonstrate level-
ing down, they show a better fairness level than
Unconstrained.

can show that IFα over G is higher than IFα over
P.

Define E = A1 × . . . × Aa−1 × Aa+1 . . . ×
Ak−1 × Ak+1 × . . . × Ap, the Cartesian product of
the protected attributes included in G but not in P.
Then for any model hθ, y ∈ Range(hθ),

max
g∈P:P (g|θ)>0

Phθ (hθ(x) = y|P = g)

=

max
g∈P:P (g|θ)>0

(cid:88)

e∈E

(cid:88)

Phθ (hθ(x) = y|E = e, g)

Phθ (E = e|g)

max

≤

max
g∈P:P (g|θ)>0

e∈E

(E=e′|g)>0

e′∈E:Phθ
(cid:0)Phθ (hθ(x) = y|E = e′, g)(cid:1) × Pθ(E = e|g)
max
=
e′∈E:Pθ(E=e′|g,θ)>0

max
g∈P:P (g|θ)>0
Phθ (hθ(x) = y|E = e′, g)

=

max
s′∈G:P (s′|θ)>0

PM,θ(M (x) = y|s′)

a

By

similar

argument,
ming∈P:P (g|θ)>0 Phθ (hθ(x) = y|P = g) ≥
ming′∈G:P (g′|θ)>0 Phθ (hθ(x) = y|g′). Applying
Corollary 1, we hence bound γ in P by the γ in G

(a) CelebA TPR

(b) Numeracy TPR

(c) Twitter Hate Speech TPR

Figure 4: Value of IFα on the test set of CelebA, Numeracy, and Twitter Hate Speech datasets for varying α ∈ [0, 1].

(a) Anxiety FPR

(b) Anxiety TPR

Figure 5: Value of IFα on the test set of Anxiety datasets for varying α ∈ [0, 1].

(a) Anxiety FPR

(b) Anxiety TPR

Figure 6: Test results over the worst-off group on Anxiety by varying the number of sensitive axes. For p binary
sensitive axis in the dataset, the total number of sensitive groups are p3 − 1. Note that in FPR, lower the value better
it is, while for TPR opposite is true.

0.00.10.20.30.40.50.60.70.80.91.00.300.350.400.450.500.55IFUnconstrainedAdversarialFairGradINLPFair MixUp0.00.10.20.30.40.50.60.70.80.91.00.300.350.400.450.500.55IFUnconstrainedAdversarialFairGradINLPFair MixUpUnconstrained_augmented0.00.10.20.30.40.50.60.70.80.91.00.20.30.40.5IFUnconstrainedAdversarialFairGradINLPFair MixUp0.00.10.20.30.40.50.60.70.80.91.00.500.550.600.65IFUnconstrainedAdversarialFairGradINLPFair MixUp0.00.10.20.30.40.50.60.70.80.91.00.4500.4750.5000.5250.5500.575IFUnconstrainedAdversarialFairGradINLPFair MixUp123sensitive axis0.30.40.50.60.7Worst FPRUnconstrainedAdversarialFairGradINLPFair MixUp123sensitive axis0.500.550.600.650.70Worst TPRUnconstrainedAdversarialFairGradINLPFair MixUp