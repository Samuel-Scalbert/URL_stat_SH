Towards Fully-fledged Archiving for RDF Datasets
Olivier Pelgrin, Luis Galárraga, Katja Hose

To cite this version:

Olivier Pelgrin, Luis Galárraga, Katja Hose. Towards Fully-fledged Archiving for RDF Datasets.
￿10.3233/SW-
Semantic Web – Interoperability, Usability, Applicability, 2021, 12 (6), pp.903-925.
210434￿. ￿hal-03500522￿

HAL Id: hal-03500522

https://inria.hal.science/hal-03500522

Submitted on 22 Dec 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Semantic Web 1 (0) 1–20
IOS Press

1

Towards Fully-ﬂedged Archiving for RDF
Datasets
Olivier Pelgrin a,*, Luis Galárraga b and Katja Hose a
a Department of Computer Science, Aalborg University, Denmark
E-mails: olivier@cs.aau.dk, khose@cs.aau.dk
b Inria, France
E-mail: luis.galarraga@inria.fr

Editors: Axel-Cyrille Ngonga Ngomo, University of Paderborn, Germany; Muhammad Saleem, University of Leipzig, Germany; Ruben
Verborgh, Ghent University – imec, Germany
Solicited reviews: Natanael Arndt, Universität Leipzig, Germany; Pascal Molli, Nantes University, France; Andre Valdestilhas, Universität
Leipzig, Germany

Abstract. The dynamicity of RDF data has motivated the development of solutions for archiving, i.e., the task of storing and
querying previous versions of an RDF dataset. Querying the history of a dataset ﬁnds applications in data maintenance and
analytics. Notwithstanding the value of RDF archiving, the state of the art in this ﬁeld is under-developed: (i) most existing
systems are neither scalable nor easy to use, (ii) there is no standard way to query RDF archives, and (iii) solutions do not exploit
the evolution patterns of real RDF data. On these grounds, this paper surveys the existing works in RDF archiving in order to
characterize the gap between the state of the art and a fully-ﬂedged solution. It also provides RDFev, a framework to study the
dynamicity of RDF data. We use RDFev to study the evolution of YAGO, DBpedia, and Wikidata, three dynamic and prominent
datasets on the Semantic Web. These insights set the ground for the sketch of a fully-ﬂedged archiving solution for RDF data.

Keywords: RDF Archiving, Knowledge Bases

1. Introduction

The amount of RDF data has steadily grown since
the conception of the Semantic Web in 2001 [17], as
more and more organizations opt for RDF [69] as the
format to publish and manage semantic data [42, 44].
For example, by July 2009 the Linked Open Data
(LOD) cloud counted a few more than 90 RDF datasets
adding up to almost 6.7B triples [18]. By 2020, these
numbers have catapulted to 1200+ datasets1 and at
least 28B triples2, although estimates based on LOD-
Stats [25] suggest more than 10K datasets and 150B+
triples if we consider the datasets with errors omitted
by the LOD Cloud [62]. This boom does not only owe

*Corresponding author. E-mail: olivier@cs.aau.dk.
1https://lod-cloud.net/
2http://lod-a-lot.lod.labs.vu.nl/

credit to the increasing number of data providers and
availability of Open Government Data [1, 4, 10], but
also to the constant evolution of the datasets in the
LOD cloud. This phenomenon is especially true for
community-driven initiatives such as DBpedia [13],
YAGO [77], or Wikidata [26], and also applies to au-
tomatically ever-growing projects such as NELL [21].
Storing and querying the entire edition history of an
RDF dataset, a task we call RDF archiving, has plenty
of applications for data producers. For instance, RDF
archives can serve as a backend for ﬁne-grained ver-
sion control in collaborative projects [9, 12, 32, 36,
52, 72]. They also allow data providers to study the
evolution of the data [29] and track errors for debug-
ging purposes. Likewise, they can be of use to RDF
streaming applications that rely on a structured history
of the data [20, 43]. But archives are also of great value
for consumer applications such as data analytics, e.g.,

1570-0844/0-1900/$35.00 © 0 – IOS Press and the authors. All rights reserved

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

2

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

mining correction patterns [64, 65] or historical trend
analysis [45].

For all the aforementioned reasons, a signiﬁcant
body of literature has started to tackle the problem
of RDF archiving. The current state of the art ranges
from systems to store and query RDF archives [3, 11,
12, 23, 34, 36, 59, 67, 72, 78, 81], to benchmarks
to evaluate such engines [29, 51], as well as tempo-
ral extensions for SPARQL [16, 30, 35, 66]. Diverse
in architecture and aim, all these works respond to
particular use cases. Examples are solutions such as
R&Wbase [72], R43ples [36], and Quit Store [12] that
provide data maintainers with distributed version con-
trol management in the spirit of Git. Conversely, other
works [34, 66] target data consumers who need to an-
swer time-aware queries such as “obtain the list of
house members who sponsored a bill from 2008”. In
this case the metadata associated to the actual triples is
used to answer domain-speciﬁc requirements.

Despite this plethora of work, there is currently no
available fully-ﬂedged solution for the management of
large and dynamic RDF datasets. This situation origi-
nates from multiple factors such as (i) the performance
and functionality limitations of RDF engines to han-
dle metadata, (ii) the absence of a standard for query-
ing RDF archives, and (iii) a disregard of the actual
evolution of real RDF data. This paper elaborates on
factors (i) and (ii) through a survey of the state of
the art that sheds light on what aspects have not yet
been explored. Factor (iii) is addressed by means of
a framework to study the evolution of RDF data ap-
plied to three large and ever-changing RDF datasets,
namely DBpedia, YAGO, and Wikidata. The idea is to
identify the most challenging settings and derive a set
of design lessons for fully-ﬂedged RDF archive man-
agement. We therefore summarize our contributions as
follows:

(cid:104)s, p, o(cid:105), (cid:104)s, p, o, ρ(cid:105)

G
g
Gi
A = {G0, G1, . . . }
u = {u+, u−}

ui, j = {u+

i, j, u−
i, j}

rv(ρ)
ts(ρ)
l(ρ), l(G)

triple and 4-tuple: subject, predicate, ob-
ject, graph revision
an RDF graph
a graph label
the i-th version or revision of graph G
an RDF graph archive
an update or changeset with sets of
added and deleted triples.
the changeset between graph revisions i
and j ( j > i)
revision number of graph revision ρ
commit time of graph revision ρ
labels of graph revision ρ and graph G

Table 1

Notation related to RDF Graphs.

ature, as well as a discussion about the challenges
in the design and implementation of such a system.

This paper is organized as follows. In Section 2 we in-
troduce preliminary concepts. Then, Section 3 presents
RDFev, addressing contribution (1). Contribution (2) is
elaborated in Section 4. In the light of the evolution of
real-world RDF data, we then survey the strengths and
weaknesses of the different state-of-the-art solutions in
Section 5 (contribution 3). Section 6 addresses contri-
bution (4). The insights from the previous sections are
then used to drive the sketch of an optimal RDF archiv-
ing system in Section 7, which addresses contribution
(5). Section 8 concludes the paper.

2. Preliminaries

This section introduces the basic concepts in RDF
archive storage and querying, and proposes some for-
malizations for the design of RDF archives.

1. RDFev, a metric-based framework to analyze the

2.1. RDF Graphs

evolution of RDF datasets;

2. A study of the evolution of DBpedia, YAGO, and

Wikidata using RDFev;

3. A detailed survey of existing work on RDF archive
management systems and SPARQL temporal exten-
sions;

4. An evaluation of Ostrich [78] on the history of
DBpedia, YAGO, and Wikidata. This was the only
system that could be tested on the experimental
datasets;

5. The sketch of a fully-ﬂedged RDF archiving system
that can satisfy the needs not addressed in the liter-

We deﬁne an RDF graph G as a set of triples t =
(cid:104)s, p, o(cid:105), where s ∈ I ∪ B, p ∈ I, and o ∈ I ∪ L ∪ B
are the subject, predicate, and object of t, respectively.
Here, I, L, and B are sets of IRIs (entity identiﬁers),
literal values (e.g., strings, integers, dates), and blank
nodes (anonymous entities) [69]. The notion of a graph
is based on the fact that G can be modeled as a directed
labeled graph where the predicate p of a triple denotes
a directed labeled edge from node s to node o. The
RDF W3C standard [69] deﬁnes a named graph as an
RDF graph that has been associated to a label l(G) =

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

3

(cid:104)s, p, o, ρ, ζ(cid:105)

D = {G0, G1, . . . }
A = {D0, D1, . . . }
D j
Gk
i

ˆu = {ˆu+, ˆu−}

U = {ˆu, u0, u1, . . . }

U+, U−

Ui, j

rv(ζ)
ts(ζ)
Υ(·)

a 5-tuple subject, predicate, object,
graph revision, and dataset revision
an RDF dataset
an RDF dataset archive
the j-th version or revision of dataset D
the i-th revision of the k-th graph in a
dataset archive

a graph changeset with sets of added and
deleted graphs
a dataset update or changeset consisting
of a graph changeset ˆu and changesets ui
associated to graphs Gi
the addition/deletion changes of U:
U+ = {ˆu+, u0+, u1+, . . . } , U− =
{ˆu−, u0−, u1−, . . . }
the dataset changeset between dataset
revisions i and j ( j > i)
revision number of dataset revision ζ
commit time of dataset revision ζ
the set of terms (IRIs, literals, and blank
nodes) present in a graph G, dataset D,
changeset u, and dataset changeset U.

i, j, u−

trary pairs of revisions i, j with i < j, and denote by
ui, j = (cid:104)u+
i, j(cid:105) the changeset such that G j = ui, j(Gi).
We remark that a graph archive can also be mod-
eled as a collection of 4-tuples (cid:104)s, p, o, ρ(cid:105), where ρ ∈
I is the RDF identiﬁer of revision i = rv(ρ) and
rv ⊂ I × N is a function that maps revision iden-
tiﬁers to natural numbers. We also deﬁne the func-
tion ts ⊂ I × N that associates a revision identi-
ﬁer ρ to its commit time, i.e., the timestamp of ap-
plication of changeset ui. Some solutions for RDF
archiving [12, 34, 36, 59, 72, 78] implement this
logical model in different ways and to different ex-
tents. For example, R43ples [36], R&WBase [72] and
Quit Store [12] store changesets and/or their associ-
ated metadata in additional named graphs using PROV-
O [24]. In contrast, x-RDF-3X [59] stores the tempo-
ral metadata in special indexes that optimize for con-
current updates at the expense of temporal consistency,
i.e., revision numbers may not always be in concor-
dance with the timestamps.

Table 2
Notation related to RDF Datasets.

2.3. RDF Dataset Archives

g ∈ I ∪B. The function l(·) returns the associated label
of an RDF graph, if any. Table 1 provides the relevant
notation related to RDF graphs.

2.2. RDF Graph Archives

i , u−

Intuitively, an RDF graph archive is a temporally-
ordered collection of all the states an RDF graph has
gone through since its creation. More formally, a graph
archive A = {Gs, Gs+i, . . . , Gs+n−1} is an ordered set
of RDF graphs, where each Gi is a revision or ver-
sion with revision number i ∈ N , and Gs (s (cid:62) 0) is
the graph archive’s initial revision. A non-initial revi-
sion Gi (i > s) is obtained by applying an update or
changeset ui = (cid:104)u+
i (cid:105) to revision Gi−1. The sets
u+
i , u−
consist of triples that should be added and
deleted respectively to and from revision Gi−1 such
i ∩ u−
that u+
i = ∅. In other words, Gi = ui(Gi−1) =
(Gi−1 ∪ u+
i ) \ u−
i . Figure 1 provides a toy RDF graph
archive A that models the evolution of the information
about the country members of the United Nations (UN)
and their diplomatic relationships (:dr). The archive
stores triples such as (cid:104) :USA, a, :Country (cid:105) or (cid:104) :USA,
:dr, :Cuba (cid:105), and consists of two revisions {G0, G1}.
G1 is obtained by applying update u1 to the initial re-
vision G0. We extend the notion of changesets to arbi-

i

In contrast to an RDF graph archive, an RDF dataset
is a set D = {G0, G1, . . . , Gm} of named graphs. Dif-
ferently from revisions in a graph archive, we use the
notation Gk for the k-th graph in a dataset, whereas
i denotes the i-th revision of Gk. The notation re-
Gk
lated to RDF datasets is detailed in Table 2. Each graph
Gk ∈ D has a label l(Gk) = gk ∈ I ∪ B. The excep-
tion to this rule is G0, known as the default graph [69],
which is unlabeled.

Most of the existing solutions for RDF archiving
can handle the history of a single graph. However,
scenarios such as data warehousing [33, 38, 39, 47–
50, 56, 57] may require to keep track of the com-
mon evolution of an RDF dataset, for example, by
storing the addition and removal timestamps of the
different RDF graphs in the dataset. Analogously to
the deﬁnition of graph archives, we deﬁne a dataset
archive A = {D0, D1, . . . , Dl−1} as a temporally or-
dered collection of RDF datasets. The j-th revision
of A ( j > 1) can be obtained by applying a dataset
update U j = {ˆu j, u0
j , . . . um
j } to revision D j−1 =
{G0
j−1, . . . Gm
j−1}. U j consists of an update per
graph plus a special changeset ˆu j = (cid:104)ˆu+
j (cid:105) that we
call the graph changeset (ˆu+
j = ∅). The sets
ˆu+
j , ˆu−
j store the labels of the graphs that should be
added and deleted in revision j respectively. If a graph
Gk is in ˆu−
j (i.e., it is scheduled for removal), then Gk

j ∩ ˆu−

j−1, G1

j , ˆu−

j , u1

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

4

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

G0

u1

(cid:104):USA, a, :Country(cid:105)

(cid:104):Cuba, a, :Country(cid:105)

(cid:104):USA, :dr, :Cuba(cid:105)

u+
1 = {(cid:104):France, a, :Country(cid:105)}
u−
1 = {(cid:104):USA, :dr, :Cuba(cid:105)}}

G1 = u1(G0)

(cid:104):USA, a, :Country(cid:105)

(cid:104):Cuba, a, :Country(cid:105)

(cid:104):France, : a, :Country(cid:105)

Fig. 1. Two revisions G0, G1 and a changeset u1 of an RDF graph archive A

as well as its corresponding changeset uk
j ∈ U j must be
empty. It follows that we can obtain revision D j by (i)
applying the individual changesets uk
j−1) for each
0 (cid:54) k (cid:54) m, (ii) removing the graphs in ˆu−
j , and (iii)
adding the graphs in ˆu+
j .

j(Gk

0, G1

Figure 2 illustrates an example of a dataset archive
with two revisions D0 and D1. D0 is a dataset with
graphs {G0
0} both at local revision 0. The dataset
update U1 generates a new global dataset revision D1.
U1 consists of three changesets: u0
1 that modiﬁes the
default graph G0, u1
1 that leaves G1 untouched, and the
graph update ˆu2 that adds graph G2 to the dataset and
initializes it at revision s = 1 (denoted by G2

1).

As proposed by some RDF engines [31, 76], we
deﬁne the master graph GM ∈ D (with label M) as
the RDF graph that stores the metadata about all the
graphs in an RDF dataset D. If we associate the cre-
ation of a graph Gk with label gk to a triple of the form
(cid:104)gk, rdf:type, η:Graph(cid:105) in GM for some namespace η,
then we can model a dataset archive as a set of 5-tuples
(cid:104)s, p, o, ρ, ζ(cid:105). Here, ρ ∈ I is the RDF identiﬁer of the
local revision of the triple in an RDF graph with la-
bel g = l(ρ) (Table 2). Conversely, ζ ∈ I identiﬁes
a (global) dataset revision j = rv(ζ). Likewise, we
overload the function ts(ζ) (deﬁned originally in Ta-
ble 1) so that it returns the timestamp associated to the
dataset revision identiﬁer ζ. Last, we notice that the ad-
dition of a non-empty graph to a dataset archive gen-
erates two revisions: one for creating the graph, and
one for populating it. A similar logic applies to graph
deletion.

2.4. SPARQL

SPARQL 1.1 is the W3C standard language to query
RDF data [74]. For the sake of brevity, we do not pro-
vide a rigorous deﬁnition of the syntax and seman-
tics of SPARQL queries; instead we brieﬂy introduce
the syntax of a subset of SELECT queries and refer
the reader to the ofﬁcial speciﬁcation [74]. SPARQL
is a graph-based language whose building blocks are
triple patterns. A triple pattern ˆt is a triple (cid:104)ˆs, ˆp, ˆo(cid:105) ∈

(I ∪ B ∪ V) × (I ∪ V) × (I ∪ B ∪ L ∪ V), where V is
a set of variables such that (I ∪ B ∪ L) ∩ V = ∅ (vari-
ables are always preﬁxed with ? or $). A basic graph
pattern (abbreviated BGP) ˆG is the conjunction of a set
of triple patterns { ˆt1 . ˆt2 . . . . ˆtm }, e.g.,

{ ?s a :Person . ?s :nationality :France }

When no named graph is speciﬁed, the SPARQL stan-
dard assumes that the BGP is matched against the de-
fault graph in the RDF dataset. Otherwise, for matches
against speciﬁc graphs, SPARQL supports the syntax
GRAPH ¯g { ˆG}, where ¯g ∈ I ∪ B ∪ V. In this paper
we call this, a named BGP denoted by ˆG¯g. A SPARQL
select query Q on an RDF dataset has the basic form
“SELECT V (FROM NAMED ¯g1 FROM NAMED
¯g2 . . . ) WHERE { ˆG(cid:48) ˆG(cid:48)(cid:48)
ˆG¯g2 . . . }, with pro-
jection variables V ⊂ V. SPARQL supports named
BGPs ˆG¯g with variables ¯g ∈ V. In some implementa-
tions [31, 76] the bindings for those variables originate
from the master graph GM. The BGPs in the expres-
sion can contain FILTER conditions, be surrounded by
OPTIONAL clauses, and be combined by means of
UNION clauses.

. . . ˆG¯g1

2.5. Queries on Archives

Queries on graph/dataset archives may combine re-
sults coming from different revisions in the history of
the data collection in order to answer an information
need. The literature deﬁnes ﬁve types of queries on
RDF archives [29, 78]. We illustrate them by means of
our example graph archive from Figure 1.

– Version Materialization. VM queries are standard
queries run against a single revision, such as what
was the list of countries according to the UN at re-
vision j?

– Delta Materialization. DM queries are standard
queries deﬁned on a changeset u j = (cid:104)u+
j (cid:105), e.g.,
which countries were added to the list at revision j?
– Version. V queries ask for the revisions where a
particular query yields results. An example of a V

j , u−

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

5

D0

U1

D1 = U1(D0)

G0

0 = {(cid:104):USA, a, :Country(cid:105),

(cid:104):Cuba, a, :Country(cid:105),

(cid:104):USA, :dr, :Cuba(cid:105)}

1 = ∅}

1 = {G2}, ˆu−

ˆu1 = {ˆu+
1 = {u0+
u0
1 = {(cid:104):France, a, :Country(cid:105)},
u0−
1 = {(cid:104): USA, :dr, :Cuba(cid:105)}}

G1

0 = {(cid:104)x:JFK, a, x:Airport(cid:105)}

u1
1 = {∅, ∅}

G0

1 = {(cid:104):USA, a, :Country(cid:105)

(cid:104):Cuba, a, :Country(cid:105)

(cid:104):France, a, :Country(cid:105)}

G1

1 = {(cid:104)x:JFK, a, x:Airport(cid:105)}

G2

1 = ∅

Fig. 2. A dataset archive A with two revisions D0, D1. The ﬁrst revision contains two graphs, the default graph G0 and G1. The dataset update
ˆU1 (i) modiﬁes G0, (ii) leaves G1 untouched, and (iii) and creates a new graph G2, all with local revision 1.

query is: in which revisions j did USA and Cuba
have diplomatic relationships?

– Cross-version. CV queries result from the combi-
nation (e.g., via joins, unions, aggregations, differ-
ences, etc.) of the information from multiple revi-
sions, e.g., which of the current countries was not in
the original list of UN members?

– Cross-delta. CD queries result from the combi-
nation of the information from multiple sets of
changes, e.g., what are the revisions j with the
largest number of UN member adhesions?

Existing solutions differ in the types of queries they
support. For example, Ostrich [78] provides native
support for queries of types VM, DM, and V on sin-
gle triple patterns, and can handle multiple triple pat-
terns via integration with external query engines. Dy-
dra [11], in contrast, has native support for all types
of queries on BGPs of any size. Even though our ex-
amples use the revision number rv(ρ) to identify a re-
vision, some solutions may directly use the revision
identiﬁer ρ or the revision’s commit time ts(ρ). This
depends on the system’s data model.

3. Framework for the Evolution of RDF Data

This section proposes RDFev, a framework to under-
stand the evolution of RDF data. The framework con-
sists of a set of metrics and a software tool to calculate
those metrics throughout the history of the data. The
metrics quantify the changes between two revisions of
an RDF graph or dataset and can be categorized into
two families: metrics for low-level changes, and met-
rics for high-level changes. Existing benchmarks, such
as BEAR [29], focus on low-level changes, that is, ad-
ditions and deletions of triples. This, however, may be
of limited use to data maintainers, who may need to

know the semantics of those changes, for instance, to
understand whether additions are creating new entities
or editing existing ones. On these grounds, we propose
to quantify changes at the level of entities and object
values, which we call high-level.

RDFev takes each version of an RDF dataset as an
RDF dump in N-triples format (our implementation
does not support multi-graph datasets and quads for the
time being). The ﬁles must be provided in chronolog-
ical order. RDFev then computes the different metrics
for each consecutive pair of revisions. The tool is im-
plemented in C++ and Python and uses the RocksBD3
key-value store as storage and indexing backend. All
metrics are originally deﬁned for RDF graphs in the
state of the art [29], and have been ported to RDF
datasets in this paper. RDFev’s source code is available
at our project website4.

3.1. Low-level Changes

Low-level changes are changes at the triple level. In-
dicators for low-level changes focus on additions and
deletions of triples and vocabulary elements. The vo-
cabulary Υ(D) ⊂ I ∪ L ∪ B of an RDF dataset D is the
set of all the terms occurring in triples of the dataset.
Tracking changes in the number of triples rather than
in the raw size of the RDF dumps is more informative
for data analytics, as the latter option is sensitive to the
serialization format. Moreover an increase in the voca-
bulary of a dataset can provide hints about the nature
of the changes and the novelty of the data incorporated
in a new revision. All metrics are deﬁned by Fernández
et al. [29] for pairs of revisions i, j with j > i.

3http://rocksdb.org
4https://relweb.cs.aau.dk/rdfev

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

6

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

Change ratio. The authors of BEAR [29] deﬁne the
change ratio between two revisions i and j of an RDF
graph G as

δi, j(G) =

i, j| + |u−
|u+
i, j|
|Gi ∪ G j|

.

(1)

δi, j compares the size of the changes between two re-
visions w.r.t. the revisions’ joint size. Large values for
δi, j denote important changes in between the revisions.
For a more ﬁne-grained analysis, Fernández et al. [29]
also proposes the insertion and deletion ratios:

δ+
i, j =

|u+
i, j|
|Gi|

(2)

δ−
i, j =

|u−
i, j|
|Gi|

.

(3)

We now adapt these metrics for RDF datasets. For
this purpose, we deﬁne the size of a dataset D as
sz(D) = (cid:80)
G∈D |G| and the size of a dataset change-
set U as sz(U) = sz(U+) + sz(U−) with sz(U+) =
(cid:80)
u∈U |u−|. With these def-
initions, the previous formulas can be ported to RDF
datasets as follows:

u∈U |u+| and sz−(U) = (cid:80)

δi, j(D) =

sz(U)

(cid:80)

G∈Di∩D j |Gi ∪ G j| + (cid:80)

G∈Di(cid:52)D j

(4)

|G|

δ+
i, j(D) =

sz(U +)
sz(Di)

(5)

δ−
i, j(D) =

sz(U −)
sz(Di)

(6)

Here, Di (cid:52) D j denotes the symmetric difference be-
tween the sets of RDF graphs in revisions i and j.

Vocabulary dynamicity. The vocabulary dynamicity
for two revisions i and j of an RDF graph is deﬁned
as [29]:

vdyni, j(G) =

|Υ(ui, j)|
|Υ(Gi) ∪ Υ(G j)|

(7)

Υ(ui, j) is the set of vocabulary terms – IRIs, literals,
or blank nodes – in the changeset ui, j (Table ??). The
literature also deﬁnes the vocabulary dynamicity for
insertions (vdyn+i, j) and deletions (vdyn-i, j):

vdyn+i, j(G) =

|Υ(u+
|Υ(Gi) ∪ Υ(G j)|

i, j)|

(8)

The formulas are analogous for RDF datasets if we re-
place G by D and ui, j by Ui, j.

Growth ratio. The grow ratio is the ratio between the
number of triples in two revisions i, j. It is calculated
as follows for graphs and datasets:

Γi, j(G) =

|G j|
|Gi|

(10)

Γi, j(D) =

sz(D j)
sz(Di)

.

(11)

3.2. High-level Changes

A high-level change confers semantics to a change-
set. For example, if an update consists of the addition
of triples about an unseen subject, we can interpret the
triples as the addition of an entity to the dataset. High-
level changes provide deeper insights about the de-
velopment of an RDF dataset than low-level changes.
In addition, they can be domain-dependent. Some ap-
proaches [63, 71] have proposed vocabularies to de-
scribe changesets in RDF data as high-level changes.
Since our approach is oblivious to the domain of the
data, we propose a set of metrics on domain-agnostic
high-level changes.

Entity changes. RDF datasets describe real-world
entities s by means of triples (cid:104)s, p, o(cid:105). Hence, an entity
is a subject for the sake of this analysis. We deﬁne the
metric entity changes between revisions i, j in an RDF
graph as:

eci, j(G) = |σi, j(G)| = |σ+

i, j(G) ∪ σ−

i, j(G)|

(12)

In the formula, σ+
i, j is the set of added entities, i.e.,
the subjects present in Υ(G j) but not in Υ(Gi) (anal-
ogously the set of deleted entities σ−
i, j is deﬁned by
swapping the roles of i and j). This metric can easily be
adapted to an RDF dataset D if we deﬁne ec(G) (with
no subscripts) as the number of different subjects in a
graph G. It follows that,

eci, j(D) =

(cid:88)

eci, j(G) +

(cid:88)

ec(G). (13)

G∈Di∩D j

G∈Di(cid:52)D j

We also propose the triple-to-entity-change score,
that is, the average number of triples that constitute a
single entity change. It can be calculated as follows for
RDF graphs:

vdyn-i, j(G) =

|Υ(u−
|Υ(Gi) ∪ Υ(G j)|

i, j)|

.

(9)

ecti, j(G) =

|(cid:104)s, p, o(cid:105) ∈ u+

i, j : s ∈ σi, j(G)|

i, j ∪ u−
eci, j(G)

(14)

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

7

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Low-level changes

High-level changes

Change ratio
Insertion and deletion ratios
Vocabulary dynamicity
Growth ratio

Entity changes
Triple-to-entity-change score
Object updates
Orphan object additions and deletions

Table 3
RDFev’s metrics

We port this metric to RDF datasets by ﬁrst deﬁning
U+ = (cid:83)
u∈U+ u and U− = (cid:83)
u∈U− u and plugging
them into the formula for ecti, j:

|(cid:104)s, p, o(cid:105) ∈ U

ecti, j(D) =

−
i, j : s ∈ σi, j(D)|

+
i, j ∪ U
eci, j(D)

(15)

Object Updates and Orphan Object Additions/Deletions.
An object update in a changeset ui, j is deﬁned by the
deletion of a triple (cid:104)s, p, o(cid:105) and the addition of a triple
(cid:104)s, p, o(cid:48)(cid:105) with o (cid:54)= o(cid:48). Once a triple in a changeset has
been assigned to a high-level change, the triple is con-
sumed and cannot be assigned to any other high-level
change. We deﬁne orphan object additions and dele-
tions respectively as those triples (cid:104)s+, p+, o+(cid:105) ∈ u+
i, j
and (cid:104)s−, p−, o−(cid:105) ∈ u−
i, j that have not been consumed
by any of the previous high-level changes. The dataset
counterparts of these metrics for two revisions i, j can
be calculated by summing the values for each of the
graphs in Di ∩ D j.

Table 3 summarizes all

the metrics deﬁned by

RDFev.

4. Evolution Analysis of RDF Datasets

Having introduced RDFev, we use it to conduct an
analysis of the revision history of three large and pub-
licly available RDF knowledge bases, namely YAGO,
DBpedia, and Wikidata. The analysis resorts to the
metrics deﬁned in Sections 3.1 and 3.2 for every pair
of consecutive revisions.

4.1. Data

We chose the YAGO [77], DBpedia [13], and Wiki-
data [26] knowledge bases for our analysis, because

of their large size, dynamicity, and central role in the
Linked Open Data initiative. We build an RDF graph
archive by considering each release of the knowledge
base as a revision. None of the datasets is provided as
a monolithic ﬁle, instead they are divided into themes.
These are subsets of triples of the same nature, e.g.,
triples with literal objects extracted with certain ex-
traction methods. We thus focus on the most popu-
lar themes. For DBpedia we use the mapping-based
objects and mapping-based literals themes, which are
available from version 3.5 (2015) onwards. Addition-
ally, we include the instance-types theme as well as
the ontology. For YAGO, we use the knowledge base’s
core, namely, the themes facts, meta facts, literal facts,
date facts, and labels available from version 2 (v.1.0
was not published in RDF). As for Wikidata, we use
the simple-statements of the RDF Exports [2] in the pe-
riod from 2014-05 to 2016-08. These dumps provide a
clean subset of the dataset useful for applications that
rely mainly on Wikidata’s encyclopedic knowledge.
All datasets are available for download in the RDFev’s
website https://relweb.cs.aau.dk/rdfev. Table 4 maps
revision numbers to releases for the sake of concise-
ness in the evolution analysis.

2s
3.0.0
3.0.1
3.0.2
3.1

Revision DBpedia YAGO Wikidata
2014-05-26
2014-08-04
2014-11-10
2015-02-23
2015-06-01
2015-08-17
2015-10-12
2015-12-28
2016-03-28
2016-06-21
2016-08-01

3.5
3.5.1
3.6
3.7
3.8
3.9
2015-04
2015-10
2016-04
2016-10
2019-08

0
1
2
3
4
5
6
7
8
9
10

Table 4
Datasets revision mapping

4.2. Low-level Evolution Analysis

Change ratio. Figures 3a, 3d and 3g depict the evo-
lution of the change, insertion, and deletion ratios for
our experimental datasets. Up to the release 3.9 (rev.
5), DBpedia exhibits a steady growth with signiﬁcantly
more insertions than deletions. Minor releases such as
3.5.1 (rev. 1) are indeed minor in terms of low-level
changes. Release 2015-04 (rev. 6) is an inﬂexion point

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

8

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

(a) Entity changes in DBpedia

(b) DBpedia’s vocabulary dynamicity

(c) Growth ratio for DBpedia

(d) Entity changes in YAGO

(e) YAGO’s vocabulary dynamicity

(f) Growth ratio for YAGO

(g) Entity changes in Wikidata

(h) Wikidata’s vocabulary dynamicity

(i) Growth ratio for Wikidata

Fig. 3. Change ratio, vocabulary dynamicity, and growth ratio

not only in terms of naming scheme (see Table 4): the
deletion rate exceeds the insertion rate and subsequent
revisions exhibit a tight difference between the rates.
This suggests a major design shift in the construction
of DBpedia from revision 6.

As for YAGO, the evolution reﬂects a different re-
lease cycle. There is a clear distinction between major
releases (3.0.0 and 3.1, i.e., rev. 1 and 4) and minor re-
leases (3.0.1 and 3.0.2, i.e., rev. 2 and 3). The magni-
tude of the changes in major releases is signiﬁcantly
higher for YAGO than for any DBpedia release. Mi-
nor versions seem to be mostly focused on corrections,
with a low number of changes.

Contrary to the other datasets, Wikidata shows a
slowly decreasing change ratio that ﬂuctuates between

5% (rev. 10) and 33% (rev. 3) within the studied period
of 2 years.

Vocabulary dynamicity. As shown in Figures 3b, 3e,
and 3h, the vocabulary dynamicity is, not surprisingly,
correlated with the change ratio. Nevertheless, the vo-
cabulary dynamicity between releases 3.9 and 2015-14
(rev. 5 and 6) in DBpedia did not decrease. This sug-
gests that DBpedia 2015-04 contained more entities,
but fewer – presumably noisy – triples about those en-
tities. The major releases of YAGO (rev. 1 and 4) show
a notably higher vocabulary dynamicity than the minor
releases. As for Wikidata, slight spikes in dynamicity
can be observed at revisions 4 and 9, however this met-

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

12345678910Revisions020406080100120140160Ratio (%)Change ratioInsertion ratioDeletion ratio12345678910Revisions0.00.20.40.60.8DynamicityDynamicityDynamicity+Dynamicity-12345678910Revisions4020020406080100Growth ratio (%)1234Revisions020406080100120140160Ratio (%)Change ratioInsertion ratioDeletion ratio1234Revisions0.00.20.40.60.8DynamicityDynamicityDynamicity+Dynamicity-1234Revisions4020020406080100Growth ratio (%)12345678910Revisions020406080100120140160Ratio (%)Change ratioInsertion ratioDeletion ratio12345678910Revisions0.00.20.40.60.8DynamicityDynamicityDynamicity+Dynamicity-12345678910Revisions4020020406080100Growth ratio (%)Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

9

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

ric remains relatively low in Wikidata compared to the
others bases.

Growth ratio. Figures 3c, 3f, and 3i depict the growth
ratio of our experimental datasets. In all cases, this
metric is mainly positive with low values for mi-
nor revisions. As pointed out by the change ratio,
the 2015-04 release in DBpedia is remarkable as the
dataset shrank and was succeeded by more conserva-
tive growth ratios. This may suggest that recent DBpe-
dia releases are more curated. We observe that YAGO’s
growth ratio is signiﬁcantly larger for major versions.
This is especially true for the 3.0.0 (rev. 1) release that
doubled the size of the knowledge base.

4.3. High-level Evolution Analysis

4.3.1. Entity changes

Figures 4a, 4d, and 4g illustrate the evolution of
the entity changes, additions, and deletions for DBpe-
dia, YAGO and Wikidata. We also show the number of
triples used to deﬁne these high-level changes (labeled
as affected triples). We observe a stable behavior for
these metrics in DBpedia except for the minor release
3.5.1 (rev. 1). Entity changes in Wikidata also display
a monotonic behavior, even though the deletion rate
tends to decrease from rev. 4. In YAGO, the number
of entity changes peaks for the major revisions (rev. 1
and 4), and is one order of magnitude larger than for
minor revisions. The minor release 3.0.2 (rev. 3) shows
the lowest number of additions, whereas deletions re-
main stable w.r.t release 3.0.1 (rev. 2). This suggests
that these two minor revisions focused on improving
the information extraction process, which removed a
large number of noisy entities.

Figure 4b shows the triple-to-entity-change score in
DBpedia. Before the 2015-14 release, this metric ﬂuc-
tuates between 2 and 12 triples without any appar-
ent pattern. Conversely subsequent releases show a de-
cline, which suggests a change in the extraction strate-
gies for the descriptions of entities. The same cannot
be said about YAGO and Wikidata (Figures 4e and
4h), where values for this metric are signiﬁcantly lower
than for DBpedia, and remain almost constant. This
suggests that minor releases in YAGO improved the
strategy to extract entities, but did not change much the
amount of extracted triples per entity.

4.3.2. Object Updates and Orphan Object

Additions/Deletions

We present the evolution of the number of object
updates for our experimental datasets in Figures 4c, 4f,

and 4i. For DBpedia, the curve is consistent with the
change ratio (Figure 3a). In addition to a drop in size,
the 2015-04 release also shows the highest number of
object updates, which corroborates the presence of a
drastic redesign of the dataset.

The results for YAGO are depicted in Figure 4f,
where we see larger numbers of object updates com-
pared to major releases in DBpedia. This is consistent
with the previous results that show that YAGO goes
through bigger changes between releases. The same
trends are observed for the number of orphan object
additions and deletions in Figures 5a and 5b. Com-
pared to the other two datasets, Wikidata’s number of
object updates, shown in Figure 4i, is much lower and
constant throughout the stream of revisions.

Finally, we remark that in YAGO and DBpedia, ob-
ject updates are 4.8 and 1.8 times more frequent than
orphan additions and deletions. This entails that the
bulk of editions in these knowledge bases aims at up-
dating existing object values. This behavior contrasts
with Wikidata, where orphan object updates are 3.7
times more common than proper object updates. As
depicted in Figure 5c, Wikidata exhibits many more
orphan object updates than the other knowledge bases.
Moreover, orphan object additions are 19 times more
common than orphan object deletions.

4.4. Conclusion

In this section we have conducted a study of the
evolution of three large RDF knowledge bases using
our proposed framework RDFev, which resorts to a
domain-agnostic analysis from two perspectives: At
the low-level it studies the dynamics of triples and vo-
cabulary terms across different versions of an RDF
dataset, whereas at the high-level it measures how
those low-level changes translate into updates to the
entities described in the experimental datasets. All in
all, we have identiﬁed different patterns of evolution.
On the one hand, Wikidata exhibits a stable release cy-
cle in the studied period, as our metrics did not exhibit
big ﬂuctuations from release to release. On the other
hand, YAGO and DBpedia have a release cycle that
distinguishes between minor and major releases. Ma-
jor releases are characterized by a large number of up-
dates in the knowledge base and may not necessarily
increase its size. Conversely, minor releases incur in at
least one order of magnitude fewer changes than major
releases and seem to focus on improving the quality of
the knowledge base, for instance, by being more con-
servative in the number of triple and entity additions.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

10

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

(a) Entity changes for DBpedia

(b) Triple-to-entity changes in DBpedia

(c) Dbpedia object updates

(d) Entity changes for YAGO

(e) Triple-to-entity changes in YAGO

(f) YAGO object updates

(g) Entity changes for Wikidata

(h) Triple-to-entity changes in Wikidata

(i) Wikidata object updates

Fig. 4. Entity changes and object updates

(a) Dbpedia

(b) YAGO

(c) Wikidata

Fig. 5. Orphan object additions and deletions

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

12345678910Revisions103104105106107108Number of changesEntity changesEntity additionsEntity deletionsAffected triples12345678910Revisions02468101214Avg triples/change12345678910Revisions0.00.51.01.52.02.5Updates1e71234Revisions103104105106107108Number of changesEntity changesEntity additionsEntity deletionsAffected triples1234Revisions02468101214Avg triples/change1234Revisions0.00.51.01.52.02.5Updates1e712345678910Revisions103104105106107108Number of changesEntity changesEntity additionsEntity deletionsAffected triples12345678910Revisions02468101214Avg triples/change12345678910Revisions0.00.51.01.52.02.5Updates1e712345678910Revisions02468Count1e6Objects additionObjects deletion1234Revisions02468Count1e6Objects additionObjects deletion12345678910Revisions02468Count1e6Objects additionObjects deletionPelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

11

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Unlike YAGO, DBpedia has shown decreases in size
across releases. We argue that an effective solution for
large-scale RDF archiving should be able to adapt to
different patterns of evolution.

5. Survey of RDF Archiving Solutions

We structure this section in three parts. Section 5.1
surveys the existing engines for RDF archiving and
discusses their strengths and weaknesses. Section 5.2
presents the languages and SPARQL extensions to ex-
press queries on RDF archives. Finally, Section 5.3
introduces various endeavors on analysis and bench-
marking of RDF archives.

5.1. RDF Archiving Systems

There are plenty of systems to store and query
the history of an RDF dataset. Except for a few ap-
proaches [11, 12, 36, 81], most available systems sup-
port archiving of a single RDF graph. Ostrich [78], for
instance, manages quads of the form (cid:104)s, p, o, rv(ρ)(cid:105).
Other solutions do not support revision numbers and
use the ρ-component ρ ∈ I to model temporal meta-
data such as insertion, deletion, and validity time-
stamps for triples [34]. In this paper we make a distinc-
tion between insertion/deletion timestamps for triples
and validity intervals. While the former are unlikely
to change, the latter are subject to modiﬁcations be-
cause they constitute domain information, e.g., the va-
lidity of a marriage statement. This is why the general
data model introduced in Section 2 only associates re-
vision numbers and commit timestamps to the fourth
component ρ, whereas other types of metadata are still
attached to the graph label g = l(ρ). We summarize
the architectural spectrum of RDF archiving systems
in Table 5 where we characterize the state-of-the-art
approaches according to the following criteria:

– Storage paradigm. The storage paradigm is proba-
bly the most important feature as it shapes the sys-
tem’s architecture. We identify three main paradigms
in the literature [29], namely independent copies
(IC), change-based (CB), and timestamp-based (TB).
Some systems [78] may fall within multiple cat-
egories, whereas Quit Store [12] implements a
fragment-based (FB) paradigm.

– Data model. It can be quads or 5-tuples with differ-
ent semantics for the fourth and ﬁfth component.

– Full BGPs. This feature determines whether the sys-
tem supports BGPs with a single triple pattern or full
BGPs with an unbounded number of triple patterns
and ﬁlter conditions.

– Query types. This criterion lists the types of queries
on RDF archives (see Section 2.5) natively sup-
ported by the solution.

– Branch & tags. It deﬁnes whether the system sup-
ports branching and tagging as in classical version
control systems.

– Multi-graph. This feature determines if the sys-
tem supports archiving of the history of multi-graph
RDF datasets.

– Concurrent updates. This criterion determines
whether the system supports concurrent updates.
This is deﬁned regardless of whether conﬂict man-
agement is done manually or automatically.

– Source available. We also specify whether the sys-
tem’s source code is available for download and is
usable, that is, whether it can be compiled and run in
modern platforms.

In the following, we discuss further details of the
state-of-the-art systems, grouped by their storage
paradigms.

5.1.1. Independent Copies Systems

In an IC-like approach, each revision Di of a dataset
archive A = {D1, D2, . . . , Dn} is fully stored as an
independent RDF dataset. IC approaches shine at the
execution of VM and CV queries as they do not in-
cur any materialization cost for such types of queries.
Conversely, IC systems are inefﬁcient in terms of disk
usage. For this reason they have mainly been proposed
for small datasets or schema version control [61, 81].
SemVersion [81], for instance, is a system that of-
fers similar functionalities as classical version control
systems (e.g., CVS or SVN), with support for mul-
tiple RDF graphs and branching. Logically, SemVer-
sion supports 5-tuples of the form (cid:104)s, p, o, l(ρ), rv(ρ)(cid:105),
in other words, revision numbers are local to each
RDF graph. This makes it difﬁcult to track the ad-
dition or deletion of named graphs in the history of
the dataset. Lastly, SemVersion provides an HTTP in-
terface to submit updates either as RDF graphs or as
changesets. Despite this ﬂexibility, new revisions are
always stored as independent copies. This makes its
disk-space consumption prohibitive for large datasets
like the ones studied in this paper.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

12

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

Storage
paradigm

Data
model

Full
BGPs

Dydra [11]

Ostrich [78]

QuitStore [12]

RDF-TX [34]

R43ples [36]

R&WBase [72]

RBDMS [46]

SemVersion [81]

Stardog [3]

v-RDFCSA [23]

x-RDF-3X [59]

TB

5-tuples

IC/CB/TB

quads

FB

TB

CB

CB

CB

IC

CB

TB

TB

5-tuples

quads
5-tuplesc
quads

quads
5-tuplesc
5-tuples

quads

quads

+
+d
+

+

+

+

+

-

+

-

+

Queries

all

VM, DM, V

all

all

all

all

all

VM, DM

all

VM, DM, V

VM, V

Branch
& tags

Multi-
graph

Concurrent
Updates

Source
available

-

-

+

-

+

+

+

+

tags

-

-

+

-

+

-

+

-

-

-

+

-

-

-

-

+

-

+

+

+

+

-

-

-

-

+

+

-
+a
+

-

-

-

-
+b

a It needs modiﬁcations to have the console client running and working
d Full BGP support is possible via integration with the Comunica query engine

b Old source code

c Graph local revisions

Table 5
Existing RDF archiving systems

5.1.2. Change-based Systems

Solutions based on the CB paradigm store a subset
ˆA ⊂ A of the revisions of a dataset archive as inde-
pendent copies or snapshots. On the contrary, all the
intermediate revisions D j (p < j < q) between two
snapshots Dp and Dq, are stored as deltas or change-
sets U j. The sequence of revisions stored as changesets
between two snapshots is called a delta chain. CB sys-
tems are convenient for DM and CD queries. Besides,
they are obviously considerably more storage-efﬁcient
than IC solutions. Their weakness lies in the high ma-
terialization cost for VM and CV queries, particularly
for long delta chains.

R&WBase [72] is an archiving system that provides
Git-like distributed version control with support for
merging, branching, tagging, and concurrent updates
with manual conﬂict resolution on top of a classical
SPARQL endpoint. R&WBase supports all types of
archive queries on full BGPs. The system uses the
PROV-Ontology (PROV-O) [24] to model the meta-
data (e.g., timestamps, parent branches) about the up-
dates of a single RDF graph. An update ui generates
two new named graphs Gi+
g containing the added
and deleted triples at revision i. Revisions can be ma-
terialized by processing the delta chain back to the ini-
tial snapshot, and they can be referenced via aliases
called virtual named graphs. In the same spirit, tags
and branches are implemented as aliases of a particular
revision. R&WBase has inspired the design of R43ples
[36]. Unlike the former, R43ples can version multiple
graphs, although revision numbers are not deﬁned at
the dataset level, i.e., each graph manages its own his-

g , Gi−

tory. Moreover, the system extends SPARQL with the
clause REVISION j ( j ∈ N ) used in conjunction with
the GRAPH clause to match a BGP against a speciﬁc
revision of a graph. Last, the approach presented by
Dong-hyuk et al. [46] relies on an RDBMS to store
snapshots and deltas of an RDF graph archive with
support for branching and tagging. Its major drawback
is the lack of support for SPARQL queries: while it
supports all the types of queries introduced in Sec-
tion 2.5, they must be formulated in SQL, which can
be very tedious for complex queries.

Stardog [3] is a commercial RDF data store with
support for dataset snapshots, tags, and full SPARQL
support. Unlike R43ples, Stardog keeps track of the
global history of a dataset, hence its logical model con-
sists of 5-tuples of the form (cid:104)s, p, o, l(ρ), ζ(cid:105) (i.e., meta-
data is stored at the dataset level). While the details of
Stardog’s internal architecture are not public, the doc-
umentation5 suggests a CB paradigm with a relational
database backend.

5.1.3. Timestamp-based Systems

TB solutions store triples with their temporal meta-
data, such as domain temporal validity intervals or in-
sertion/deletion timestamps. Like in CB solutions, re-
visions must be materialized at a high cost for VM
and CV queries. V queries are usually better supported,
whereas the efﬁciency of materializing deltas depends
on the system’s indexing strategies.

5https://github.com/stardog-union/stardog-examples/tree/
d7ac8b562ecd0346306a266d9cc28063fde7edf2/examples/cli/
versioning

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

13

x-RDF-3X [59] is a system based on the RDF-3X
[58] engine. Logically x-RDF-3X supports quads of
the form (cid:104)s, p, o, ρ(cid:105) where ρ ∈ I is associated to all
the revisions where the triple was present as well as
to all addition and deletion timestamps. The system
is a fully-ﬂedged query engine optimized for highly
concurrent updates with support for snapshot isolation
in transactions. However, x-RDF-3X does not support
versioning for multiple graphs, neither branching nor
tagging.

Dydra [11] is a TB archiving system that supports
archiving of multi-graph datasets. Logically, Dydra
stores 5-tuples of the form (cid:104)s, p, o, l(ρ), ζ(cid:105), that is, re-
vision metadata lies at the dataset level. In its physi-
cal design, Dydra indexes quads (cid:104)s, p, o, l(ρ)(cid:105) and as-
sociates them to visibility maps and creation/deletion
timestamps that determine the revisions and points in
time where the quad was present. The system relies
on six indexes – gspo, gpos, gosp, spog, posg, and
ospg implemented as B+ trees – to support arbitrary
SPARQL queries (g = l(ρ) is the graph label). More-
over, Dydra extends the query language with the clause
REVISION x, where x can be a variable or a con-
stant. This clause instructs the query engine to match
a BGP against the contents of the data sources bound
to x, namely a single database revision ζ, or a dataset
changeset U j,k. A revision can be identiﬁed by its IRI ζ,
its revision number rv(ζ) or by a timestamp τ(cid:48). The lat-
ter case matches the revision ζ with the largest times-
tamp τ = ts(ζ) such that τ (cid:54) τ(cid:48). Alas, Dydra’s source
is not available for download and use.

RDF-TX [34] supports single RDF graphs and
uses a multiversion B-tree (MVBT) to index triples
and their time metadata (insertion and deletion time-
stamps). An MVBT is actually a forest where each
tree indexes the triples that were inserted within a time
interval. RDF-TX implements an efﬁcient compres-
sion scheme for MVBTs, and proposes SPARQL-T, a
SPARQL extension that adds a fourth component ˆg to
BGPs. This component can match only time objects τ
of type timestamp or time interval. The attributes of
such objects can be queried via built-in functions, e.g.,
year(τ). While RDF-TX offers interval semantics at
the query level, it stores only timestamps.

v-RDFCSA [23] is a lightweight and storage-efﬁcient
TB approach that relies on sufﬁx-array encoding [19]
for efﬁcient storage with basic retrieval capabilities
(much in the spirit of HDT [27]). Each triple is asso-
ciated to a bitsequence of length equals the number of
revisions in the archive. That is, v-RDFCSA logically
stores quads of the form (cid:104)s, p, o, rv(ρ)(cid:105). Its query func-

tionalities are limited since it supports only VM, DM,
and V queries on single triple patterns.

5.1.4. Hybrid and Fragment-based Systems

Some approaches can combine the strengths of the
different storage paradigms. One example is Ostrich
[78], which borrows inspirations from IC, CB, and
TB systems. Logically, Ostrich supports quads of the
form (cid:104)s, p, o, rv(ρ)(cid:105). Physically, it stores snapshots of
an RDF graph using HDT [27] as serialization format.
Delta chains are stored as B+ trees timestamped with
revision numbers in a TB-fashion. These delta chains
are redundant, i.e., each revision in the chain is stored
as a changeset containing the changes w.r.t. the latest
snapshot – and not the previous revision as proposed
by Dong-hyuk et al. [46]. Ostrich alleviates the cost of
redundancy using compression. All these design fea-
tures make Ostrich query and space efﬁcient, however
its functionalities are limited. Its current implementa-
tion does not support more than one (initial) snapshot
and a single delta chain, i.e., all revisions except for
revision 0 are stored as changesets of the form u0,i.
Multi-graph archiving as well as branching/tagging are
not possible. Moreover, the system’s querying capabil-
ities are restricted to VM, DM, and V queries on single
triple patterns. Support for full BGPs is possible via
integration with the Comunica query engine6.

Like R43ples [36], Quit Store [12] provides collab-
orative Git-like version control for multi-graph RDF
datasets, and uses PROV-O for metadata management.
Unlike R43ples, Quit Store provides a global view of
the evolution of a dataset, i.e., each commit to a graph
generates a new dataset revision. The latest revision is
always materialized in an in-memory quad store. Quit-
Store is implemented in Python with RDFlib and pro-
vides full support for SPARQL 1.1. The dataset history
(RDF graphs, commit tree, etc.) is physically stored in
text ﬁles (i.e. N-quads ﬁles resp. N-triples ﬁles in the
latest implementation) and is accessible via a SPARQL
endpoint on a set of virtual graphs. However, the sys-
tem only stores snapshots of the modiﬁed ﬁles in the
spirit of fragment-based storage. Quit Store is tailored
for collaborative construction of RDF datasets, but its
high memory requirements make it unsuitable as an
archiving backend. As discussed in Section 7, fully-
ﬂedged RDF archiving can provide a backend for this
type of applications.

6https://github.com/rdfostrich/comunica-actor-init-sparql-ostrich

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

14

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

5.2. Languages to Query RDF Archives

Multiple research endeavors have proposed alterna-
tives to succinctly formulate queries on RDF archives.
The BEAR benchmark [29] uses AnQL to express the
query types described in Section 2.5. AnQL [82] is a
SPARQL extension based on quad patterns (cid:104)ˆs, ˆp, ˆo, ˆg(cid:105).
AnQL is more general than SPARQL-T (proposed by
RDF-TX [34]) because the ˆg-component can be bound
to any term u ∈ I ∪ L (not only time objects). For in-
stance, a DM query asking for the countries added at
revision 1 to our example RDF dataset from Figure 1
could be written as follows:
SELECT * WHERE {

{ (?x a :Country): [1] } MINUS
{ (?x a :Country): [0] }
}

T-SPARQL [35] is a SPARQL extension inspired
by the query language TSQL2 [75]. T-SPARQL al-
lows for the annotation of groups of triple patterns
with constraints on temporal validity and commit time,
i.e., it supports both time-intervals and timestamps as
time objects. T-SPARQL deﬁnes several comparison
operators between time objects, namely equality, pre-
cedes, overlaps, meets, and contains. Similar exten-
sions [16, 66] also offer support for geo-spatial data.

SPARQ-LTL [30] is a SPARQL extension that
makes two assumptions, namely that (i) triples are
annotated with revision numbers, and (ii) revisions
are accessible as named graphs. When no revision is
speciﬁed, BGPs are iteratively matched against every
revision. A set of clauses on BGPs can instruct the
SPARQL engine to match a BGP against other revi-
sions at each iteration. For instance the clause PAST in
the expression PAST{ q } MINUS { q } with q = (cid:104)?x
a :Country(cid:105) will bind variable ?x to all the countries
that were ever deleted from the RDF dataset, even if
they were later added.

5.3. Benchmarks and Tools for RDF Archives

BEAR [29] is the state-of-the-art benchmark for
RDF archive solutions. The benchmark provides three
real-world RDF graphs (called BEAR-A, BEAR-B,
and BEAR-C) with their corresponding history, as well
as a set of VM, DM, and V queries on those histories.
In addition, BEAR allows system designers to com-
pare their solutions with baseline systems based on
different storage strategies (IC, CB, TB, and hybrids
TB/CB, IC/CB) and platforms (Jena TDB and HDT).
Despite its multiple functionalities and its dominant

position in the domain, BEAR has some limitations: (i)
It assumes single-graph RDF datasets; (ii) it does not
support CV and CD queries, moreover VM, DM, and
V queries are deﬁned on single triple patterns; and (iii)
it cannot simulate datasets of arbitrary size and query
workloads.

EvoGen [51] tackles the latter limitation by extend-
ing the Lehigh University Benchmark (LUBM) [37] to
a setting where both the schema and the data evolve.
Users can not only control the size and frequency of
that evolution, but can also deﬁne customized query
workloads. EvoGen supports all the types of queries
on archives presented in Section 2.5 on multiple triple
patterns.

A recent approach [79] proposes to use FCA (For-
mal Concept Analysis) and several data fusion tech-
niques to produce summaries of the evolution of en-
tities across different revisions of an RDF archive. A
summary can, for instance, describe groups of subjects
with common properties that change over time. Such
summaries are of great interest for data maintainers as
they convey edition patterns in RDF data through time.

6. Evaluation of the Related Work

In this section, we conduct an evaluation of the state-
of-the-art RDF archiving engines. We ﬁrst provide a
global analysis of the systems’ functionalities in Sec-
tion 6.1. Section 6.2 then provides a performance eval-
uation of Ostrich (the only testable solution) on our
experimental RDF archives from Table 4. This evalu-
ation is complementary to the Ostrich’s evaluation on
BEAR (available in [78]), as it shows the performance
of the system in three real-world large RDF datasets.

6.1. Functionality Analysis

As depicted in Table 5, existing RDF archiving so-
lutions differ greatly in design and functionality. The
ﬁrst works [22, 59, 81] offered mostly storage of old
revisions and support for basic VM queries. Conse-
quently, subsequent efforts focused on extending the
query capabilities and allowing for concurrent updates
as in standard version control systems [12, 36, 46, 72].
Such solutions are attractive for data maintainers in
collaborative projects, however they still lack scalabil-
ity, e.g., they cannot handle large datasets and change-
sets, besides conﬂict management is still delegated
to users. More recent works [23, 78] have therefore
focused on improving storage and querying perfor-

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

15

(a) Disk-space usage

(b) Ingestion time

Fig. 6. Ostrich’s performance on multiple revisions of DBpedia and YAGO

mance, alas, at the expense of features. For example,
Ostrich [78] is limited to a single snapshot and delta
chain. In addition to the limitations in functionality,
Table 5 shows that most of the existing systems are
not available because their source code is not pub-
lished. While this still leaves us with Ostrich [78], Quit
Store [12], R&WBase [72], R43ples [36] and x-RDF-
3X as testable solutions, only [78] was able to run on
our experimental datasets. To carry out a fair compar-
ison with the other systems, we tried Quit Store in the
persistence mode, which ingests the data graphs into
main memory at startup – allowing us to measure in-
gestion times. Unfortunately, the system crashes for all
our experimental datasets7. We also tested Quit Store
in its default lazy loading mode, which loads the data
into main memory at query time. This option throws a
Python MemoryError for our experimental queries. In
regards to R43ples, we had to modify its source code
to handle large ﬁles8. Despite this change, the sys-
tem could not ingest a single revision of DBpedia after
four days of execution. R&WBase, on the other hand,
accepts updates only through a SPARQL endpoint,
which cannot handle the millions of update statements
required to ingest the changesets. Finally, x-RDF-3X’s
source code does not compile out of the box in modern
platforms, and even after successful compilation, it is
unable to ingest one DBpedia changeset.

6.2. Performance Analysis

We evaluate the performance of Ostrich on our ex-
perimental datasets in terms of storage space, inges-
tion time – the time to generate a new revision from
an input changeset – and query response time. The
changesets were computed with RDFev from the dif-

7The Python interpreter reports a UnboundLocalError.
8The code creates an array that exceeds the maximal array size in

Java.

ferent versions of DBpedia, YAGO, and Wikidata (Ta-
ble 4). All the experiments were run on a server with
a 4-core CPU (Intel Xeon E5-2680 v3@2.50GHz) and
64 GB of RAM.

Storage space. Figure 6a shows the amount of stor-
age space (in GB) used by Ostrich for the selected re-
visions of our experimental datasets. We provide the
raw sizes of the RDF dumps of each revision for ref-
erence. Storing each version of YAGO separately re-
quires 36 GB, while Ostrich uses only 4.84 GB. For
DBpedia compression goes from 39 GB to 5.96 GB.
As for Wikidata, it takes 131 GB to stores the raw ﬁles,
but only 7.88 GB with Ostrich. This yields a compres-
sion rate of 87% for YAGO, 84% for DBpedia and
94% for Wikidata. This space efﬁciency is the result of
using HDT [27] for snapshot storage, as well as com-
pression for the delta chains.

Ingestion time. Figure 6b shows Ostrich’s ingestion
times. We also provide the number of triples of each
revision as reference. The results suggest that this mea-
sure depends both on the changeset size, and the length
of the delta chain. However, the latter factor becomes
more prominent as the length of the delta chain in-
creases. For example, we can observe that Ostrich
requires ∼22 hours to ingest revision 9 of DBpedia
(2.43M added and 2.46M deleted triples) while it takes
only ∼14 hours to ingest revision 5 (12.85M added and
5.95M deleted triples). This conﬁrms the trends ob-
served in [78] where ingestion time increases linearly
with the number of revisions. This is explained by the
fact that Ostrich stores the i-th revision of an archive as
a changeset of the form u0,i. In consequence, Ostrich’s
changesets are constructed from the triples in all pre-
vious revisions, and can only grow in size. This fact
makes it unsuitable for very long histories.

Query runtime. We run Ostrich on 100 randomly
generated VM, V, and DM queries on our experimental

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

012345678910051015DB Size (GB)YAGO Ostrich's sizeYAGO data sizeDBpedia Ostrich's sizeDBpedia data sizeWikidata Ostrich's sizeWikidata data size0123456789100102030Ingestion time (hours)YAGO timeYAGO #triplesDBpedia timeDBpedia #triplesWikidata timeWikidata #triples20406080100120140Millions of triples16

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

Triple Patterns
? p ?
? <top p> o
? p o
s p ?

VM

92.81(0)
112.81(0)
96.74(0)
94.99(0)

DBpedia
V

118.64(0)
283.59(0)
92.04(0)
91.67(0)

DM

91.78(0)
130.88(0)
91.93(0)
92.81(0)

VM

2.9(3)
35.08(2)
2.38(4)
2.42(2)

YAGO
V

- (5)
69.65(4)
2.35(4)
2.36(3)

DM

VM

1.82(3)
137.16(4)
2.35(2)
2.41(1)

281.41(0)
347.06(0)
282.12(0)
284.74(0)

Wikidata
V

302.26(0)
499.77(0)
281.63(0)
281.4(0)

DM

303.73(0)
285.02(0)
281.45(0)
281.2(0)

Table 6
Ostrich’s Query Performance in seconds

datasets. Ostrich does not support queries on full BGPs
natively, thus the queries consisted of single triple pat-
terns of the most common forms, namely (cid:104) ?, p, ? (cid:105),
(cid:104) s, p, ? (cid:105), and (cid:104) ?, p, o (cid:105) in equal numbers. We also
considered queries (cid:104) ?, <top p>, o (cid:105), where <top p>
corresponds to the top 5 most common predicates in
the dataset. Revision numbers for all queries were also
randomly generated. Table 6 shows Ostrich’s average
runtime in seconds for the different types of queries.
We set a timeout of 1 hour for each query, and show the
number of timeouts in parentheses next to the runtime,
which excludes queries that timed out. We observe that
Ostrich is roughly one order of magnitude faster on
YAGO than on DBpedia and Wikidata. To further un-
derstand the factors that impact Ostrich’s runtime, we
computed the Spearman correlation score between Os-
trich’s query runtime and a set of features relevant to
query execution. These features include the length of
the delta chain, the average size of the relevant change-
sets, the size of the initial revision, the average number
of deleted and added triples in the changesets, and the
number of query results. The results show that the most
correlated features are the length of the delta chain, the
standard deviation of the changeset size, and the aver-
age number of deleted triples. This suggests that Os-
trich’s runtime performance will degrade as the history
of the archive grows and that massive deletions actu-
ally aggravate that phenomenon. Finally, we observe
some timeouts in YAGO in contrast to DBpedia and
Wikidata. We believe this is mainly caused by the sizes
of the changesets, which are on average of 3.72GB for
YAGO, versus 2.09GB for DBpedia and 1.86GB for
Wikidata. YAGO’s changesets at revisions 1 and 4 are
very large as shown in Section 4.

7. Towards Fully-ﬂedged RDF Archiving

We now build upon the ﬁndings from previous sec-
tions to derive a set of lessons towards the design of
a scalable fully-ﬂedged solution for archiving of large
RDF datasets. We structure this section in two parts.

Section 7.1 discusses the most important functional-
ities that such a solution may offer, whereas Section
7.2 discusses the algorithmic and design challenges of
providing those functionalities.

7.1. Functionalities

Global and local history. Our survey in Section 5.1
shows that R43ples [36] and Quit Store [12] are the
only available solutions that support both archiving of
the local and joint (global) history of multiple RDF
graphs. We argue that such a feature is vital for proper
RDF archiving: It is not only of great value for dis-
tributed version control in collaborative projects, but
can also be useful for the users and maintainers of data
warehouses. Conversely, existing solutions are strictly
focused on distributed version control and their Git-
based architectures make them unsuitable to archive
the releases of large datasets such as YAGO, DBpe-
dia, or Wikidata as explained in Section 6. From an en-
gineering and algorithmic perspective, this implies to
redesign RDF solutions to work with 5-tuples instead
of triples. We discuss the technical challenges of such
requirement in Section 7.2.

Temporal domain-speciﬁc vs. revision metadata. Sys-
tems and language extensions for queries with time
constraints [34, 35], treat both domain-speciﬁc meta-
data (e.g., triple validity intervals) and revision-related
annotations (e.g., revision numbers) in the same way.
We highlight, however, that revision metadata is im-
mutable and should therefore be logically placed at a
different level. In this line of thought we propose to as-
sociate revision metadata for graphs and datasets, e.g.,
commit time, revision numbers, or branching & tag-
ging information, to the local and global revision iden-
tiﬁers ρ and ζ, whereas depending on the application,
domain-speciﬁc time objects could be modeled ei-
ther as statements about the revisions or as statements
about the graph labels g = l(ρ). The former alternative
enforces the same temporal domain-speciﬁc metadata
to all the triples added in a changeset, whereas the lat-

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

17

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

ter option makes sense if all the triples with the same
graph label are supposed to share the same domain-
speciﬁc information – which can still be edited by an-
other changeset on the master graph. We depict both
alternatives in Figure 7. We remark that such associa-
tions are only deﬁned at the logical level.

Provenance. Revision metadata is part of the history
of a triple within a dataset. Instead, its complete his-
tory is given by its workﬂow provenance. The W3C
offers the PROV-O ontology [24] to model the his-
tory of a triple from its sources to its current state in
an RDF dataset. Pretty much like temporal domain-
speciﬁc metadata, provenance metadata can be log-
ically linked to either the (local or global) revision
identiﬁers or to the graph labels (Figure 7). This de-
pends on whether we want to deﬁne provenance for
changesets because the triples added to an RDF graph
may have different provenance workﬂows. A hybrid
approach could associate a default provenance history
to a graph and use the revision identiﬁers to override
or extend that default history for new triples. More-
over, the global revision identiﬁer ζ provides an ad-
ditional level of metadata that allow us to model the
provenance of a dataset changeset.

Concurrent updates & modularity. We can group the
existing state-of-the-art solutions in three categories
regarding their support for concurrent updates, namely
(i) solutions with limited or no support for concurrent
updates [11, 23, 34, 67, 78], (ii) solutions inspired by
version control systems such as Git [12, 36, 46, 72, 81],
and (iii) approaches with full support for highly con-
current updates [59]. Git-like solutions are particu-
larly interesting for collaborative efforts such as DB-
pedia, because it is feasible to delegate users the task
of conﬂict management. Conversely, fully automati-
cally constructed KBs such as NELL [21] or data-
intensive (e.g., streaming) applications may need the
features of solutions such as x-RDF-3X [59]. Conse-
quently, we propose a modular design that separates
the concurrency layer from the storage backend. Such
a middleware could take care of enforcing a consis-
tency model for concurrent commits either automati-
cally or via user-based conﬂict management. The layer
could also manage the additional metadata for features
such as branching and tagging. In that light, collabora-
tive version control systems for RDF [12, 36, 72] be-
come an application of fully-ﬂedged RDF archiving.

Formats for publication and querying. A fully func-
tional archiving solution should support the most po-
pular RDF serialization formats for data ingestion and
dumping. For metadata enhanced RDF, this should in-
clude support for N-quads, singleton properties, and
RDF-star. Among those, RDF-star [40] is the only one
that can natively support multiple levels of metadata
(still in a very verbose fashion). For example RDF-star
could serialize the tuple (cid:104):USA, :dr, :Cuba, ρ, ζ(cid:105) with
graph label (:gl) l(ρ) = :UN and global timestamp
(:ts) ts(ζ) =2020-07-09 as follows:

<<<:USA :dr :Cuba> :gl :UN> :ts “2020-07-09”ˆˆxsd:date>

The authors of [40] propose this serialization as part
of the Turtle-star format. Moreover,
they propose
SPARQL-star that allows for nested triple patterns.
While SPARQL-star enables the deﬁnition of meta-
data constraints at different levels, a fully archive-
compliant language could offer further syntactic sugar
such as the clauses REVISION [11, 36] or DELTA to
bind the variables of a BGP to the data in particular re-
visions or deltas. We propose to build such an archive-
compliant language upon SPARQL-star.

Support for different types of archive queries. Most
of the studied archiving systems can answer all the
query types deﬁned in the literature of RDF archives [29,
78]. That said, more complex queries such as CD
and CV queries, or queries on full BGPs are some-
times supported via query middlewares and external li-
braries [12, 78]. We endorse this design philosophy be-
cause it eases modularity. Existing applications in min-
ing archives [45, 64, 65] already beneﬁt from support
for V, VM, and DM queries on single triple patterns.
By guaranteeing scalable runtime for such queries,
we can indirectly improve the runtime of more com-
plex queries. Further optimizations can be achieved by
proper query planning.

7.2. Challenges

Trade-offs on storage, query runtime, and ingestion
time. RDF archiving differs from standard RDF
management in an even more imperative need for scal-
ability, in particular storage efﬁciency. As shown by
Taelman et al. [78], the existing storage paradigms
shine at different types of queries. Hence, supporting
arbitrary queries while being storage-efﬁcient requires
the best from the IC, CB, FB, and TB philosophies.
A hybrid approach, however, will inevitably be more

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

18

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

(a) Revision identiﬁers hold revision-related metadata.
Provenance as well as temporal domain-speciﬁc metadata
are in green.

(b) Revision identiﬁers hold all the metadata.

Fig. 7. Two logical models to handle metadata in RDF archives. The namespace prov: corresponds to the PROV-O namespace.

complex and introduce further parameters and trade-
offs. The authors of Ostrich [78], for instance, chose
to beneﬁt faster version materialization via redundant
deltas at the expense of larger ingestion times. Users
requiring shorter ingestion times could, on the other
hand, opt for non-redundant changesets, or lazy non-
asynchronous ingestion (at the expense of data avail-
ability). We argue that the most crucial algorithmic
challenge for a CB archiving solution is to decide when
to store a revision as a snapshot or as a delta, which is
tantamount to trading disk space for faster VM queries.
This could be formulated as an (multi-objective) mini-
mization problem whose objective might be a function
of response time for triple patterns in VM, CV and V
queries with constraints on available disk space and
average ingestion time. When high concurrency is im-
perative, the objective function could also take query
throughput into account. In the same vibe, a TB solu-
tion could trigger the construction of further indexes
(e.g., new combinations of components, incremental
indexes in the concurrent setting) based on a careful
consideration of disk consumption and runtime gain.
Such scenarios would not only require the conception
of a novel cost model for query runtime in the archiv-
ing setting, but also the development of approximation
algorithms for the underlying optimization problems,
which are likely NP-Hard. Finally, since fresh data is
likely to be queried more often than stale data, we be-
lieve that fetch time complexity9 on the most recent(s)
version(s) of the dataset should not depend on the size
of the archive history. Hence, and depending on the
host available main memory, an RDF archiving sys-
tem could keep the latest revision(s) of a dataset (or
parts of it) in main memory or in optimized disk-based
stores for faster query response time (as done by Quit-

9For single triple patterns on VM queries

Store [12]). Hence, main memory consumption could
also be part of the optimization objective.

Internal serialization. Archiving multi-graph datasets
requires the serialization of 5-tuples, which complex-
iﬁes the trade-offs between space (i.e., disk and main
memory consumption) and runtime efﬁciency (i.e., re-
sponse time, ingestion time). For example, dealing
with more columns increases the number of possible
index combinations. Also, it leads to more data redun-
dancy, since a triple can be associated to multiple val-
ues for the fourth and ﬁfth component. Classical solu-
tions for metadata in RDF include reiﬁcation [70], sin-
gleton properties [60], and named graphs [69]. Reiﬁ-
cation assigns each RDF statement (triple or quad) an
identiﬁer t ∈ I that can be then used to link the triple
to its ρ and ζ components in the 5-tuples data model
introduced in Section 2.3. While simple and fully com-
patible with the existing RDF standards, reiﬁcation is
well-known to incur serious performance issues for
storage and query efﬁciency, e.g., it would quintuple
the number of triple patterns in SPARQL queries. On
those grounds, Nguyen et al. [60] proposes singleton
properties to piggyback the metadata in the predicate
component. In this strategy, predicates take the form
p#m ∈ I for some m ∈ N and every triple with p in
the dataset. This scheme gives p#m the role of ρ in the
aforementioned data model reducing the overhead of
reiﬁcation. However, singleton properties would still
require an additional level of reiﬁcation for the ﬁfth
component ζ. The same is true for a solution based on
named graphs. A more recent solution is HDTQ [28],
which extends HDT with support for quads. An addi-
tional extension could account for a ﬁfth component.
Systems such as Dydra [11] or v-RDFCSA [23] re-
sort to bit vectors and visibility maps for triples and
quads. We argue that vector and matrix representations
may be suitable for scalable RDF archiving as they al-

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

19

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

low for good compression in the presence of high re-
dundancy: If we assume a binary matrix from triples
(rows) to graph revisions (columns) where a one de-
notes the presence of a triple in a revision, we would
expect rows and columns to contain many contiguous
ones – the logic is analogous for removed triples.

Accounting for evolution patterns. As our study
in Section 4 shows, the evolution patterns of RDF
archives can change throughout time leading even, to
decreases in dataset size. With that in mind, we en-
vision an adaptive data-oriented system that adjusts
its parameters according to the archive’s evolution for
the sake of efﬁcient resource comsumption. Parameter
tuning could rely on the metrics proposed in Section 3.
Nonetheless, these desiderata translate into some de-
sign and engineering considerations. For example, we
saw in Section 6 that a large number of deletions
can negatively impact Ostrich’s query runtime, hence,
such an event could trigger the construction of a com-
plete snapshot of the dataset in order to speed-up VM
queries (assuming the existence of a cost model for
query runtime). In the same spirit and assuming some
sort of dictionary encoding, an increase in the vocabu-
lary dynamicity could increase the number of bits used
to encode the identiﬁers of RDF terms in the dictio-
nary. Those changes could be automatically carried out
by the archiving engine, but could also be manually set
up by the system administrator after an analysis with
RDFev. A design philosophy that we envision to ex-
plore divides the history of each graph in the dataset
in intervals such that each interval is associated to a
block ﬁle. This ﬁle contains a full snapshot plus all the
changesets in the interval. It follows that the applica-
tion of a new changeset may update the latest block
ﬁle or create a new one (old blocks could be merged
into snapshots to save disk space). This action could be
automatically executed by the engine or triggered by
the system administrator. For instance, if the archive is
the backend of a version control system, new branches
may always trigger the creation of snapshots. This base
architecture should be enhanced with additional in-
dexes to speed up V queries and adapted compression
for the dictionary and the triples.

Finally as we expect long dataset histories, it is vital
for solutions to improve their ingestion time complex-
ity, which should depend on the size of the changesets
rather than on history size—contrary to what we ob-
served in Section 6 for Ostrich. This constraint could
be taken into account by the storage policy for the cre-
ation of storage structures such as deltas, snapshots,

or indexes (e.g., by reducing the length of delta chains
for redundant changesets). Nevertheless, very large
changesets may still be challenging, specially in the
concurrent scenario. This may justify the creation of
temporary incremental (in-memory) indexes and data
structures optimized for asynchronous batch updates
as proposed in x-RDF-3X [59].

8. Conclusions

In this paper we have discussed the importance of
RDF archiving for both maintainers and consumers
of RDF data. Besides, we have discussed the impor-
tance of evolution patterns in the design of a fully-
ﬂedged RDF archiving solution. On these grounds, we
have proposed a metric-based framework to character-
ize the evolution of RDF data, and we have applied
our framework to study the history of three challeng-
ing RDF datasets, namely DBpedia, YAGO, and Wiki-
data. This study has allowed us to characterize the his-
tory of these datasets in terms of changes at the level
of triples, vocabulary terms, and entities. It has also al-
lowed us to identify design shifts in their release his-
tory. Those insights can be used to optimize the allo-
cation of resources for archiving, for example, by trig-
gering the creation of a new snapshot as a response to
a large changeset.

In other matters, our survey and study of the exist-
ing solutions and benchmarks for RDF archiving has
shown that only a few solutions are available for down-
load and use, and that among those, only Ostrich can
store the release history of very large RDF datasets.
Nonetheless, its design still does not scale to long his-
tories and does not exploit the data evolution patterns.
R43ples [36], R&WBase [72], Quit Store [12], and
x-RDF-3X [59] are also available, however they are
still far from tackling the major challenges of this task,
mainly because, they are conceived for collaborative
version control, which is an application of RDF archiv-
ing in itself. Our survey also reveals that the state of the
art lacks a standard to query RDF archives. We think
that a promising solution is to use SPARQL-star com-
bined with additional syntactic sugar as proposed by
some approaches [11, 30, 36]

Finally, we have used all these observations to de-
rive a set of design lessons in order to overcome the
gap between the literature and a fully functional so-
lution for large RDF archives. All in all, we believe
that such a solution should (i) support global histories
for RDF datasets, (i) resort to a modular architecture

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

20

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

that decouples the storage from the application lay-
ers, (iii) handle provenance and domain-speciﬁc tem-
poral metadata, (iv) implement a SPARQL extension
to query archives, (v) use a metric-based approach to
monitor the data evolution and adapt resource con-
sumption accordingly, and (vi) provide a performance
that does not depend on the length of the history. The
major algorithmic challenges in the ﬁeld lie in how
to handle the inherent trade-offs between disk usage,
ingestion time, and query runtime. With this detailed
study and the derived guidelines, we aim at paving the
way towards an ultimate solution for this problem. In
this sense, we envision archiving solutions to not only
serve as standalone single server systems but also as
components of the RDF ecosystem on the Web in all
its ﬂavors covering federated [5, 55, 68, 73] and client-
server architectures [6, 14, 15, 41, 53, 54, 80] as well
as peer-to-peer [7, 8] solutions.

Acknowledgements

This research was partially funded by the Danish
Council for Independent Research (DFF) under grant
agreement no. DFF-8048-00051B and the Poul Due
Jensen Foundation.

References

[1] European Open Data Portal. https://data.europa.eu/data/. Ac-

cessed: 2020-06-09.

[2] RDF Exports from Wikidata. Available at tools.wmﬂabs.org/

wikidata-exports/rdf/index.html.

[3] Stardog. http://stardog.com. Accessed: 2020-06-09.
[4] USA Data Gov. portal.

https://www.data.gov/. Accessed:

2020-06-09.

[5] Maribel Acosta, Maria-Esther Vidal, Tomas Lampo, Julio
Castillo, and Edna Ruckhaus. ANAPSID: An Adaptive Query
In The Seman-
Processing Engine for SPARQL Endpoints.
tic Web - ISWC 2011 - 10th International Semantic Web Con-
ference, Bonn, Germany, October 23-27, 2011, pages 18–34,
2011. doi:10.1007/978-3-642-25073-6\_2.
[6] Christian Aebeloe, Ilkcan Keles, Gabriela Montoya, and Katja
Hose. Star Pattern Fragments: Accessing Knowledge Graphs
through Star Patterns. CoRR, abs/2002.09172, 2020. URL:
https://arxiv.org/abs/2002.09172, arXiv:2002.09172.
[7] Christian Aebeloe, Gabriela Montoya, and Katja Hose. A De-
centralized Architecture for Sharing and Querying Semantic
Data. In ESWC, volume 11503, pages 3–18. Springer, 2019.
doi:10.1007/978-3-030-21348-0\_1.

[8] Christian Aebeloe, Gabriela Montoya, and Katja Hose. De-
centralized Indexing over a Network of RDF Peers. In ISWC,
volume 11778, pages 3–20. Springer, 2019. doi:10.1007/
978-3-030-30793-6\_1.

[9] Christian Aebeloe, Gabriela Montoya, and Katja Hose.
ColChain: Collaborative Linked Data Networks. In WWW ’21:
The Web Conference 2021. ACM / IW3C2, 2021. to appear.

[10] Alex B. Andersen, Nurefsan Gür, Katja Hose, Kim Ahlstrøm
Jakobsen, and Torben Bach Pedersen. Publishing Danish Agri-
cultural Government Data as Semantic Web Data. In Seman-
tic Technology - 4th Joint International Conference, JIST 2014,
Chiang Mai, Thailand, November 9-11, 2014. Revised Selected
Papers, pages 178–186. Springer, 2014. doi:10.1007/
978-3-319-15615-6\_13.
[11] James Anderson and Arto Bendiken.

Transaction-Time
Queries in Dydra. In MEPDaW/LDQ@ESWC, volume 1585 of
CEUR Workshop Proceedings, pages 11–19. CEUR-WS.org,
2016.

[12] Natanael Arndt, Patrick Naumann, Norman Radtke, Michael
Martin, and Edgard Marx. Decentralized collaborative knowl-
Journal of Web Semantics,
edge management using git.
54:29 – 47, 2019. Managing the Evolution and Preser-
vation of the Data Web. URL: http://www.sciencedirect.
com/science/article/pii/S1570826818300416, doi:https:
//doi.org/10.1016/j.websem.2018.08.002.
[13] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann,
Richard Cyganiak, and Zachary Ives. DBpedia: A Nucleus for
a Web of Open Data. In The Semantic Web, pages 722–735.
2007. doi:10.1007/978-3-540-76298-0_52.
[14] Amr Azzam, Christian Aebeloe, Gabriela Montoya, Ilkcan Ke-
les, Axel Polleres, and Katja Hose. WiseKG: Balanced Access
to Web Knowledge Graphs. In WWW ’21: The Web Conference
2021. ACM / IW3C2, 2021. to appear.

[15] Amr Azzam, Javier D. Fernández, Maribel Acosta, Martin
Beno, and Axel Polleres. SMART-KG: Hybrid Shipping for
SPARQL Querying on the Web. In WWW ’20: The Web Con-
ference 2020, pages 984–994. ACM / IW3C2, 2020. doi:
10.1145/3366423.3380177.

[16] Konstantina Bereta, Panayiotis Smeros,

and Manolis
Representation and Querying of Valid
Koubarakis.
In Ex-
Time of Triples in Linked Geospatial Data.
tended Semantic Web Conference, pages 259–274, 2013.
doi:10.1007/978-3-642-38288-8_18.

[17] Tim Berners-Lee, James Hendler, Ora Lassila, et al. The Se-
mantic Web. Scientiﬁc American, 284(5):28–37, 2001. doi:
10.1007/978-0-387-30440-3_478.

[18] Christian Bizer. The Emerging Web of Linked Data. IEEE In-
telligent Systems, 24(5):87–92, 2009. doi:10.1109/MIS.
2009.102.

[19] Nieves R. Brisaboa, Ana Cerdeira-Pena, Antonio Fariña, and
Gonzalo Navarro. A Compact RDF Store Using Sufﬁx Arrays.
In String Processing and Information Retrieval, pages 103–
115, 2015. doi:10.1007/978-3-319-23826-5_11.

[20] Jörg Brunsmann. Archiving Pushed Inferences from Sensor
Data Streams. In Proceedings of the International Workshop
on Semantic Sensor Web, pages 38–46. INSTICC, 2010. doi:
10.5220/0003116000380046.

[21] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles,
Estevam R. Hruschka Jr., and Tom M. Mitchell. Toward an Ar-
chitecture for Never-Ending Language Learning. In Proceed-
ings of the Twenty-Fourth Conference on Artiﬁcial Intelligence,
2010. doi:10.5555/2898607.2898816.

[22] Steve Cassidy and James Ballantine. Version Control for
RDF Triple Stores. ICSOFT (ISDM/EHST/DC), 7:5–12, 2007.
doi:10.1142/S0218194012500040.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

21

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

[23] Ana Cerdeira-Pena, Antonio Fariña, Javier D. Fernández, and
Miguel A. Martínez-Prieto. Self-Indexing RDF Archives. In
DCC, pages 526–535. IEEE, 2016. doi:10.1109/DCC.
2016.40.

[24] The World Wide Web Consortium. PROV-O: The PROV On-

tology. http://www.w3.org/TR/prov-o, 2013.

[25] Ivan Ermilov, Jens Lehmann, Michael Martin, and Sören Auer.
In Proceedings
LODStats: The Data Web Census Dataset.
of 15th International Semantic Web Conference - Resources
Track, 2016. URL: http://svn.aksw.org/papers/2016/ISWC_
LODStats_Resource_Description/public.pdf.

[26] Fredo Erxleben, Michael Günther, Markus Krötzsch, Ju-
lian Mendez, and Denny Vrandeˇci´c.
Introducing Wiki-
In International Seman-
data to the Linked Data Web.
tic Web Conference, pages 50–65, 2014. doi:10.1007/
978-3-319-11964-9_4.

[27] Javier D. Fernández, Miguel A. Martínez-Prieto, Claudio
Gutiérrez, Axel Polleres, and Mario Arias. Binary RDF Rep-
resentation for Publication and Exchange (HDT). Web Seman-
tics: Science, Services and Agents on the World Wide Web,
19:22–41, 2013. doi:10.1016/j.websem.2013.01.
002.

[28] Javier D. Fernández, Miguel A. Martínez-Prieto, Axel Polleres,
and Julian Reindorf. HDTQ: Managing RDF Datasets in Com-
In The Semantic Web, pages 191–208, 2018.
pressed Space.
doi:10.1007/978-3-319-93417-4_13.

[29] Javier D Fernández, Jürgen Umbrich, Axel Polleres, and Mag-
nus Knuth. Evaluating Query and Storage Strategies for RDF
Archives. In Proceedings of the 12th International Conference
on Semantic Systems, pages 41–48, 2016. doi:10.3233/
SW-180309.

[30] Valeria Fionda, Melisachew Wudage Chekol, and Giuseppe
Pirrò. Gize: A Time Warp in the Web of Data. In International
Semantic Web Conference (Posters & Demos), volume 1690,
2016.

[31] The Apache Software Foundation. Apache Jena Semantic Web

Framework. jena.apache.org.

[32] Johannes Frey, Marvin Hofer, Daniel Obraczka,

Jens
Lehmann, and Sebastian Hellmann. DBpedia FlexiFusion the
In The Seman-
Best of Wikipedia > Wikidata > Your Data.
tic Web – ISWC 2019, pages 96–112, 2019. doi:10.1007/
978-3-030-30796-7_7.

[33] Luis Galárraga, Kim Ahlstrøm, Katja Hose, and Torben Bach
Pedersen. Answering Provenance-Aware Queries on RDF
In The Semantic Web
Data Cubes Under Memory Budgets.
doi:10.1007/
– ISWC 2018, pages 547–565, 2018.
978-3-030-00671-6_32.

[34] Shi Gao, Jiaqi Gu, and Carlo Zaniolo. RDF-TX: A Fast, User-
Friendly System for Querying the History of RDF Knowledge
Bases. In Proceedings of the Extended Semantic Web Confer-
ence, pages 269–280, 2016. doi:10.1016/j.ic.2017.
08.012.

[35] Fabio Grandi. T-SPARQL: A TSQL2-like Temporal Query
In Local Proceedings of the Fourteenth
Language for RDF.
East-European Conference on Advances in Databases and In-
formation Systems, pages 21–30, 2010. doi:10.1145/
3180374.3181338.

[36] Markus Graube, Stephan Hensel, and Leon Urbas. R43ples:
Revisions for Triples. In Proceedings of the 1st Workshop on
Linked Data Quality co-located with 10th International Con-
ference on Semantic Systems (SEMANTiCS), 2014.

[37] Yuanbo Guo, Zhengxiang Pan, and Jeff Heﬂin. LUBM: A
Journal of
benchmark for OWL knowledge base systems.
Web Semantics, 3(2):158–182, 2005. doi:10.1016/j.
websem.2005.06.005.

[38] Nurefsan Gür, Jacob Nielsen, Katja Hose, and Torben Bach
Pedersen. GeoSemOLAP: Geospatial OLAP on the Seman-
tic Web Made Easy. In Proceedings of the 26th International
Conference on World Wide Web Companion, pages 213–217.
ACM, 2017. doi:10.1145/3041021.3054731.
[39] Nurefsan Gür, Torben Bach Pedersen, Esteban Zimányi, and
Katja Hose. A foundation for spatial data warehouses on the
semantic web. Semantic Web, 9(5):557–587, 2018. doi:10.
3233/SW-170281.

[40] Olaf Hartig. Foundations of RDF(cid:63) and SPARQL(cid:63): An Al-
ternative Approach to Statement-Level Metadata in RDF. In
Proc. of the 11th Alberto Mendelzon International Workshop
on Foundations of Data Management, AMW, 2017.

[41] Olaf Hartig and Carlos Buil Aranda. Bindings-restricted triple
In On the Move to Meaningful Internet
pattern fragments.
Systems: OTM 2016 Conferences - Confederated International
Conferences: CoopIS, C&TC, and ODBASE 2016, pages 762–
779, 2016. doi:10.1007/978-3-319-48472-3\_48.
[42] Olaf Hartig, Katja Hose, and Juan F. Sequeda. Linked Data
Management. In Sherif Sakr and Albert Y. Zomaya, editors,
Encyclopedia of Big Data Technologies. Springer, 2019. doi:
10.1007/978-3-319-63962-8\_76-1.

[43] Ian Horrocks, Thomas Hubauer, Ernesto Jiménez-Ruiz,
Evgeny Kharlamov, Manolis Koubarakis, Ralf Möller, Kon-
stantina Bereta, Christian Neuenstadt, Özgür Özçep, Mikhail
Roshchin, Panayiotis Smeros, and Dmitriy Zheleznyakov. Ad-
dressing Streaming and Historical Data in OBDA Systems:
Optique’s Approach (Statement of Interest). In Workshop on
Knowledge Discovery and Data Mining Meets Linked Open
Data (Know@LOD), 2013.

[44] Katja Hose and Ralf Schenkel. RDF stores.

In Ling Liu
and M. Tamer Özsu, editors, Encyclopedia of Database Sys-
doi:10.1007/
tems, Second Edition. Springer, 2018.
978-1-4614-8265-9\_80676.

[45] Thomas Huet, Joanna Biega, and Fabian M. Suchanek. Mining
History with Le Monde. In Proceedings of the Workshop on
Automated Knowledge Base Construction, pages 49–54, 2013.
doi:10.1145/2509558.2509567.

[46] Dong hyuk Im, Sang won Lee, and Hyoung joo kim. A Version
Management Framework for RDF Triple Stores. International
Journal of Software Engineering and Knowledge Engineering,
22, 04 2012. doi:10.1142/S0218194012500040.
[47] Dilshod Ibragimov, Katja Hose, Torben Bach Pedersen, and
Esteban Zimányi. Towards Exploratory OLAP Over Linked
In Enabling Real-Time Busi-
Open Data - A Case Study.
ness Intelligence - International Workshops, BIRTE 2013, vol-
ume 206, pages 114–132. Springer, 2014. doi:10.1007/
978-3-662-46839-5\_8.

[48] Dilshod Ibragimov, Katja Hose, Torben Bach Pedersen, and
Esteban Zimányi. Processing Aggregate Queries in a Feder-
In The Semantic Web. Latest
ation of SPARQL Endpoints.
Advances and New Domains - 12th European Semantic Web
Conference, ESWC 2015, Portoroz, Slovenia, May 31 - June
4, 2015. Proceedings, volume 9088, pages 269–285. Springer,
2015. doi:10.1007/978-3-319-18818-8\_17.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

22

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

[49] Dilshod Ibragimov, Katja Hose, Torben Bach Pedersen, and
Esteban Zimányi. Optimizing Aggregate SPARQL Queries
Using Materialized RDF Views. In The Semantic Web - ISWC
2016 - 15th International Semantic Web Conference, Kobe,
Japan, October 17-21, 2016, Proceedings, Part I, pages 341–
359, 2016. doi:10.1007/978-3-319-46523-4\_21.
[50] Kim Ahlstrøm Jakobsen, Alex B. Andersen, Katja Hose, and
Torben Bach Pedersen. Optimizing RDF Data Cubes for Efﬁ-
cient Processing of Analytical Queries. In International Work-
shop on Consuming Linked Data (COLD) co-located with 14th
International Semantic Web Conference (ISWC 2105), vol-
ume 1426. CEUR-WS.org, 2015. URL: http://ceur-ws.org/
Vol-1426/paper-02.pdf.

[51] Marios Meimaris. EvoGen: A Generator for Synthetic Ver-
the
In Proceedings of
sioned RDF.
EDBT/ICDT 2016 Joint Conference, EDBT/ICDT Workshops
2016, volume 1558, 2016. URL: http://ceur-ws.org/Vol-1558/
paper9.pdf.

the Workshops of

[52] Steffen Metzger, Katja Hose, and Ralf Schenkel. Colledge: a
vision of collaborative knowledge networks. In Proceedings of
the 2nd International Workshop on Semantic Search over the
Web, pages 1–8. ACM, 2012. doi:10.1145/2494068.
2494069.

[53] Thomas Minier, Hala Skaf-Molli, and Pascal Molli. SaGe: Web
Preemption for Public SPARQL Query Services. In The World
Wide Web Conference, WWW 2019, pages 1268–1278. ACM,
2019. doi:10.1145/3308558.3313652.

[54] Gabriela Montoya, Christian Aebeloe, and Katja Hose. To-
wards Efﬁcient Query Processing over Heterogeneous RDF In-
terfaces. In Proceedings of the 2nd Workshop on Decentral-
izing the Semantic Web co-located with the 17th International
Semantic Web Conference, DeSemWeb@ISWC, volume 2165
of CEUR Workshop Proceedings. CEUR-WS.org, 2018. URL:
http://ceur-ws.org/Vol-2165/paper4.pdf.

[55] Gabriela Montoya, Hala Skaf-Molli, and Katja Hose. The
Odyssey Approach for Optimizing Federated SPARQL
Queries. In International Semantic Web Conference, volume
10587, pages 471–489. Springer, 2017. doi:10.1007/
978-3-319-68288-4\_28.

[56] Rudra Pratap Deb Nath, Katja Hose, Torben Bach Pedersen,
and Oscar Romero. SETL: A programmable semantic extract-
transform-load framework for semantic data warehouses. Inf.
Syst., 68:17–43, 2017. doi:10.1016/j.is.2017.01.
005.

[57] Rudra Pratap Deb Nath, Katja Hose, Torben Bach Pedersen,
Oscar Romero, and Amrit Bhattacharjee. SETLBI : An Inte-
In Pro-
grated Platform for Semantic Business Intelligence.
ceedings of The 2020 World Wide Web Conference, 2020.
doi:10.1145/3366424.3383533.

[58] Thomas Neumann and Gerhard Weikum. RDF-3X: a RISC-
style engine for RDF. Proceedings of the VLDB Endow-
ment, 1(1):647–659, 2008. doi:10.14778/1453856.
1453927.

[59] Thomas Neumann and Gerhard Weikum. x-rdf-3x: Fast query-
ing, high update rates, and consistency for rdf databases. Pro-
ceedings of the VLDB Endowment, 3(1-2):256–263, 2010.
doi:10.14778/1920841.1920877.

[60] Vinh Nguyen, Olivier Bodenreider, and Amit Sheth. Don’t
Like RDF Reiﬁcation?: Making Statements About Statements
Using Singleton Property. In Proceedings of the 23rd Interna-

tional Conference on World Wide Web, pages 759–770, 2014.
doi:10.1145/2566486.2567973.

[61] Natasha Noy and Mark Musen. Ontology Versioning in an
Ontology Management Framework. Intelligent Systems, IEEE,
19:6–13, 08 2004. doi:10.1109/MIS.2004.33.

[62] George Papadakis, Konstantina Bereta, Themis Palpanas, and
Manolis Koubarakis. Multi-core Meta-blocking for Big Linked
In Proceedings of the 13th International Conference
Data.
on Semantic Systems, pages 33–40, 2017. doi:10.1145/
3132218.3132230.

[63] Vicky Papavassiliou, Giorgos Flouris, Irini Fundulaki, Dim-
itris Kotzinos, and Vassilis Christophides. On Detecting High-
level Changes in RDF/S KBs. In International Semantic Web
Conference (ISWC), pages 473–488, 2009. doi:10.1007/
978-3-642-04930-9_30.

[64] Thomas Pellissier Tanon, Camille Bourgaux, and Fabian
Suchanek. Learning How to Correct a Knowledge Base from
the Edit History. In The World Wide Web Conference, pages
1465–1475, 2019. doi:10.1145/3308558.3313584.

[65] Thomas Pellissier Tanon and Fabian M. Suchanek. Querying
the Edit History of Wikidata. In Extended Semantic Web Con-
ference, 2019. doi:10.1007/978-3-030-32327-1_
32.

[66] Matthew Perry, Prateek Jain, and Amit P. Sheth. SPARQL-ST:
In
Extending SPARQL to Support Spatio-temporal Queries.
Geospatial Semantics and the Semantic Web, volume 12, pages
61–86, 2011. doi:10.1007/978-1-4419-9446-2_3.
[67] Maria Psaraki and Yannis Tzitzikas. CPOI: A Compact
Method to Archive Versioned RDF Triple-Sets, 2019. arXiv:
1902.04129.

[68] Bastian Quilitz and Ulf Leser. Querying Distributed RDF
In The Semantic Web: Re-
Data Sources with SPARQL.
search and Applications, 5th European Semantic Web Confer-
ence, ESWC 2008, Tenerife, Canary Islands, Spain, June 1-5,
2008, Proceedings, pages 524–538, 2008. doi:10.1007/
978-3-540-68234-9\_39.

[69] Yves Raimond and Guus Schreiber. RDF 1.1 primer. W3C
recommendation, 2014. http://www.w3.org/TR/2014/NOTE-
rdf11-primer-20140624/.

[70] Yves Raimond and Guus Schreiber. RDF 1.1 Semantics. W3C
http://www.w3.org/TR/2014/REC-

recommendation, 2014.
rdf11-mt-20140225/.

[71] Yannis Roussakis, Ioannis Chrysakis, Kostas Stefanidis, Gior-
gos Flouris, and Yannis Stavrakas. A Flexible Framework for
In
Understanding the Dynamics of Evolving RDF Datasets.
International Semantic Web Conference (ISWC), 2015. doi:
10.1007/978-3-319-25007-6_29.

[72] Miel Vander Sande, Pieter Colpaert, Ruben Verborgh, Sam
Coppens, Erik Mannens, and Rik Van de Walle. R&Wbase:
Git for triples. Linked Data on the Web Workshop, 2013.
[73] Andreas Schwarte, Peter Haase, Katja Hose, Ralf Schenkel,
and Michael Schmidt. FedX: Optimization Techniques for
Federated Query Processing on Linked Data. In International
Semantic Web Conference, volume 7031, pages 601–616.
Springer, 2011. doi:10.1007/978-3-642-25073-6\
_38.

SPARQL 1.1
[74] Andy Seaborne
query language.
W3C recommendation, W3C, 2013.
http://www.w3.org/TR/2013/REC-sparql11-query-20130321/.

and Steven Harris.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

Pelgrin et al. / Towards Fully-ﬂedged Archiving for RDF Datasets

23

[75] Richard T. Snodgrass, Ilsoo Ahn, Gad Ariav, Don S. Ba-
tory, James Clifford, Curtis E. Dyreson, Ramez Elmasri, Fabio
Grandi, Christian S. Jensen, Wolfgang Käfer, Nick Kline,
Krishna G. Kulkarni, T. Y. Cliff Leung, Nikos A. Lorent-
zos, John F. Roddick, Arie Segev, Michael D. Soo, and
TSQL2 Language Speciﬁca-
Suryanarayana M. Sripada.
tion. SIGMOD Record, 23(1):65–86, 1994. doi:10.1145/
181550.181562.

[76] OpenLink Software. OpenLink Virtuoso.

http://virtuoso.

openlinksw.com.

[77] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum.
Yago: A Large Ontology from Wikipedia and Wordnet. Web
Semantics: Science, Services and Agents on the World Wide
doi:10.1016/j.websem.
Web, 6(3):203–217, 2008.
2008.06.001.

[78] Ruben Taelman, Miel Vander Sande, and Ruben Verborgh. OS-
TRICH: Versioned Random-Access Triple Store. In Compan-
ion of the The Web Conference 2018, WWW, pages 127–130,
2018. doi:10.1145/3184558.3186960.

[79] Mayesha Tasnim, Diego Collarana, Damien Graux, Fabrizio
Orlandi, and Maria-Esther Vidal. Summarizing Entity Tempo-

ral Evolution in Knowledge Graphs. In Companion Proceed-
ings of The 2019 World Wide Web Conference, pages 961–965,
2019. doi:10.1145/3308560.3316521.

[80] Ruben Verborgh, Miel Vander Sande, Olaf Hartig,
Joachim Van Herwegen, Laurens De Vocht, Ben De Meester,
Gerald Haesendonck, and Pieter Colpaert.
Triple Pat-
tern Fragments: A low-cost knowledge graph interface
J. Web Semant., 37-38:184–206, 2016.
for
doi:10.1016/j.websem.2016.03.003.

the Web.

[81] Max Volkel, Wolf Winkler, York Sure, Sebastian Ryszard
Kruk, and Marcin Synak. SemVersion: A Versioning System
for RDF and Ontologies. Second European Semantic Web Con-
ference, 2005.

[82] Antoine Zimmermann, Nuno Lopes, Axel Polleres, and Um-
berto Straccia. A General Framework for Representing, Rea-
soning and Querying with Annotated Semantic Web Data. Web
Semantics, 11:72–95, 2012. doi:10.1016/j.websem.
2011.08.006.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

