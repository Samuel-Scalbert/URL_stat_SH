Fully Decentralized Joint Learning of Personalized
Models and Collaboration Graphs
Valentina Zantedeschi, Aurélien Bellet, Marc Tommasi

To cite this version:

Valentina Zantedeschi, Aurélien Bellet, Marc Tommasi. Fully Decentralized Joint Learning of Per-
sonalized Models and Collaboration Graphs. AISTATS 2020 - The 23rd International Conference on
Artificial Intelligence and Statistics, Aug 2020, Palerme / Virtual, Italy. ￿hal-03100057￿

HAL Id: hal-03100057

https://inria.hal.science/hal-03100057

Submitted on 6 Jan 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Fully Decentralized Joint Learning of
Personalized Models and Collaboration Graphs

Valentina Zantedeschi
GE Global Research, USA 1

Aur´elien Bellet
Inria, France

Marc Tommasi
Universit´e de Lille & Inria, France

Abstract

We consider the fully decentralized machine
learning scenario where many users with per-
sonal datasets collaborate to learn models
through local peer-to-peer exchanges, with-
out a central coordinator. We propose to
train personalized models that leverage a col-
laboration graph describing the relationships
between user personal tasks, which we learn
jointly with the models. Our fully decen-
tralized optimization procedure alternates be-
tween training nonlinear models given the
graph in a greedy boosting manner, and up-
dating the collaboration graph (with con-
trolled sparsity) given the models. Through-
out the process, users exchange messages only
with a small number of peers (their direct
neighbors when updating the models, and a
few random users when updating the graph),
ensuring that the procedure naturally scales
with the number of users. Overall, our ap-
proach is communication-eﬃcient and avoids
exchanging personal data. We provide an
extensive analysis of the convergence rate,
memory and communication complexity of
our approach, and demonstrate its beneﬁts
compared to competing techniques on syn-
thetic and real datasets.

1 INTRODUCTION

In the era of big data, the classical paradigm is to
build huge data centers to collect and process user
data. This centralized access to resources and datasets
is convenient to train machine learning models, but
also comes with important drawbacks. The service

provider needs to gather, store and analyze the data on
a large central server, which induces high infrastructure
costs. As the server represents a single point of entry,
it must also be secure enough to prevent attacks that
could put the entire user database in jeopardy. On the
user end, disadvantages include limited control over
one’s personal data as well as possible privacy risks,
which may come from the aforementioned attacks but
also from potentially loose data governance policies
on the part of service providers. A more subtle risk
is to be trapped in a “single thought” model which
fades individual users’ speciﬁcities or leads to unfair
predictions for some of the users.

For these reasons and thanks to the advent of powerful
personal devices, we are currently witnessing a shift to
a diﬀerent paradigm where data is kept on the users’
devices, whose computational resources are leveraged to
train models in a collaborative manner. The resulting
data is not typically balanced nor independent and
identically distributed across machines, and additional
constraints arise when many parties are involved. In
particular, the speciﬁcities of each user result in an
increase in model complexity and size, and information
needs to be exchanged across users to compensate for
the lack of local data. In this context, communication is
usually a major bottleneck, so that solutions aiming at
reaching an agreement between user models or requiring
a central coordinator should be avoided.

In this work, we focus on fully decentralized learning,
which has recently attracted a lot of interest (Duchi
et al., 2012; Wei and Ozdaglar, 2012; Colin et al., 2016;
Lian et al., 2017; Jiang et al., 2017; Tang et al., 2018;
Lian et al., 2018). In this setting, users exchange in-
formation through local peer-to-peer exchanges in a
sparse communication graph without relying on a cen-
tral server that aggregates updates or coordinates the
protocol. Unlike federated learning which requires such
central coordination (McMahan et al., 2017; Koneˇcn`y

Proceedings of the 23rdInternational Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2020, Palermo, Italy.
PMLR: Volume 108. Copyright 2020 by the author(s).

1 This work was carried out while the author was aﬃli-
ated with Univ Lyon, UJM-Saint-Etienne, CNRS, Institut
d Optique Graduate School, Laboratoire Hubert Curien
UMR 5516, France

Fully Decentralized Joint Learning

et al., 2016; Kairouz et al., 2019), fully decentralized
learning naturally scales to large numbers of users with-
out single point of failure or communication bottlenecks
(Lian et al., 2017).

The present work stands out from existing approaches
in fully decentralized learning, which train a single
global model that may not be adapted to all users.
Instead, our idea is to leverage the fact that in many
large-scale applications (e.g., predictive modeling in
smartphones apps), each user exhibits distinct behav-
iors/preferences but is suﬃciently similar to some other
peers to beneﬁt from sharing information with them.
We thus propose to jointly discover the relationships
between the personal tasks of users in the form of
a sparse collaboration graph and learn personalized
models that leverage this graph to achieve better gen-
eralization performance. For scalability reasons, the
collaboration graph serves as an overlay to restrict the
communication to pairs of users whose tasks appear to
be suﬃciently similar. In such a framework, it is cru-
cial that the graph is well-aligned with the underlying
similarity between the personal tasks to ensure that
the collaboration is fruitful and avoid convergence to
poorly-adapted models.

We formulate the problem as the optimization of a
joint objective over the models and the collaboration
graph, in which collaboration is achieved by introduc-
ing a trade-oﬀ between (i) having the personal model of
each user accurate on its local dataset, and (ii) making
the models and the collaboration graph smooth with
respect to each other. We then design and analyze a
fully decentralized algorithm to solve our collaborative
problem in an alternating procedure, in which we iter-
ate between updating personalized models given the
current graph and updating the graph (with controlled
sparsity) given the current models. We ﬁrst propose
an approach to learn personalized nonlinear classiﬁers
as combinations of a set of base predictors inspired
from l1-Adaboost (Shen and Li, 2010).
In the pro-
posed decentralized algorithm, users greedily update
their personal models by incorporating a single base
predictor at a time and send the update only to their
direct neighbors in the graph. We establish the conver-
gence rate of the procedure and show that it requires
very low communication costs (linear in the number of
edges in the graph and logarithmic in the number of
base classiﬁers to combine). We then propose an ap-
proach to learn a sparse collaboration graph. From the
decentralized system perspective, users update their
neighborhood of similar peers by communicating only
with small random subsets of peers obtained through
a peer sampling service (Jelasity et al., 2007). Our
approach is ﬂexible enough to accommodate various
graph regularizers allowing to easily control the spar-

sity of the learned graph, which is key to the scalability
of the model update step. For strongly convex regu-
larizers, we prove a fast convergence for our algorithm
and show how the number of random users requested
from the peer sampling service rules a trade-oﬀ between
communication and convergence speed.

To summarize, we propose the ﬁrst approach to train
in a fully decentralized way, i.e. without any central
server, personalized and nonlinear models in a collabo-
rative way while also learning the collaboration graph.
Our main contributions are as follows. (1) We formal-
ize the problem of learning with whom to collaborate,
together with personalized models for collaborative de-
centralized learning. (2) We propose and analyze a fully
decentralized algorithm to learn nonlinear personalized
models with low communication costs. (3) We derive
a generic and scalable approach to learn sparse collab-
oration graphs in the decentralized setting. (4) We
show that our alternating optimization scheme leads
to better personalized models at lower communication
costs than existing methods on several datasets.

2 RELATED WORK

Federated multi-task learning. Our work can be
seen as multi-task learning (MTL) where each user
is considered as a task. In MTL, multiple tasks are
learned simultaneously with the assumption that a
structure captures task relationships. A popular ap-
proach in MTL is to jointly optimize models for all tasks
while enforcing similar models for similar tasks (Ev-
geniou and Pontil, 2004; Maurer, 2006; Dhillon et al.,
2011). Task relationships are often considered as known
a priori but recent work also tries to learn this struc-
ture (see Zhang and Yang, 2017, and references therein).
However, in classical MTL approaches data is collected
on a central server where the learning algorithm is per-
formed (or it is iid over the machines of a computing
cluster). Recently, distributed and federated learning
approaches (Wang et al., 2016b,a; Baytas et al., 2016;
Smith et al., 2017) have been proposed to overcome
these limitations. Each node holds data for one task
(non iid data) but these approaches still rely on a central
server to aggregate updates. The federated learning
approach of (Smith et al., 2017) is closest to our work
for it jointly learns personalized (linear) models and
pairwise similarities across tasks. However, the simi-
larities are updated in a centralized way by the server
which must regularly access all task models, creating a
signiﬁcant communication and computation bottleneck
when the number of tasks is large. Furthermore, the
task similarities do not form a valid weighted graph
and are typically not sparse. This makes their problem
formulation poorly suited to the fully decentralized
setting, where sparsity is key to ensure scalability.

Valentina Zantedeschi, Aur´elien Bellet, Marc Tommasi

Decentralized learning. There has been a recent
surge of interest in fully decentralized machine learn-
ing. In most existing work, the goal is to learn the same
global model for all users by minimizing the average
of the local objectives (Duchi et al., 2012; Wei and
Ozdaglar, 2012; Colin et al., 2016; Lafond et al., 2016;
Lian et al., 2017; Jiang et al., 2017; Tang et al., 2018;
Lian et al., 2018). In this case, there is no personal-
ization: the graph merely encodes the communication
topology without any semantic meaning and only af-
fects the convergence speed. Our work is more closely
inspired by recent decentralized approaches that have
shown the beneﬁts of collaboratively learning person-
alized models for each user by leveraging a similarity
graph given as input to the algorithm (Vanhaesebrouck
et al., 2017; Li et al., 2017; Bellet et al., 2018; Almeida
and Xavier, 2018). As in our approach, this is achieved
through a graph regularization term in the objective. A
severe limitation to the applicability of these methods
is that a relevant graph must be known beforehand,
which is an unrealistic assumption in many practical
scenarios. Crucially, our approach lifts this limitation
by allowing to learn the graph along with the models.
In fact, as we demonstrate in our experiments, our
decentralized graph learning procedure of Section 5
can be readily combined with the algorithms of (Van-
haesebrouck et al., 2017; Li et al., 2017; Bellet et al.,
2018; Almeida and Xavier, 2018) in our alternating op-
timization procedure, thereby broadening their scope.
It is also worth mentioning that (Vanhaesebrouck et al.,
2017; Li et al., 2017; Bellet et al., 2018; Almeida and
Xavier, 2018) are restricted to linear models and have
per-iteration communication complexity linear in the
data dimension. Our boosting-based approach (Sec-
tion 4) learns nonlinear models with logarithmic com-
munication cost, providing an interesting alternative
for problems of high dimension and/or with complex
decision boundaries, as illustrated in our experiments.

3 PROBLEM SETTING AND

NOTATIONS

In this section, we formally describe the problem
of interest. We consider a set of users (or agents)
[K] = {1, . . . , K}, each with a personal data distri-
bution over some common feature space X and label
space Y deﬁning a personal supervised learning task.
For example, the personal task of each user could be
to predict whether he/she likes a given item based
on features describing the item. Each user k holds a
local dataset Sk of mk labeled examples drawn from
its personal data distribution over X × Y, and aims
to learn a model parameterized by αk ∈ Rn which
generalizes well to new data points drawn from its
distribution. We assume that all users learn models

from the same hypothesis class, and since they have
datasets of diﬀerent sizes we introduce a notion of
“conﬁdence” ck ∈ R+ for each user k which should be
thought of as proportional to mk (in practice we simply
set ck = mk/ maxl ml). In a non-collaborative setting,
each user k would typically select the model parameters
that minimize some (potentially regularized) loss func-
tion Lk(αk; Sk) over its local dataset Sk. This leads
to poor generalization performance when local data is
scarce. Instead, we propose to study a collaborative
learning setting in which users discover relationships
between their personal tasks which are leveraged to
learn better personalized models. We aim to solve this
problem in a fully decentralized way without relying
on a central coordinator node.

Decentralized collaborative learning. Following
the standard practice in the fully decentralized litera-
ture (Boyd et al., 2006), each user regularly becomes
active at the ticks of an independent local clock which
follows a Poisson distribution. Equivalently, we con-
sider a global clock (with counter t) which ticks each
time one of the local clock ticks, which is convenient for
stating and analyzing the algorithms. We assume that
each user can send messages to any other user (like on
the Internet) in a peer-to-peer manner. However, in
order to scale to a large number of users and to achieve
fruitful collaboration, we consider a semantic overlay
on the communication layer whose goal is to restrict
the message exchanges to pairs of users whose tasks are
most similar. We call this overlay a collaboration graph,
which is modeled as an undirected weighted graph
Gw = ([K], w) in which nodes correspond to users and
edge weights wk,l ≥ 0 should reﬂect the similarity be-
tween the learning tasks of users k and l, with wk,l = 0
indicating the absence of edge. A user k only sends
messages to its direct neighbors Nk = {l : wk,l > 0}
in Gw, and potentially to a small random set of peers
obtained through a peer sampling service (see Jelasity
et al., 2007, for a decentralized version). Importantly,
we do not enforce the graph to be connected: diﬀerent
connected components can be seen as modeling clusters
of unrelated users. In our approach, the collaboration
graph is not known beforehand and iteratively evolves
(controlling its sparsity) in a learning scheme that al-
ternates between learning the graph and learning the
models. This scheme is designed to solve a global, joint
optimization problem that we introduce below.

Objective function. We propose to learn the per-
sonal classiﬁers α = (α1, . . . , αK) ∈ (Rn)K and the
collaboration graph w ∈ RK(K−1)/2 to minimize the
following joint optimization problem:
J(α, w) = (cid:80)K

k=1 dk(w)ckLk(αk; Sk)

minα∈M
w∈W

+

µ1
2

(cid:80)

k<l wk,l(cid:107)αk − αl(cid:107)2 + µ2g(w),

(1)

Fully Decentralized Joint Learning

where M = M1 × · · · × MK and W = {w ∈
RK(K−1)/2 : w ≥ 0} are the feasible domains for the
models and the graph, d(w) = (d1(w), . . . , dK(w)) ∈
RK is the degree vector with dk(w) = (cid:80)K
l=1 wk,l, and
µ1, µ2 ≥ 0 are trade-oﬀ hyperparameters.

The joint objective function J(α, w) in (1) is composed
of three terms. The ﬁrst one is a (weighted) sum of
loss functions, each involving only the personal model
and local dataset of a single user. The second term
involves both the models and the graph: it enables col-
laboration by encouraging two users k and l to have a
similar model for large edge weight wk,l. This principle,
known as graph regularization, is well-established in
the multi-task learning literature (Evgeniou and Pontil,
2004; Maurer, 2006; Dhillon et al., 2011). Importantly,
the factor dk(w)ck in front of the local loss Lk of each
user k implements a useful inductive bias: users with
larger datasets (large conﬁdence) will tend to connect
to other nodes as long as their local loss remains small
so that they can positively inﬂuence their neighbors,
while users with small datasets (low conﬁdence) will
tend to disregard their local loss and rely more on in-
formation from other users. Finally, the last term g(w)
introduces some regularization on the graph weights
w used to avoid degenerate solutions (e.g., edgeless
graphs) and control structural properties such as spar-
sity (see Section 5 for concrete examples). We stress
the fact that the formulation (1) allows for very ﬂexible
notions of relationships between the users’ tasks. For
instance, as µ1 → +∞ the problem becomes equivalent
to learning a shared model for all users in the same
connected component of the graph, by minimizing the
sum of the losses of users independently in each compo-
nent. On the other hand, setting µ1 = 0 corresponds
to having each user k learn its classiﬁer αk based on
its local dataset only (no collaboration). Intermediate
values of µ1 let each user learn its own personal model
but with the models of other (strongly connected) users
acting as a regularizer.

While Problem (1) is not jointly convex in α and w in
general, it is typically bi-convex. Our approach thus
solves it by alternating decentralized optimization on
the models α and the graph weights w.2

Outline. In Section 4, we propose a decentralized al-
gorithm to learn nonlinear models given the graph in a
greedy boosting manner with communication-eﬃcient
updates. In Section 5, we design a decentralized algo-
rithm to learn a (sparse) collaboration graph given the
models with ﬂexible regularizers g(w). We discuss re-
lated work in Section 2, and present some experiments
in Section 6.

2Alternating optimization converges to a local optimum
under mild technical conditions, see (Tseng, 2001; Tseng
and Yun, 2009; Razaviyayn et al., 2013).

4 DECENTRALIZED

COLLABORATIVE BOOSTING
OF PERSONALIZED MODELS

In this section, given some ﬁxed graph weights w ∈ W,
we propose a decentralized algorithm for learning per-
sonalized nonlinear classiﬁers α = (α1, . . . , αK) ∈ M
in a boosting manner which is essential to ensure only
logarithmic communication complexity in the number
of model parameters while optimizing expressive mod-
els. For simplicity, we focus on binary classiﬁcation
with Y = {−1, 1}. We propose that each user k learns
a personal classiﬁer as a weighted combination of a set
of n real-valued base predictors H = {hj : X → R}n
j=1,
i.e. a mapping x (cid:55)→ sign((cid:80)n
j=1[αk]jhj(x)) parameter-
ized by αk ∈ Rn. The base predictors can be for
instance weak classiﬁers (e.g., decision stumps) as in
standard boosting, or stronger predictors pre-trained
on separate data (e.g., public, crowdsourced, or col-
lected from users who opted in to share personal data).
We denote by Ak ∈ Rmk×n the matrix whose (i, j)-
th entry gives the margin achieved by the j-th base
classiﬁer on the i-th training sample of user k, so that
j=1[αk]jhj(xi) gives the
for i ∈ [mk], [Akαk]i = yi
margin achieved by the classiﬁer αk on the i-th data
point (xi, yi) in Sk. Only user k has access to Ak.

(cid:80)n

Adapting the formulation of l1-Adaboost (Shen and
Li, 2010; Wang et al., 2015) to our personalized setting,
we instantiate the local loss Lk(αk; Sk) and the feasible
domain Mk = {αk ∈ Rn : (cid:107)αk(cid:107)1 ≤ β} for each user k
as follows:

Lk(αk; Sk) = log (cid:0)

e−[Akαk]i(cid:1),

(2)

mk(cid:88)

i=1

where β ≥ 0 is a hyperparameter to favor sparse models
by controlling their l1-norm. Since the graph weights
are ﬁxed in this section, with a slight abuse of notation
we denote by f (α) := J(α, w) the objective function in
(1) instantiated with the loss function (2). Note that
f is convex and continuously diﬀerentiable, and the
domain M = M1 × · · · × MK is a compact and convex
subset of (Rn)K.

4.1 Decentralized Algorithm

We propose a decentralized algorithm based on Frank-
Wolfe (FW) (Frank and Wolfe, 1956; Jaggi, 2013), also
known as conditional gradient descent. Our approach
is inspired from a recent FW algorithm to solve l1-
Adaboost in the centralized and non-personalized set-
ting (Wang et al., 2015). For clarity of presentation,
we set aside the decentralized setting for a moment and
derive the FW update with respect to the model of a
single user.

Classical FW update. Let t ≥ 1 and denote by

Valentina Zantedeschi, Aur´elien Bellet, Marc Tommasi

∇[f (α(t−1))]k the partial derivative of f with respect
to the k-th block of coordinates corresponding to the
model α(t−1)
of user k. For step size γ ∈ [0, 1], a FW
update for user k takes the form of a convex combina-
+ γs(t)
tion α(t)

k = (1 − γ)α(t−1)

k with

k

k

s(t)
k = arg min(cid:107)s(cid:107)1≤β s(cid:62)∇[f (α(t−1))]k
)ej(t)
= β sign(−(∇[f (α(t−1))]k)j(t)
k ,

k

(3)

k = arg maxj[|∇[f (α(t−1))]k|]j and ej(t)

where j(t)
is the
unit vector with 1 in the j(t)
k -th entry (Clarkson, 2010;
Jaggi, 2013). In other words, FW updates a single co-
ordinate of the current model α(t−1)
which corresponds
to the maximum absolute value entry of the partial
gradient ∇[f (α(t−1))]k. In our case, we have:

k

k

∇[f (α(t−1))]k = −dk(w)ckη(cid:62)

k Ak + µ1(dk(w)α(t−1)
− (cid:80)
l wk,lα(t−1)

),

k

l

(4)

k

k

)

)i

(cid:80)mk

exp(−Akα(t−1)
i=1 exp(−Akα(t−1)

. The ﬁrst term in

with ηk =
∇[f (α(t−1))]k plays the same role as in standard Ad-
aboost: the j-th entry (corresponding to the base pre-
dictor hj) is larger when hj achieves a large margin on
the training sample Sk reweighted by ηk (i.e., points
that are currently poorly classiﬁed get more impor-
tance). On the other hand, the more hj is used by
the neighbors of k, the larger the j-th entry of the
second term. The FW update (3) thus preserves the
ﬂavor of boosting (incorporating a single base classi-
ﬁer at a time which performs well on the reweighted
sample) with an additional bias towards selecting base
predictors that are popular amongst neighbors in the
collaboration graph. The relative importance of the
two terms depends on the user conﬁdence ck.

Decentralized FW. We are now ready to state our
decentralized FW algorithm to optimize f . Each user
corresponds keeps its personal dataset locally. The
ﬁxed collaboration graph Gw plays the role of an over-
lay: user k only needs to communicate with its direct
neighborhood Nk in Gw. The size of Nk, |Nk|, is typ-
ically small so that updates can occur in parallel in
diﬀerent parts of the network, ensuring that the proce-
dure scales well with the number of users.

Our algorithm proceeds as follows. Let us denote by
α(t) ∈ M the current models at time step t. Each
personal classiﬁer is initialized to some feasible point
α(0)
k ∈ Mk (such as the zero vector). Then, at each
step t ≥ 1, a random user k wakes up and performs
the following actions:

1. Update step: user k performs a FW update on its
local model based on the most recent information

α(t−1)
l

received from its neighbors l ∈ Nk:

α(t)
k = (1 − γ(t))α(t−1)
with s(t)
k as in (3) and γ(t) = 2K/(t + 2K).

+ γ(t) s(t)
k ,

k

2. Communication step: user k sends its updated
to its neighborhood Nk.

model α(t)
k

Importantly, the above update only requires the knowl-
edge of the models of neighboring users, which were
received at earlier iterations.

4.2 Convergence Analysis, Communication

and Memory Costs

The convergence analysis of our algorithm essentially
follows the proof technique proposed in (Jaggi, 2013)
and reﬁned in (Lacoste-Julien et al., 2013) for the
case of block coordinate Frank-Wolfe. It is based on
deﬁning a surrogate for the optimality gap f (α)−f (α∗),
where α∗ ∈ arg minα∈M f (α). Under an appropriate
notion of smoothness for f over the feasible domain,
the convergence is established by showing that the gap
decreases in expectation with the number of iterations,
because at a given iteration t the block-wise surrogate
gap at the current solution is minimized by the greedy
update s(t)
k . We obtain that our algorithm achieves
an O(1/t) convergence rate (see supplementary for the
proof).

Theorem 1. Our decentralized Frank-Wolfe algo-
rithm takes at most 6K(C ⊗
f + p0)/ε iterations to
in
ﬁnd an approximate solution α that satisﬁes,
expectation, f (α) − f (α∗) ≤ ε, where C ⊗
f ≤
4β2 (cid:80)K
k=1 dk(w)(ck(cid:107)Ak(cid:107)2 + µ1) and p0 = f (α(0)) −
f (α∗) is the initial sub-optimality gap.

Theorem 1 shows that large degrees for users with low
conﬁdence and small margins penalize the convergence
rate much less than for users with large conﬁdence and
large margins. This is rather intuitive as users in the
latter case have greater inﬂuence on the overall solution
in Eq. (1).

Remarkably, using a few tricks in the representation
of the sparse updates, the communication and mem-
ory cost needed by our algorithm to converge to an
(cid:15)-approximate solution can be shown to be linear in
the number of edges of the graph and logarithmic in
the number of base predictors. We refer to the sup-
plementary material for details. For the classic case
where base predictors consist of a constant number
of decisions stumps per feature, this translates into
a logarithmic cost in the dimensionality of the data
leading to signiﬁcantly better complexities than the
state-of-the-art (see the experiments of Section 6).

Fully Decentralized Joint Learning

Remark 1 (Other loss functions). We focus on the
Adaboost log loss (2) to emphasize that we can learn
nonlinear models while keeping the formulation convex.
We point out that our algorithm and analysis readily
extend to other convex loss functions, as long as we
keep an L1-constraint on the parameters.

respect to block wk,K. We now state our algorithm.
We start from some arbitrary weight vector w(0) ∈ W,
each user having a local copy of its K − 1 weights.
At each time step t, a random user k wakes up and
performs the following actions:

5 DECENTRALIZED LEARNING
OF COLLABORATION GRAPH

In the previous section, we have proposed and analyzed
an algorithm to learn the model parameters α given
a ﬁxed collaboration graph w. To make our fully de-
centralized alternating optimization scheme complete,
we now turn to the converse problem of optimizing the
graph weights w given ﬁxed models α. We will work
with ﬂexible graph regularizers g(w) that are weight
and degree-separable:
g(w) = (cid:80)

k<l gk,l(wk,l) + (cid:80)K
where gk,l : R → R and gk : R → R are convex and
smooth. This generic form allows to regularize weights
and degrees in a ﬂexible way (which encompasses some
recent work from the graph signal processing commu-
nity (Dong et al., 2016; Kalofolias, 2016; Berger et al.,
2018)), while the separable structure is key to the de-
sign of an eﬃcient decentralized algorithm that relies
only on local communication. We denote the graph
learning objective function by h(w) := J(α, w) for ﬁxed
models α. Note that h(w) is convex in w.

k=1 gk(dk(w)),

Decentralized algorithm. Our goal is to design a
fully decentralized algorithm to update the collabora-
tion graph Gw. We thus need users to communicate
beyond their current direct neighbors in Gw to discover
new relevant neighbors. In order to preserve scalability
to large numbers of users, a user can only communicate
with small random batches of other users. In a decen-
tralized system, this can be implemented by a classic
primitive known as a peer sampling service (Jelasity
et al., 2007; Kermarrec et al., 2011). Let κ ∈ [1..K − 1]
be a parameter of the algorithm, which in practice is
much smaller than K. At each step, a random user k
wakes up and samples uniformly and without replace-
ment a set K of κ users from the set {1, . . . , K} \ {k}
using the peer sampling service. We denote by wk,K
the κ-dimensional subvector of a vector w ∈ RK(K−1)/2
corresponding to the entries {(k, l)}l∈K. Let ∆k,K =
((cid:107)αk−αl(cid:107)2)l∈K, pk,K = (ckLk(αk; Sk)+clLl(αl; Sl))l∈K
and vk,K(w) = (g(cid:48)
k,l(wk,l))l∈K.
The partial derivative of the objective h(w) with respect
to the variables wk,K can be written as follows:

k(dk(w)) + g(cid:48)

l(dl(w)) + g(cid:48)

[∇h(w)]k,K = pk,K + (µ1/2)∆k,K + µ2vk,K(w).

(5)

1. Draw a set K of κ users and request their current

models, loss value and degree.
2. Update the associated weights:

k,K ← max (cid:0)0, w(t)
w(t+1)

k,K − (1/Lk,K)[∇h(w(t))]k,K

(cid:1).

3. Send each updated weight w(t+1)

to the associated

k,l

user in l ∈ K.

The algorithm is fully decentralized. Indeed, no global
information is needed to update the weights: the in-
formation requested from users in K at step 1 of the
algorithm is suﬃcient to compute (5). Updates can
thus happen asynchronously and in parallel.

Convergence, communication and memory. Our
analysis proceeds as follows. We ﬁrst show that our
algorithm can be seen as an instance of proximal coor-
dinate descent (PCD) (Tseng and Yun, 2009; Richt´arik
and Tak´ac, 2014) on a slightly modiﬁed objective func-
tion. Unlike the standard PCD setting which focuses on
disjoint blocks, our coordinate blocks exhibit a speciﬁc
overlapping structure that arises as soon as κ > 1 (as
each weight is shared by two users). We build upon the
PCD analysis due to (Wright, 2015), which we adapt
to account for our overlapping block structure. The de-
tails of our analysis can be found in the supplementary
material. For the case where g is strongly convex, we
obtain the following convergence rate.3

Theorem 2. Assume that g(w) is σ-strongly con-
vex. Let T > 0 and h∗ be the optimal objective
value. Our algorithm cuts the expected suboptimal-
ity gap by a constant factor ρ at each iteration: we
have E[h(w(T )) − h∗] ≤ ρT (h(w(0)) − h∗) with ρ =
1 −

with Lmax = max(k,K) Lk,K.

2κσ
K(K−1)Lmax

The rate of Theorem 2 is typically faster than the sub-
linear rate of the boosting subproblem (Theorem 1),
suggesting that a small number of updates per user
is suﬃcient to reach reasonable optimization error be-
fore re-updating the models given the new graph. In
the supplementary, we further analyze the trade-oﬀ
between communication and memory costs and the
convergence rate ruled by κ.

Proposed regularizer.
In our experiments, we
use a graph regularizer deﬁned as g(w) = λ(cid:107)w(cid:107)2 −
1(cid:62) log(d(w) + δ), which is inspired from (Kalofolias,
2016). The log term ensures that all nodes have nonzero

3For the general convex case, we can obtain a slower

We denote by Lk,K is the Lipschitz constant of ∇h with

O(1/T ) convergence rate.

Valentina Zantedeschi, Aur´elien Bellet, Marc Tommasi

degrees (the small positive constant δ is a simple trick to
make the logarithm smooth on the feasible domain, see
e.g., (Koriche, 2018)) without ruling out non-connected
graphs with several connected components. Crucially,
λ > 0 provides a direct way to tune the sparsity of
the graph: the smaller λ, the more concentrated the
weights of a given user on the peers with the closest
models. This allows us to control the trade-oﬀ between
accuracy and communication in the model update step
of Section 4, whose communication cost is linear in the
number of edges. The resulting objective is strongly
convex and block-Lipschitz continuous (see supplemen-
tary for the derivation of the parameters and analysis
of the trade-oﬀs). Finally, as discussed in (Kalofolias,
2016), tuning the importance of the log-degree term
with respect to the other graph terms has simply a
scaling eﬀect, thus we can simply set µ2 = µ1 in (1).
Remark 2 (Reducing the number of variables). To
reduce the number of variables to optimize, each user
can keep to 0 the weights corresponding to users whose
current model is most diﬀerent to theirs. This heuristic
has a negligible impact on the solution quality in sparse
regimes (small λ).

6 EXPERIMENTS

In this section, we study the practical behavior of
our approach. Denoting our decentralized Adaboost
method introduced in Section 4 as Dada, we study
two variants: Dada-Oracle (which uses a ﬁxed oracle
graph given as input) and Dada-Learned (where the
graph is learned along with the models). We com-
pare against various competitors, which learn either
global or personalized models in a centralized or decen-
tralized manner. Global-boost and Global-lin learn a
single global l1-Adaboost model (resp. linear model)
over the centralized dataset S = ∪kSk. Local-boost
and Local-lin learn (Adaboost or linear) personalized
models independently for each user without collabo-
ration. Finally, Perso-lin is a decentralized method
for collaboratively learning personalized linear mod-
els (Vanhaesebrouck et al., 2017). This approach re-
quires an oracle graph as input (Perso-lin-Oracle) but
it can also directly beneﬁt from our graph learning
approach of Section 5 (we denote this new variant by
Perso-lin-Learned). We use the same set of base predic-
tors for all boosting-based methods, namely n simple
decision stumps uniformly split between all D dimen-
sions and value ranges. For all methods we tune the
hyper-parameters with 3-fold cross validation. Models
are initialized to zero vectors and the initial graphs
of Dada-Learned and Perso-lin-Learned are learned
using the purely local classiﬁers, and then updated
after every 100 iterations of optimizing the classiﬁers,
with κ = 5. All reported accuracies are averaged over

Table 1: Test accuracy (%) on real data, averaged over
3 runs. Best results in boldface, second best in italic.

DATASET

HARWS VEH. COMP.

SCH.

Global-linear
Local-linear
Perso-linear-Learned
Global-Adaboost
Local-Adaboost
Dada-Learned

93.64
92.69
96.87
94.34
93.16
95.57

87.11
90.38
91.45
88.02
90.59
91.04

62.18
60.68
69.10
69.16
66.61
73.55

57.06
70.43
71.78
69.96
70.69
72.47

users. Additional details and results can be found in
the supplementary. The source code is available at
https://github.com/vzantedeschi/Dada.

Figure 1: Results on the Moons dataset. Top: Training
and test accuracy w.r.t. iterations (we display the per-
formance of non-collaborative baselines at convergence
with a straight line). Global-lin is oﬀ limits at ∼50%
accuracy. Bottom: Average number of neighbors w.r.t.
iterations for Dada-Learned.

Synthetic data. To study the behavior of our ap-
proach in a controlled setting, our ﬁrst set of experi-
ments is carried out on a synthetic problem (Moons)
constructed from the classic two interleaving Moons
dataset which has nonlinear class boundaries. We con-
sider K = 100 users, clustered in 4 groups of 10, 20, 30
and 40 users. Users in the same cluster are associated
with a similar rotation of the feature space and hence
have similar tasks. We construct an oracle collabora-
tion graph based on the diﬀerence in rotation angles
between users, which is given as input to Dada-Oracle
and Perso-lin-Oracle. Each user k obtains a training
sample random size mk ∼ U(3, 15). The data dimen-
sion is D = 20 and the number of base predictors is
n = 200. We refer to the supplementary material for

Fully Decentralized Joint Learning

Table 2: Test accuracy (%) with diﬀerent ﬁxed communication budgets (# bits) on real datasets.

BUDGET MODEL

HARWS VEHICLE COMPUTER SCHOOL

DZ × 160

DZ × 500

DZ × 1000

Perso-lin-Learned
Dada-Learned
Perso-lin-Learned
Dada-Learned
Perso-lin-Learned
Dada-Learned

-
95.70
81.06
95.70
87.55
95.70

-
75.11
89.82
89.57
90.52
90.81

-
52.03
-
62.22
68.95
68.83

-
56.83
-
71.90
71.90
72.22

more details on the dataset generation. Figure 1 (left)
shows the accuracy of all methods. As expected, all lin-
ear models (including Perso-lin) perform poorly since
the tasks have highly nonlinear decision boundaries.
The results show the clear gain in accuracy provided by
our method: both Dada-Oracle and Dada-Learned are
successful in reducing the overﬁtting of Local-boost,
and also achieve higher test accuracy than Global-boost.
Dada-Oracle outperforms Dada-Learned as it makes
use of the oracle graph computed from the true data
distributions. Despite the noise introduced by the ﬁnite
sample setting, Dada-Learned eﬀectively makes up for
not having access to any knowledge on the relations be-
tween the users’ tasks. Figure 1 (right) shows that the
graph learned by Dada-Learned remains sparse across
time (in fact, always sparser than the oracle graph),
ensuring a small communication cost for the model
update steps. Figure 2 (left) conﬁrms that the graph
learned by Dada-Learned is able to approximately re-
cover the ground-truth cluster structure. Figure 2
(right) provides a more detailed visualization of the
learned graph. We can clearly see the eﬀect of the
inductive bias brought by the conﬁdence-weighted loss
term in Problem (1) discussed in Section 3. In partic-
ular, nodes with high conﬁdence and high loss values
tend to have small degrees while nodes with low conﬁ-
dence or low loss values are more densely connected.

Real data. We present results on real datasets that
are naturally collected at the user level: Human Activ-
ity Recognition With Smartphones (Harws, K = 30,
(Anguita et al., 2013), Vehicle Sen-
D = 561)
sor (Duarte and Hu, 2004) (K = 23, D = 100), Com-
puter Buyers (K = 190, D = 14) and School (Gold-
stein, 1991) (K = 140, D = 17). As shown in Table 1,
Dada-Learned and Perso-lin-Learned, which both make
use of our alternating procedure, achieve the best per-
formance. This demonstrates the wide applicability
of our graph learning approach, for it enables the use
of Perso-lin (Vanhaesebrouck et al., 2017) on datasets
where no prior information is available to build a pre-
deﬁned collaboration graph. Thanks to its logarithmic
communication, our approach Dada-Learned achieves
higher accuracy under limited communication budgets,
especially on higher-dimensional data (Table 2). More
details and results are given in the supplementary.

Figure 2: Graph learned on Moons. Top: Graph
weights for the oracle and learned graph (with users
grouped by cluster). Bottom: Visualization of the
graph. The node size is proportional to the conﬁdence
ck and the color reﬂects the relative value of the local
loss (greener = smaller loss). Nodes are labeled with
their rotation angle, and a darker edge color indicates
a higher weight.

7 FUTURE WORK

We plan to extend our approach to (functional) gra-
dient boosting (Friedman, 2001; Wang et al., 2015)
where the graph regularization term would need to
be applied to an inﬁnite set of base predictors. An-
other promising direction is to make our approach
diﬀerentially-private (Dwork, 2006) to formally guaran-
tee that personal datasets cannot be inferred from the
information sent by users. As our algorithm communi-
cates very scarcely, we think that the privacy/accuracy
trade-oﬀ may be better than the one known for linear
models (Bellet et al., 2018).

54474343345365394246125129139136152137125125146148153130149132141136144129147141315301327332335320310291325335329314309323318284313309324314308308313288318298321284323318232213230230226237214223203242236234231221222242238228226260217208226223225219210214219228236212210229217246226225223225Valentina Zantedeschi, Aur´elien Bellet, Marc Tommasi

Acknowledgments

Dwork, C. (2006). Diﬀerential Privacy.

In ICALP,

The authors would like to thank R´emi Gilleron for
his useful feedback. This research was partially sup-
ported by grants ANR-16-CE23-0016-01 and ANR-15-
CE23-0026-03, by the European Union’s Horizon 2020
Research and Innovation Program under Grant Agree-
ment No. 825081 COMPRISE and by a grant from
CPER Nord-Pas de Calais/FEDER DATA Advanced
data science and technologies 2015-2020.

References

Almeida, I. and Xavier, J. (2018). DJAM: Distributed
Jacobi Asynchronous Method for Learning Personal
Models. IEEE Signal Processing Letters, 25(9):1389–
1392.

Anguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes-
Ortiz, J. L. (2013). A public domain dataset for
human activity recognition using smartphones. In
ESANN.

Baytas, I. M., Yan, M., Jain, A. K., and Zhou, J. (2016).

Asynchronous Multi-task Learning. In ICDM.

Bellet, A., Guerraoui, R., Taziki, M., and Tommasi,
M. (2018). Personalized and Private Peer-to-Peer
Machine Learning. In AISTATS.

Berger, P., Buchacher, M., Hannak, G., and Matz, G.
(2018). Graph Learning Based on Total Variation
Minimization. In ICASSP.

Boyd, S. P., Ghosh, A., Prabhakar, B., and Shah, D.
(2006). Randomized gossip algorithms. IEEE Trans-
actions on Information Theory, 52(6):2508–2530.

Clarkson, K. L. (2010). Coresets, sparse greedy ap-
proximation, and the Frank-Wolfe algorithm. ACM
Transactions on Algorithms, 6(4):1–30.

Colin, I., Bellet, A., Salmon, J., and Cl´emen¸con, S.
(2016). Gossip dual averaging for decentralized opti-
mization of pairwise functions. In ICML.

Dhillon, P. S., Sellamanickam, S., and Selvaraj, S. K.
(2011). Semi-supervised multi-task learning of struc-
tured prediction models for web information extrac-
tion. In CIKM, pages 957–966.

Dong, X., Thanou, D., Frossard, P., and Van-
dergheynst, P. (2016). Learning Laplacian matrix in
smooth graph signal representations. IEEE Trans-
actions on Signal Processing, 64(23):6160–6173.

Duarte, M. F. and Hu, Y. H. (2004). Vehicle classi-
ﬁcation in distributed sensor networks. Journal of
Parallel and Distributed Computing, 64(7):826–838.

Duchi, J. C., Agarwal, A., and Wainwright, M. J.
(2012). Dual Averaging for Distributed Optimization:
Convergence Analysis and Network Scaling. IEEE
Transactions on Automatic Control, 57(3):592–606.

volume 2.

Evgeniou, T. and Pontil, M. (2004). Regularized multi-

task learning. In KDD.

Frank, M. and Wolfe, P. (1956). An algorithm for
quadratic programming. Naval Research Logistics
(NRL), 3:95–110.

Friedman, J. H. (2001). Greedy function approxima-
tion: A gradient boosting machine. The Annals of
Statistics, 29(5):1189–1232.

Goldstein, H. (1991). Multilevel modelling of survey
data. Journal of the Royal Statistical Society. Series
D (The Statistician), 40(2):235–244.

Jaggi, M. (2013). Revisiting Frank-Wolfe: Projection-

Free Sparse Convex Optimization. In ICML.

Jelasity, M., Voulgaris, S., Guerraoui, R., Kermarrec,
A.-M., and van Steen, M. (2007). Gossip-based peer
sampling. ACM Trans. Comput. Syst., 25(3).

Jiang, Z., Balu, A., Hegde, C., and Sarkar, S. (2017).
Collaborative Deep Learning in Fixed Topology Net-
works. In NIPS.

Kairouz, P., McMahan, H. B., Avent, B., Bellet, A.,
Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles,
Z., Cormode, G., Cummings, R., D’Oliveira, R. G. L.,
Rouayheb, S. E., Evans, D., Gardner, J., Garrett, Z.,
Gasc´on, A., Ghazi, B., Gibbons, P. B., Gruteser, M.,
Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson,
B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Kho-
dak, M., Koneˇcn´y, J., Korolova, A., Koushanfar, F.,
Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri,
M., Nock, R., ¨Ozg¨ur, A., Pagh, R., Raykova, M.,
Qi, H., Ramage, D., Raskar, R., Song, D., Song,
W., Stich, S. U., Sun, Z., Suresh, A. T., Tram`er,
F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z.,
Yang, Q., Yu, F. X., Yu, H., and Zhao, S. (2019).
Advances and Open Problems in Federated Learning.
Technical report, arXiv:1912.04977.

Kalofolias, V. (2016). How to learn a graph from

smooth signals. In AISTATS.

Kermarrec, A., Leroy, V., and Thraves, C. (2011).
Converging quickly to independent uniform random
topologies. In PDP.

Koneˇcn`y, J., McMahan, H. B., Yu, F. X., Richt´arik,
P., Suresh, A. T., and Bacon, D. (2016). Federated
learning: Strategies for improving communication
eﬃciency. arXiv preprint arXiv:1610.05492.

Koriche, F. (2018). Compiling Combinatorial Predic-

tion Games. In ICML.

Lacoste-Julien, S., Jaggi, M., Schmidt, M., and
Pletscher, P. (2013). Block-Coordinate Frank-Wolfe
Optimization for Structural SVMs. In ICML.

Fully Decentralized Joint Learning

Lafond, J., Wai, H.-T., and Moulines, E. (2016). D-FW:
Communication eﬃcient distributed algorithms for
high-dimensional sparse optimization. In ICASSP.

Wang, J., Kolar, M., and Srebro, N. (2016a). Dis-
tributed Multi-Task Learning with Shared Represen-
tation. arXiv preprint arXiv:1603.02185.

Wang, J., Kolar, M., and Srebro, N. (2016b). Dis-

tributed Multitask Learning. In AISTATS.

Wei, E. and Ozdaglar, A. E. (2012). Distributed Alter-
nating Direction Method of Multipliers. In CDC.

Wright, S. J. (2015). Coordinate descent algorithms.

Mathematical Programming, 151(1):3–34.

Zhang, Y. and Yang, Q. (2017). A survey on multi-task

learning. arXiv preprint arXiv:1707.08114.

Li, J., Arai, T., Baba, Y., Kashima, H., and Miwa, S.
(2017). Distributed Multi-task Learning for Sensor
Network. In ECML/PKDD.

Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang,
W., and Liu, J. (2017). Can Decentralized Algo-
rithms Outperform Centralized Algorithms? A Case
Study for Decentralized Parallel Stochastic Gradient
Descent. In NIPS.

Lian, X., Zhang, W., Zhang, C., and Liu, J. (2018).
Asynchronous Decentralized Parallel Stochastic Gra-
dient Descent. In ICML.

Maurer, A. (2006). The Rademacher Complexity of

Linear Transformation Classes. In COLT.

McMahan, H. B., Moore, E., Ramage, D., Hampson,
S., and Ag¨uera y Arcas, B. (2017). Communication-
eﬃcient learning of deep networks from decentralized
data. In AISTATS.

Razaviyayn, M., Hong, M., and Luo, Z.-Q. (2013).
A uniﬁed convergence analysis of block successive
minimization methods for nonsmooth optimization.
SIAM Journal on Optimization, 23(2):1126–1153.

Richt´arik, P. and Tak´ac, M. (2014). Iteration complex-
ity of randomized block-coordinate descent methods
for minimizing a composite function. Mathematical
Programming, 144(1-2):1–38.

Shen, C. and Li, H. (2010). On the dual formulation
of boosting algorithms. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 32(12):2216–
2231.

Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar,
A. S. (2017). Federated Multi-Task Learning. In
NIPS.

Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.
(2018). D2: Decentralized Training over Decentral-
ized Data. In ICML.

Tseng, P. (2001). Convergence of a block coordinate
descent method for nondiﬀerentiable minimization.
Journal of Optimization Theory and Applications,
109(3):475–494.

Tseng, P. and Yun, S. (2009). Block-coordinate gradient
descent method for linearly constrained nonsmooth
separable optimization. Journal of Optimization
Theory and Applications, 140(3):140–513.

Vanhaesebrouck, P., Bellet, A., and Tommasi, M.
(2017). Decentralized Collaborative Learning of Per-
sonalized Models over Networks. In AISTATS.

Wang, C., Wang, Y., Schapire, R., et al. (2015). Func-
tional Frank-Wolfe Boosting for General Loss Func-
tions. arXiv preprint arXiv:1510.02558.

