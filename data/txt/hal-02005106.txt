Speech enhancement with variational autoencoders and
alpha-stable distributions
Simon Leglaive, Umut Şimşekli, Antoine Liutkus, Laurent Girin, Radu Horaud

To cite this version:

Simon Leglaive, Umut Şimşekli, Antoine Liutkus, Laurent Girin, Radu Horaud. Speech enhancement
with variational autoencoders and alpha-stable distributions. ICASSP 2019 - 44th IEEE International
Conference on Acoustics, Speech and Signal Processing, May 2019, Brighton, United Kingdom. pp.541-
545, ￿10.1109/ICASSP.2019.8682546￿. ￿hal-02005106￿

HAL Id: hal-02005106

https://inria.hal.science/hal-02005106

Submitted on 8 Feb 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

SPEECH ENHANCEMENT WITH VARIATIONAL AUTOENCODERS
AND ALPHA-STABLE DISTRIBUTIONS

Simon Leglaive1, Umut ¸Sim¸sekli2, Antoine Liutkus3, Laurent Girin1,4, Radu Horaud1

1Inria Grenoble Rhône-Alpes, France,
3Inria and LIRMM, France,

2LTCI, Télécom ParisTech, Université Paris-Saclay, France

4Univ. Grenoble Alpes, Grenoble INP, GIPSA-lab, France

ABSTRACT

This paper focuses on single-channel semi-supervised speech en-
hancement. We learn a speaker-independent deep generative speech
model using the framework of variational autoencoders. The noise
model remains unsupervised because we do not assume prior knowl-
edge of the noisy recording environment. In this context, our con-
tribution is to propose a noise model based on alpha-stable distribu-
tions, instead of the more conventional Gaussian non-negative ma-
trix factorization approach found in previous studies. We develop a
Monte Carlo expectation-maximization algorithm for estimating the
model parameters at test time. Experimental results show the supe-
riority of the proposed approach both in terms of perceptual quality
and intelligibility of the enhanced speech signal.

Index Terms— Speech enhancement, variational autoencoders,

alpha-stable distribution, Monte Carlo expectation-maximization.

1. INTRODUCTION

Speech enhancement is one of the central problems in audio signal
processing [1]. The goal is to recover a clean speech signal after ob-
serving a noisy mixture.
In this work, we address single-channel
speech enhancement, which can be seen as an under-determined
source separation problem, where the sources are of different nature.
One popular statistical approach for source separation combines
a local Gaussian model of the time-frequency signal coefﬁcients
with a variance model [2]. In this framework, non-negative matrix
factorization (NMF) techniques have been used to model the time-
frequency-dependent signal variance [3, 4]. Recently, discriminative
approaches based on deep neural networks (DNNs) have also been
investigated for speech enhancement, with the aim of estimating ei-
ther clean spectrograms or time-frequency masks, given noisy spec-
trograms [5, 6, 7]. As a representative example, a DNN is used in
[6] to map noisy speech log-power spectrograms into clean speech
log-power spectrograms.

Even more recently, generative models based on deep learning,
and in particular variational autoencoders (VAEs) [8], have been
used for single-channel [9, 10] and multi-channel speech enhance-
ment [11, 12]. These generative model-based approaches provide
important advantages and justify the interest of semi-supervised
methods for speech enhancement.
Indeed, as shown in [9, 10],
fully-supervised methods such as [6] may have issues for gener-
alizing to unseen noise types. The method proposed in [10] was
shown to outperform both a semi-supervised NMF baseline and the
fully-supervised deep learning approach [6].

In most cases, probabilistic models for source separation or
speech enhancement rely on a Gaussianity assumption, which turns

This work is supported by the ERC Advanced Grant VHIA #340113.

out to be restrictive for audio signals [13]. As a result, heavy-
tailed distributions have started receiving increasing attention in the
In particular, α-stable
audio processing community [14, 15, 16].
distributions (cf. Section 3) are becoming popular heavy-tailed
models for audio modeling due to their nice theoretical properties
[17, 13, 14, 18, 19, 20, 21].

In this work, we investigate the combination of a deep learning-
based generative speech model with a heavy-tailed α-stable noise
model. The rationale for introducing a noise model based on heavy-
tailed distributions as opposed to a structured NMF approach as
in [10] is to avoid relying on restricting assumptions regarding sta-
tionarity or temporal redundancy of the noisy environment, that may
be violated in practice, leading to errors in the estimates. In addition,
we let the noise model remain unsupervised in order to avoid the
aforementioned generalization issues regarding the noisy recording
environment. We develop a Monte Carlo expectation-maximization
algorithm [22] for performing maximum likelihood estimation at test
time. Experiments performed under challenging conditions show
that the proposed approach outperforms the competing approaches
in terms of both perceptual quality and intelligibility.

2. SPEECH MODEL

We work in the short-term Fourier transform (STFT) domain where
B = {0, ..., F − 1}×{0, ..., N − 1} denotes the set of time-frequency
bins. For (f, n) ∈ B, f denotes the frequency index and n the time-
frame index. We use sf n, bf n, xf n ∈ C to denote the complex STFT
coefﬁcients of the speech, noise, and mixture signals, respectively.
As in [9, 10], independently for all (f, n) ∈ B, we consider the
following generative speech model involving a latent random vector
hn ∈ RL, with L ≪ F :

hn ∼ N (0, I);
sf n ∣ hn ∼ Nc(0, σ2

s,f (hn)),

(1)

(2)

where N (x; µ, Σ) denotes the multivariate Gaussian distribution
for a real-valued random vector, I is the identity matrix of appro-
priate size, and Nc(x; µ, σ2
) denotes the univariate complex proper
Gaussian distribution. As represented in Fig. 1a, {σ2
↦
R+}
F −1
f =0 is a set of non-linear functions corresponding to a neural
network which takes as input hn ∈ RL. This variance term can be
understood as a model for the short-term power spectral density of
speech [23]. We denote by θs the parameters of this generative neu-
ral network.

s,f ∶ RL

An important contribution of VAEs [8] is to provide an efﬁcient
way of learning the parameters of such a generative model. Let
s = {sn ∈ CF
N −1
n=0 be a training dataset of clean-speech STFT time
}
frames and h = {hn ∈ RL
N −1
n=0 the set of associated latent random
}

(a) Generative network.

(b) Recognition network.

Fig. 1: Generative and recognition networks. Beside each layer is
indicated its size and the activation function.

vectors. Taking ideas from variational inference, VAEs estimate the
parameters θs by maximizing a lower bound of the log-likelihood
ln p(s; θs) deﬁned by:
L (θs, ψ)=Eq(h∣s;ψ) [ln p (s ∣ h; θs)]−DKL (q (h ∣ s; ψ) ∥ p(h)) ,
(3)
where q (h ∣ s; ψ) denotes an approximation of the intractable true
posterior distribution p(h ∣ s; θs), and DKL(q ∥ p) = Eq[ln(q/p)]
is the Kullback-Leibler divergence. Independently for all the dimen-
sions l ∈ {0, ..., L − 1} and all the time frames n ∈ {0, ..., N − 1},
q(h ∣ s; ψ) is deﬁned by:

hl,n ∣ sn ∼ N (˜µl (∣sn∣

⊙2

) , ˜σ2

l (∣sn∣

⊙2

)) ,

(4)

⊙⋅ denotes element-wise exponentia-
where hl,n = (hn)l and (⋅)
+ ↦ R}
tion. As represented in Figure 1b, {˜µl ∶ RF
l=0 and {˜σ2
∶
RF
+ ↦ R+}
L−1
l=0 are non-linear functions corresponding to a neural
network which takes as input the speech power spectrum at a given
time frame. ψ denotes the parameters of this recognition network,
which also have to be estimated by maximizing the variational lower
bound deﬁned in (3). Using (1), (2) and (4) we can develop this ob-
jective function as follows:

L−1

l

L (θs, ψ)

c
= −

F −1
∑
f =0

N −1
∑
n=0

Eq(hn∣sn;ψ) [dIS (∣sf n∣

2 ; σ2

f (hn))]

1
2

L−1
∑
l=0

N −1
∑
n=0

+

[ln ˜σ2

l (∣sn∣

⊙2

) − ˜µl (∣sn∣

⊙2

2
)

− ˜σ2

l (∣sn∣

⊙2

)] ,

(5)

where dIS(x; y) = x/y − ln(x/y) − 1 is the Itakura-Saito divergence.
Finally, using the so-called “reparametrization trick” [8] to approxi-
mate the intractable expectation in (5), we obtain an objective func-
tion which is differentiable with respect to both θs and ψ and can be
optimized using gradient-ascent-based algorithms. It is important to
note that the only reason why the recognition network is introduced
is to learn the parameters of the generative network.

3. NOISE AND MIXTURE MODELS

In the previous section we have seen how to learn the parameters
of the generative model (1)-(2). This model can then be used as a
speech signal probabilistic prior for a variety of applications. In this
paper we are interested in single-channel speech enhancement.

We do not assume prior knowledge about the recording environ-
ment, so that the noise model remains unsupervised. Independently
for all (f, n) ∈ B, the STFT coefﬁcients of noise are modeled as
complex circularly symmetric α-stable random variables [24]:

bf n ∼ SαS(σb,f ),
(6)
where α ∈]0, 2] is the characteristic exponent and σb,f ∈ R+ is
the scale parameter. As proposed in [20] for multichannel speech

enhancement, this scale parameter is only frequency-dependent, it
does not depend on time. For algorithmic purposes, the noise model
(6) can be conveniently rewritten in an equivalent scale mixture of
Gaussians form [25], by making use of the product property of the
symmetric α-stable distribution [24]:

φf n ∼ P

α
2 S(2 cos(πα/4)

2/α

);

bf n ∣ φf n ∼ Nc(0, φf nσ2

b,f ),

(7)

(8)

where φf n ∈ R+ is called the impulse variable. It locally modu-
lates the variance of the conditional distribution of bf n given in (8).
α
2 S denotes a positive stable distribution of characteristic expo-
P
nent α/2. It corresponds to a right-skewed heavy-tailed distribution
deﬁned for non-negative random variables [26]. These impulse vari-
ables can be understood as carrying uncertainty about the stationary
noise assumption made in the marginal model (6), where the scale
parameter does not depend on the time-frame index.

The observed mixture signal is modeled as follows for all

(f, n) ∈ B:

√gnsf n + bf n,

xf n =

(9)
where gn ∈ R+ represents a frame-dependent but frequency-
independent gain. The importance of this parameter was experi-
mentally shown in [10]. We further consider the conditional inde-
pendence of the speech and noise STFT coefﬁcients so that:

xf n ∣ hn, φf n ∼ Nc (0, gnσ2

s,f (hn) + φf nσ2

b,f ) .

(10)

4. INFERENCE

n=0 , σ2
N −1

b = {σ2

b,f ∈ R+}

Let θu = {g = {gn ∈ R+}
F −1
f =0 } be the set
of model parameters to be estimated. For maximum likelihood
estimation, in this section we develop a Monte-Carlo expectation
maximization (MCEM) algorithm [22], which iteratively applies
the so-called E- and M-steps until convergence, which we detail
below. Remember that the speech generative model parameters
θs have been learned during a training phase (see Section 2).
We denote by x = {xf n}(f,n)∈B the set of observed data while
F −1
z = {hn, φn = {φf n}
is the set of latent variables. We will
f =0 }
F −1
also use xn = {xf n}
f =0 and zn = {hn, φn} to respectively denote
the set of observed and latent variables at a given time frame n.

N −1

n=0

Let θ⋆
Monte Carlo E-Step.
u be the current (or the ini-
tial) value of the model parameters. At the E-step of a standard
expectation-maximization algorithm, we would compute the fol-
lowing conditional expectation of the complete-data log-likelihood
Q(θu; θ⋆
this
expectation cannot be here computed analytically. We therefore
approximate Q(θu; θ⋆

u) [ln p(x, z; θs, θu)]. However,

u) using an empirical average:

u) = Ep(z∣x;θs,θ⋆

˜Q(θu; θ⋆
u)

c
= −

1
R

R
∑
r=1

∑
(f,n)∈B

[ ln (gnσ2

s,f (h(r)

n ) + φ(r)

f n σ2

b,f )

2
+ ∣xf n∣

(gnσ2

s,f (h(r)

n ) + φ(r)

f n σ2

b,f )

−1

],

(11)

F −1

n , φ(r)

where c
= denotes equality up to an additive constant, and z(r)
=
{h(r)
n = {φ(r)
f =0 }, r ∈ {1, ..., R}, is a sample drawn from
f n }
the posterior p(zn ∣ xn; θs, θ⋆
u) using a Markov Chain Monte Carlo
(MCMC) method. This approach forms the basis of the MCEM al-
gorithm [22]. Note that unlike the standard EM algorithm, it does

n

not ensure an improvement in the likelihood at each iteration. Nev-
ertheless, some convergence results in terms of stationary points of
the likelihood can be obtained under suitable conditions [27].

n

f n ∼ p(φf n ∣ xf n, h(m)

In this work we use a (block) Gibbs sampling algorithm [28].
From an initialization z(0)
n , it consists in iteratively sampling from
the so-called full conditionals. More precisely, at the m-th itera-
tion of the algorithm and independently for all n ∈ {0, ..., N − 1},
we ﬁrst sample h(m)
∼ p(hn ∣ xn, φ(m−1)
u). Then, we
sample φ(m)
u) independently for all
f ∈ {0, ..., F − 1}. Those two full conditionals are unfortunately an-
alytically intractable, but we can use one iteration of the Metropolis-
Hastings algorithm to sample from them. This approach corresponds
to the Metropolis-within-Gibbs sampling algorithm [28, p. 393].
One iteration of this method is detailed in Algorithm 1. The pro-
posal distributions for hn and φf n are respectively given in lines 2
and 6. The two acceptance probabilities required in lines 3 and 7 are
computed as follows:

n ; θs, θ⋆

; θs, θ⋆

n

α(h)

n = min

p (

˜hn)

p (h(m−1)

n

)

⎛
⎜
⎜
1,
⎜
⎜
⎝

F −1
∏
f =0

F −1
∏
f =0

p (xf n ∣

˜hn, φ(m−1)
f n

; θs, θ⋆

u)

p (xf n ∣ h(m−1)

n

, φ(m−1)
f n

; θs, θ⋆

;

⎞
⎟
⎟
⎟
⎟
u)
⎠
(12)

n = min ⎛
α(φ)
⎝

1,

p(xf n ∣ h(m)
p(xf n ∣ h(m)

n , ˜φf n; θs, θ⋆
u)
n , φ(m−1)
; θs, θ⋆
f n

u)

.

⎞
⎠

(13)

The two distributions involved in the computation of those accep-
tance probabilities are deﬁned in (1) and (10). Finally, we only keep
the last R samples for computing ˜Q(θu; θ⋆
u), i.e. we discard the
samples drawn during a so-called burn-in period.

˜Q(θu; θ⋆

At the M-step we want to minimize −

M-Step.
u) with
respect to θu. Let C(θu) denote the cost function associated with
this non-convex optimization problem. Similar to [29], we adopt a
majorization-minimization approach. Let us introduce the two fol-
lowing sets of auxiliary variables c = {c(r)
f n ∈ R+}r,f,n and λ =
{λ(r)
k,f n ∈ R+}k,r,f,n. We can show using standard concave/convex
inequalities (see e.g. [29]) that C(θu) ≤ G(θu, c, λ), where:

G(θu, c, λ) =

1
R

+

ln (c(r)
f n )

∑
(f,n)∈B

⎡
⎢
⎢
⎢
⎢
⎣
s,f (h(r)
(gnσ2

R
∑
r=1
1
c(r)
f n

n ) + φ(r)

f n σ2

b,f − c(r)
f n )

+ ∣xf n∣

2⎛
⎝

2

(λ(r)
1,f n)
s,f (h(r)
n )

gnσ2

2
(λ(r)
2,f n)
φ(r)
f n σ2
b,f

+

.

⎞
⎠

⎤
⎥
⎥
⎥
⎥
⎦

(14)

Moreover, this upper bound is tight, i.e. C(θu) = G(θu, c, λ), for

c(r)
f n = gnσ2

s,f (h(r)

n ) + φ(r)

f n σ2

b,f ;

λ(r)
1,f n = gnσ2

s,f (h(r)

n ) (gnσ2

s,f (h(r)

n ) + φ(r)

f n σ2

b,f )

−1

(15)

;

(16)

2,f n = φ(r)
λ(r)

f n σ2

b,f (gnσ2

s,f (h(r)

n ) + φ(r)

f n σ2

b,f )

−1

.

(17)

Minimizing G(θu, c, λ) with respect to the model parameters θu is
a convex optimization problem. By zeroing the partial derivatives of
G with respect to each scalar in θu we obtain update rules that de-
pend on the auxiliary variables. We then replace these auxiliary vari-
ables with the formulas given in (15)-(17), which makes the upper

Algorithm 1 m-th iteration of the Metropolis-within-Gibbs sam-
pling algorithm

1: independently for all n ∈ {0, ..., N − 1} do
2:

Sample ˜hn ∼ N (h(m−1)
n
Compute acceptance probability α(h)
n > u(h)

, (cid:15)2I)

Set h(m)

n

=

if α(h)
otherwise

⎧⎪⎪
˜hn
⎨
h(m−1)
⎪⎪⎩
n

n (equation (12))
n ∼ U([0, 1])

independently for all f ∈ {0, ..., F − 1} do
α
2 S(2 cos(πα/4)
)

Sample ˜φf n ∼ P
Compute acceptance probability α(φ)
n > u(φ)

2/α

Set φ(m)

f n =

if α(φ)
otherwise

⎧⎪⎪
˜φf n
⎨
φ(m−1)
⎪⎪⎩
f n

n (equation (13))
n ∼ U([0, 1])

3:

4:

5:

6:

7:

8:

end for

9:
10: end for

bound G tight. This procedure ensures that the cost C(θu) decreases
[30]. The resulting ﬁnal updates are given as follows:

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
F −1
∑
f =0

b,f ← σ2
σ2

b,f

gn ← gn

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣

N −1
∑
n=0

2

∣xf n∣

R
∑
r=1

f n (v(r)
φ(r)

x,f n)

−2

N −1
∑
n=0

R
∑
r=1

f n (v(r)
φ(r)

x,f n)

−1

2
∣xf n∣

R
∑
r=1

s,f (h(r)
σ2

n ) (v(r)

x,f n)

F −1
∑
f =0

R
∑
r=1

s,f (h(r)
σ2

n ) (v(r)

x,f n)

−1

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

1/2

;

−2

(18)

1/2

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

,

(19)

where we introduced v(r)
b,f in order to
ease the notations. Non-negativity is ensured provided that the pa-
rameters are initialized with non-negative values.

x,f n = gnσ2

n ) + φ(r)

s,f (h(r)

f n σ2

Speech reconstruction. Once the unsupervised model parameters
θu are estimated with the MCEM algorithm, we need to estimate
√gnsf n be the
the clean speech signal. For all (f, n) ∈ B, let ˜sf n =
scaled version of the speech STFT coefﬁcients. We estimate these
variables according to their posterior mean, given by:

ˆ˜sf n = Ep(˜sf n∣xf n;θu,θs)[˜sf n]

= Ep(zn∣xn;θu,θs) [Ep(˜sf n∣zn,xn;θu,θs)[˜sf n]]

= Ep(zn∣xn;θu,θs) [

gnσ2
s,f (hn)
s,f (hn) + φf nσ2

b,f

gnσ2

] xf n.

(20)

As before, this expectation cannot be computed analytically so it
is approximated using the Metropolis-within-Gibbs sampling algo-
rithm detailed in Algorithm 1. This estimate corresponds to a proba-
bilistic Wiener ﬁltering averaged over all possible realizations of the
latent variables according to their posterior distribution.

5. EXPERIMENTS

Reference method: The proposed approach is compared with the re-
cent method [10]. The speech signal in this paper is modeled in the
exact same manner as in the current work, only the noise model dif-
fers. This latter is a Gaussian model with an NMF parametrization
of the variance [3]: bf n ∼ Nc (0, (WbHb)f,n), where Wb ∈ RF ×K
and Hb ∈ RK×N
. It is also unsupervised in the sense that both Wb

+

+

Fig. 2: Speech enhancement results obtained with the proposed method as a function of the characteristic exponent α in the noise model (6).
α = 2.0 actually corresponds to α = 1.999. The value of the median is indicated within each boxplot.

these last layers is therefore real-valued, which is the reason why we
consider logarithm of variances. For learning the parameters θs and
φ, we use the Adam optimizer [33] with a step size of 10−3, expo-
nential decay rates for the ﬁrst and second moment estimates of 0.9
and 0.999 respectively, and an epsilon of 10−7 for preventing divi-
sion by zero. 20% of the TIMIT training set is kept as a validation
set, and early stopping with a patience of 10 epochs is used. Weights
are initialized using the uniform initializer described in [34].

Results: The estimated speech signal quality is evaluated in
terms of standard energy ratios expressed in decibels (dBs) [35]: the
signal-to-distortion (SDR), signal-to-interference (SIR) and signal-
to-artifact (SAR) ratios. We also consider the perceptual evaluation
of speech quality (PESQ) [36] measure (in [−0.5, 4.5]), and the
short-time objective intelligibility (STOI) measure [37] (in [0, 1]).
For all measures, the higher the better. We ﬁrst study the per-
formance of the proposed method according to the choice of the
characteristic exponent α in the noise model (6). Results presented
in Fig. 2 indicate that according to the PESQ and STOI measures, the
best performance is obtained for α = 2 (Gaussian case).1 The SDR
indicates that we should choose α = 1.8. Indeed, for greater values
of α, the SIR starts to decrease (∼1dB difference between α = 1.8
and α = 2) while the SAR remains stable (results are not shown
here due to space constraints). Therefore, in Fig. 3 we compare the
results obtained with the reference [10] and the proposed method
using α = 1.8. With the proposed method, the estimated speech
signal contains more interferences (SIR is lower) but less artifacts
(SAR is higher). According to the SDR both methods are equivalent.
But for intelligibility and perceptual quality, artifacts are actually
more disturbing than interferences [38], which is the reason why the
proposed method obtains better results in terms of both STOI and
PESQ measures. For reproducibility, a Python implementation of
our algorithm and audio examples are available online.2

6. CONCLUSION

In this work, we proposed a speech enhancement method exploiting
a speech model based on VAEs and a noise model based on alpha-
stable distributions. At the expense of more interferences, the pro-
posed α-stable noise model reduces the amount of artifacts in the
estimated speech signal, compared to the use of a Gaussian NMF-
based noise model as in [10]. Overall, it is shown that the proposed
approach improves the intelligibility and perceptual quality of the
enhanced speech signal. Future works include extending the pro-
posed approach to a multi-microphone setting using multivariate α-
stable distributions [18, 20].

1Actually α = 1.999 because for α = 2 the positive α-stable distribution in
(6) is degenerate.
2https://team.inria.fr/perception/icassp2019-asvae/

Fig. 3: Results obtained with the reference and the proposed meth-
ods (using α = 1.8). The median is indicated within each boxplot.

and Hb are estimated from the noisy mixture signal. This method
also relies on an MCEM algorithm. By comparing the proposed
method with [10], we fairly investigate which noise model leads to
the best speech enhancement results. Note that the method proposed
in [10] was shown to outperform both a semi-supervised NMF base-
line and the fully-supervised deep learning approach [6], the latter
having difﬁculties for generalizing to unseen noise types. We do not
include here the results obtained with these two other methods.

Database: The supervised speech model parameters in the pro-
posed and the reference methods are learned from the training set
of the TIMIT database [31]. It contains almost 4 hours of 16-kHz
speech signals, distributed over 462 speakers. For the evaluation
of the speech enhancement algorithms, we mixed clean speech sig-
nals from the TIMIT test set and noise signals from the DEMAND
database [32], corresponding to various noisy environments: domes-
tic, nature, ofﬁce, indoor public spaces, street and transportation. We
created 168 mixtures at a 0 dB signal-to-noise ratio (one mixture per
speaker in the TIMIT test set). Note that both speakers and sentences
are different than in the training set.

Parameter setting: The STFT is computed using a 64-ms sine
window (i.e. F = 513) with 75%-overlap. Based on [10], the latent
dimension in the speech generative model (1)-(2) is ﬁxed to L = 64.
In this reference method, the NMF rank of the noise model is ﬁxed
to K = 10. The NMF parameters are randomly initialized. For both
the proposed and the reference method, the gain gn is initialized
to one for all time frames n. For the proposed method, the noise
scale parameter σb,f is also initialized to one for all frequency bins.
We run 200 iterations of the MCEM algorithm. At each Monte-
Carlo E-Step, we run 40 iterations of the Metropolis-within-Gibbs
algorithm and we discard the ﬁrst 30 samples as the burn-in period.
The parameter (cid:15)2 in line 2 of Algorithm 1 is set to 0.01.

Neural network: The structure of the generative and recognition
networks is the same as in [10] and is represented in Fig. 1. Hid-
den layers use hyperbolic tangent (tanh(⋅)) activation functions and
output layers use identity activation functions (Id(⋅)). The output of

1.01.11.21.31.41.51.61.71.81.92.005101520253035SDRin dB9.510.310.811.211.812.113.413.514.214.013.51.01.11.21.31.41.51.61.71.81.92.01.01.52.02.53.03.54.0PESQ in [-0.5, 4.5]2.342.362.402.472.582.632.672.722.772.772.801.01.11.21.31.41.51.61.71.81.92.00.50.60.70.80.91.0STOI in [0, 1]0.850.850.860.870.870.890.890.900.900.910.91051015202530SDR in dB14.214.2010203040SIR in dB19.718.11015202530SAR in dB15.917.21.52.02.53.03.54.0PESQ in [-0.5, 4.5]2.652.770.50.60.70.80.91.0STOI in [0, 1]0.870.907. REFERENCES

[1] P. C. Loizou, Speech enhancement: theory and practice, CRC press,

2007.

[20] M. Fontaine, F.-R. Stöter, A. Liutkus, U. ¸Sim¸sekli, R. Serizel, and R.
Badeau, “Multichannel Audio Modeling with Elliptically Stable Ten-
sor Decomposition,” in Proc. Int. Conf. Latent Variable Analysis and
Signal Separation (LVA/ICA), 2018.

[2] E. Vincent, M. G. Jafari, S. A. Abdallah, M. D. Plumbley, and M. E.
Davies, “Probabilistic modeling paradigms for audio source separa-
tion,” in Machine Audition: Principles, Algorithms and Systems, W.
Wang, Ed., pp. 162–185. IGI Global, 2010.

[21] U. ¸Sim¸sekli, H. Erdo˘gan, S. Leglaive, A. Liutkus, R. Badeau, and
G. Richard, “Alpha-stable low-rank plus residual decomposition for
speech enhancement,” in Proc. IEEE Int. Conf. Acoust., Speech, Sig-
nal Process. (ICASSP), 2018.

[3] C. Févotte, N. Bertin, and J.-L. Durrieu, “Nonnegative matrix factor-
ization with the Itakura-Saito divergence: With application to music
analysis,” Neural computation, vol. 21, no. 3, pp. 793–830, 2009.
[4] N. Mohammadiha, P. Smaragdis, and A. Leijon, “Supervised and unsu-
pervised speech enhancement using nonnegative matrix factorization,”
IEEE Trans. Audio, Speech, Language Process., vol. 21, no. 10, pp.
2140–2151, 2013.

[5] D. Wang and J. Chen, “Supervised speech separation based on deep
learning: An overview,” IEEE Trans. Audio, Speech, Language Pro-
cess., vol. 26, no. 10, pp. 1702–1726, 2018.

[6] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “A regression approach to
speech enhancement based on deep neural networks,” IEEE Trans.
Audio, Speech, Language Process., vol. 23, no. 1, pp. 7–19, 2015.
[7] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le Roux, J. R.
Hershey, and B. Schuller, “Speech enhancement with LSTM recurrent
neural networks and its application to noise-robust ASR,” in Proc.
Int. Conf. Latent Variable Analysis and Signal Separation (LVA/ICA),
2015, pp. 91–99.

[8] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” in

Proc. Int. Conf. Learning Representations (ICLR), 2014.

[9] Y. Bando, M. Mimura, K. Itoyama, K. Yoshii, and T. Kawahara, “Sta-
tistical speech enhancement based on probabilistic integration of vari-
ational autoencoder and non-negative matrix factorization,” in Proc.
IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), 2018, pp.
716–720.

[10] S. Leglaive, L. Girin, and R. Horaud, “A variance modeling framework
based on variational autoencoders for speech enhancement,” Proc.
IEEE Int. Workshop Machine Learning Signal Process. (MLSP), 2018.

[11] K. Sekiguchi, Y. Bando, K. Yoshii, and T. Kawahara, “Bayesian multi-
channel speech enhancement with a deep speech prior,” in Proc. Asia-
Paciﬁc Signal and Information Processing Association Annual Summit
and Conference (APSIPA ASC), 2018, pp. 1233–1239.

[12] S. Leglaive, L. Girin, and R. Horaud, “Semi-supervised multichannel
speech enhancement with variational autoencoders and non-negative
matrix factorization,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal
Process. (ICASSP), 2019.

[13] A. Liutkus and R. Badeau, “Generalized Wiener ﬁltering with frac-
tional power spectrograms,” in Proc. IEEE Int. Conf. Acoust., Speech,
Signal Process. (ICASSP), 2015, pp. 266–270.

[14] U. ¸Sim¸sekli, A. Liutkus, and A. T. Cemgil, “Alpha-stable matrix fac-
torization,” IEEE Signal Process. Lett., vol. 22, no. 12, pp. 2289–2293,
2015.

[15] A. Liutkus, D. Fitzgerald, and R. Badeau, “Cauchy nonnegative ma-
trix factorization,” in Proc. IEEE Workshop Applicat. Signal Process.
Audio Acoust. (WASPAA), New Paltz, NY, USA, 2015, pp. 1–5.
[16] K. Yoshii, K. Itoyama, and M. Goto, “Student’s t nonnegative matrix
factorization and positive semideﬁnite tensor factorization for single-
channel audio source separation,” in Proc. IEEE Int. Conf. Acoust.,
Speech, Signal Process. (ICASSP), 2016, pp. 51–55.

[17] E. E. Kuruoglu, Signal processing in alpha-stable noise environments:
a least lp-norm approach, Ph.D. thesis, University of Cambridge,
1999.

[18] S. Leglaive, U. ¸Sim¸sekli, A. Liutkus, R. Badeau, and G. Richard,
“Alpha-stable multichannel audio source separation,” in Proc. IEEE
Int. Conf. Acoust., Speech, Signal Process. (ICASSP), 2017, pp. 576–
580.

[19] M. Fontaine, A. Liutkus, L. Girin, and R. Badeau, “Explaining the pa-
rameterized Wiener ﬁlter with alpha-stable processes,” in Proc. IEEE
Workshop Applicat. Signal Process. Audio Acoust. (WASPAA), 2017.

[22] G. C. Wei and M. A. Tanner,

“A Monte Carlo implementation of
the EM algorithm and the poor man’s data augmentation algorithms,”
Journal of the American statistical Association, vol. 85, no. 411, pp.
699–704, 1990.

[23] A. Liutkus, R. Badeau, and G. Richard, “Gaussian processes for un-
derdetermined source separation,” IEEE Trans. Signal Process., vol.
59, no. 7, pp. 3155–3167, 2011.

[24] G. Samorodnitsky and M. S. Taqqu, Stable non-Gaussian random pro-
cesses: stochastic models with inﬁnite variance, vol. 1, CRC press,
1994.

[25] D. F. Andrews and C. L. Mallows, “Scale mixtures of normal distribu-
tions,” Journal of the Royal Statistical Society. Series B (Methodolog-
ical), vol. 36, no. 1, pp. 99–102, 1974.

[26] P. Magron, R. Badeau, and A. Liutkus, “Lévy NMF for robust non-
negative source separation,” in Proc. IEEE Workshop Applicat. Sig-
nal Process. Audio Acoust. (WASPAA), New Paltz, NY, United States,
2017, pp. 259–263.

[27] K. Chan and J. Ledolter, “Monte Carlo EM estimation for time series
models involving counts,” Journal of the American Statistical Associ-
ation, vol. 90, no. 429, pp. 242–252, 1995.

[28] C. P. Robert and G. Casella, Monte Carlo Statistical Methods,

Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.

[29] C. Févotte and J. Idier, “Algorithms for nonnegative matrix factoriza-
tion with the β-divergence,” Neural computation, vol. 23, no. 9, pp.
2421–2456, 2011.

[30] D. R. Hunter and K. Lange, “A tutorial on MM algorithms,” The

American Statistician, vol. 58, no. 1, pp. 30–37, 2004.

[31] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett,
N. L. Dahlgren, and V. Zue, “TIMIT acoustic phonetic continuous
speech corpus,” in Linguistic data consortium, 1993.

[32] J. Thiemann, N. Ito, and E. Vincent,

“The Diverse Environments
Multi-channel Acoustic Noise Database (DEMAND): A database of
multichannel environmental noise recordings,” in Proc. Int. Cong. on
Acoust., 2013.

[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” in Proc. Int. Conf. Learning Representations (ICLR), 2015.

[34] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep
feedforward neural networks,” in Proc. Int. Conf. Artif. Intelligence
and Stat., 2010, pp. 249–256.

[35] E. Vincent, R. Gribonval, and C. Févotte, “Performance measurement
in blind audio source separation,” IEEE Trans. Audio, Speech, Lan-
guage Process., vol. 14, no. 4, pp. 1462–1469, 2006.

[36] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual
evaluation of speech quality (PESQ)-a new method for speech quality
assessment of telephone networks and codecs,” in Proc. IEEE Int.
Conf. Acoust., Speech, Signal Process. (ICASSP), 2001, pp. 749–752.

[37] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “An algorithm
for intelligibility prediction of time–frequency weighted noisy speech,”
IEEE Trans. Audio, Speech, Language Process., vol. 19, no. 7, pp.
2125–2136, 2011.

[38] S. Venkataramani, R. Higa, and P. Smaragdis, “Performance based
cost functions for end-to-end speech separation,” in Proc. Asia-Paciﬁc
Signal and Information Processing Association Annual Summit and
Conference (APSIPA ASC), 2018, pp. 350–355.

