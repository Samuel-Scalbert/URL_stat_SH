Compressed k-Nearest Neighbors Ensembles for
Evolving Data Streams
Maroua Bahri, Silviu Maniu, Albert Bifet, Rodrigo Fernandes de Mello,

Nikolaos Tziortziotis

To cite this version:

Maroua Bahri, Silviu Maniu, Albert Bifet, Rodrigo Fernandes de Mello, Nikolaos Tziortziotis. Com-
pressed k-Nearest Neighbors Ensembles for Evolving Data Streams. ECAI 2020 - 24th European
Conference on Artificial Intelligence, Aug 2020, Santiago de Compostella / Virtual, Spain.
￿hal-
03189997￿

HAL Id: hal-03189997

https://hal.science/hal-03189997

Submitted on 5 Apr 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Compressed k-Nearest Neighbors Ensembles for Evolving
Data Streams

Maroua Bahri1 and Albert Bifet1 2 and Silviu Maniu3 4 5 and Rodrigo F. de Mello6 and Nikolaos Tziortziotis7

Abstract. The unbounded and multidimensional nature, the evolu-
tion of data distributions with time, and the requirement of single-
pass algorithms comprise the main challenges of data stream classi-
ﬁcation, which makes it impossible to infer learning models in the
same manner as for batch scenarios. Data dimensionality reduction
arises as a key factor to transform and select only the most relevant
features from those streams in order to reduce algorithm space and
time demands. In that context, Compressed Sensing (CS) encodes
an input signal into lower-dimensional space, guaranteeing its recon-
struction up to some distortion factor (cid:15). This paper employs CS on
data streams as a pre-processing step to support a k-Nearest Neigh-
bors (kNN) classiﬁcation algorithm, one of the most often used al-
gorithms in the data stream mining area – all this while ensuring
the key properties of CS hold. Based on topological properties, we
show that our classiﬁcation algorithm also preserves the neighbor-
hood (withing an (cid:15) factor) of kNN after reducing the stream dimen-
sionality with CS. As a consequence, end-users can set an acceptable
error margin while performing such projections for kNN. For further
improvements, we incorporate this method into an ensemble classi-
ﬁer, Leveraging Bagging, by combining a set of different CS matri-
ces which increases the diversity inside the ensemble. An extensive
set of experiments is performed on various datasets, and the results
were compared against those yielded by current state-of-the-art ap-
proaches, conﬁrming the good performance of our approaches.

1

Introduction

Data streams are unbounded sequences of multidimensional obser-
vations made available along time, hence, it is impossible to main-
tain and process them using the main memory. In practice, mining
tasks reduce time and space requirements while only processing rel-
evant features out of those redundant streams, what corresponds to
data summaries generally obtained as follows: (i) either by selecting
only a subsample (sampling) of the input data along time; or (ii) by
reducing/selecting data attributes via dimensionality reduction tech-
niques, what turns out to work along features. Naturally, the choice
of the technique depends on the problem being solved [13]. The di-
mensionality reduction process inherently arises when one deals with
a large number of attributes, especially when data are sparse. In this

1 T´el´ecom Paris,

IP-Paris, Paris, France,

email: {maroua.bahri,

albert.bifet}@telecom-paris.fr

2 University of Waikato, Hamilton, New Zealand
3 Universit´e Paris-Saclay, LRI, CNRS, Orsay, France, email:

sil-

viu.maniu@lri.fr
4 Inria, Paris, France
5 ENS DI, CNRS, ´Ecole Normale Sup´erieure, Universit´e PSL, Paris, France
6 Universidade de S˜ao Paulo, Brazil, email: mello@icmc.usp.br
7 Tradelab, France, email: ntziorzi@gmail.com

scenario, this is achieved by selecting only the most relevant features,
or by transforming them into a smaller set.

A common taxonomy organizes those approaches as follows: (i)
Feature selection, which consists in selecting a subset of the input
features, i.e., the most relevant, non-redundant, without operating
any sort of data transformation [5, 27]; and (ii) Feature extraction,
which consists in transforming the input attributes into a new set
of features in some lower dimensional space [26]. The main draw-
back of feature selection is that the simple feature removal could
lead to data loss, when one inadvertently performs such selection that
is even more challenging when dealing with potentially inﬁnite and
high-dimensional data streams that boost the use of computational
resources, most precisely the memory and the processing time. In
order to address this limitation, we apply Compressed Sensing (CS)
(also referred to as Compressed Sampling) which is a feature ex-
traction strategy that provides theoretical lower and upper bounds
on pairwise data transformations. This data reduction is highly rele-
vant in the context of data streams mining since it helps to diminish
resource demands while ensuring the quality of learning (e.g. classi-
ﬁcation accuracy), addressing the evolving data streams framework
requirements and avoiding more than a single data pass. This last
issue would lead an algorithm either to lose information from next
observations, given some sub-sampling along time, or to have an ex-
ponential amount of main memory available to save the stream, thus
making sure that all observations were processed properly.

CS is a technique that has attracted a lot of attention and is based
on the concept that a data compression method has to deal with re-
dundancy while transforming and reconstructing data [15]. The ba-
sic idea is to use orthogonal features, i.e. complementary features,
to provably and properly represent data as well as reconstruct them
from small number of samples. CS has been widely studied and used
throughout different domains in the ofﬂine framework, such as image
processing [31], face recognition [25], and vehicle classiﬁcation [35].
Hence, we aim to ﬁnd the best trade-off over three aspects: (i) the
classiﬁer accuracy: the proportion of correctly predicted instances;
(ii) the memory usage: the cost of keeping the transformed data in
main memory; and (iii) the overall processing time: comprising the
data transformation, learning, and classiﬁcation. All such aspects are
strongly related, so the drastic reduction of time and space complexi-
ties would make our algorithm much faster than using all features. Of
course, one should weigh the accuracy while assessing these factors.

The main contributions of this paper are the following:

• Compressed-k-Nearest Neighbors (CS-kNN): a new kNN algo-
rithm to support data stream classiﬁcation. Our main focus con-
sists in improving its resource usage by compressing input streams
using CS while theoretically guaranteeing close approximation to
the accuracy that would be obtained using the original stream;

• Bagging with Compressed Sensing (CSB): an ensemble technique
based on Leveraging Bagging [8] where we combine several CS-
kNN instances built upon different CS independent matrices;
• Empirical results: we present experiments to show the abilities of
our proposals in obtaining a good trade-off among the three axes
(accuracy, memory and time) against several popular baselines;

The remainder of this work is organized as follows. We present
the related work for dimensionality reduction and streaming classi-
ﬁcation algorithms in Section 2.2. Section 3 provides the basics of
compressed sensing, followed by its application in conjunction with
the kNN algorithm for evolving data streams. Section 4 discusses the
experimental results performed on both synthetic and real datasets
followed by the concluding remarks in Section 5.

2 Preliminaries and Related work

2.1 Notation

In the following, we assume a data stream X is a sequence of in-
stances x1, x2, . . . , xN , where N denotes the number of available
observations so far. Each instance xi is composed of a vector of d at-
tributes x1
i . The dimensionality reduction comprises the pro-
cess of ﬁnding some transformation function (or map) A : Rd → Rp,
where p (cid:28) d, to be applied on each instance xi of X.

i , . . . , xd

2.2 Related Work

There are two main different types of technique for addressing the di-
mensionality problem: (i) based on random projections, a.k.a. data-
independent, that are not derived from data; and (ii) data-dependent
strategies derived from data to achieve the transformation itself.
Among data-dependent techniques, we mention feature extraction
based on component analysis where several variations have been
proposed to handle evolving distributions such as IPCA [38] and
IKPCA [19]. The most popular and straightforward technique is
the Principal Component Analysis (PCA) [20] which aims to ﬁnd
a lower-dimensional basis in which the sum of square distances be-
tween the original data and their projections is minimized, i.e. being
as close as possible to zero while maximizing the variances.

Random Projection (RP) is a cost-efﬁcient alternative to PCA
since it is data-independent and satisﬁes the Johnson-Lindenstrauss
(JL) lemma [21]: Let (cid:15) ∈ [0, 1], X = {x1, ..., xN } ∈ Rd. Given
a number p ≥ log(N/(cid:15)2), ∀xi, xj ∈ X there is a linear map
A : Rd → Rp such that:

Another well-known technique is the Hashing Trick (HT) [36],
also known as feature hashing. It has been used to make the analysis
of sparse and large data tractable in practice by mapping sparse in-
stances or vectors into a lower feature space using a hash function.
Given a list of keys that represents a set of features from the input
instances, it calculates then the hash function for each key, which
will ensure its mapping to a speciﬁc cell of a ﬁxed size vector that
constitutes the new compressed instance.

Those techniques could be combined to several machine learn-
ing algorithms while dealing with evolving data streams. Besides, an
important issue to address in the data stream scenario is the com-
putational efﬁciency of classiﬁers because of the potentially inﬁ-
nite nature of evolving data streams. Quite a number of classiﬁca-
tion algorithms for static datasets, that have already been thoroughly
studied, proved to be of limited effectiveness when dealing with big
data streams. Therefore, some of them have been extended to handle
evolving data streams [6, 3], among others, Self-Adjusting Memory
kNN (samkNN) [28] which uses a dual-memory model to capture the
drift in data streams. Hoeffding tree [14], also known as Very Fast
Decision Tree (VFDT), which is an incremental, anytime decision
tree induction algorithm that uses the Hoeffding bound to select the
optimal splitting attributes. On the other hand, there exists ensemble
learners which are popular when learning from evolving data streams
because they achieve a high learning performance, such as Lever-
aging Bagging (LB) [8] and Adaptive Random Forest (ARF) [18].
However, their major drawback is the high computational demand.

3 Compressed Sensing Classiﬁcation

3.1 Basic Notions of Compressed Sensing
The goal of compressed sensing, given a sparse vector x ∈ Rd, is to
measure y ∈ Rp and then reconstruct x, for p (cid:28) d, as follows:

y = Ax,

(2)

where A ∈ Rp×d is called a measurement (sampling or sensing) ma-
trix. A is used to transform instances from high-dimensional space
(x vectors) to a lower dimensional space (y vectors). Three concepts
are crucial to the recovery of the stream with high probability [15]:

Sparsity: CS exploits the fact that data may be sparse and hence
compressible in a concise representation. For an instance X with
support supp(X) = {l : xl (cid:54)= 0}, we deﬁne the (cid:96)0-norm (cid:107)X(cid:107)0=
|supp(X)|, so X is s-sparse if (cid:107)X(cid:107)0≤ s. The implication of sparsity
is important to remove irrelevant data without much loss.

Restricted Isometry Property (RIP): A is said to respect RIP if

(1 − (cid:15))(cid:107)xi − xj(cid:107)2

2≤ (cid:107)Axi − Axj(cid:107)2

2≤ (1 + (cid:15))(cid:107)xi − xj(cid:107)2
2.

there exists (cid:15) ∈ [0, 1] such that:

(1)

The JL lemma (1) asserts that N instances from an Eu-
clidean space can be projected into a lower dimensional space of
O(log N/(cid:15)2) dimensions under which pairwise distances are pre-
served within a multiplicative factor 1 ± (cid:15).

As detailed in the following section, random projection matri-
ces have been used before in conjunction with compressed sens-
ing [37]. This approach applies a random linear transformation on
vectors, changing their original space and leading to signiﬁcant re-
sults, outperforming PCA. For example, given data in a 104 dimen-
sional space, two random projections will give a perfect recovery
while two PCA projections will only recover data with a probability
equals to 2/104. In short, RP achieves good performance with few
projections whereas the PCA performance increases linearly with the
number of output dimension which makes it slower [37].

(1 − (cid:15))(cid:107)x(cid:107)2

2≤ (cid:107)Ax(cid:107)2

2≤ (1 + (cid:15))(cid:107)x(cid:107)2
2.

(3)

This property holds for all s-sparse data x ∈ Rd.

CS relies on the aforementioned principles that provide, with high
probability, a good data reconstruction from a limited number of in-
coherent and possibly noisy measurements. Mathematically, the de-
compression of the data that obeys the linear relation in Equation
(2) consists in approximating the error by (cid:96)1-norm minimization that
provides a convex relaxation and when data are sparse, the recovery
via (cid:96)1-minimization is provably exact [34]:

arg min
x∈Rd

(cid:107)x(cid:107)1

s.t. y = Ax.

The goal is to ﬁnd an efﬁcient representation for each instance
such that the sum of their reconstruction errors is minimized. The

Data Stream

Compressed Sensing
(Compression)

Feature Extraction

kNN

Figure 1: Compressed kNN Scheme.

RIP guarantees the proper computation of the above-mentioned re-
covery problem [10].

3.2 Construction of Sampling (Sensing) Matrices

The RIP is both a necessary and sufﬁcient condition for an efﬁcient
data recovery. Randomization is a key ingredient in the construction
of most of RIP matrices used in the CS transformation process [11].
In what follows, we cite examples of CS matrices:

• Fourier matrix is obtained by applying Fourier transform on data
and thereafter selecting uniformly at random p rows from a d di-
mensional Fourier matrix;

• Random Gaussian matrix is generated randomly from a Gaussian
distribution having independent and identically distributed (i.i.d)
entries with zero mean and variance one: Ai,j ∼ N (0, 1);

• Random Bernoulli matrix has entries which are randomly sam-
pled from a Bernoulli distribution with equal probability: Ai,j ∈
{1/

p, −1/

√

√

p}.

For the data-independent random matrices, it has been proved that
any matrix A satisfying the JL lemma (1) will also satisfy the RIP
in CS with high probability if p = O(s log(d)) [1]. A comparison
of the results obtained with these matrices and an explanation of the
choice of the matrix used in this work are provided in the following.

3.3 Compressed Classiﬁcation Using kNN

The k-Nearest Neighbors (kNN) is one of the most often used al-
gorithms that has been adapted to the stream setting [32]. It does not
require any work during training but its ofﬂine version uses the entire
dataset to predict class labels for test instances. The challenge with
adapting kNN to evolving data streams lies in the impracticality of
storing the entire stream for prediction. To tackle this issue, an adap-
tive solution ﬁts new instances once they arrive into a ﬁxed-length
window and merge them with the closest ones already in memory.
The prediction of the class label for an instance is therefore made by
taking the majority vote of its nearest neighbors, using a deﬁned dis-
tance metric. Yet, the search for the nearest neighbors is still costly in
terms of time and memory [9, 3]. Thus, for high-dimensional data, a
dimension reduction is imperative to avoid the curse of dimensional-
ity which may increase the use of computational resources. The main
idea to mitigate this drawback and improve kNN’s performance is to
use a simple strategy with relevant properties such as CS.

We focus on the analysis of an inﬁnite stream of instances xi ∈ Rd
from which we wish to construct a low dimensional space Rp, where
p (cid:28) d. We assume that instances are s-sparse in some basis, so we
can use CS with a RIP matrix and work in a lower dimension of
O(s log(d)). It is important to perform such reduction because it is
related to the number of dimensions and independent of the stream

Algorithm 1 Compressed-kNN algorithm. Symbols: X =
{x1, x2, . . .}: stream; C = {c1, c2, . . .}: set of labels; w: window;
k: the neighborhood size; p: the target dimension; S: subset of w.
1: function CS-kNN(X, w, k, p)
2:
3:
4:
5:
6:

Init w ← ∅
for all xi ∈ X do
yi ← CS(xi)
for all yj ∈ w do

(cid:46) apply CS
(cid:46) ∀ j (cid:54)= i
(cid:46) Equation (5)

compute Dyj (yi)

7:

8:

DS,k(yi)

c ← max
c∈C

w ← yi

(cid:46) Equation (6)

(cid:46) maintain the compressed xi in w

size, making it useful in applications for data streams where the size
is unknown. This transformation can lead to information loss, except
if the sensing matrix respects the RIP, then with high probability, the
information loss is minimal, and the original signal can be recovered.
Figure 1 presents the main ﬂow of the proposed approach com-
bining the simplicity of kNN and the strong properties of CS to ob-
tain the compressed kNN classiﬁer, called CS-kNN in the following.
Fundamentally, CS is composed of two phases: (i) the compression
phase, where the data are projected onto a low-dimensional space;
and (ii) the decompression phase, where the data are recovered. Nev-
ertheless, the compressed nature of CS makes the paradigm a bet-
ter ﬁt to classiﬁcation than the reconstruction. In this paper, we are
only concerned with the ﬁrst stage, so the extracted features from the
high-dimensional space are fed to kNN classiﬁer which predicts tar-
get class labels. This does not prevent, however, the guarantees over
the recovery to hold true. Algorithm 1 shows the pseudo-code of the
CS-kNN. We apply CS on each instance xi of the stream (lines 3-4),
then we apply kNN by computing the distance of each yj in w with
yi (line 6) and thus report the most frequent class label to yi (line 7).
Finally, we feed the compressed version yi to w (line 8).

In this work, we opt to make kNN more efﬁcient in terms of
memory and speed taking into account the online aspect of evolv-
ing data streams. Our approach consists of the CS application on
high-dimensional data obtained by compressing every new arrived
instance via solving Equation (2). In order that our approach works
well, we need to use an effective sampling matrix that gives sufﬁ-
ciently good (or with minor loss in) accuracy and potentially leads
to computational savings. In recent work [2], authors reviewed dif-
ferent sampling matrices performance where the experiment results
show that Gaussian random matrices perform nicely.

To motivate our choice in the following, we perform experiments
to assess different sampling matrices. For this, we ﬁrst generate syn-
thetic random Bernoulli and Gaussian matrices, and we also con-
struct the Fourier matrices on some datasets (see the description in
Table 2). Thus, for each dataset, we build projections for 5 different
settings of the target dimension {10, 20, . . . , 50}. Table 1 shows the
results for kNN (with k = 5) along with the overall average over
the different targeting dimensions for each matrix. We notice that
with the random Bernoulli matrix, kNN performs worse on average,
conﬁrming previous studies [2, 30], compared to the Random Gaus-
sian and Fourier matrices which are very close to the kNN, in terms
of accuracy, using the whole data without projections. Nevertheless,
Fourier transformation relies on data, i.e., it requires the presence
of all instances which is unrealistic in the context of data streams.
In [17], authors proposed a recursive scheme for using Fourier ma-
trices with data streams which constructs successive windows and
uses the measurement in the previous window to obtain the next one.
However, this approach is expensive in terms of memory since it

keeps data on windows and it is still not as accurate as using a Gaus-
sian matrix.

Table 1: Accuracy (%) comparison of compressed sensing with Bernoulli,
Gaussian, Fourier matrices, and the entire dataset.

Dataset
Tweet1
Tweet2
Tweet3
CNAE
Enron
Overall ∅

Bernoulli
64.78
64.59
60.24
27.04
95.88
62.51

Gaussian
77.89
77.17
75.59
64.59
95.97
78.24

Fourier
77.90
79.53
77.13
58.49
95.91
77.79

Whole data
79.80
79.20
78.86
73.33
96.18
81.58

In this work, we want to use a data-independent matrix to en-
sure fast processing. Following these experiments, we focus on the
Gaussian matrix which not only provides good accuracy but satis-
ﬁes with high probability the RIP and therefore allows recovery of
instances [4]. The matrix A in Equation (2) satisﬁes the RIP so x
can be recovered with minimum error from y, i.e., y preserves the
important information that x contains.

First, we need to bound the probability of error related to the esti-
mated instance and its expected value using the Hoeffding inequality.
Given xi, ∀i ∈ [1, N ], where xj
i are bounded by the interval [aj, bj],
then for any (cid:15), the probability of error is upper bounded as follows:

P (|xi − ˆxi|≥ (cid:15)) ≤ 2 exp

−

(cid:32)

2(cid:15)2
l=1(bl − al)2

(cid:80)d

(cid:33)

,

(4)

where ˆxi is the reconstructed instance and N can simply be the size
of a “sliding window” over which the error guarantee is provided.

To make the link between the kNN and the use of RIP matrices,
we point out that the JL lemma [21] asserts that a random projection
preserves the distance between pairs of instances up to 1 ± (cid:15) guaran-
tee with high probability. Similarly, the kNN algorithm is based on
a function that measures the distances between instances to predict.
Thus, we aim to provide theoretical guarantees on the connection be-
tween kNN and stream recovery by showing that the CS transforma-
tion using Gaussian matrix preserves the distance function and also
approximately maintains the shape –in the neighborhood sense– as
the original space, based on the concept of persistent homology.

Persistent homology [16] is one of the main tools used to extract
information from topological features of a space at different scales
for an effective shape description. Given a dataset in some metric
space, computing the persistent homology naturally involves nearest
neighbors since we are constructing the topological space by building
open balls around instances. In this regards, it has been shown in [33]
that the persistent homology of a distance such as in the JL lemma (1)
is (1 ± (cid:15))-preserved under random projection into O(log N/(cid:15)2) di-
mensions. The basic idea in [33] consists in preserving the radius of
the minimum enclosing open ball of data up to a factor of (1 ± 4(cid:15)).
In the following, we deal with the Euclidean distance function in
both kNN and data reconstruction guarantees. Given a window w,
the distance between instances xi and xj is deﬁned as follows:

Dxj (xi) = (cid:112)(cid:107)xi − xj(cid:107)2.

Similarly, the k-nearest neighbors distance is deﬁned as follows:

Dw,k(xi) = min

(w
k),xj ∈w

k
(cid:88)

j=1

Dxj (xi),

(5)

(6)

where (cid:0)w
bors to the instance xi in w.

k

(cid:1) denotes the subset of w of size k, i.e., the k-nearest neigh-

CS random matrices satisfy RIP, so we need to show that our
matrix preserves the neighborhood for kNN without signiﬁcant loss
through the JL lemma (1). This would allow us to conserve distances
among instances and ﬁnally ensure distance-preservation among all
neighbors. In [4], authors have indeed established a connection be-
tween the expressions in (1) and (3), and proved that the JL lemma
implies the RIP for s-sparse data within an (cid:15)-multiplicative factor. A
converse result has been proved in [24] wherein matrices having the
RIP respect the JL lemma, i.e., preserve the distances in the trans-
formation between any pairs of instances up to a (1 ± (cid:15))-factor with
target dimension in O(s log(d)).

Application to persistent homology. Now we want to prove,
based on the aforementioned result [24] derived from Equation (3),
that CS preserves as well the distances between all instances up to
(1 ± (cid:15))-error and not only distances between pairs of instances. In
other words, we prove that, given a RIP matrix, the resulting com-
pressed instances preserve the kNN neighborhood of the data.

Theorem 1 Given a set of instances in a sliding window w = {xi},
i ∈ [1, N ] and (cid:15) ∈ [0, 1], if there exists a transformation matrix
A : Rd → Rp having the RIP, such that p = O(s log(d)), where s
is the sparsity of data, then ∀xi ∈ w:

(1 − (cid:15))D2

w,k(x) ≤ D2

w,k(Ax) ≤ (1 + (cid:15))D2

w,k(x).

(7)

Proof. Assume that x1, x2, · · · , xk are the k-nearest neighbors to
an instance t ∈ w. We have:

(1 − (cid:15))(cid:107)t − xi(cid:107)2≤ (cid:107)At − Axi(cid:107)2≤ (1 + (cid:15))(cid:107)t − xi(cid:107)2.

By summing these inequalities k times, we obtain:

(1 − (cid:15))

k
(cid:88)

i=1

(cid:107)t − xi(cid:107)2≤

k
(cid:88)

(cid:107)At − Axi(cid:107)2≤ (1 + (cid:15))

i=1

k
(cid:88)

(cid:107)t − xi(cid:107)2.

i=1

The distance of At to its k-nearest neighbors in w is minimal, so we
have the lower bound as follows:

D2

w,k(At) ≤

k
(cid:88)

(cid:107)At − Axi(cid:107)2.

i=1

For the upper bound, we have:

D2

w,k(At) ≤

k
(cid:88)

(cid:107)At − Axi(cid:107)2≤ (1 + (cid:15))

i=1

k
(cid:88)

(cid:107)t − xi(cid:107)2,

i=1

D2

w,k(At) ≤

k
(cid:88)

(cid:107)At − Axi(cid:107)2≤ (1 + (cid:15))D2

w,k(t).

i=1

Assume that Az1, Az2, · · · , Azk are the k-nearest neighbors to

At, where z1, z2, · · · , zk ∈ w. So we have:

(1 − (cid:15))

k
(cid:88)

(cid:107)y − zi(cid:107)≤

k
(cid:88)

(cid:107)At − Azi(cid:107)2= D2

w,k(At).

i=1

i=1

Given the fact that x1, x2, · · · , xk are the k-nearest neighbors to t,

we found the lower bound as follows:

D2

w,k(t) =

k
(cid:88)

(cid:107)t − xi(cid:107)2≤

k
(cid:88)

(cid:107)t − zi(cid:107)2.

i=1

i=1

(cid:3)
This completes the proof.
So far, we demonstrated that the CS-kNN linked to geometrical
properties by achieving homology preservation while being scale-
invariant in terms of distances, captures the neighborhood up to some
((cid:15))-divergence between the original and the compressed instances.

Bagging CS-kNN. Another application of this framework is to
use the CS in an ensemble method which applies CS-kNN as a base
learner under Leveraging Bagging (LB) [8], denoted CS-kNNLB.

To increase the diversity inside this LB ensemble, in addition to
sampling with the Poisson distribution (λ), where λ ≥ 1, we can
use several random matrices by generating a different CS matrix for
each ensemble member (CS-kNN) instead of using only one for all
the learners (the case of CS-kNNLB). We refer to the aforementioned
approach in the following as Compressed Sensing Bagging Ensem-
ble (CSB), (CSB-kNN). The properties assessing the neighborhood
preservation for CS-kNN hold also for the CSB-kNN.

4 Experimental Results

We conduct extensive experiments to evaluate the performance of our
proposals. We are interested in three main results: the accuracy, the
memory (megabytes), and the time (seconds).

4.1 Data

We use 4 synthetic and 5 real-world datasets from different scenar-
ios. Table 2 presents a short description of each dataset, and further
details are provided in what follows.

Table 2: Overview of the datasets

Dataset
Tweets1
Tweets2
Tweets3
RBF
CNAE
Enron
IMDB
Spam
Covt

#Instances
1,000,000
1,000,000
1,000,000
1,000,000
1,080
1,702
120,919
9,324
581,012

#Attributes
500
1,000
1,500
200
856
1,000
1,001
39,916
54

#Classes
2
2
2
10
9
2
2
2
7

Type
Synthetic
Synthetic
Synthetic
Synthetic
Real
Real
Real
Real
Real

Tweets. Tweets was created using the text data generator provided
by MOA [7]. It simulates sentiment analysis on tweets, where mes-
sages can be classiﬁed into two categories depending on whether they
convey positive or negative feelings. Tweets1, Tweets2, and Tweets3
produce instances of 500, 1, 000, and 1, 500 attributes, respectively.
RBF. The Radial Basis Function generator creates centroids at
random positions, and each one has a standard deviation, a weight
and a class label.

CNAE. CNAE is the national classiﬁcation of economic activi-
ties dataset, initially used in [12]. Instances represent descriptions
of Brazilian companies categorized into 9 classes. The original texts
were pre-processed to obtain the current highly sparse dataset.

Enron. The Enron corpus is a cleaned version of a large set of
emails that was made public during the legal investigation concern-
ing the Enron corporation [23].

IMDB. IMDB movie reviews dataset was ﬁrst proposed for senti-
ment analysis [29], where reviews have been pre-processed, and each
review is encoded as a sequence of word indexes (integers).

Spam. The spam corpus is the result of a text mining on an online
news dissemination system which intends on creating an incremen-
tal ﬁltering of e-mails classifying them as spam or not [22]. Each
attribute represents the presence of a word in the instance (an e-mail).

Covt. The forest covertype dataset obtained from US Forest Ser-

vice Region 2 Resource Information System (RIS) data.

4.2 Results

The experiments were conducted on a machine equipped with an In-
tel Core i5 CPU and 4GB of main memory. They were implemented
and evaluated in Java by extending the MOA framework [6, 7]. We
used the online evaluation setting for Test-Then-Train method, where
each instance is used ﬁrst for testing and then for training.

Results with non-ensemble methods. We compare the perfor-
mance of our proposed classiﬁer, CS-kNN, to commonly-used tech-
niques in the literature; self-adjusting memory kNN (CS-samkNN)
with CS, kNN using hashing trick (HT-kNN), principal component
analysis (PCA-kNN), and the standard kNN without projection as
well (using the entire data). The streaming kNN has two principal
parameters: the number of neighbors k and the window size w.

Table 3: Performance of kNN with different window sizes.

Accuracy (%)

Overall ∅

w=100
75.48

w=1000
80.33

w=5000
82.44

Time (sec)

Overall ∅

w=100
537.76

w=1000
6592.72

w=5000
20028.01

Memory (MB)

Overall ∅

w=100
46.85

w=1000
269.65

w=5000
2000

Table 3 presents the results for distinct sizes of w and shows that;
for shorter windows (w = 100), the accuracy degrades, while for
bigger windows the accuracy slightly increases. On the other hand,
the processing time and memory usage increase as well. Therefore
this parameter selection implies an accuracy-time-memory trade-off.
The following experiments are performed with w = 1000 for kNN,
because using a greater window size yields indeed to a better accu-
racy but the resource consumption is more signiﬁcant.

Tables 4, 5, and 6 report the ﬁnal accuracies, memory consump-
tion, and speed of the classiﬁcation models in a 40-dimensional space
after the projections based on two setups of k = 5, 11. We choose 40
dimensions because we noticed that, starting from this size of space,
improvements are statistically insigniﬁcant as showed in Figure 3.
The latter illustrates a detailed comparison with ﬁve different values
of output dimension (10, 20, · · · , 50) on Tweet2 and Enron datasets.
Results with ensemble methods. We compare the proposed LB
with CS-kNN as a base learner (CS-kNNLB) and the CSB-kNN with
a different CS matrix for each learner, both using 10 learners (the size
of ensemble) and k = 5, against popular ensemble methods such as
the adaptive random forest (ARF) [18] and leveraging bagging us-
ing Hoeffding tree [14] as base learner (HTreeLB), with 30 and 10
ensemble members, respectively. Tables 7, 8 and 9 display the per-
formance of the ensembles. In this evaluation, each of the ensemble
member uses the same CS matrix to perform the reduction into 40
dimensions, except CSB-kNN which sets up a different matrix for
each member in attempt to assess the ensemble diversity impact.

4.3 Discussion

Our proposed CS-kNN has more accurate results (Table 4) than HT-
kNN for all datasets and it is slightly outperformed by CS-samkNN,
the standard kNN (without projection) and PCA-kNN; this quite a

Table 4: Accuracy (%) comparison of CS-kNN, CS-samkNN, HT-kNN, PCA-kNN, and kNN over the whole dataset.

Dataset

Tweet1
Tweet2
Tweet3
RBF
CNAE
Enron
IMDB
Spam
Covt
Overall ∅

CS-kNN

CS-samkNN

HT-kNN

PCA-kNN

kNN

k = 5
78.82
78.13
76.75
98.90
70.00
96.02
69.86
85.39
91.36
82.80

k = 11
78.88
78.36
76.16
97.31
68.70
95.65
72.32
81.01
89.92
82.04

k = 5
76.02
75.74
73.03
99.87
73.77
96.23
74.29
91.34
90.47
83.42

k = 11
74.31
74.13
72.56
99.78
72.19
96.06
74.53
90.48
87.71
82.42

k = 5
73.77
73.02
72.40
19.20
65.00
95.76
69.65
83.82
77.18
69.98

k = 11
73.14
72.61
72.36
19.20
65.28
95.48
72.03
80.63
76.59
69.70

k = 5
80.43
80.06
81.93
99.00
75.83
94.59
70.57
96.00
91.55
85.55

k = 11
79.43
78.89
82.38
97.86
72.08
93.18
72.81
94.66
90.16
84.61

k = 5
79.80
79.20
78.86
98.89
73.33
96.18
70.94
81.17
91.67
83.34

k = 11
78.17
77.74
77.73
97.33
71.48
96.00
72.51
77.32
90.30
82.06

Table 5: Time (sec) comparison of CS-kNN, CS-samkNN, HT-kNN, PCA-kNN, and kNN over the whole dataset.

Dataset

Tweet1
Tweet2
Tweet3
RBF
CNAE
Enron
IMDB
Spam
Covt
Overall ∅

CS-kNN

CS-samkNN

HT-kNN

PCA-kNN

kNN

k = 5
62.55
107.48
126.73
59.47
0.87
1.58
95.62
159.92
30.94
71.68

k = 11
91.06
112.97
142.95
80.52
0.92
1.63
120.66
183.19
51.08
87.22

k = 5
41.81
74.92
83.01
60.08
0.56
1.31
80.82
197.22
39.25
64.33

k = 11
59.20
99.77
101.43
77.00
0.63
1.57
103.51
208.94
45.55
75.29

k = 5
93.24
120.83
154.22
168.31
0.95
1.81
125.62
194.07
88.17
105.25

k = 11
99.78
127.95
165.11
169.88
1.02
1.90
129.27
216.37
90.85
111.42

k = 5
622.65
705.71
988.25
243.26
3.97
7.21
1686.88
11329.91
161.00
1749.87

k = 11
629.60
712.84
995.93
258.12
4.14
7.28
1692.28
14820.26
164.16
2142.73

k = 5
1198.78
2029.82
2864.55
284.34
32.19
86.08
7892.96
34231.45
252.69
5430.32

k = 11
1432.47
2502.92
3643.26
439.23
35.04
91.99
8217.06
35031.76
268.28
5740.22

Table 6: Memory (MB) comparison of CS-kNN, CS-samkNN, HT-kNN,
PCA-kNN, and kNN over the whole dataset.

Dataset
Tweet1
Tweet2
Tweet3
RBF
CNAE
Enron
IMDB
Spam
Covt
Overall ∅

CS-kNN CS-samkNN HT-kNN PCA-kNN kNN

2.52
2.52
2.52
2.52
2.52
2.52
2.52
2.52
2.52
2.52

8.86
10.48
10.52
10.31
10.22
9.84
10.28
10.57
9.96
10,12

2.52
2.52
2.52
2.52
2.52
2.52
2.52
2.52
2.52
2.52

3.03
5.97
8.84
8.86
3.09
3.51
8.81

34.64
70.97
103.19
13.18
61.37
70.60
70.65
245.22 1476.11
3.47
211.57

3.02
32.26

natural result since kNN processes the whole data stream and PCA-
kNN formally tries to ﬁnd a lower-dimensional space under which
the sum of square distances – representing the error, between the
original data and its projection – is minimized. CS-kNN is moder-
ately less accurate than CS-samkNN for some datasets containing
drifts, because the latter deals with different types of concept drift
which makes it stronger facing changes in data distributions.

To assess the beneﬁts in terms of computational resources – where
small values are desirable – Tables 5 and 6 point out the improve-
ments of CS-kNN in terms of memory and time against CS-samkNN,
PCA-kNN, and kNN which are signiﬁcant enough to justify rela-
tively minor losses in accuracy. In fact, CS-samkNN maintains mod-
els for current and past concepts which makes it memory inefﬁcient.
PCA-kNN performs worse, in terms of resource usage, than RP since
it incrementally stores and updates the eigenvectors and eigenval-
ues, conﬁrming previous studies [37]. Our proposed approach is also
faster than HT-kNN, although they have similar memory behavior,
because both are based on RP and do not rely on data.

For some datasets such as Spam, CS-kNN outperforms kNN (us-
ing the whole data) simply because ﬁnding relevant combinations of
existing features and presenting them in a different space can help
supervised models to improve accuracy. Even if data are not sparse,
CS surprisingly performs projections on suitable dimensions.

Figures 3a and 3d depict the typical trade-off for accuracy: a small
feature space cannot properly represent data, therefore it can signif-
icantly degrade the accuracy; whereas a higher dimensional space
(e.g., 50) increases the accuracy and makes it closer to the results
with kNN. We also notice the stability of our CS-kNN, i.e., the ac-
curacy is linearly boosted with the target space size and converges to
the accuracy of kNN. On the other hand, CS-samkNN, HT-kNN and
PCA-kNN have different behaviors, clearly illustrated in Figure 3a;
this results deduce that, in practice, it may be hard to ﬁx a proper
space size. We show that kNN, PCA-kNN and HT-kNN are outper-
formed in terms of processing time (Figures 3b and 3e) and that CS-
kNN requires also less memory compared to these baselines. For in-
stance, with Tweet2 and Enron in Figures 3c and 3f respectively, we
observe large gains compared to kNN, PCA-kNN, and CS-samkNN,
albeit our proposal has the same memory usage as HT-kNN because
both do not rely on data. We also observe that the behavior of mem-
ory usage is correlated to the running time trends, i.e., when the mem-
ory usage increases, the processing time also increases accordingly.
Tables 7, 8 and 9 show, using only 10 learners, CSB-kNN per-
forms better than the reputed CS-ARF [18] using 30 learners (trees)
on most of the datasets. We noticed that with CSB-kNN, when the
features set is large (e.g. Spam), the memory usage is relatively high.
On the other hand, for large datasets (e.g. Tweets), CS-ARF and CS-
HTreeLB require more memory whereas our approaches use less,
making them useful for the stream setting. CS-kNNLB is the most
memory efﬁcient and even proving competitive with CS-HTreeLB

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3: Sorted plots of accuracy, time and memory over different output dimensions. a Accuracy Tweet2. b Time Tweet2. c Memory Tweet2. d Accuracy
Enron. e Time Enron. f Memory Enron.

Table 7: Accuracy (%) comparison of CS-kNNLB , CSB-kNN, CS-
HTreeLB, and CS-ARF

Dataset
Tweets1
Tweets2
Tweets3
RBF
CNAE
Enron
IMDB
Spam
Covt
Overall ∅

CS-kNNLB
78.94
78.24
76.06
98.90
71.64
95.94
70.02
86.08
91.09
82.99

CSB-kNN CS-HTreeLB

81.80
81.28
80.40
99.68
81.48
96.00
74.27
90.28
91.76
86.33

81.35
80.39
78.59
99.24
65.70
96.17
74.80
90.02
88.48
83.86

CS-ARF
81.53
80.75
79.54
99.25
62.55
95.88
74.88
89.04
88.01
83.49

Table 8: Time (SEC) comparison of CS-kNNLB, CSB-kNN, CS-HTreeLB,
and CS-ARF

Dataset
Tweets1
Tweets2
Tweets3
RBF
CNAE
Enron
IMDB
Spam
Covt
Overall ∅

CS-kNNLB
1130.44
1449.44
1668.26
735.21
8.99
20.07
1552.81
359.07
612.62
837.43

CSB-kNN CS-HTreeLB
1251.77
1526.30
1825.41
772.62
11.02
21.92
1649.94
2194.93
694.02
1105.33

82.18
105.87
127.19
90.22
1.80
2.11
90.17
218.16
41.69
84.37

CS-ARF
170.52
212.69
239.97
223.08
4.66
3.78
174.54
270.15
115.3
108.70

Table 9: Memory (MB) comparison of CS-kNNLB, CSB-kNN, CS-
HTreeLB, and CS-ARF

Dataset
Tweets1
Tweets2
Tweets3
RBF
CNAE
Enron
IMDB
Spam
Covt
Overall ∅

CS-kNNLB
6.16
6.16
6.16
6.16
6.15
6.15
6.16
5.38
6.16
6.07

CSB-kNN CS-HTreeLB

27.13
28.97
30.80
25.96
28.11
28.59
28.60
151.91
24.10
41.57

60.71
66.75
73.89
9.91
0.48
1.59
5.60
5.15
4.44
25.39

CS-ARF
175.71
177.32
176.92
25.90
1.31
4.10
18.63
10.44
11.66
66.89

and CS-ARF. However, this is at the price of being slower. Also,
computational resources of CSB-kNN with different CS matrices in-
crease considerably to allow higher accuracy and diversity (enabling
the ensemble to generalize well).

In conclusion, our CSB-kNN ensemble method has good overall
performance compared to other methods. We showed that our pro-
posal can be used to classify accurately data streams with a large
number of attributes using a relatively small number of base learners,
in contrast with CS-ARF the number of base trees can considerably
affect the classiﬁcation performance.

5 Conclusions

In this work, we presented a scheme to enable the k-nearest neigh-
bors algorithm to be efﬁcient with evolving high-dimensional data
streams in terms of classiﬁcation performance and computational re-
sources (memory and time) after space transformations provided by

compressed sensing given its ability to ensure theoretical lower and
upper bounds on pairwise data transformations. Our ﬁrst contribution
is the mix of two main ingredients: compressed sensing and kNN,
thus resulting in the CS-kNN algorithm designed to work on evolv-
ing data streams while operating on a reduced feature space.

We proposed also an ensemble method, CSB-kNN, that uses CS-
kNN as base learner under the Leveraging Bagging, where each en-
semble member has a different CS matrix to help increasing the over-
all accuracy. We showed theoretically that CS-kNN using Gaussian
matrices, the neighborhood distance used in kNN is preserved up to
some 1 ± (cid:15)-factor. The key idea is to show that squared kNN dis-
tances, in the original data, are too within the same factor. Conse-
quently, our CS-kNN algorithm also conserves such distances.

We evaluated the proposed algorithms via extensive experiments
using synthetic and real-world datasets with different parameters.
Results show the potential of the CS-kNN and CSB-kNN algorithms
to obtain close approximations to what it would be obtained using
the input instances from data streams. We compared our propos-
als against well-known approaches from the literature, showing im-
provements along 3 dimensions: accuracy, memory usage, and time.
In future work, we intend to investigate how to optimize the pro-
cessing time of CSB-kNN algorithm and provide guarantees along
the number of output dimensions. Once we ﬁx the latter, we could
efﬁciently project the data streams knowing the input dimensions.

Acknowledgements

This work was done in the context of IoTA AAP Emergence Digi-
Cosme Project and was funded by Labex DigiCosme.

References

[1] Dimitris Achlioptas, ‘Database-friendly random projections: Johnson-
lindenstrauss with binary coins’, JCSS, 66(4), 671–687, (2003).
[2] Youness Arjoune, Naima Kaabouch, Hassan El Ghazi, and Ahmed
Tamtaoui, ‘A performance comparison of measurement matrices in
compressive sensing’, International Journal of Communication Sys-
tems, 31(10), e3576, (2018).

[3] Maroua Bahri, Silviu Maniu, and Albert Bifet, ‘A sketch-based naive
bayes algorithms for evolving data streams’, in International Confer-
ence on Big Data, pp. 604–613. IEEE, (2018).

[4] Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael
Wakin, ‘A simple proof of the restricted isometry property for random
matrices’, Constructive Approximation, 28(3), 253–263, (2008).
Jean Paul Barddal, Heitor Murilo Gomes, Fabr´ıcio Enembreck, and
Bernhard Pfahringer, ‘A survey on feature drift adaptation: Deﬁnition,
benchmark, challenges and future directions’, Journal of Systems and
Software, 127, 278–294, (2017).

[5]

[6] Albert Bifet, Ricard Gavald`a, Geoff Holmes, and Bernhard Pfahringer,
Machine learning for data streams: with practical examples in MOA,
MIT Press, 2018.

[7] Albert Bifet, Geoff Holmes, Richard Kirkby, and Bernhard Pfahringer,
‘Moa: Massive online analysis’, JMLR, 11(May), 1601–1604, (2010).
[8] Albert Bifet, Geoff Holmes, and Bernhard Pfahringer, ‘Leveraging bag-
ging for evolving data streams’, in Joint European conference on ma-
chine learning and knowledge discovery in databases, pp. 135–150.
Springer, (2010).

[9] Albert Bifet, Bernhard Pfahringer, Jesse Read, and Geoff Holmes, ‘Ef-
ﬁcient data stream classiﬁcation via probabilistic adaptive windows’, in
SIGAPP, pp. 801–806. ACM, (2013).

[10] Alfred M Bruckstein, David L Donoho, and Michael Elad, ‘From sparse
solutions of systems of equations to sparse modeling of signals and
images’, SIAM review, 51(1), 34–81, (2009).

[11] Avishy Y Carmi, Lyudmila Mihaylova, and Simon J Godsill, Com-

pressed sensing and sparse ﬁltering, Springer, 2014.

[12] Patrick Marques Ciarelli and Elias Oliveira, ‘Agglomeration and elim-
ination of terms for dimensionality reduction’, in ISDA, pp. 547–552.
IEEE, (2009).

[13]

John P Cunningham and Zoubin Ghahramani, ‘Linear dimensionality
reduction: Survey, insights, and generalizations’, JMLR, 16(1), 2859–
2900, (2015).

[14] Pedro Domingos and Geoff Hulten, ‘Mining high-speed data streams’,

in SIGKDD, pp. 71–80. ACM, (2000).

[15] David L Donoho, ‘Compressed sensing’, IEEE Transactions on Infor-

mation Theory, 52(4), 1289–1306, (2006).

[16] Herbert Edelsbrunner and John Harer, ‘Persistent homology-a survey’,

Contemporary Mathematics, 453, 257–282, (2008).

[17] Nikolaos M Freris, Orhan Oc¸al, and Martin Vetterli, ‘Compressed sens-
ing of streaming data’, in 51st Annual Allerton Conference on Commu-
nication, Control, and Computing, pp. 1242–1249. IEEE, (2013).
[18] Heitor M Gomes, Albert Bifet, Jesse Read, Jean Paul Barddal,
Fabr´ıcio Enembreck, Bernhard Pfharinger, Geoff Holmes, and Talel
Abdessalem, ‘Adaptive random forests for evolving data stream clas-
siﬁcation’, Machine Learning, 1–27, (2017).

[19] Simon G¨unter, Nicol N Schraudolph, and SVN Vishwanathan, ‘Fast
iterative kernel principal component analysis’, JMLR, 8(8), 1893–1918,
(2007).

[20] Harold Hotelling, ‘Analysis of a complex of statistical variables into
principal components.’, Journal of Educational Psychology, 24(6), 417,
(1933).

[21] William B Johnson, Joram Lindenstrauss, and Gideon Schechtman,
‘Extensions of lipschitz maps into banach spaces’, Israel Journal of
Mathematics, 54(2), 129–138, (1986).
Ioannis Katakis, Grigorios Tsoumakas, Evangelos Banos, Nick Bassil-
iades, and Ioannis Vlahavas, ‘An adaptive personalized news dissemi-
nation system’, Journal of Intelligent Information Systems, 32(2), 191–
212, (2009).

[22]

[23] Bryan Klimt and Yiming Yang, ‘The enron corpus: A new dataset for
email classiﬁcation research’, in ECML, pp. 217–226. Springer, (2004).
[24] Felix Krahmer and Rachel Ward, ‘New and improved johnson–
lindenstrauss embeddings via the restricted isometry property’, Mathe-
matical Analysis, 43(3), 1269–1281, (2011).

[25] Hanxi Li, Chunhua Shen, and Qinfeng Shi, ‘Real-time visual tracking

using compressive sensing’, in CVPR, pp. 1305–1312. IEEE, (2011).

[26] Huan Liu and Hiroshi Motoda, Feature extraction, construction and
selection: A data mining perspective, volume 453, Springer Science &
Business Media, 1998.

[27] Huan Liu and Hiroshi Motoda, Computational methods of feature se-

lection, CRC Press, 2007.

[28] Viktor Losing, Barbara Hammer, and Heiko Wersing, ‘Knn classiﬁer
with self adjusting memory for heterogeneous concept drift’, in ICDM,
pp. 291–300. IEEE, (2016).

[29] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, An-
drew Y Ng, and Christopher Potts, ‘Learning word vectors for senti-
ment analysis’, in ACL-HLT, pp. 142–150. Association for Computa-
tional Linguistics, (2011).

[30] Thu LN Nguyen and Yoan Shin, ‘Deterministic sensing matrices in

compressive sensing: a survey’, The Scientiﬁc World Journal, (2013).

[31] Chenlu Qiu, Wei Lu, and Namrata Vaswani, ‘Real-time dynamic mr
image reconstruction using kalman ﬁltered compressed sensing’, in
ICASSP, pp. 393–396. IEEE, (2009).
Jesse Read, Albert Bifet, Bernhard Pfahringer, and Geoff Holmes,
‘Batch-incremental versus instance-incremental learning in dynamic
and evolving data’, in IDA, pp. 313–323, (2012).

[32]

[33] Donald R Sheehy, ‘The persistent homology of distance functions under

[34]

random projection’, in SoCG, p. 328. ACM, (2014).
Jean-Luc Starck, Fionn Murtagh, and Jalal Fadili, Sparse image and
signal processing: Wavelets and related geometric multiscale analysis,
Cambridge university press, 2015.

[35] M Uttarakumari, Ashray V Achary, Sujata D Badiger, DS Avinash, An-
isha Mukherjee, and Nancy Kothari, ‘Vehicle classiﬁcation using com-
pressive sensing’, in RTEICT, pp. 692–696. IEEE, (2017).

[36] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and
Josh Attenberg, ‘Feature hashing for large scale multitask learning’, in
ICML, pp. 1113–1120. ACM, (2009).

[37] Yair Weiss, Hyun Sung Chang, and William T Freeman, ‘Learn-
ing compressed sensing’, in Snowbird Learning Workshop. Citeseer,
(2007).
Juyang Weng, Yilu Zhang, and Wey-Shiuan Hwang,
‘Candid
covariance-free incremental principal component analysis’, TPAMI,
25(8), 1034–1040, (2003).

[38]

