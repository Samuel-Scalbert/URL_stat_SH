Self-Concordant Analysis of Generalized Linear Bandits
with Forgetting
Yoan Russac, Louis Faury, Olivier Cappé, Aurélien Garivier

To cite this version:

Yoan Russac, Louis Faury, Olivier Cappé, Aurélien Garivier. Self-Concordant Analysis of Generalized
Linear Bandits with Forgetting. AISTATS 2021 - International Conference on Artificial Intelligence
and Statistics, Apr 2021, San Diego / Virtual, United States. ￿hal-02984117v2￿

HAL Id: hal-02984117

https://hal.science/hal-02984117v2

Submitted on 3 Mar 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Yoan Russac*
DI ENS, CNRS, Inria,
ENS, Université PSL

Louis Faury*
Criteo AI Lab,
LTCI TélécomParis

Olivier Cappé
DI ENS, CNRS, Inria,
ENS, Université PSL

Aurélien Garivier
UMPA, CNRS,
Inria, ENS Lyon

Abstract

Contextual sequential decision problems with
categorical or numerical observations are ubiq-
uitous and Generalized Linear Bandits (GLB)
oﬀer a solid theoretical framework to address
them. In contrast to the case of linear bandits,
existing algorithms for GLB have two draw-
backs undermining their applicability. First,
they rely on excessively pessimistic concentra-
tion bounds due to the non-linear nature of
the model. Second, they require either non-
convex projection steps or burn-in phases to
enforce boundedness of the estimators. Both
of these issues are worsened when considering
non-stationary models, in which the GLB pa-
rameter may vary with time. In this work, we
focus on self-concordant GLB (which include
logistic and Poisson regression) with forget-
ting achieved either by the use of a sliding
window or exponential weights. We propose
a novel conﬁdence-based algorithm for the
maximum-likehood estimator with forgetting
and analyze its perfomance in abruptly chang-
ing environments. These results as well as the
accompanying numerical simulations highlight
the potential of the proposed approach to ad-
dress non-stationarity in GLB.

1

INTRODUCTION

In recent years, linear bandits (Abbasi-Yadkori et al.,
2011; Chu et al., 2011; Dani et al., 2008; Rusmevichien-
tong and Tsitsiklis, 2010) have become the go-to
paradigm to balance exploration and exploitation in
contextual sequential decision making problems. Linear
bandits have typically found applications for content-
based recommendations (Li et al., 2010; Valko et al.,

Proceedings of the 24th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2021, San Diego,
California, USA. PMLR: Volume 130. Copyright 2021 by
the author(s). *Equal contribution.

2014), real-time bidding (Flajolet and Jaillet, 2017)
and even mobile-health interventions (Tewari and Mur-
phy, 2017). Concurrently, Generalized linear bandits
(GLB) have been introduced as a generalization of lin-
ear bandits, able to describe broader reward models of
considerable practical relevance, in particular binary
or categorical rewards (Filippi et al., 2010; Li et al.,
2017). GLB are for instance a natural option in on-
line advertising applications where the rewards take
In this
the form of clicks (Chapelle and Li, 2011).
work, we focus on deterministic algorithms and refer
to (Chapelle and Li, 2011; Kveton et al., 2020) for
randomized algorithms applicable to GLB. Compared
to the linear bandits case, there are two distinctive
drawbacks of GLB algorithms. The ﬁrst is (1) the
presence of a problem-dependent constant, imposed by
the non-linear nature of the model, that is possibly pro-
hibitively large and has a negative impact both on the
design of algorithms and on their analysis. The second
is (2) the need to modify the Maximum Likelihood
Estimator (MLE) to ensure that it has a bounded norm.
Usually this is achieved by resorting to an additional
non-convex projection program applied to the MLE
(Filippi et al., 2010). These distinctions correspond
to a fundamental diﬀerence between the models, and
explain why methods developed for linear bandits may
fail in the case of GLB.

The ﬁrst drawback (1) was recently addressed by Faury
et al. (2020), in the speciﬁc case of logistic bandits.
They showed that in this particular setting, the regret
bounds of carefully designed algorithms could be signif-
icantly improved only at the cost of minor algorithmic
modiﬁcations. Their analysis tightens the gap with the
linear case, and takes a signiﬁcant step towards the
development of eﬃcient GLB algorithms.

The second drawback (2) has seen little treatment in
the literature, except for the work of Li et al. (2017) who
proved that the projection step of Filippi et al. (2010)
could be avoided by resorting to random initialization
phases. However, a careful examination of the required
conditions shows that these initialization phases can
be prohibitively long to be deployed in scenarios of

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

practical interest.

The aforementioned improvements to the original GLB
algorithm of Filippi et al. (2010) were developed under
a stationarity assumption. However, non-stationary
environments are ubiquitous in real-world applications
of contextual bandits. In the linear bandits literature,
this has motivated the development of adequate algo-
rithms, able to handle changes in the structure of the
reward signal (Cheung et al., 2019b; Russac et al., 2019;
Zhao et al., 2020). Russac et al. (2020) generalized such
approaches to GLB, but without addressing neither (1)
nor (2). As a result, the practical relevance of their
approach remains questionable and the development
of eﬃcient and non-stationary GLB algorithms stands
incomplete.

This paper aims at closing this gap. We study a broad
family of GLB, known as self-concordant (which in-
cludes for instance the logistic and Poisson bandits), in
environments where the parameter is allowed to switch
arbitrarily over time. Under this setting, we answer (1)
by providing a non-trivial extension of the concentra-
tion results from Faury et al. (2020). We also leverage
the self-concordance property to remove the projection
step, henceforth overcoming (2). This is made possi-
ble by an improved characterization of the, possibly
weighted, MLE in (self-concordant) generalized linear
models. Combined together, these two contributions
lead to the design of eﬃcient GLB algorithms, with
improved regret bounds and which do not require to
solve hard (i.e. non-convex) optimization programs.
In doing so, we also answer the long-standing issue of
providing proper conﬁdence regions centered around
the pristine MLE in GLB.

2 BACKGROUND

2.1 Setting and Assumptions

At each time step, the environment provides a time-
dependent action set At and the agent plays a d-
dimensional action at ∈ At. We will assume that the
reward’s distribution belongs to a canonical exponen-
tial family with respect to a reference measure ν, such
that dPθ(r|a) = exp(ra(cid:62)θ − b(a(cid:62)θ) + c(r))dν(r). Here,
the function c(·) is real-valued and b(·) is assumed to
be twice continuously diﬀerentiable. Thanks to the
properties of exponential families, b is convex and can
be related to the function µ = ˙b, itself referred to as
the inverse link or mean function. A key feature of
this description is that given a ground-truth parameter
θ(cid:63), selecting an action at at time t yields a reward rt+1
conditionally independent on the past and such that
E[rt+1|at] = µ(a(cid:62)
The non-stationary nature of the considered environ-

t θ(cid:63)).

ments is characterized as follows: the bandit parameter
θ(cid:63) is allowed to change in an arbitrary fashion up to
ΓT times within the horizon T . In the following, θ(cid:63)
will be indexed by t to clearly exhibit its dependency
w.r.t round t, and the reward signal will follow
E[rt+1|at] = µ(a(cid:62)

t θ(cid:63)

t ) .

The focus of this paper is the dynamic regret deﬁned
as

RT =

T
(cid:88)

t=1

max
a∈At

µ(cid:0)a(cid:62)θ(cid:63)

t

(cid:1) − µ(cid:0)a(cid:62)

t θ(cid:63)
t

(cid:1) .

Note that in this setting, there is no ﬁxed best arm,
both due to the non-stationarity of the environment
and to the fact that the action set At may vary with
time. We will work under the following assumptions.
Assumption 1 (Bounded actions and bandit parame-
ters).

∀t ≥ 1, (cid:107)θ(cid:63)

t (cid:107)2 ≤ S and ∀a ∈ At, (cid:107)a(cid:107)2 ≤ 1 .

We deﬁne the admissible parameter space Θ = (cid:8)θ ∈
Rd, (cid:107)θ(cid:107)2 ≤ S(cid:9).
Assumption 2 (Bounded rewards).

∃m ∈ R+such that ∀t ≥ 1, 0 ≤ rt ≤ m .
Assumption 3. The mean function µ : R (cid:55)→ R is
continuously diﬀerentiable, Lipschitz with constant kµ
and such that

cµ =

inf
θ∈Θ,(cid:107)a(cid:107)2≤1

˙µ(cid:0)a(cid:62)θ(cid:1) > 0 .

The quantity cµ is crucial in the analysis, as it repre-
sents the (worst case) sensitivity of the mean function.
Our last assumption diﬀers from most of existing works
as we focus here on self-concordant GLMs. This as-
sumption on the curvature of the mean function is
rather mild, and covers for instance the logistic and
Poisson models.
Assumption 4 (Generalized self-concordance). The
mean function veriﬁes |¨µ| ≤ ˙µ .

In order to estimate the unknown bandit parameter
, we will adopt a weighted regularized maximum-
θ(cid:63)
t
likelihood principle. Formally, we deﬁne ˆθt for λ > 0
and γ ∈ (0, 1] as the solution of the strictly convex
program

ˆθt = arg min

θ∈Rd

s=1

t−1
(cid:88)

−

γt−1−s log Pθ(rs+1|as) +

λ
2

(cid:107)θ(cid:107)2

2 .

(1)
Equivalently, ˆθt may be deﬁned as the minimizer of
− (cid:80)t−1
, with time-
independent increasing weights γ−s and time-varying
regularization λγ−(t−1), which is more handy for anal-
ysis purposes, see (Russac et al., 2019).

s=1 γ−s log Pθ(rs+1|as) + λγ−(t−1)

(cid:107)θ(cid:107)2
2

2

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

2.2 Stationary GLB

GLB were ﬁrst considered in the seminal work of
Filippi et al. (2010) who proposed GLM-UCB, an op-
timistic algorithm with a regret upper bound of the
form ˜O(c−1
T ). A key characteristic of GLM-UCB is a
µ d
projection step, used to map the MLE onto the set of
admissible parameters Θ. Formally, when the MLE ˆθt
is not in Θ, it needs to be replaced by

√

˜θt = arg min

θ∈Θ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

s=1

(cid:104)
µ(cid:0)a(cid:62)

s θ(cid:1) − µ(cid:0)a(cid:62)

s

(cid:1)(cid:105)

ˆθt

as

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vt

−1

(2)

where Vt is an invertible d × d square matrix.
With GLM-UCB, both the size of the conﬁdence set (thus
the exploration bonus) and the regret bound scale as
. However, this constant can be prohibitively large.
c−1
µ
In the cases of the logistic and Poisson bandits, one
has c−1
µ ≥ eS, revealing an exponential dependency on
S. If we consider the example of click prediction in
is of the
online advertising with the logistic GLB, c−1
µ
order 103, corresponding to typical click rates of less
than a percent.

µ

d

√

√

T + c−1

T ) and (cid:101)O(d

This critical dependency was addressed by Faury
et al. (2020) for the logistic bandit. They introduce
LogUCB1 and LogUCB2 for which they respectively prove
(cid:101)O(c−1/2
µ ) regret upper bounds.
Their analysis relies on the self-concordance property of
the logistic log-likelihood. Self-concordance oﬀers a re-
ﬁned way to control the curvature of the log-likelihood,
and has been used in batch statistical learning (Bach,
2010) and online optimization (Bach and Moulines,
2013) (see also (Boyd and Vandenberghe, 2004, Section
9.6) for a broader picture). However, the analysis of
Faury et al. (2020) does not use the self-concordance
to its fullest and a projection step is still required, as
detailed in Section 5.

Since the mean function µ can be non-convex (as for
example in the case of logistic regression), the projec-
tion step deﬁned in Equation (2) generally involves the
minimization of a non-convex function. Solving this
program can be arduous and ﬁnding ways to bypass it
is desirable. This was achieved by Li et al. (2017) using
a burn-in phase corresponding to an initial number of
rounds during which the agent plays randomly. This
ensures that ˆθt stays in Θ for subsequent rounds and
therefore avoids the projection step. This technique
was re-used in other recent works, such as (Kveton
et al., 2020; Zhou et al., 2019). A major drawback
of this approach however is the length of this burn-in
phase, which typically grows with c−2
(Kveton et al.,
µ
2020, Section 4.5). In the previously cited example
of click-prediction, this would lead the agent to act
randomly for approximately 106 rounds.

2.3 Forgetting in Non-Stationary

Environments

Motivated by the non-stationary nature of most real-life
applications of contextual bandits, a consequent theory
for linear bandits in non-stationary environments has
been recently developed (Cheung et al., 2019a; Rus-
sac et al., 2019; Zhao et al., 2020). We focus here on
forgetting policies, a broader perspective is discussed
in Section 5. In (Cheung et al., 2019a), a sliding win-
dow is used and the estimator is constructed based on
the most recent observations only. In (Russac et al.,
2019) exponentially increasing weights are used to give
more importance to most recent observations. In (Zhao
et al., 2020) the algorithm is restarted on a regular ba-
sis. These contributions were generalized to GLB by
Russac et al. (2020); Cheung et al. (2019a); Zhao et al.
(2020). However, the approach of Russac et al. (2020)
still suﬀers from the aforementioned limitations (depen-
dency w.r.t. cµ and need for a projection step) while
the analysis of both Cheung et al. (2019a) and Zhao
et al. (2020) are missing key features of the problem at
hand (see (Russac et al., 2020, Section 1)).

The non-stationary nature of the problem rules out the
use of burning phases as changes in the GLB parameter
can lead ˆθt to leave Θ, even when well initialized. This
also accentuates the inconveniences brought by the pro-
jection step, as ˆθt leaving Θ is more likely to happen.
This is why ﬁnding alternatives without projection is
even more attractive in this particular setting. Further-
more, a generalization of the improvements brought by
Faury et al. (2020) to non-stationary world is missing,
and it is unclear if the dependency in cµ can still be
reduced in this harder setting.

2.4 Contributions

The present paper addresses these challenges, focusing
on the use of exponential weights to adapt to changes in
the model. First, we extend in Theorem 3 the Bernstein-
like tail-inequality of (Faury et al., 2020, Theorem 1)
to weighted self-normalized martingales. We then lever-
age the self-concordance property (Assumption 4) to
provide an improved characterization of the maximum-
likelihood estimator (Proposition 1). This allows to
provide concentration guarantees without projecting
ˆθt back to Θ. Combining these results leads to the
SC-D-GLUCB strategy (Algorithm 1), which does not re-
sort to a non-convex projection step and enjoys an
d2/3Γ1/3
˜O(c−1/3
T T 2/3) worst case regret upper bound
µ
(Theorem 2). A O(c−1/2
ΓT T ) regret bound is
also obtained (Theorem 1) under an additional minimal
gap ∆ > 0 assumption (Assumption 5). A summary of
our contributions and comparison with prior work are
given in Table 1.

µ ∆−1d

√

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Algorithm
GLM-UCB
Filippi et al. (2010)
LogUCB1
Faury et al. (2020)
D-GLUCB
Russac et al. (2020)
SC-D-GLUCB
(this paper)
SC-D-GLUCB
(this paper)

Setting
Stationary
GLM
Stationary
Logistic
Non-Stationary
GLM
Non-Stationary
GLM + SC + Ass. 5
Non-Stationary
GLM + SC

Projection

Regret Upper Bound

Non-convex

Non-convex

Non-convex

(cid:101)O

No projection

(cid:16)
µc−1
µc−1
c−1
µ · d ·

(cid:101)O

√

(cid:17)

T

(cid:16)
µc−1/2
c−1/2
µc−1/2
µ

(cid:101)O

· d ·

√

(cid:17)

(cid:16)
µ · d2/3 · Γ1/3
µc−1
µc−1
c−1
T
√
(cid:16)
µc−1/2
c−1/2
µc−1/2
µ

· d ·

(cid:101)O

T
· T 2/3(cid:17)
(cid:17)

ΓT T

No projection

(cid:16)
µc−1/3
c−1/3
µc−1/3
µ

(cid:101)O

· d2/3 · Γ1/3

T

· T 2/3(cid:17)

Table 1: Comparison of regret guarantees for diﬀerent algorithms in the GLM setting with respect to the degree
of non-linearity cµ, the dimension d, the horizon T and the number ΓT of abrupt changes. In the table SC stands
for self-concordant. Regret guarantees for SC-SW-GLUCB are the same than for SC-D-GLUCB.

3 ALGORITHM AND RESULTS

Algorithm 1 SC-D-GLUCB

3.1 Algorithms

In this section, we consider the abruptly changing en-
vironments deﬁned in Section 2. We propose two algo-
rithms: SC-D-GLUCB, which is based on discount factors,
and SC-SW-GLUCB using a sliding window. Due to space
limitation constraints, the pseudo-code of SC-SW-GLUCB
and the corresponding theoretical results are reported
in Appendix C. Associated with the weighed MLE de-
ﬁned in Equation (1), deﬁne the weighted design matrix
as

Input: Probability δ, dimension d, regularization
λ, upper bound for bandit parameters S, discount
factor γ.
Initialize: V0 = (λ/cµ)Id, ˆθ0 = 0Rd .
for t = 1 to T do

Receive At, compute ˆθt according to (1)
Play at = arg maxa∈At µ(a(cid:62) ˆθt)+ βδ
βδ
T
Receive reward rt+1
Update: Vt+1 ← ata(cid:62)

deﬁned in Equation (4)

t + γVt + λ
cµ

T√
cµ

(1 − γ)Id

(cid:107)a(cid:107)V−1

t

with

end for

Vt =

t−1
(cid:88)

s=1

γt−1−sasa(cid:62)

s +

λ
cµ

Id .

(3)

We detail in this section the performance guarantees
for SC-D-GLUCB. Deﬁne

3.2 Regret Upper Bounds

The SC-D-GLUCB algorithm proceeds as follows. First,
based on the previous rewards and actions, ˆθt is com-
puted. After receiving the action set At, the action at
is chosen optimistically as the maximizer of the cur-
rent estimate µ(a(cid:62) ˆθt) of each arm’s reward inﬂated
by the conﬁdence bonus c−1/2
. Finally, the
µ
reward rt+1 is received and the matrix Vt is updated.
The expression of βδ
is a consequence of our novel
T
concentration result and is deﬁned in Equation (4).
A pseudo-code of the algorithm is presented in Algo-
rithm 1.

βδ
T (cid:107)a(cid:107)V−1

t

(cid:32)

√

λ

1 + ¯S +

(cid:114)

βδ
T = kµ

1 + ¯S
λ

ρδ
T +

(cid:18) ρδ
T√
λ

(cid:19)2(cid:33)3/2

with

and where

ρδ
T =

+

√

λ
2m
dm
√
λ

¯S = S +

2Skµ + m
T λ(1 − γ)

,

log

+

2m
√
λ

(cid:18)

log

1 +

(cid:19)

+

(cid:18) T
δ

2m
√
λ
(cid:19)
kµ(1 − T −2)
dλ(1 − γ2)

.

d log(2)

(4)

(5)

There are two diﬀerences between SC-D-GLUCB and the
algorithm proposed in Russac et al. (2020). First, we
directly use ˆθt to make predictions about the arms’
performances, whether it belongs to Θ or not. Second,
the exploration term scales as c−1/2
),
as in Faury et al. (2020). The latter has a direct impact
on the regret-bound of SC-D-GLUCB, to be stated below.

(instead of c−1
µ

µ

The latter expression is a direct consequence of the
concentration result presented in Theorem 3 below.
The diﬀerence between ¯S and S is a bias term due to
the non-stationarity.

Before stating our ﬁrst theorem, we add an additional
assumption on the minimal gap. This assumption is
discussed in Section 5 and is only used in Theorem 1.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Assumption
mina∈At,µ(a(cid:62)θ(cid:63)
ﬁes

5. The

t )<µ(a(cid:62)

(cid:63) θ(cid:63)

t ) µ(a(cid:62)

t ) − µ(a(cid:62)θ(cid:63)
t )

reward
(cid:63) θ(cid:63)

gaps ∆t

=
satis-

where C1 and C2 are universal constants independent
of cµ and γ with only logarithmic terms in T .

∀t ≤ T, ∆t ≥ ∆ > 0 .

In particular, setting γ = 1 −

(cid:19)2/3

(cid:18) c1/2
µ ΓT
dT

and λ =

Theorem 1. Under Assumption 5, the regret of the
SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1)
with probability at least 1 − δ by

d log(T ) leads to

RT = (cid:101)O(cid:0)c−1/3

µ

d2/3Γ1/3

T T 2/3(cid:1) .

RT ≤ C1

+ C3

+ C4

ΓT
1 − γ
√

βδ
T
√

dT
cµ∆
d(βδ
T )2
cµ∆

+ C2

1
T (1 − γ)2∆

(cid:115)

(cid:18)

T log(1/γ) + log

1 +

(cid:19)

1
dλ(1 − γ)

(cid:16)

T log(1/γ) + log

(cid:16)

1 +

1
dλ(1 − γ)

(cid:17)(cid:17)

,

where C1, C2, C3, C4 are universal constants indepen-
dent of cµ, γ with only logarithmic terms in T .

In particular, setting γ = 1 −
leads to

RT = (cid:101)O(cid:0)∆−1c−1/2

µ

(cid:112)

d

ΓT T (cid:1) .

and λ = d log(T )

√

cµΓT
√
T
d

i

√

There is a strong link between the cost of non-
stationarity in the K-arm setting and the one observed
in the more general GLB setting. In the K-arm setting,
any sub-optimal arm i is played at most O(∆−2
log(T ))
times (e.g (Munos, 2014, Proposition 1.1)), whereas in
any abruptly changing environment, forgetting policies
play a sub-optimal arm i at most (cid:101)O((∆T (i))−2
ΓT T )
(Garivier and Moulines, 2011). ∆T (i) is the minimum
distance between the mean of the optimal arm and
the mean of the suboptimal arm i over the entire time
horizon. For GLBs, in the stationary case Filippi et al.
(2010, Theorem 1) give a gap-dependent bound on the
µ d2 log(T )). Here, the bound
regret scaling as O(∆−1c−2
of Theorem 1 is of order O(∆−1c−1/2
ΓT T ). The
d
reduced dependency in cµ in the latter bound is a di-
rect consequence of the use of self-concordance. Also
note that when the inverse link function is the identity
and the action set is the canonical basis, our analysis
recovers the results of Garivier and Moulines (2011).

√

µ

We give an upper bound for the worst case regret
of Algorithm 1 in the following theorem; its proof is
deferred to the appendix.

Theorem 2. The regret of the SC-D-GLUCB algorithm
is bounded for all γ ∈ (1/2, 1) with probability at least
1 − δ by

RT ≤ C1

+ C2

ΓT
1 − γ
√

βδ
T
√

dT
cµ

(cid:115)

T log

(cid:19)

(cid:18) 1
γ

(cid:18)

+ log

1 +

1
dλ(1 − γ)

(cid:19)

,

As in the linear case, this regret bound highlights the
existence of two mechanisms of diﬀerent nature. The
ﬁrst term is due to non-stationarity, the number of
changes ΓT being multiplied by 1/(1 − γ), which is a
rough measure of the forgetting time induced by the
exponential weights. The second term characterizes
the rate at which the weighted MLE ˆθt approaches
. By balancing both terms, we can characterize the
θ(cid:63)
t
asymptotic behavior of the regret bound.

In Theorem 2, optimally tuning γ yields the asymptotic
worst case rate of T 2/3. This is similar to the asymp-
totic rate achievable in the linear case with a diﬀerent
measure of non-stationarity (Russac et al., 2019) and
the same dependency is attained with a sliding window
for MDPs in abruptly changing environments (Gajane
et al., 2018) and with restart factors (Auer et al., 2008).

√

√

1 + 2S(

Remark 1. The proof of Theorem 2 reveals that
for rounds t where ˆθt lies in Θ, it is possible to ob-
tain a (usually) tighter concentration result (depend-
ing on the values of λ and S) by replacing βδ
T with
kµ
T ). This cannot be used to im-
prove the result of Theorem 2, as one doesn’t know in
advance for which rounds the condition will be satisﬁed,
but this minor modiﬁcation of Algorithm 1 is most often
advisable in practice. See Section B.4 in Appendix for
more details.

λS + ρδ

4 KEY ARGUMENTS

In this section, we detail some key elements of our
analysis. First, we describe the concentration result in
its most generic form. Then, we explain the main steps
to derive the upper bound of the regret of SC-D-GLUCB.

4.1 A Tail-Inequality for Self-Normalized

Weighted Martingales

To reduce the dependency in cµ, it is essential to take
into account the actual conditional variance of the
generalized linear model (Faury et al., 2020). With
exponentially increasing weights, we also need time-
dependent regularization parameters to avoid a van-
ishing eﬀect of the regularization (Russac et al., 2019).
Carefully combining these two elements yields the fol-
lowing concentration result.

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Theorem 3. Let t be a ﬁxed time instant. Let {Fu}t
u=1
be a ﬁltration. Let {au}t
u=1 be a stochastic process on
Rd such that au is Fu measurable and (cid:107)au(cid:107)2 ≤ 1. Let
{(cid:15)u}t
u=2 be a martingale diﬀerence sequence such that
(cid:15)u+1 is Fu+1 measurable. Assume that the weights are
non-decreasing, strictly positive and the time horizon
is known. Furthermore, assume that conditionally on
Fu we have |(cid:15)u+1| ≤ m a.s. Let {λu}t
u=1 be a deter-
ministic sequence of regularization terms and denote
t = E (cid:2)(cid:15)2
(cid:3).
σ2
Let (cid:101)Ht = (cid:80)t−1
(cid:80)t−1

s + λt−1Id and St =

s=1 w2

s asa(cid:62)

t+1|Ft

sσ2

s=1 ws(cid:15)s+1as, then for any δ ∈ (0, 1],
(cid:32)

+

2mwt−1
(cid:112)λt−1

log

(cid:33)

det( (cid:101)Ht)1/2
δλd/2
t

(cid:107)St(cid:107)

(cid:101)H−1
t

≥

+

(cid:112)λt−1
2mwt−1
2mwt−1
(cid:112)λt−1

d log(2)

with probability smaller than δ.

4.2 Upper Bounding the Regret of SC-D-GLUCB

In a non-stationary environment, each change in the
parameter will necessarily result in a number of rounds
where the bias of the weighted MLE estimator cannot
be controlled. This gives rise to the ﬁrst term in the
upper bound in Theorem 2. To make this observation
more explicit, for D ≥ 1, deﬁne T (γ) = {1 ≤ t ≤
for t − D ≤ s ≤ t − 1} the set of
T, such that θ(cid:63)
time instants that are at least D steps away from the
previous closest breakpoint. Central in the analysis of
weighted GLBs is the matrix

s = θ(cid:63)
t

Gt(ˆθt, θ(cid:63)

t ) =

t−1
(cid:88)

s=1

where

γt−1−sα(as, ˆθt, θ(cid:63)

t )asa(cid:62)

s + λId,

α(as, ˆθt, θ(cid:63)

t ) =

(cid:90) 1

0

˙µ(a(cid:62)

s ((1 − v)θ(cid:63)

t + v ˆθt))dv.

As in the linear case, we deﬁne its analogue with
squared exponential weights,

(cid:101)Gt(ˆθt, θ(cid:63)

t ) =

t−1
(cid:88)

s=1

γ2(t−1−s)α(as, ˆθt, θ(cid:63)

t )asa(cid:62)

s + λId .

We add the subscript t − D : t to a quantity when the
sum is for time instants between t − D and t − 1. In
this subsection, for space constraints, we will denote
equivalently (cid:101)Gt(ˆθt, θ(cid:63)
t )) by (cid:101)Gt (resp.
Gt). As for linear bandits, the exploration bonus is
designed to mitigate the impact of prediction errors.
We focus below on upper bounding the prediction error
in ˆθt deﬁned as ∆t(a, ˆθt) = |µ(a(cid:62) ˆθt) − µ(a(cid:62)θ(cid:63)
t )|. The

t ) (resp. Gt(ˆθt, θ(cid:63)

exact link between the regret and this quantity is made
explicit in Proposition 9 in the appendix. By deﬁning
gt(θ) = (cid:80)t−1
s θ)as + λθ, when t ∈ T (γ)
one can upper bound the prediction error in ˆθt.

s=t−D γt−1−sµ(a(cid:62)

∆t(a, ˆθt) ≤

cγD
1 − γ

+ kµ (cid:107)gt(ˆθt) − gt(θ(cid:63)

t )(cid:107)

(cid:124)

(cid:123)(cid:122)
1

(cid:101)G−1

t−D:t
(cid:125)

(cid:107)a(cid:107)G−1
(cid:124) (cid:123)(cid:122) (cid:125)
2

t

The ﬁrst term corresponds to the bias due to non-
stationarity. 1 is a measure of the deviation of ˆθt from
adapted to the non-linear nature of the problem.
θ(cid:63)
t
Note that gt(ˆθt) − gt(θ(cid:63)
t ) involves a martingale diﬀer-
ence sequence (thanks to the optimality condition of the
MLE) that can be controlled using Theorem 3. How-
ever, to bound 1 using Theorem 3 one needs to link
the matrix (cid:101)Gt−D:t with (cid:101)Ht−D:t , the self-concordance
allows exactly to do this.

Self-Concordance More precisely, the use of self-
concordance oﬀers a sharp relation (independent of
cµ) between the ﬁrst derivative of the mean function
evaluated at diﬀerent points. Using Lemma 4 reported
in Appendix D, standard calculations yield:

1
√
λ

(cid:107)gt(ˆθt) − gt(θ(cid:63)

(cid:101)Gt−D:t ≥ (cid:0)1 + C +

(cid:101)Ht−D:t
(6)
Note that Equation (6) involves the deviation term
that we want to control. Here, C is a residual bias due
to the non-stationarity of the environment.

t )(cid:107)

(cid:101)G−1

t−D:t

(cid:1)

Better Characterization of the MLE By leverag-
ing Equation (6) to bound the deviation gt(ˆθt) − gt(θ(cid:63)
t )
in the (cid:101)G−1
-norm, one obtains an implicit equation.
Solving it leads to the following proposition.

t−D:t

Proposition 1. When t ∈ T (γ), the following holds,

(cid:107)gt(ˆθt)−gt(θ(cid:63)

t )(cid:107)

(cid:101)G−1

t−D:t(ˆθt,θ(cid:63)

t ) ≤

√

1 + Cρδ

T +

1
√
λ

(cid:1)2

(cid:0)ρδ

T

,

where C is a residual term due to non-stationarity.

Remark. In stark contrast with previously existing
works (see (Filippi et al., 2010, Proposition 1)), de-
viations from the true parameter θ(cid:63)
t are characterized
uniquely by the MLE (and not by its projected counter-
part). This can be done whether ˆθt belongs to Θ or not
and without any projection. This is not speciﬁc to the
non-stationary nature of the problem but fundamentally
relies on an improved analysis of the MLE. Similar
guarantees can be obtained in any stationary environ-
ment. See Section 5 for a more detailed comparison of
the possible uses of the self-concordance property.

1 can be upper bounded using Proposition 1. To

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

upper bound 2 we use the following inequality.

(cid:18)

(cid:19)−1

1
√
λ

t−D:t

(cid:101)G−1

t )(cid:107)

(cid:107)gt(ˆθt) − gt(θ(cid:63)

Gt ≥

1 + C +

cµVt .
(7)
Combining Proposition 1 with Equation (7) gives the
upper bound for 2 . Putting everything together, we
obtain the form of βδ
given in Equation (4). The regret
T
bound is then obtained by summing the exploration
bonus for the diﬀerent time instants. Applying the so-
called elliptical lemma (see (Lattimore and Szepesvári,
2019, Chap. 19)) and letting D = log(T )/ log(1/γ)
completes the proof.

5 DISCUSSION

√

Assumption on the Gaps. Assumptions similar to
our Assumption 5 requiring a minimum gap are fre-
quent in non-stationary bandits. First, note that ∆ is
not required for the algorithm but only for the theoreti-
cal analysis. Second, similar assumptions can be found
for K-arm bandits in several works to obtain the opti-
mal (cid:101)O(
ΓT T ) regret bound. This is in particular the
case for change-points detection methods: (Cao et al.,
2019, Corollary 1) and (Zhou et al., 2020, Corollary
4.3) is proved under an assumption on the minimal gap.
This remains true for forgetting strategies: the bound of
Garivier and Moulines (2011) is gap-dependent, Trovo
et al. (2020) achieve a O(∆−1
T ΓT ) regret. More
demanding, the LM-DSEE and SW-UCB# algorithms
from Wei and Srivatsva (2018) require the minimum
gap as an input of the algorithm. Generally speak-
ing, none of those works provide an analysis when the
minimum gap can depend on the time horizon T and
when the mean of diﬀerent arms can be arbitrarily
close. We suspect that forgetting policies would obtain
a O(Γ1/3
T T 2/3) worst case dependency as in Theorem 2
and that changepoint detection methods are likely to
fail in such a case.

√

Tightness of the Bound. For problems with a ﬁ-
nite number of actions, Auer et al. (2018) have devel-
oped an algorithm that does not require the knowledge
of the number of breakpoints nor assumption on the
gaps. This was extended to the K-arm setting by
Auer et al. (2019) and to the more general contex-
tual bandits by Chen et al. (2019). Both works (Auer
et al. (2019); Chen et al. (2019)) achieve the optimal
ΓT T ) regret bound. Yet, their analysis does not
(cid:101)O(
apply to the GLB framework. Furthermore, both works
rely on replaying phases that are incompatible with
time-dependent action sets as considered here. Addi-
tionally, in (Chen et al., 2019) the regret is deﬁned
with respect to the best policy in some ﬁnite class,
whereas our results apply to the general setting where

√

actions can change over time and the regret bench-
mark is the ground-truth of the environment. The best
lower-bound for forgetting policies in abruptly changing
environments with time-dependent action sets remains
unknown. While it is known that forgetting policies are
minimax optimal when non-stationarity is measured
through the so-called variational budget (see Cheung
et al. (2019b); Russac et al. (2019)), whether such meth-
ods are optimal in abruptly changing environments is
unclear. Nonetheless, the bound obtained by Gariv-
ier and Moulines (2011) in the K-arm setting yields
a worst case regret bound that can be shown to be of
order O(Γ1/3

T T 2/3) (see Appendix E).

Knowledge of ΓT Optimizing the choice of the
forgetting parameter γ (w.r.t. the regret bound) re-
quires the knowledge of ΓT . The Bandit over Ban-
dit (BOB) framework introduced by Cheung et al.
(2019b) can be used to circumvent this requirement.
When the assumption 5 is satisﬁed, following the proof
from Cheung et al. (2019a) one would obtain a re-
(cid:112)T max(ΓT , T 1/2))
gret bound of order (cid:101)O(∆−1dc−1/2
(see (Auer et al., 2019, Remark 2)). Similarly, in
the absence of Assumption 5 an upper bound of or-
der (cid:101)O(c−1/3
d2/3T 2/3 max(ΓT , d−1/2T 1/4)1/3) can be
achieved (see (Zhao et al., 2020, Theorem 4)).

µ

µ

Self-Concordance The analysis of Faury et al.
(2020) does not use self-concordance to its fullest. We
present an improved analysis valid in any stationary
time frame, proving that a better treatment of the self-
concordance removes the need for the inconvenient pro-
jection. Informally, the self-concordance links µ(x(cid:62) ˆθt)
to µ(x(cid:62)θ(cid:63)) without resorting to global bounds on ˙µ
(e.g kµ and cµ). In Faury et al. (2020), this takes the
form of a Taylor-like expansion:

µ(x(cid:62)θt) ≤ µ(x(cid:62)θ(cid:63)) +

|x(cid:62)(θ(cid:63) − θt)|
1 + 2S

˙µ(x(cid:62)θ(cid:63)) ,

where θt is a projected version of ˆθt in Θ. The denomi-
nator of the r.h.s. is reminiscent of this projection step.
We show here that a ﬁner analysis yields the following,
more implicit but powerful bound:

µ(x(cid:62) ˆθt) ≤ µ(x(cid:62)θ(cid:63)) +

|x(cid:62)(θ(cid:63) − ˆθt)|
1 + |x(cid:62)(θ(cid:63) − ˆθt)|

˙µ(x(cid:62)θ(cid:63)) .

Note that when ˆθt ∈ Θ (i.e there is no need for a projec-
tion), our bound implies the one of Faury et al. (2020).
The kind of relationship displayed in the above equation
allows us to derive a tail inequality for the deviation
from ˆθt to θ(cid:63) without projecting ˆθt, by solving an im-
plicit equation. We believe that this new approach is
of interest in other settings involving self-concordant

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

(a) c−1

µ = 400

(b) c−1

µ = 1000

Figure 1: Regret of the diﬀerent algorithms in a 2D abruptly changing environment averaged on 200 independent
experiments and the 25% associated quantiles.

GLBs. The self-concordance assumption (Assumption
4) is not particularly restrictive and goes beyond logis-
tic functions. Under the classical Assumption 1 (i.e.
bounded features) all GLMs are self-concordant (cf.
Sec. 2 of Bach (2014)) with constants that depend on
the link function.

6 EXPERIMENTS

In this section, we illustrate the empirical perfor-
mance of SC-D-GLUCB in a simulated, abruptly chang-
ing environment with a logistic link function µ(x) =
In this two-dimensional problem,
1/(1 + exp(−x)).
there is a switch in the reward distribution at t = 4000
(red dashed line on Figure 1).

SC-D-GLUCB (Algorithm 1) is compared with GLM-UCB
from Filippi et al. (2010), LogUCB1 from Faury et al.
(2020) and with D-GLUCB from Russac et al. (2020).
SC-D-GLUCB (resp. D-GLUCB) is related with LogUCB1
(resp. GLM-UCB) in the sense that the exploration terms
have the same scaling but the former incorporate the
exponential weights making it possible to adapt to
changes. The average regret of the diﬀerent policies
together with their central 50% quantiles, averaged on
200 independent runs, are reported in Figure 1 for two
diﬀerent parameter values.

In Fig. 1a, θ(cid:63) starts on the circle of radius S = 6
µ = exp(S) ≈ 400) with an angle
(corresponding to c−1
of 2π/3 and jumps at t = 4000 to an angle of 4π/3.
The experiment reported on Fig. 1b is identical with
a radius S = 7 corresponding to a c−1
µ ≈ 1000. As
previously discussed, using such values of S is required
in situation where the actions return binary rewards

with expected values in the range 10−3 – 10−2, which is
typically the case in web advertising or recommendation
applications.

For both experiments, at every time steps, 50 randomly
generated actions in the unit circle are proposed to the
learner. For SC-D-GLUCB and D-GLUCB the asymptot-
ically optimal choice of the discount factors is used:
γ = 1 − (ΓT /(d × T ))2/3 with d = 2, ΓT = 2 and
T = 8000. To speed up the learning that is hard
with those values of cµ, all the algorithms have their
exploration bonus divided by 5.

As expected, the algorithms tuned for non stationary
situations (SC-D-GLUCB, D-GLUCB) perform worse than
their stationary counterparts (LogUCB1 and GLM-UCB)
during the ﬁrst stationary phase. More precisely, with
the choice made for γ the estimation of ˆθt for algorithms
that use exponential weights is roughly based on the
1/(1 − γ) ≈ 400 most recent observations. In contrast,
LogUCB1 and GLM-UCB use all the observations from
the start to compute the MLE, which eventually leads
to a more precise estimation. Right after the change,
the bias caused by the non-stationarity results in a
signiﬁcant increase in regret. Unweighted algorithms
are aﬀected much more deeply by this phenomenon
that will eventually cause large losses in performance
due to the persistence of obsolete information.

The theoretical analysis of Section 3.2 suggests that
the advantage of SC-D-GLUCB is all the more signiﬁcant
in strongly non-linear (large c−1
) non-stationary en-
µ
vironments. This is obvious in Figure 1, particularly
when comparing Fig. 1a and Fig. 1b, which diﬀer by
the range on which the logistic function is used for

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

making reward predictions. Note that, on average, for
these two simulated scenarios the fact that the MLE ˆθt
does not belong to Θ happens for several hundred of
rounds. All the algorithms except SC-D-GLUCB would
require non convex projection steps at these instants,
or equivalently, one should inﬂate S (and thus c−1
) to
µ
ensure the compliance of these algorithms with the as-
sociated theory. In producing Figure 1, this projection
step was simply bypassed, which provides an optimistic
evaluation of the performance of the competitors of
SC-D-GLUCB. Interestingly, the observation that the dis-
persion of performance of SC-D-GLUCB is slightly higher
than that of D-GLUCB can be traced back to the use
of Remark 1 in these simulations: SC-D-GLUCB adapts
to the events {ˆθt /∈ Θ} (rather than pretending that
these did not happen) and thus its performance is made
somewhat dependent on the actual occurrence of these
events.

7 CONCLUSION

In this paper, we design GLB algorithms for piece-
wise stationary environments by resorting to forgetting
mechanisms. We improve existing solutions by circum-
venting important drawbacks aﬀecting their applicabil-
ity in real-life scenarios. More precisely, under a generic
self-concordance assumption, we remove the need for
burdensome non-convex projections and leverage re-
ﬁned and exponentially deﬂated conﬁdence regions. At
the heart of our approach are a reﬁned characterization
of the maximum-likelihood estimator and an exten-
sion of a Bernstein-like tail inequality to weighted self-
normalized martingales. We believe that both can be
of independent interest and leveraged in other settings
(e.g drifting environments). We can see two natural
extensions of our work; the ﬁrst involves achieving sim-
ilar success for other nature of non-stationarity such
as drifting environments. The second could try to
replicate the recent progress of Abeille et al. (2020) in
the stationary logistic case, and provide minimax rates
w.r.t cµ in non-stationary self-concordant GLBs.

References

Yasin Abbasi-Yadkori, Dávid Pál,

and Csaba
Szepesvári. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Process-
ing Systems, NeurIPS 2011, pages 2312–2320, 2011.
Marc Abeille, Louis Faury, and Clément Calauzènes.
Instance-wise minimax-optimal algorithms for logis-
tic bandits. arXiv preprint arXiv:2010.12642, 2020.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-
optimal regret bounds for reinforcement learning. In
Advances in neural information processing systems,
NeurIPS, pages 89–96, 2008.

Peter Auer, Pratik Gajane, and Ronald Ortner. Adap-
tively tracking the best arm with an unknown number
of distribution changes. In European Workshop on
Reinforcement Learning, EWRL 2018, 2018.

Peter Auer, Pratik Gajane, and Ronald Ortner. Adap-
tively tracking the best bandit arm with an unknown
number of distribution changes. In Conference on
Learning Theory, COLT 2019, pages 138–158, 2019.
Francis Bach. Self-concordant analysis for logistic re-
gression. Electron. J. Statist., 4:384–414, 2010. doi:
10.1214/09-EJS521.

Francis Bach. Adaptivity of averaged stochastic gra-
dient descent to local strong convexity for logistic
regression. Journal of Machine Learning Research,
15(19):595–627, 2014.

Francis Bach and Eric Moulines. Non-strongly-convex
smooth stochastic approximation with convergence
rate o (1/n). In Advances in Neural Information
Processing Systems, NeurIPS 2013, pages 773–781,
2013.

S.P. Boyd and L. Vandenberghe. Convex optimization.

Cambridge Univ Press, 2004.

Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie.
Nearly optimal adaptive procedure with change de-
tection for piecewise-stationary bandit. Proceedings
of the 22nd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2019, 2019.
O. Chapelle and L. Li. An empirical evaluation of
Thompson Sampling. In Advances in Neural Infor-
mation Processing Systems, NeurIPS 2011, 2011.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-
Yu Wei. A new algorithm for non-stationary contex-
tual bandits: Eﬃcient, optimal, and parameter-free.
Proceedings of the 32nd Conference on Learning The-
ory, COLT 2019, 2019.

Wang Chi Cheung, David Simchi-Levi, and Ruihao
Zhu. Hedging the drift: Learning to optimize under
non-stationarity. arXiv preprint arXiv:1903.01461,
2019a.

Wang Chi Cheung, David Simchi-Levi, and Ruihao
Zhu. Learning to optimize under non-stationarity.
In Proceedings of the 22nd International Conference
on Artiﬁcial Intelligence and Statistics, AISTATS
2019, 2019b.

Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire.
Contextual bandits with linear payoﬀ functions. In
Proceedings of the Fourteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2011, pages 208–214, 2011.

Varsha Dani, Thomas P Hayes, and Sham M Kakade.
Stochastic linear optimization under bandit feed-

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Yoan Russac, Olivier Cappé, and Aurélien Garivier.
Algorithms for non-stationary generalized linear ban-
dits. arXiv preprint arXiv:2003.10113, 2020.

Ambuj Tewari and Susan A Murphy. From ads to
interventions: Contextual bandits in mobile health.
In Mobile Health, pages 495–517. Springer, 2017.
Francesco Trovo, Stefano Paladino, Marcello Restelli,
and Nicola Gatti. Sliding-window thompson sam-
pling for non-stationary settings. Journal of Artiﬁcial
Intelligence Research, 68:311–364, 2020.

Michal Valko, Rémi Munos, Branislav Kveton, and
Tomáš Kocák. Spectral bandits for smooth graph
functions. In Proceedings of the 31st International
Conference on Machine Learning, ICML 2014, pages
46–54, 2014.

Lai Wei and Vaibhav Srivatsva. On abruptly-changing
and slowly-varying multiarmed bandit problems. In
2018 Annual American Control Conference (ACC),
pages 6291–6296. IEEE, 2018.

Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua
Zhou. A simple approach for non-stationary linear
bandits. In Proceedings of the 23rd International
Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2020, 2020.

Huozhi Zhou, Lingda Wang, Lav R Varshney, and Ee-
Peng Lim. A near-optimal change-detection based al-
gorithm for piecewise-stationary combinatorial semi-
bandits. AAAI, 2020.

Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet.
Learning in generalized linear contextual bandits
with stochastic delays. In Advances in Neural Infor-
mation Processing Systems, NeurIPS 2019, 2019.

back. In 21st Annual Conference on Learning Theory,
COLT 2008, pages 355–366, 2008.

Louis Faury, Marc Abeille, Clément Calauzènes, and
Olivier Fercoq. Improved optimistic algorithms for
logistic bandits. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020,
2020.

Sarah Filippi, Olivier Cappé, Aurélien Garivier, and
Csaba Szepesvári. Parametric bandits: The general-
ized linear case. In Advances in Neural Information
Processing Systems, NeurIPS 2010, pages 586–594,
2010.

Arthur Flajolet and Patrick Jaillet. Real-time bidding
with side information. In Advances in Neural In-
formation Processing Systems, NeurIPS 2017, pages
5168–5178, 2017.

Pratik Gajane, Ronald Ortner, and Peter Auer. A
sliding-window algorithm for markov decision pro-
cesses with arbitrarily changing rewards and transi-
tions. arXiv preprint arXiv:1805.10066, 2018.

Aurélien Garivier and Eric Moulines. On upper-
conﬁdence bound policies for switching bandit prob-
lems. In International Conference on Algorithmic
Learning Theory, ALT 2011, pages 174–188, 2011.
Branislav Kveton, Manzil Zaheer, Csaba Szepesvari,
Lihong Li, Mohammad Ghavamzadeh, and Craig
Boutilier. Randomized exploration in generalized
linear bandits. Proceedings of the 23nd International
Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2020, 2020.

Tor Lattimore and Csaba Szepesvári. Bandit Algo-

rithms. Cambridge University Press, 2019.

Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. A contextual-bandit approach to personal-
ized news article recommendation. In WWW, 2010.
Lihong Li, Yu Lu, and Dengyong Zhou. Provably
optimal algorithms for generalized linear contextual
bandits.
In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, pages
2071–2080, 2017.

Rémi Munos. From bandits to Monte-Carlo Tree Search:
The optimistic principle applied to optimization and
planning. 2014.

Paat Rusmevichientong and John N Tsitsiklis. Linearly
parameterized bandits. Mathematics of Operations
Research, pages 395–411, 2010.

Yoan Russac, Claire Vernade, and Olivier Cappé.
Weighted linear bandits for non-stationary environ-
ments. In Advances in Neural Information Processing
Systems, NeurIPS 2019, pages 12017–12026, 2019.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Self-Concordant Analysis of Generalized Linear Bandits with
Forgetting: Supplementary Material

The Appendix is structured as follows. In Section A, our new concentration result for self-normalized weighted
martingales with time dependent regularization parameters is presented. In Section A.3, similar concentration
results are established when a sliding window is used. Section B studies the regret with discount factors through
our improved characterization of the MLE. Section C gives similar results with a sliding window. Section D
gathers some technical results, in particular the main properties resulting from the self-concordance assumption.
Finally in Section E, a worst case bound for a sliding window policy in the K-arm setting is presented.

A TAIL-INEQUALITY FOR SELF-NORMALIZED WEIGHTED

MARTINGALES

While keeping in mind our objective of obtaining a deviation inequality with exponentially increasing weights, we
give more generic results under two assumptions on the weights.

Assumption 6. The time horizon T is known in advance.

Assumption 7. The weights are deterministic, strictly positive and non-decreasing, i.e,

We recall the statement of the corresponding concentration result.

∀1 ≤ t ≤ T, 0 < w1 ≤ wt ≤ wt+1 ≤ wT .

Theorem 3. Let t be a ﬁxed time instant. Let {Fu}t
u=1 be a stochastic process on
Rd such that au is Fu measurable and (cid:107)au(cid:107)2 ≤ 1. Let {(cid:15)u}t
u=2 be a martingale diﬀerence sequence such that (cid:15)u+1
is Fu+1 measurable. Assume that the weights are non-decreasing, strictly positive and the time horizon is known.
Furthermore, assume that conditionally on Fu we have |(cid:15)u+1| ≤ m a.s. Let {λu}t
u=1 be a deterministic sequence
t = E (cid:2)(cid:15)2
of regularization terms and denote σ2
Let (cid:101)Ht = (cid:80)t−1

(cid:3).
s + λt−1Id and St = (cid:80)t−1

u=1 be a ﬁltration. Let {au}t

s asa(cid:62)

t+1|Ft

sσ2

s=1 w2

s=1 ws(cid:15)s+1as, then for any δ ∈ (0, 1],
(cid:32)

(cid:33)

+

2mwt−1
(cid:112)λt−1

log

det( (cid:101)Ht)1/2
δλd/2
t

(cid:107)St(cid:107)

(cid:101)H−1
t

≥

+

(cid:112)λt−1
2mwt−1
2mwt−1
(cid:112)λt−1

d log(2)

with probability smaller than δ.

Theorem 3 is a non-trivial extension of Faury et al. (2020, Theorem 1) allowing for the use of time-dependent
regularization parameters and weights. We now state several lemmas that are useful for establishing Theorem 3.

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

A.1 Useful Lemmas

As a ﬁrst step we ﬁx a time instant t. Let M t

u(ξ) for ξ ∈ Rd and 1 ≤ u ≤ t be deﬁned as

M t

u(ξ) = exp

(cid:18) 1

mwt−1

ξ(cid:62)Su −

1
m2w2

t−1

ξ(cid:62) (cid:101)Hu(0)ξ

(cid:19)

,

(8)

with Su = (cid:80)u−1
We prefer the notation M t
u
the notation Mt to M t
t

s=1 ws(cid:15)s+1as and (cid:101)Hu(0) = (cid:80)u−1

s = E[(cid:15)2
to Mu to clearly indicate the dependency on the weight wt−1. When u = t, we prefer

s+1|Fs].

where σ2

s=1 w2

s asa(cid:62)
s

sσ2

. For the entire appendix, we use the notation B2(d) = {a ∈ Rd, (cid:107)a(cid:107)2 ≤ 1}.

Lemma 1. For all ξ ∈ B2(d) and 2 ≤ u ≤ t, under Assumption 6 and 7, we have

E (cid:2)M t

u(ξ)|Fu−1

(cid:3) ≤ M t

u−1(ξ) . a.s

Proof.

E (cid:2)M t

u(ξ)|Fu−1

(cid:3) = M t

u−1(ξ) exp

(cid:18)

−

1
m2w2

t−1

ξ(cid:62)w2

u−1σ2

u−1au−1a(cid:62)

u−1ξ

(cid:19)

(cid:20)

exp

× E

(cid:18) 1

mwt−1

ξ(cid:62)wu−1(cid:15)uau−1

(cid:19)

(cid:21)

|Fu−1

.

The equality holds because au−1 is Fu−1 measurable and (cid:15)u−1 is Fu−1 measurable. With ˜(cid:15)u = (cid:15)u/m and
v = wu−1
wt−1

ξ(cid:62)au−1, the conditions of Lemma 3 (stated below) are met and we have,

(cid:20)

exp

E

(cid:18) 1

mwt−1

ξ(cid:62)wu−1(cid:15)uau−1

(cid:19)

(cid:21)

|Fu−1

= E [exp(v˜(cid:15)u)|Fu−1] ≤ 1 +

v2
m2 σ2

u−1 .

|v| ≤ 1 holds because of Assumption 7 and both ξ and au−1 ∈ B2(d). Therefore,

E (cid:2)M t

u(ξ)|Fu−1

(cid:3) ≤ M t

u−1(ξ) exp

(cid:18)

−

1
m2w2

t−1

ξ(cid:62)w2

u−1σ2

u−1au−1a(cid:62)

u−1ξ

(cid:19)

where the last inequality uses 1 + x ≤ exp(x).

≤ M t

u−1(ξ)

(cid:18)

×

1 +

u−1ξ(cid:62)au−1a(cid:62)
σ2

u−1ξ

(cid:19)

w2
u−1
m2w2

t−1
(a.s) ,

Hence, for all 1 ≤ u ≤ t and ξ ∈ B2(d), E [Mt(ξ)] ≤ E [M t
For 1 ≤ u ≤ t we deﬁne,

u(ξ)] ≤ E [M t

1(ξ)] = 1.

¯M t

u =

(cid:90)

ξ

M t

u(ξ)dhu(ξ) .

(9)

Here, hu is the density of an isotropic normal distribution of precision 2λu−1
m2w2
N (hu) its normalization constant.
Lemma 2. Let t be a ﬁxed time instant, for all 1 ≤ u ≤ t, under assumptions 6 and 7, with {hu}t
of an isotropic normal distribution of precision 2λu−1
m2w2

truncated on B2(d) we have,

t−1

t−1

truncated on B2(d). We will denote

u=1 the density

E (cid:2) ¯M t

u

(cid:3) ≤ 1 .

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Proof.

(cid:19)

M t

u(ξ)dhu(ξ)

dP(w)

dhu(ξ)

(Fubini)

(cid:90)

Ω

(cid:90)

¯M t

udP(w) =
(cid:18)(cid:90)

(cid:90)

(cid:18)(cid:90)

Ω

Rd
(cid:19)

Rd

(cid:90)

Ω

(cid:18)(cid:90)

M t

u(ξ)dP(w)
(cid:19)

1dP(w)

E (cid:2) ¯M t

u

(cid:3) =

≤

≤

≤

Rd

(cid:90)

Rd

dhu(ξ)

(Lemma 1 + hu deﬁned on B2(d))

Ω

dhu(ξ) = 1 .

(hu is a probability density function)

Remark 2. Allowing time-dependent regularization parameters is essential in our analysis to avoid the vanishing
eﬀect of the regularization with exponentially increasing weights for example. This is a fundamental diﬀerence
with the deviation result provided in Faury et al. (2020). Furthermore, allowing the regularization parameters
to be time-dependent comes at a cost here, we loose the property E (cid:2) ¯M t
u−1 that would hold with a
ﬁxed regularization parameter (as in Faury et al. (2020)). In the linear bandit setting, this issue was discussed in
Lemma 2 in Russac et al. (2019).

(cid:3) ≤ ¯M t

u|Fu−1

In particular, applying Lemma 2 for u = t gives,

E (cid:2) ¯Mt

(cid:3) = E (cid:2) ¯M t

t

(cid:3) ≤ 1 .

(10)

Lemma 3 (Lemma 7 of Faury et al. (2020)). Let ε be a centered random variable of variance σ2 and such that
|ε| ≤ 1 almost surely. Then for all v ∈ [−1, 1],

E [exp(vε)] ≤ 1 + v2σ2 .

Remark 3. We stress out that v ∈ [−1, 1] is required for Lemma 3 to hold. It has strong consequences in our
setting with the weights as the normalization 1/wt−1 and 1/w2
u are needed to ensure
that v = (wu−1/wt−1)ξ(cid:62)au that appears in the proof of Lemma 1 will be smaller than 1. As a consequence, the
stopping trick presented in Abbasi-Yadkori et al. (2011) can not be applied to ¯M t
u because of its dependency on t.
For this reason, the deviation result presented in Theorem 3 is only valid for a ﬁxed time instant t. To obtain a
deviation result on the entire trajectory an union bound is required.

t−1 in the deﬁnition of M t

A.2 Proof of Theorem 3

The proof of this theorem follows the line of proof of Faury et al. (2020). The main diﬀerences are the time-
dependent regularization parameters and the presence of weights. We recall that in Equation (9) ht is the density
of an isotropic normal distribution of precision 2λt−1
truncated on B2(d) and denote N (ht) its normalization
m2w2
constant.

t−1

The following holds,

¯Mt =

1
N (ht)

(cid:90)

Rd

1 [ξ ∈ B2(d)] exp

(cid:18) 1

mwt−1

ξ(cid:62)St −

1
m2w2

t−1

(cid:19)

dξ .

ξ(cid:62) (cid:101)Htξ

(11)

Let ft : Rd (cid:55)→ R be deﬁned as ft(ξ) = 1
for ξ(cid:63) = arg max(cid:107)ξ(cid:107)2≤1/2 ft(ξ),

mwt−1

ξ(cid:62)St −

1

m2w2

t−1

ξ(cid:62) (cid:101)Htξ. As a quadratic function, ft can be rewritten

ft(ξ) = ft(ξ(cid:63)) + ∇ft(ξ(cid:63))(cid:62)(ξ − ξ(cid:63)) +

1
2

(ξ − ξ(cid:63))(cid:62)∇2ft(ξ(cid:63))(ξ − ξ(cid:63)) .

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Using ∀ξ ∈ B2(d), ∇2ft(ξ) = − 2
m2w2

t−1

(cid:101)Ht,

¯Mt =

=

≥

≥

(cid:90)

(cid:90)

Rd

eft(ξ(cid:63))
N (ht)
eft(ξ(cid:63))
N (ht)
eft(ξ(cid:63))
N (ht)
eft(ξ(cid:63))N (gt)
N (ht)

Rd

Rd

(cid:90)

1 [(cid:107)ξ(cid:107)2 ≤ 1] exp

(cid:18)

∇ft(ξ(cid:63))(cid:62)(ξ − ξ(cid:63)) −

1
m2w2

t−1

(cid:107)ξ − ξ(cid:63)(cid:107)2
(cid:101)Ht

(cid:19)

dξ

1 [(cid:107)ξ + ξ(cid:63)(cid:107)2 ≤ 1] exp

1 [(cid:107)ξ(cid:107)2 ≤ 1/2] exp

(cid:18)

∇ft(ξ(cid:63))(cid:62)ξ −

1
m2w2

t−1

(cid:107)ξ(cid:107)2
(cid:101)Ht

(cid:19)

dξ

(cid:18)

∇ft(ξ(cid:63))(cid:62)ξ −

1
m2w2

t−1

(cid:107)ξ(cid:107)2
(cid:101)Ht

(cid:19)

dξ

Eξ∼gt

(cid:2)exp (cid:0)∇ft(ξ(cid:63))(cid:62)ξ(cid:1)(cid:3) .

The second equality is obtained after a change of variable ξ (cid:55)→ ξ − ξ(cid:63). In the last inequality, gt is the density of a
d-dimensional normal distribution with precision matrix

(cid:101)Ht truncated on {a ∈ Rd, (cid:107)a(cid:107)2 ≤ 1/2}.

2

m2w2

t−1

¯Mt ≥

eft(ξ(cid:63))N (gt)
N (ht)

exp (cid:0)Eξ∼gt

(cid:2)∇ft(ξ(cid:63))(cid:62)ξ(cid:3)(cid:1) .

(Jensen’s inequality)

gt is symmetric which implies Eξ∼gt [ξ] = 0. Hence,

Therefore,

¯Mt ≥

eft(ξ(cid:63))N (gt)
N (ht)

.

(12)

(cid:18)

(cid:18)

(cid:18)

(cid:18)

δ ≥ P

≥ P

= P

≥ P

¯Mt ≥

(cid:19)

1
δ

ft(ξ(cid:63)) ≥ log

(Equation (10) + Markov’s Inequality)

(cid:19)

(cid:18) 1
δ

+ log

(cid:19)(cid:19)

(cid:18) N (ht)
N (gt)

(Equation (12))

max
(cid:107)ξ(cid:107)2≤1/2

ft(ξ) ≥ log

(cid:19)

(cid:18) 1
δ

+ log

ft(ξ0) ≥ log

(cid:19)

(cid:18) 1
δ

+ log

(cid:18) N (ht)
N (gt)

(cid:18) N (ht)
N (gt)
(cid:19)(cid:19)

.

(cid:19)(cid:19)

In the last inequality ξ0 is deﬁned as ξ0 =

(cid:101)Ht ≥ λt−1Id. We also have,

√

λt
2

(cid:101)H−1
(cid:107)St(cid:107)

t St
−1
(cid:102)H
t

, such that (cid:107)ξ0(cid:107)2 ≤ 1/2 holds. This can be seen by using

ft(ξ0) =

1
mwt−1

ξ(cid:62)
0 St −

1
m2w2

t−1

ξ(cid:62)
0 (cid:101)Htξ0 =

(cid:112)λt−1
2mwt−1

(cid:107)St(cid:107)

(cid:101)H−1
t

−

λt−1
4m2w2

t−1

.

Therefore,

(cid:32)

P

(cid:107)St(cid:107)

(cid:101)H−1
t

≥

(cid:112)λt−1
2mwt−1

+

2mwt−1
(cid:112)λt−1

log(1/δ) +

2mwt−1
(cid:112)λt−1

log

(cid:18) N (ht)
N (gt)

(cid:19)(cid:33)

≤ δ .

(13)

We conclude using Proposition 2.
Proposition 2. Let ht be the density of a d-dimensional isotropic normal distribution of precision 2λt−1
m2w2
2
(cid:101)Ht
truncated on B2(d). Let gt be the density of a d-dimensional normal distribution with precision matrix
truncated on {a ∈ Rd, (cid:107)a(cid:107)2 ≤ 1/2}. The following inequality holds,

m2w2

t−1

t−1

log

(cid:19)

(cid:18) N (ht)
N (gt)

(cid:32)

≤ log

(cid:33)

det( (cid:101)Ht)
λd/2
t−1

+ d log(2) .

(14)

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Proof.

N (ht) =

=

(cid:90)

1 [(cid:107)ξ(cid:107)2 ≤ 1] exp

Rd
(cid:18) m2w2
t−1
2λt−1

(cid:19)d/2 (cid:90)

(cid:18)

−

1
2

2λt−1
m2w2

t−1

(cid:107)ξ(cid:107)2
2

(cid:34)

1

(cid:107)ξ(cid:107)2 ≤

(cid:112)2λt−1
mwt−1

(cid:19)

(cid:35)

dξ

(cid:18)

exp

−

(cid:19)

1
2

(cid:107)ξ(cid:107)2
2

dξ .

N (gt) =

(cid:90)

Rd

1 [(cid:107)ξ(cid:107)2 ≤ 1/2] exp

=

≥

Therefore,

(cid:90)

Rd

(cid:17)(cid:12)
(cid:12)
(cid:12)

2

1
(cid:16) √
mwt−1 (cid:101)H1/2
(cid:19)d/2

t

t−1

(cid:12)
(cid:12)
(cid:12)det
(cid:18) m2w2
2

Rd

(cid:18)

−

1
2
(cid:34)

2
m2w2

t−1

(cid:19)

ξ(cid:62) (cid:101)Htξ

1

(cid:107)ξ(cid:107)2 ≤

(cid:112)2λt−1
mwt−1

1
2

dξ

(cid:35)

(cid:18)

exp

−

(cid:19)

dξ

(cid:107)ξ(cid:107)2
2

1
2

det( (cid:101)Ht)−1/2

(cid:34)

1

(cid:90)

Rd

(cid:107)ξ(cid:107)2 ≤

(cid:35)

(cid:112)2λt−1
mwt−1

1
2

(cid:18)

exp

−

(cid:19)

1
2

(cid:107)ξ(cid:107)2
2

dξ .

N (ht)
N (gt)

≤

det( (cid:101)Ht)
λd/2
t−1

(cid:82)

Rd

1

(cid:20)
(cid:107)ξ(cid:107)2 ≤

(cid:20)

1

Rd

(cid:107)ξ(cid:107)2 ≤ 1
2

(cid:82)

(cid:124)

√

(cid:21)

2λt−1
mwt−1
√

2λt−1
mwt−1
(cid:123)(cid:122)
R

2 (cid:107)ξ(cid:107)2

2

(cid:1) dξ

exp (cid:0)− 1
(cid:21)

exp (cid:0)− 1

2 (cid:107)ξ(cid:107)2
2

.

(cid:1) dξ

(cid:125)

(15)

The last step consists in upper bounding the ratio of the integrals R. Following, (Faury et al., 2020, Lemma 6),
one gets R = 2d.

We conclude by using this equality in Equation (15) and applying the logarithm on both sides.

A.3 A Unifying Concentration Result for Discount Factors and Sliding-Window

In this section, we explain how Theorem 3 can be used with self-concordant GLBs to obtain a concentration
inequality that encapsulates the analysis for both discount-factors and the sliding-window.

Up to now, we have stated the results in the most generic way. Actually, in our analysis we will use a weaker
version of the concentration inequality established in Theorem 3.
Theorem 4. Let t be a ﬁxed time instant. Let {Fu}t
u=1 be a stochastic process on
Rd such that au is Fu measurable and (cid:107)au(cid:107)2 ≤ 1. Let {(cid:15)u}t
u=2 be a martingale diﬀerence sequence such that
(cid:15)u+1 is Fu+1 measurable. Assume that the weights are non-decreasing, positive and the time horizon is known.
Furthermore, assume that conditionally on Fu we have |(cid:15)u+1| ≤ m a.s. Let {λu}t
u=1 be a deterministic sequence
t = E (cid:2)(cid:15)2
of regularization terms and denote σ2
Let (cid:101)Ht−t0:t = (cid:80)t−1
s=t−t0
Then for any δ ∈ (0, 1],

t+1|Ft
s + λt−1Id and St−t0:t = (cid:80)t−1

u=1 be a ﬁltration. Let {au}t

ws(cid:15)s+1as.

s asa(cid:62)

s=t−t0

sσ2

w2

(cid:3).

(cid:32)

P

(cid:107)St−t0:t(cid:107)

(cid:101)H−1

t−t0:t

≥

(cid:112)λt−1
2mwt−1

+

2mwt−1
(cid:112)λt−1

log

(cid:32)

det( (cid:101)Ht−t0:t)1/2
δλd/2
t−1

(cid:33)

+

2mwt−1
(cid:112)λt−1

(cid:33)

d log(2)

≤ δ .

Proof. The arguments used to establish Theorem 4 are the same than for Theorem 3. We only give the main
term that diﬀers from the proof of Theorem 3.

With t a ﬁxed time instant, for any u such that t − t0 ≤ u ≤ t, M t
u

is deﬁned as

M t

u(ξ) = exp

(cid:32)

1
mwt−1

ξ(cid:62)St−t0:u −

1
m2w2

t−1

ξ(cid:62)

u−1
(cid:88)

s=t−t0

(cid:33)

w2

sasa(cid:62)
s ξ

,

with St−t0:u = (cid:80)u−1
gives the result.

s=t−t0

ws(cid:15)s+1as. Following the steps of the proof of Theorem 3 with these slight diﬀerences

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Discount Factors Let t0 = D be the equivalent of the sliding window length with exponential weights, wt = γ−t
and λt = λγ−2t for 0 < γ < 1. Even when γ depends on T , the weights satisfy the assumptions 6 and 7. We can
obtain:

Corollary 1 (Concentration result with discount factors). Under the same assumption than Theorem 4, when
deﬁning (cid:101)Ht−D:t = (cid:80)t−1

s=t−D γ−s(cid:15)s+1as. For any δ ∈ (0, 1],

s=t−D γ2(t−1−s) ˙µ(a(cid:62)

(cid:32)

(cid:13)
(cid:13)γt−1St−D:t

P

s )asa(cid:62)
s θ(cid:63)
√

s + λId and St−D:t = (cid:80)t−1
(cid:33)

(cid:32)

(cid:13)
(cid:13) (cid:101)H−1

t−D:t

≥

λ
2m

+

2m
√
λ

log

det( (cid:101)Ht−D:t)1/2
δλd/2

(cid:33)

d log(2)

≤ δ .

2m
√
λ

+

Sliding Window With t0 = τ the length of the sliding window, with the weights satisfying wt = 1 for
t − τ ≤ s ≤ t − 1 and λt = λ, we have:
Corollary 2 (Concentration result with a sliding window). Under the same assumption than Theorem 4, when
deﬁning Ht = (cid:80)t−1

s=max(1,t−τ ) (cid:15)s+1as. For any δ ∈ (0, 1],

s + λId and St = (cid:80)t−1

s=max(1,t−τ ) ˙µ(a(cid:62)

s θ(cid:63)

s )asa(cid:62)
√

(cid:32)

P

(cid:107)St(cid:107)H−1

t

≥

λ
2m

+

2m
√
λ

log

(cid:18) det(Ht)1/2
δλd/2

(cid:19)

+

2m
√
λ

(cid:33)

d log(2)

≤ δ .

B REGRET ANALYSIS WITH DISCOUNT FACTORS

In this section we detail the regret analysis of SC-D-GLUCB. First we recall the main notation.

B.1 Notation

For any θ ∈ Rd,

(cid:101)Ht(θ) =

t−1
(cid:88)

s=1

γ2(t−1−s) ˙µ(a(cid:62)

s θ)asa(cid:62)

s + λId .

Ht(θ) =

(cid:101)Vt =

t−1
(cid:88)

s=1

t−1
(cid:88)

s=1

γt−1−s ˙µ(a(cid:62)

s θ)asa(cid:62)

s + λId .

γ2(t−1−s)asa(cid:62)

s +

λ
cµ

Id .

For any θ1, θ2 ∈ Rd,

Vt =

t−1
(cid:88)

s=1

γt−1−sasa(cid:62)

s +

λ
cµ

Id .

g1:t(θ) =

t−1
(cid:88)

s=1

γt−1−sµ(a(cid:62)

s θ)as + λθ .

St =

t−1
(cid:88)

s=1

γ−s(cid:15)s+1as .

α(a, θ1, θ2) =

(cid:90) 1

0

˙µ(va(cid:62)θ2 + (1 − v)a(cid:62)θ1)dv .

Gt(θ1, θ2) =

t−1
(cid:88)

s=1

γt−1−sα(as, θ1, θ2)asa(cid:62)

s + λId .

(16)

(17)

(18)

(19)

(20)

(21)

(cid:101)Gt(θ1, θ2) =

t−1
(cid:88)

s=1

γ2(t−1−s)α(as, θ1, θ2)asa(cid:62)

s + λId .

(22)

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Let (cid:101)Ht be deﬁned as

Let us deﬁne T (γ) as

(cid:101)Ht =

t−1
(cid:88)

s=1

γ2(t−1−s) ˙µ(a(cid:62)

s θ(cid:63)

s )asa(cid:62)

s + λId .

T (γ) = {1 ≤ t ≤ T, such that ∀s, t − D ≤ s ≤ t − 1, θ(cid:63)

s = θ(cid:63)

t } .

(23)

(24)

Remark. t ∈ T (γ) when t is a least D steps away from the closest previous breakpoint. On the contrary to the
analysis with the sliding window (see Appendix C) the bias does not completely cancel out when we are far enough
from a breakpoint.

D is an analysis parameter and will be speciﬁed later in the diﬀerent theorems. For the entire section we will use
the notation t − D : t when the sum concerns time instants s such that t − D ≤ s ≤ t − 1. In the weighted setting,
we construct an estimator based on a weighted penalized log-likelihood. ˆθt is deﬁned as the unique maximizer of

t−1
(cid:88)

s=1

γt−1−s log Pθ(rs+1|as) −

λ
2

(cid:107)θ(cid:107)2

2 .

By using the deﬁnition of the GLM and thanks to the concavity of this equation in θ, ˆθt is the unique solution of

t−1
(cid:88)

s=1

γt−1−s(rs+1 − µ(a(cid:62)

s θ))as − λθ = 0 .

This can be summarized with

g1:t(ˆθt) =

t−1
(cid:88)

s=1

γt−1−srs+1as = γt−1St +

t−1
(cid:88)

s=1

γt−1−sµ(a(cid:62)

s θ(cid:63)

s )as .

(25)

B.2 Analysis of the Regret of SC-D-GLUCB

In this section, we present the main ideas to obtain an analysis of the regret of the SC-D-GLUCB algorithm when
the projection step is avoided.

We deﬁne

and also,

(cid:32) √

λ
2m

ρδ
T =

+

2m
√
λ

log

(cid:19)

(cid:18) T
δ

+

dm
√
λ

(cid:18)

log

1 +

kµ(1 − γ2D)
dλ(1 − γ2)

(cid:19)

+

2m
√
λ

(cid:33)

d log(2)

,

¯S = S +

γD(2Skµ + m)
λ(1 − γ)

.

(26)

(27)

is deﬁned such that thanks to Corollary 1 with high probability for all t in T (γ), (cid:107)γt−1St−D:t(cid:107)

and ¯S given here coincide with the expression in the main paper when D = log(T )/ log(1/γ).
≤ ρδ
T

(cid:101)H−1

t−D:t

The expression of ρδ
T
ρδ
T
holds.

The next result uses the self-concordance to relate the ﬁrst derivative of the link function evaluated at diﬀerent
points. This relation is independent of cµ and only depends on the distance between the parameters.
Proposition 3. When ˆθt is the maximum likelihood as deﬁned in Equation (1) and t ∈ T (γ), we have

α(a, θ(cid:63)

t , ˆθt) ≥

(cid:18)

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

where ¯S is deﬁned in Equation (27).

(cid:101)G−1

t−D:t(θ(cid:63)

t ,ˆθt)

(cid:19)−1

˙µ(a(cid:62)θ(cid:63)

t ) ,

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Proof. In the proof, we will replace the notation (cid:101)Gt−D:t(θ(cid:63)
Gt(θ(cid:63)

t , ˆθt) with Gt. Using Lemma 4 we have,

t , ˆθt) with (cid:101)Gt−D:t and (cid:101)Gt(θ(cid:63)

t , ˆθt) with (cid:101)Gt but also

Combining this with the mean value theorem gives

α(a, θ(cid:63)

t , ˆθt) ≥

(cid:16)

1 +

(cid:12)
(cid:12)
(cid:12)a(cid:62)(ˆθt − θ(cid:63)
(cid:12)
(cid:12)
t )
(cid:12)

(cid:17)−1

˙µ(a(cid:62)θ(cid:63)

t ) .

α(a, θ(cid:63)

t , ˆθt) ≥

(cid:16)

1 +

(cid:16)

g1:t(ˆθt) − g1:t(θ(cid:63)
t )

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:17)−1

˙µ(a(cid:62)θ(cid:63)

t ) .

(cid:12)
(cid:12)a(cid:62)G−1
(cid:12)
(cid:16)

t

(cid:17)

Next, it is possible to upper bound |a(cid:62)G−1

t

g1:t(ˆθt) − g1:t(θ(cid:63)
t )

| using the triangle inequality and Equation (25).

(cid:12)
(cid:12)a(cid:62)G−1
(cid:12)

t

(cid:16)

g1:t(ˆθt) − g1:t(θ(cid:63)
t )

(cid:17)(cid:12)
(cid:12)
(cid:12) ≤

The ﬁrst term is controlled as follows,

a(cid:62)G−1

t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:124)

t−1
(cid:88)

s=1

t

+

a(cid:62)G−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:124)
+ (cid:12)
(cid:12)a(cid:62)G−1
(cid:124)

γt−1−s(µ(a(cid:62)

s θ(cid:63)

s ) − µ(a(cid:62)

s θ(cid:63)

t ))as

(cid:123)(cid:122)
b1,t(a)

(cid:32)

−λθ(cid:63)

t +

t−D−1
(cid:88)

γt−1−s(cid:15)s+1as

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

s=1
(cid:123)(cid:122)
b2,t(a)
(cid:12)
t γt−1St−D:t
(cid:12)
(cid:125)

(cid:123)(cid:122)
b3,t(a)

b1,t(a) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t−1
(cid:88)

a(cid:62)G−1

t

γt−1−s(µ(a(cid:62)

s θ(cid:63)

s ) − µ(a(cid:62)

s θ(cid:63)

t ))as

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s=1

t−1
(cid:88)
(cid:107)

s=1

≤ (cid:107)a(cid:107)G−1

t

γt−1−s(µ(a(cid:62)

s θ(cid:63)

s ) − µ(a(cid:62)

s θ(cid:63)

t ))as(cid:107)G−1

t

1
√
λ

(cid:107)

t−D−1
(cid:88)

s=1

γt−1−s(µ(a(cid:62)

s θ(cid:63)

s ) − µ(a(cid:62)

s θ(cid:63)

t ))as(cid:107)G−1

t

(Cauchy-Schwarz ineq.)

(Gt ≥ λId and t ∈ T (γ))

1
λ

t−D−1
(cid:88)

s=1

γt−1−s|α(as, θ(cid:63)

s , θ(cid:63)

t )| × |a(cid:62)

s (θ(cid:63)

t − θ(cid:63)

s )| × (cid:107)as(cid:107)2

(Triangle ineq. + Gt ≥ λId)

≤

≤

≤

2Skµ
λ

t−D−1
(cid:88)

γt−1−s

(θ(cid:63)
s

and θ(cid:63)

t ∈ Θ)

s=1
γD
1 − γ
Using similar arguments, one can upper bound b2,t(a).

2Skµ
λ

≤

.

b2,t(a) =

(cid:32)

a(cid:62)G−1

t

−λθ(cid:63)

t +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t−D−1
(cid:88)

s=1

γt−1−s(cid:15)s+1as

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t−D−1
(cid:88)

≤ S + (cid:107)

s=1
γD
1 − γ

m
λ

≤ S +

γt−1−s(cid:15)s+1as(cid:107)G−2

t

.

(|(cid:15)s+1| ≤ m)

Before upper bounding, b3,t(a), we need the following relation.
When 0 < γ < 1, γ2(t−1−s) ≤ γt−1−s for s smaller than t − 1 which implies

∀θ1, θ2 ∈ Rd, (cid:101)Gt(θ1, θ2) ≤ Gt(θ1, θ2) .

(28)

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

We have,

t (cid:101)G1/2

t

t (cid:101)GtG−1

t

γt−1St−D:t|

(cid:101)G−1/2
t
(cid:107)γt−1St−D:t(cid:107)

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1
t

b3,t(a) = |a(cid:62)G−1
≤ (cid:107)a(cid:107)G−1
≤ (cid:107)a(cid:107)G−1
1
√
λ

≤

t

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t

.

(Gt ≥ λId)

(Cauchy-Schwarz ineq.)

(cid:101)G−1
t
(Equation (28))

By combining all the results we have,

α(a, θ(cid:63)

t , ˆθt) ≥

(cid:18)

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t

(cid:19)−1

˙µ(a(cid:62)θ(cid:63)

t ) .

Corollary 3. When ˆθt is the maximum likelihood as deﬁned in Equation (1), and t ∈ T (γ), we have

(cid:101)Gt−D:t(θ(cid:63)

t , ˆθt) ≥

(cid:18)

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t(θ(cid:63)

t ,ˆθt)

(cid:19)−1

(cid:101)Ht−D:t .

This proposition establishes a useful link between (cid:101)Gt−D:t(θ(cid:63)

t , ˆθt) and (cid:101)Ht−D:t.

Proof. Thanks to Proposition 3,

α(as, θ(cid:63)

t , ˆθt) ≥

(cid:18)

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1
t

(ˆθt,θ(cid:63)
t )

(cid:19)−1

˙µ(a(cid:62)

s θ(cid:63)

t ) .

Therefore,

t−1
(cid:88)

s=t−D

γ2(t−1−s)α(as, θ(cid:63)

t , ˆθt)asa(cid:62)

s ≥

(cid:18)

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1
t

(ˆθt,θ(cid:63)
t )

(cid:19)−1

t−1
(cid:88)

×

s=t−D

γ2(t−1−s) ˙µ(a(cid:62)

s θ(cid:63)

t )asa(cid:62)
s .

We obtain the announced result by using θ(cid:63)
regularization terms.

s = θ(cid:63)
t

for t − D ≤ s ≤ t − 1 because t ∈ T (γ) and by adding the

Using Proposition 3 and Corollary 3, we can now prove Proposition 1. The proposition establishes an upper
bound for the deviation of the MLE (through γt−1St−D:t) that only depends on ρδ
the high probability upper
T
bound obtained using Corollary 1.

Proposition 1. For any δ ∈ (0, 1], with probability higher than 1 − δ,

∀t ∈ T (γ), (cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t(ˆθt,θ(cid:63)

t ) ≤

(cid:112)

1 + ¯Sρδ

T +

1
√
λ

(cid:1)2

(cid:0)ρδ

T

,

T is deﬁned in Equation (26).

where ρδ
Remark. Here, note that the left-hand side is controlled under the norm (cid:101)G−1
t ), whereas the right hand
side is the consequence of the upper bound of the same term controlled in the (cid:101)H−1
t−D:t-norm (Corollary 1). Linking
those two matrices independently from cµ is not-straightforward. The self-concordance is the key ingredient to
obtain this bound.

t−D:t(ˆθt, θ(cid:63)

Proof. Applying Corollary 3,

(cid:107)γt−1St−D:t(cid:107)2

(cid:101)G−1

t−D:t(ˆθt,θ(cid:63)
t )

(cid:18)

≤

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t(ˆθt,θ(cid:63)
t )

(cid:19)

(cid:107)γt−1St−D:t(cid:107)2

(cid:101)H−1

t−D:t

.

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Let X = (cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t(ˆθt,θ(cid:63)
t )

, it gives the following constraint,

∀X, X 2 −

1
√
λ

(cid:107)γt−1St−D:t(cid:107)2

(cid:101)H−1

t−D:t

X − (cid:0)1 + ¯S(cid:1) (cid:107)γt−1St−D:t(cid:107)2

(cid:101)H−1

t−D:t

≤ 0 .

Solving this polynomial inequality yields

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1
t

t ,ˆθt) ≤
(θ(cid:63)

1
√
λ

(cid:107)γt−1St−D:t(cid:107)2

(cid:101)H−1

t−D:t

+

(cid:112)

1 + ¯S(cid:107)γt−1St−D:t(cid:107)

(cid:101)H−1

t−D:t

.

The result is then obtained by applying Corollary 1.

Corollary 4. When ˆθt is the maximum likelihood as deﬁned in Equation (1) and t ∈ T (γ), we have

Gt(θ(cid:63)

t , ˆθt) ≥

(cid:18)

1 + ¯S +

1
√
λ

Proof. Similar to the proof of Corollary 3.

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t(θ(cid:63)

t ,ˆθt)

(cid:19)−1

cµVt .

In the next proposition, we give an upper bound for ∆t(a, ˆθt) the prediction error in ˆθt which is directly connected
to the instantaneous regret.

Here, βδ
T
Equation (26) and (27).

is deﬁned as in the main paper in Equation (4) but we replace ρδ
T

and ¯S with the expressions stated

Proposition 4. For any δ ∈ (0, 1], with probability higher than 1 − δ,

∀t ∈ T (γ), ∆t(a, ˆθt) ≤

kµ
λ

γD
1 − γ

(2Skµ + m) +

βδ
T√
cµ

(cid:107)a(cid:107)V−1

t

.

Proof. We denote Gt = Gt(θ(cid:63)

t , ˆθt) and we have,

∆t(a, ˆθt) = |µ(a(cid:62)θ(cid:63)
≤ kµ|a(cid:62)(θ(cid:63)
= kµ|a(cid:62)G−1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t ) − µ(a(cid:62) ˆθt)|
t − ˆθt)|
t (g1:t(θ(cid:63)
(cid:32)t−1
(cid:88)

a(cid:62)G−1

= kµ

s=1

t

t ) − g1:t(ˆθt))|

(Mean-Value Theorem)

γt−1−s(µ(a(cid:62)

s θ(cid:63)

t ) − µ(a(cid:62)

s θ(cid:63)

s ))as + λθ(cid:63)

t − γt−1St

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

In the last equality, we have used the characterization of the MLE (Equation (25)).

∆t(a, ˆθt) ≤ kµ

a(cid:62)G−1

t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:124)

t−1
(cid:88)

s=1

γt−1−s(µ(a(cid:62)

s θ(cid:63)

t ) − µ(a(cid:62)

s θ(cid:63)

s ))as

(cid:123)(cid:122)
c1,t(a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

+ kµ

a(cid:62)G−1

t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:124)

t−D−1
(cid:88)

s=1

(cid:123)(cid:122)
c2,t(a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

γt−1−s(cid:15)s+1as

+kµ

(cid:12)
(cid:12)a(cid:62)G−1
(cid:124)

t

(cid:0)γt−1St−D:t − λθ(cid:63)

t

(cid:123)(cid:122)
c3,t(a)

.

(cid:1)(cid:12)
(cid:12)
(cid:125)

We will bound the diﬀerent terms.

c1,t(a) can be bounded like b1,t(a) in the proof of Proposition 3.

c1,t(a) ≤

2Skµ
λ

γD
1 − γ

.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

c2,t(a) can be bounded like b2,t(a) in the proof of the same proposition.

The last term requires more work. (cid:101)Gt(θ(cid:63)

t , ˆθt) will be denoted (cid:101)Gt for simplicity.

c2,t(a) ≤

m
λ

γD
1 − γ

.

c3,t(a) = (cid:12)

(cid:12)a(cid:62)G−1

t

(cid:0)γt−1St−D:t − λθ(cid:63)

(cid:12)
(cid:1)(cid:12)
(cid:12)a(cid:62)G−1
(cid:12)
(cid:12) =
(cid:107)γt−1St−D:t − λθ(cid:63)
t (cid:107)

t

t (cid:101)GtG−1

t

≤ (cid:107)a(cid:107)G−1
≤ (cid:107)a(cid:107)G−1

t

≤ (cid:107)a(cid:107)G−1

t

(cid:107)γt−1St−D:t − λθ(cid:63)
t (cid:107)
(cid:16)√

(cid:101)G−1
t
λS + (cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1
t
(Equation (28))

(cid:17)

( (cid:101)Gt ≥ λId and Assumption 1)

(cid:101)G−1
t

t (cid:101)G1/2

t

(cid:101)G−1/2

t

(cid:0)γt−1St−D:t − λθ(cid:63)

t

(cid:1)(cid:12)
(cid:12)
(cid:12)

(cid:115)

≤

(cid:107)a(cid:107)V−1
t√
cµ

1 + ¯S +

1
√
λ

(cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t

(cid:16)√

λS + (cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t

(cid:17)

.

with
In the last inequality we used Corollary 4. The next step consists in upper bounding (cid:107)γt−1St−D:t(cid:107)
Proposition 1 and to combine this with the high probability upper bound from Corollary 1. Therefore, with
probability higher than 1 − δ,

(cid:101)G−1

t−D:t

c3,t(a) ≤

(cid:107)a(cid:107)V−1
t√
cµ

(cid:115)

(cid:114)

1 + ¯S +

1 + ¯S
λ

ρδ
T +

1
λ

(ρδ

T )2

(cid:16)√

λS + (cid:107)γt−1St−D:t(cid:107)

(cid:101)G−1

t−D:t

(cid:17)

√

√

λ
cµ

λ
cµ

(cid:107)a(cid:107)V−1

t

(cid:115)

(cid:32)

1 + ¯S +

(cid:107)a(cid:107)V−1

t

1 + ¯S +

(cid:114)

(cid:114)

1 + ¯S
λ

1 + ¯S
λ

ρδ
T +

ρδ
T +

1
λ

1
λ

(cid:32)

(cid:114)

(ρδ

T )2

S +

1 + ¯S
λ

ρδ
T +

1
λ

(ρδ

T )2

(cid:33)

(cid:33)3/2

(ρδ

T )2

.

≤

√

≤

√

The ﬁrst term of the right hand side of Proposition 4 is a bias term resulting from the non-stationarity of the
environment. The second term results from the concentration results we have established in Section A combined
with the self-concordance assumption.

With βδ
T

deﬁned in Equation (4), the algorithm SC-D-GLUCB selects the action at time t as follows,

at = arg max

a∈At

= arg max

a∈At

(cid:18)

(cid:18)

µ(a(cid:62) ˆθt) +

µ(a(cid:62) ˆθt) +

βδ
T√
cµ
βδ
T√
cµ

(cid:107)a(cid:107)V−1

t

+

kµ
λ

γD
1 − γ

(cid:19)

(2Skµ + m)

(cid:19)

.

(cid:107)a(cid:107)V−1

t

(29)

Note that the bias term is independent of the action. Nevertheless, this term will appear in the upper bound for
the regret. Equation (29) explains how the actions are chosen in Algorithm 1.

We can now give the main theorem.

Theorem 2. The regret of the SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1) with probability at least 1 − δ
by

RT ≤

2 log(T )
1 − γ

ΓT +

2kµ(2Skµ + m)
λ
(cid:18)

(cid:19)(cid:115)

1
1 − γ

(cid:18)

(cid:115)

dT

2 max

1,

T log(1/γ) + log

1 +

+

√

2βδ
T√
cµ

1
λ

1
dλ(1 − γ)

(cid:19)

.

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

In particular, setting γ = 1 −

(cid:19)2/3

(cid:18) c1/2
µ ΓT
dT

and λ = d log(T ) leads to

RT = (cid:101)O(cid:0)c−1/3

µ

d2/3Γ1/3

T T 2/3(cid:1) .

Proof. Using Proposition 4, we obtain a high probability upper bound for ∆t(a, ˆθt). We recall that the exploration
bonus of SC-D-GLUCB is deﬁned as,

1
√
cµ

βδ
T (cid:107)at(cid:107)V−1

t

+

kµ
λ

γD
1 − γ

(2Skµ + m) .

Furthermore, the estimator used by SC-D-GLUCB is the MLE ˆθt as deﬁned in Equation (1), all the conditions
required for applying Proposition 9 are met. Hence when t ∈ T (γ),

rt ≤

2
√
cµ

βδ
T (cid:107)at(cid:107)V−1

t

+

2kµ
λ

γD
1 − γ

(2Skµ + m) .

The dynamic regret can then be upper bounded by,

RT =

T
(cid:88)

t=1

rT =

(cid:88)

rt +

(cid:88)

rt ≤ ΓT D +

(cid:88)

rt

t∈T (γ)

(cid:88)

(cid:107)at(cid:107)V−1

t

t /∈T (γ)

(2Skµ + m)T +

(2Skµ + m)T +

2βδ
T√
cµ

2βδ
T√
cµ

t∈T (γ)
√

(cid:115) (cid:88)

T

t∈T (γ)

(cid:107)at(cid:107)2

V−1
t

(Cauchy-Schwarz ineq.)

(2Skµ + m)T +

√

T

2βδ
T√
cµ

(cid:107)at(cid:107)2

V−1
t

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:115)

(2Skµ + m)T +

√

2βδ
T√
cµ

T

2 max

1,

(cid:18)

(cid:19)

1
λ

log

(cid:18) det(VT +1)
γdT λd

(cid:19)

.

≤ ΓT D +

≤ ΓT D +

≤ ΓT D +

≤ ΓT D +

t∈T (γ)
γD
1 − γ

2kµ
λ

2kµ
λ

γD
1 − γ

2kµ
λ

γD
1 − γ

2kµ
λ

γD
1 − γ

The last inequality uses Lemma 7. Next, we use Corollary 8 to upper bound the determinant,

γdT λd ≤ γ−dT
Applying the logarithm function on both sides yields

det(VT +1)

(cid:18)

1 +

1 − γT
λd(1 − γ)

(cid:19)d

.

RT ≤ ΓT D +

2kµ(2Skµ + m)
λ

+

√

2βδ
T√
cµ

(cid:115)

(cid:18)

dT

2 max

1,

1
λ

T

γD
1 − γ
(cid:19)(cid:115)

T log(1/γ) + log

1 +

(cid:18)

1
dλ(1 − γ)

(cid:19)

.

With the additional constraint 1/2 < γ < 1, by setting D = log(T )/ log(1/γ), noticing that 0 < 1/γ − 1 < 1 and
using log(1 + x) ≥ x/2 for 0 < x < 1, we have

log(1/γ) = log(1 + 1/γ − 1) ≥

1 − γ
2γ

.

Therefore, we have D ≤ 2γ log(T )
By properly balancing the bias term due to the non-stationarity and the rate at which the weighted MLE
approaches the true bandit parameter, the asymptotic behavior of SC-D-GLUCB can be characterized as follows:

1−γ

.

By setting γ = 1 −

and λ = d log(T ), we have:

(cid:19)2/3

(cid:18) c1/2
µ ΓT
dT

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

• 2 log(T )

1−γ ΓT scales as (cid:101)O(c−1/3

µ

d2/3Γ1/3

T T 2/3).

• 2kµ(2Skµ+m)

λ

1
1−γ

scales as (cid:101)O(c−1/3
(cid:114)

µ

d2/3Γ−2/3
T

T 2/3).

√

(cid:113)

• 2βδ
T√
cµ

2 max (cid:0)1, 1
mic factors and constant terms.

dT

(cid:1)

λ

T log(1/γ) + log

(cid:16)

1 +

1
dλ(1−γ)

(cid:17)

scales as

1√
cµ

dT (cid:112)log(1/γ) when omitting logarith-

Using − log(1 − x) ≤ x−1
x

for 0 ≤ x < 1, we also have

(cid:112)log(1/γ) = (cid:112)− log(1 − (1 − γ)) ≤

(cid:114) 1 − γ
γ

≤ (cid:112)2(1 − γ) .

(cid:112)log(1/γ) scales as (cid:101)O(c1/6
ing the diﬀerent terms concludes the proof.

µ d−1/3Γ1/3

T T −1/3). Hence scales c−1/2

µ

dT (cid:112)log(1/γ) as (cid:101)O(c−1/3

µ

d2/3Γ1/3

T T 2/3). Combin-

Using Assumption 5, we can obtain reﬁned regret bounds.

B.3 Gap-Dependent Bound

Theorem 1. Under Assumption 5, the regret of the SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1) with
probability at least 1 − δ by

1
T (1 − γ)2∆

+ C3

(cid:115)

βδ
T
√

√

dT
cµ∆

T log(1/γ) + log

1 +

(cid:18)

(cid:19)

1
dλ(1 − γ)

+ C2

RT ≤ C1

+ C4

ΓT
1 − γ
d(βδ
T )2
cµ∆

(cid:0)T log(1/γ) + log(1 +

1
dλ(1 − γ)

)(cid:1) ,

where C1, C2, C3, C4 are universal constants independent of cµ, γ with only logarithmic terms in T .

In particular, setting γ = 1 −

√

cµΓT
√
T
d

and λ = d log(T ) leads to

RT = (cid:101)O(cid:0)∆−1c−1/2

µ

(cid:112)

d

ΓT T (cid:1) .

Proof. First note that for any suboptimal action a ∈ At,

µ(a(cid:62)

(cid:63),tθ(cid:63)

t ) − µ(a(cid:62)θ(cid:63)

t ) ≥ ∆ .

This implies

rt = µ(a(cid:62)

(cid:63),tθ(cid:63)

t ) − µ(a(cid:62)

t θ(cid:63)

t ) ≤

(cid:0)µ(a(cid:62)

(cid:63),tθ(cid:63)

t θ(cid:63)

t )(cid:1)2

t ) − µ(a(cid:62)
∆

=

r2
t
∆

.

(30)

Using Proposition 9 one has,

This implies in particular,

rt ≤

2
√
cµ

βδ
T (cid:107)at(cid:107)V−1

t

+

2kµ
λ

γD
1 − γ

(2Skµ + m) .

r2
t ≤

4
cµ
(cid:124)

(βδ

T )2(cid:107)at(cid:107)2
(cid:123)(cid:122)
r1,t

V−1
t
(cid:125)

+

4k2
µ
λ2
(cid:124)

γ2D
(1 − γ)2 (2Skµ + m)2
(cid:125)

(cid:123)(cid:122)
r2,t

+

8kµ
λ

βδ
T√
cµ

γD
1 − γ

(cid:124)

(2Skµ + m)(cid:107)at(cid:107)V−1

t

.

(31)

(cid:123)(cid:122)
r3,t

(cid:125)

The dynamic regret can then be upper bounded by,

RT =

T
(cid:88)

t=1

rT =

(cid:88)

rt +

(cid:88)

rt ≤ ΓT D +

(cid:88)

(µ(a(cid:62)

(cid:63),tθ(cid:63)

t ) − µ(a(cid:62)

t θ(cid:63)

t ))

≤ ΓT D +

t∈T (γ)
1
∆

(cid:88)

t∈T (γ)

t /∈T (γ)

t∈T (γ)

r2
t .

(Equation (30))

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

By applying Equation (31), the regret can be separated in 4 diﬀerent terms.

When summing for the diﬀerent time instants r1,t becomes

T
(cid:88)

t=1

r1,t ≤

≤

8
cµ

8d
cµ

(βδ

T )2 max

(βδ

T )2 max

(cid:18)

1,

(cid:18)

1,

1
λ

1
λ

(cid:19)

log

(cid:18) det(VT +1)
γdT λd

(cid:19)

(Lemma 7)

(cid:19) (cid:18)

(cid:18)

T log(1/γ) + log

1 +

(cid:19)(cid:19)

1
dλ(1 − γ)

.

(Corollary 8)

For r2,t, we have

T
(cid:88)

t=1

r2,t ≤

4k2
µ
λ2

γ2DT
(1 − γ)2 (2Skµ + m)2 .

Furthermore, r3,t is treated as follows:

T
(cid:88)

t=1

r3,t ≤

8kµ
λ

βδ
T√
cµ

γD
1 − γ

(2Skµ + m)

T
(cid:88)

t=1

(cid:107)at(cid:107)V−1

t

8kµ
λ

βδ
T√
cµ

γD
1 − γ

√

(2Skµ + m)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

T

t=1

(cid:107)at(cid:107)2

V−1
t

8kµβδ
T
√
cµ
λ

γD
1 − γ

(cid:115)

(cid:18)

(2Skµ + m)

2dT max

1,

(cid:19)(cid:115)

1
λ

T log

(cid:19)

(cid:18) 1
γ

(cid:18)

+ log

1 +

1
dλ(1 − γ)

(cid:19)

.

≤

≤

When λ = d log(T ), D = log(T )
log(1/γ)
of Theorem 2.

and γ = 1 −

√

cµΓT
√
T
d

With those choices,

1. ΓT D scales as (cid:101)O(c−1/2

µ

dΓ1/2

T T 1/2)

, we can upper bound the diﬀerent terms following the proof

2. (cid:80)T

t=1 r1,t scales as (cid:101)O(c−1/2

µ

dΓ1/2

T T 1/2)

3. (cid:80)T

t=1 r2,t scales as (cid:101)O(c−1

µ Γ−1
T )

4. (cid:80)T

t=1 r3,t scales as (cid:101)O(d1/4c−3/4

µ

Γ−1/4
T

T 1/4)

Keeping the highest order term in T and dividing by ∆ yields the announced result.

B.4 Reﬁned Exploration Bonus when ˆθt ∈ Θ

As brieﬂy explained in Remark 1 in the main paper, when the MLE is an admissible parameter (ˆθt ∈ Θ) it is
possible to obtain a usually tighter concentration result. In this section, we explain exactly how this can be done.
Note that this improvement is mostly useful for the design of the algorithm and has no impact on the regret
guarantees.

We deﬁne

√

¯βδ
T = kµ

1 + 2S

(cid:16)√

λS + ρδ
T

(cid:17)

,

(32)

where ρδ
T

is deﬁned in Equation (26).

Proposition 5. For any δ ∈ (0, 1], with probability higher than 1 − δ,

∀t ∈ T (γ) s.t ˆθt ∈ Θ, ∆t(a, ˆθt) ≤

kµ
λ

γD
1 − γ

(2Skµ + m) +

¯βδ
T√
cµ

(cid:107)a(cid:107)V−1

t

.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Proof. We use the notation Gt (respectively (cid:101)Gt) instead of Gt(θ(cid:63)
same steps as for the proof of Proposition 4, one gets

t , ˆθt) (respectively (cid:101)Gt(θ(cid:63)

t , ˆθt)). Following the

∆t(a, ˆθt) ≤

≤

≤

kµ
λ
kµ
λ
kµ
λ

γD
1 − γ
γD
1 − γ
γD
1 − γ

(2Skµ + m) + kµ|a(cid:62)G−1

t (γt−1St−D:t − λθ(cid:63)

t )|

(2Skµ + m) + (cid:107)a(cid:107)G−1

t (cid:101)GtG−1

t

(cid:107)γt−1St−D:t − λθ(cid:63)
t (cid:107)

(cid:101)G−1
t

(2Skµ + m) + (cid:107)a(cid:107)G−1

t

(cid:107)γt−1St−D:t − λθ(cid:63)
t (cid:107)

(cid:101)G−1
t

.

(Equation (28))

Here, with the additional assumption ˆθt ∈ Θ, the self-concordance can be used to obtain an easier relation between
(cid:101)Gt and (cid:101)Ht as stated in Lemma 6.

∆t(a, ˆθt) ≤

≤

kµ
λ
kµ
λ

γD
1 − γ
γD
1 − γ

(2Skµ + m) +

(2Skµ + m) +

√

√

1 + 2S(cid:107)a(cid:107)G−1

t

(cid:107)γt−1St−D:t − λθ(cid:63)
t (cid:107)

(cid:101)H−1
t

(Lemma 6)

1 + 2S(cid:107)a(cid:107)G−1

t

(cid:107)γt−1St−D:t − λθ(cid:63)
t (cid:107)

(cid:101)H−1

t−D:t

.

The last inequality uses (cid:101)Ht−D:t ≤ (cid:101)Ht. Now by applying Corollary 1, ∆t(a, ˆθt) can be further upper bounded.

∆t(a, ˆθt) ≤

kµ
λ

γD
1 − γ

(2Skµ + m) +

√

1 + 2S(cid:107)a(cid:107)G−1

t

(cid:16)√

λS + ρδ
T

(cid:17)

.

The ﬁnal step consists in using Gt = Gt(θ(cid:63)

t , ˆθt) ≥ cµVt which holds because both ˆθt and θ(cid:63)

t

are in Θ.

Consequently, when ˆθt ∈ Θ, the action at at time t can be chosen according to:

at = arg max

a∈At

= arg max

a∈At

(cid:18)

(cid:18)

µ(a(cid:62) ˆθt) +

µ(a(cid:62) ˆθt) +

¯βδ
T√
cµ
¯βδ
T√
cµ

(cid:107)a(cid:107)V−1

t

+

kµ
λ

γD
1 − γ

(cid:19)

(2Skµ + m)

(cid:19)

.

(cid:107)a(cid:107)V−1

t

(33)

C REGRET ANALYSIS WITH A SLIDING WINDOW

In the main paper only the analysis with discount factors is discussed. However as in the linear bandit literature,
the analysis with exponential weights and a sliding window share similarities, in particular they have the same
form of guarantees for the regret. For the sake of completeness, we give a detailed analysis of the results achievable
with a sliding window.

C.1 Notation

Let us ﬁrst introduce the main notations. For any value of θ ∈ Rd, we deﬁne,

Ht(θ) =

t−1
(cid:88)

s=max(1,t−τ )

˙µ(a(cid:62)

s θ)asa(cid:62)

s + λId .

t−1
(cid:88)

Vt =

s=max(1,t−τ )

asa(cid:62)

s +

λ
cµ

Id .

gt(θ) =

t−1
(cid:88)

s=max(1,t−τ )

µ(a(cid:62)

s θ)as + λθ .

(34)

(35)

(36)

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

For any θ1, θ2 ∈ Rd,

t−1
(cid:88)

St =

(cid:15)s+1as .

s=max(1,t−τ )

α(a, θ1, θ2) =

(cid:90) 1

˙µ(va(cid:62)θ2 + (1 − v)a(cid:62)θ1)dv .

Gt(θ1, θ2) =

0

t−1
(cid:88)

s=max(1,t−τ )

α(as, θ1, θ2)asa(cid:62)

s + λId .

Let Ht be deﬁned as

Let us deﬁne T (τ ) as

t−1
(cid:88)

Ht =

s=max(1,t−τ )

˙µ(a(cid:62)

s θ(cid:63)

s )asa(cid:62)

s + λId .

T (τ ) = {1 ≤ t ≤ T, ∀s, such that t − τ ≤ s ≤ t − 1, θ(cid:63)

s = θ(cid:63)

t } .

(37)

(38)

(39)

(40)

t ∈ T (τ ) when t is a least τ steps away from the closest previous breakpoint. When focusing on time instants in
T (τ ) the bias due to non-stationarity disappears. In the sliding window setting, we construct an estimator based
on a truncated penalized log-likelihood. In this section, ˆθt is deﬁned as the unique maximizer of

t−1
(cid:88)

s=max(1,t−τ )

log Pθ(rs+1|as) −

λ
2

(cid:107)θ(cid:107)2

2 .

(41)

By using the deﬁnition of the GLM and thanks to the concavity of this equation in θ, ˆθt is the unique solution of

t−1
(cid:88)

(rs+1 − µ(a(cid:62)

s θ))as − λθ = 0 .

s=max(1,t−τ )

This can be summarized with

gt(ˆθt) =

t−1
(cid:88)

rs+1as = St +

t−1
(cid:88)

s=max(1,t−τ )

s=max(1,t−τ )

C.2 Algorithm

µ(a(cid:62)

s θ(cid:63)

s )as .

The SC-SW-GLUCB algorithm proceeds as follows. First, based on the τ last rewards and actions, ˆθt is computed
using Equation (41). Then, after receiving the action set At the action at is chosen optimistically. Finally, by
proposing this action a reward rt+1 is received and the design matrix is updated. The pseudo code of SC-SW-GLUCB
is reported in Algorithm 2.

Algorithm 2 SC-SW-GLUCB

Input: Probability δ, dimension d, regularization λ, upper bound for bandit parameters S, sliding window τ .
Initialize: V0 = (λ/cµ)Id, ˆθ0 = 0Rd .
for t = 1 to T do

(cid:107)a(cid:107)V−1

t

with βδ
t

deﬁned in Equation (43)

Receive At, compute ˆθt according to (41)
Play at = arg maxa∈At µ(a(cid:62) ˆθt) + βδ
Receive reward rt+1
Update:
if t < τ then

t√

cµ

Vt+1 ← ata(cid:62)

t + Vt

else

Vt+1 ← ata(cid:62)

t − at−τ a(cid:62)

t−τ + Vt

end if
end for

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

C.3 Analysis of the Regret of SC-SW-GLUCB

In Section B, the self-concordance is the key tool to obtain an analysis without using a projection step. In the
next proposition, we link the matrix Gt(ˆθt, θ(cid:63)
Proposition 6. When ˆθt is the maximum likelihood estimator as deﬁned in Equation (41) and t ∈ T (τ ), we
have:

t ) independently from cµ.

t ) with Ht(θ(cid:63)

α(a, θ(cid:63)

t , ˆθt) ≥

(cid:18)

1 + S +

1
√
λ

(cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:19)−1

˙µ(a(cid:62)θ(cid:63)

t ) .

Note that the main diﬀerence with Proposition 3 is that ¯S is now replaced by S. This is due to the fact that the
bias disappears when using a sliding window for t ∈ T (τ ).

Proof. Thanks to Lemma 4, we have:

α(a, θ(cid:63)

t , ˆθt) ≥

≥

≥

≥

≥

≥

(cid:18)

(cid:18)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

1 +

1 +

(cid:12)
(cid:12)a(cid:62)(θ(cid:63)
(cid:12)
(cid:12)
(cid:12)a(cid:62)G−1
(cid:12)

(cid:12)
t − ˆθt)
(cid:12)
(cid:12)

(cid:17)−1

˙µ(a(cid:62)θ(cid:63)
t )

t (θ(cid:63)

t , ˆθt)(gt(θ(cid:63)

(cid:12)
t ) − gt(ˆθt))
(cid:12)
(cid:12)

(cid:17)−1

1 + (cid:107)a(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:13)
(cid:13)gt(θ(cid:63)
(cid:13)

t ) − gt(ˆθt)

(cid:13)
(cid:13)
(cid:13)G−1

1 + λ−1/2 (cid:13)
(cid:13)gt(θ(cid:63)
(cid:13)

t ) − gt(ˆθt)

(cid:13)
(cid:13)
(cid:13)G−1

t

t ,ˆθt)

(θ(cid:63)
t
(cid:19)−1

t ,ˆθt)

(θ(cid:63)
(cid:17)−1

1 + λ−1/2 (cid:107)St − λθ(cid:63)

t (cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

˙µ(a(cid:62)θ(cid:63)
t )

(t ∈ T (τ ))

1 + S + λ−1/2 (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:17)−1

˙µ(a(cid:62)θ(cid:63)

t ) .

˙µ(a(cid:62)θ(cid:63)
t )
(cid:19)−1

(Mean-Value Theorem)

˙µ(a(cid:62)θ(cid:63)
t )

(Cauchy–Schwarz)

˙µ(a(cid:62)θ(cid:63)
t )

(Gt(θ(cid:63)

t , ˆθt) ≥ λId)

Corollary 5. When ˆθt is the maximum likelihood estimator as deﬁned in Equation (41), when t ∈ T (τ ) and Ht
is deﬁned in Equation (39), we have,

Furthermore,

Gt(θ∗

t , ˆθt) ≥

(cid:18)

1 + S +

1
√
λ

(cid:19)−1

(cid:107)St(cid:107)G−1

t

(θ∗

t ,ˆθt)

Ht .

∀t ≤ T, (cid:107)St(cid:107)G−1

t

t ,ˆθt) ≤
(θ(cid:63)

√

1 + S (cid:107)St(cid:107)H−1

t

+

1
√
λ

(cid:107)St(cid:107)2

H−1
t

.

Proof. Using Proposition 6 and summing for time instants s such that max(1, t − τ ) ≤ s ≤ t − 1,

t−1
(cid:88)

s=t−τ

α(as, θ(cid:63)

t , ˆθt)asa(cid:62)

s ≥

(cid:16)

1 + S + λ−1/2 (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:17)−1 t−1
(cid:88)

s=t−τ

˙µ(a(cid:62)

s θ(cid:63)

s )asa(cid:62)
s .

Where we use θ(cid:63)
the regularization term on both sides. Note that

s = θ(cid:63)
t

(cid:16)

for t − τ ≤ s ≤ t − 1 thanks to the assumption t ∈ T (τ ). The next step consists in adding

1 + S + λ−1/2 (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:17)

λ ≥ λ and obtain,

Gt(θ(cid:63)

t , ˆθt) ≥

(cid:16)

1 + S + λ−1/2 (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:17)−1

Ht .

This in turn implies,

(cid:107)St(cid:107)2
t ,ˆθt) ≤
G−1
(θ(cid:63)
t
⇐⇒ (cid:107)St(cid:107)2

G−1
t

(θ(cid:63)

(cid:16)

1 + S + λ−1/2 (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:17)

t ,ˆθt) − λ−1/2 (cid:107)St(cid:107)2

H−1
t

(cid:107)St(cid:107)G−1

t

(θ(cid:63)

(cid:107)St(cid:107)2
H−1
t
t ,ˆθt) − (1 + S) (cid:107)St(cid:107)2

≤ 0 .

H−1
t

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Solving this polynomial inequality (in (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

) ﬁnally gives,

(cid:107)St(cid:107)G−1

t

t ,ˆθt) ≤
(θ(cid:63)

√

1 + S (cid:107)St(cid:107)H−1

t

+

1
√
λ

(cid:107)St(cid:107)2

H−1
t

.

Using this technique, we have established an explicit link between Gt(θ(cid:63)
ˆθt on Θ when t ∈ T (τ ).
We deﬁne

t , ˆθt) and Ht without the need to project

ρδ
t =

and

(cid:32) √

λ
2m

+

2m
√
λ

log

(cid:19)

(cid:18) T
δ

+

dm
√
λ

(cid:18)

log

1 +

kµ min(t, τ )
dλ

(cid:19)

+

2m
√
λ

√

βδ
t = kµ

(cid:32)

λ

1 + S +

(cid:114)

1 + S
λ

ρδ
t +

(cid:18) ρδ
t√
λ

(cid:19)2(cid:33)3/2

.

(cid:33)

d log(2)

,

(42)

(43)

In the next proposition, we give an upper bound for ∆t(a, ˆθt).
Proposition 7. For any δ ∈ (0, 1], with probability higher than 1 − δ,

∀t ∈ T (τ ), ∆t(a, ˆθt) ≤

βδ
t√
cµ

(cid:107)a(cid:107)V−1

t

.

Proof.

∆t(a, ˆθt) = |µ(a(cid:62)θ(cid:63)

t − ˆθt)|

t ) − µ(a(cid:62) ˆθt)| ≤ kµ|a(cid:62)(θ(cid:63)
t , ˆθt)(gt(θ(cid:63)
t (θ(cid:63)
t ,ˆθt)(cid:107)gt(θ(cid:63)
t ,ˆθt)(cid:107)St − λθ(cid:63)

t ) − gt(ˆθt))|
t ) − gt(ˆθt)(cid:107)G−1
(θ(cid:63)
t ,ˆθt) .
t (cid:107)G−1
(θ(cid:63)

(θ(cid:63)

(θ(cid:63)

t

t

t

= kµ|a(cid:62)G−1
≤ kµ(cid:107)a(cid:107)G−1
≤ kµ(cid:107)a(cid:107)G−1

t

(Mean-Value Theorem)

(Cauchy-Schwarz ineq.)

t ,ˆθt)

(t ∈ T (τ ))

We can use Corollary 5 to link (cid:107)a(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

with (cid:107)a(cid:107)H−1

t

.

(cid:115)

∆t(a, ˆθt) ≤ kµ

1 + S +

(cid:107)St(cid:107)G−1

t

t ,ˆθt)(cid:107)a(cid:107)H−1
(θ(cid:63)

t

(cid:16)√

λS + (cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:17)

1
√
λ

√

√

≤ kµ

≤ kµ

(cid:115)

λ

1 + S +

(cid:18)

λ

1 + S +

1
√
λ

1
√
λ

(cid:18)

(cid:107)St(cid:107)G−1

t

t ,ˆθt)(cid:107)a(cid:107)H−1
(θ(cid:63)

t

S +

(cid:19)3/2

(cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:107)a(cid:107)H−1

t

.

1
√
λ

(cid:107)St(cid:107)G−1

t

(θ(cid:63)

t ,ˆθt)

(cid:19)

Then, using Corollary 5 we can upper bound (cid:107)St(cid:107)G−1
Recall that Corollary 2 gives with probability higher than 1 − δ , for all t in T (τ ), (cid:107)St(cid:107)H−1

with a combination of terms depending on (cid:107)St(cid:107)H−1

≤ ρδ
t

.

t ,ˆθt)

(θ(cid:63)

t

t

t

.

∆t(a, ˆθt) ≤ kµ

√

(cid:32)

(cid:114)

λ

1 + S +

(cid:33)3/2

1 + S
λ

ρδ
t +

1
λ

(ρδ

t )2

(cid:107)a(cid:107)H−1

t

.

The proof is completed using Ht ≥ cµVt, which holds thanks to Assumption 1 on the bandit parameters.

Finally, we give an upper bound for the regret enjoyed by SC-SW-GLUCB.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Theorem 5. The regret of the SC-SW-GLUCB algorithm is bounded with probability at least 1 − δ by,

RT ≤ ΓT τ +

dT (cid:112)(cid:100)T /τ (cid:101)

(cid:115)

(cid:18)

2 max

1,

√

2βδ
T√
cµ

(cid:19)(cid:114)

log

1
λ

(cid:16)

1 +

(cid:17)

,

τ
dλ

where βδ

t is deﬁned in Equation (43).

Proof. The proof essentially follows the steps of the proof of Theorem 2. The main diﬀerence is that βδ
from
t
Equation (43) is used and the elliptical lemma is diﬀerent because the design matrix is designed with a sliding
window instead of weights.

Applying Proposition 9 when t ∈ T (τ ), with probability higher than 1 − δ,

rt ≤

2
√
cµ

βδ
t (cid:107)at(cid:107)V−1

t

.

(44)

The dynamic regret can then be upper bounded by,

RT =

T
(cid:88)

t=1

rT =

(cid:88)

rt +

(cid:88)

rt ≤ ΓT τ +

t∈T (τ )

t /∈T (τ )

(cid:88)

rt

t∈T (τ )

≤ ΓT τ +

≤ ΓT τ +

≤ ΓT τ +

2βδ
T√
cµ

2βδ
T√
cµ

2βδ
T√
cµ

(cid:88)

(cid:107)at(cid:107)V−1

t

(Equation (44))

t∈T (τ )
√

(cid:115) (cid:88)

T

t∈T (τ )

(cid:107)at(cid:107)2

V−1
t

(Cauchy-Schwarz ineq.)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

√

T

T
(cid:88)

t=1

(cid:107)at(cid:107)2

V−1
t

≤ ΓT τ + +

√

2βδ
T√
cµ

dT (cid:112)(cid:100)T /τ (cid:101)

(cid:115)

(cid:18)

2 max

1,

(cid:19)

1
λ

(cid:16)

1 +

log

(cid:17)

.

τ
dλ

(Lemma 8)

Corollary 6 (Asymptotic bound). If ΓT is known, by choosing τ =

(cid:19)2/3

(cid:18)

dT
c1/2
µ ΓT

and λ = d log(T ), the regret of

SC-SW-GLUCB scales as

If ΓT is unknown, by choosing τ =

RT = (cid:101)O(c−1/3
(cid:19)2/3

µ

d2/3Γ1/3

T T 2/3) .

, the regret of SC-SW-GLUCB scales as

(cid:18)

dT
c1/2
µ

RT = (cid:101)O(c−1/3

µ

d2/3ΓT T 2/3) .

Proof. When ΓT is known, we set λ = d log(T ) and τ =

(cid:16) dT√

cµΓT

(cid:17)2/3

. With those choices,

1. βδ
T

scales as (cid:112)d log(T ).

2. ΓT τ scales as (cid:101)O(c−1/3
(cid:113)

√

µ

d2/3Γ2/3

T T 2/3).

3. βδ
T√
cµ

T

d T
τ

scales as (cid:101)O(c−1/3

µ

d2/3Γ1/3

T T 2/3).

The proof is similar when ΓT is unknown.

When the reward gaps are bounded from below we can obtain the following gap-dependent upper bound:

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Theorem 6. Under Assumption 5, when setting τ = d

√
T√
cµΓT

the regret of the SC-SW-GLUCB algorithm satisﬁes:

Proof. First note that for any suboptimal action a ∈ At,

RT = (cid:101)O(cid:0)∆−1c−1/2

µ

(cid:112)

d

ΓT T (cid:1) .

This implies

µ(a(cid:62)

(cid:63),tθ(cid:63)

t ) − µ(a(cid:62)θ(cid:63)

t ) ≥ ∆ .

rt = µ(a(cid:62)

(cid:63),tθ(cid:63)

t ) − µ(a(cid:62)

t θ(cid:63)

t ) ≤

(cid:0)µ(a(cid:62)

(cid:63),tθ(cid:63)

t θ(cid:63)

t )(cid:1)2

t ) − µ(a(cid:62)
∆

=

r2
t
∆

.

(45)

Using Proposition 9 one has,

rt ≤

2
√
cµ

βδ
t (cid:107)at(cid:107)V−1

t

.

The dynamic regret can then be upper bounded by,

RT ≤ ΓT τ +

1
∆

(cid:88)

r2
t

t∈T (τ )

(Equation (45))

≤ ΓT τ +

≤ ΓT τ +

T )2
4(βδ
cµ∆
8(βδ
T )2
cµ∆

T
(cid:88)

t=1

(cid:107)at(cid:107)2

V−1
t

(cid:18)

max

1,

(cid:19)

1
λ

d(cid:100)T /τ (cid:101) log

(cid:16)

1 +

(cid:17)

τ
λd

.

(Lemma 8)

We set λ = d log(T ) and τ = d

√
T√
cµΓT

. With those choices,

1. βδ
T

scales as (cid:112)d log(T ).

2. ΓT τ scales as (cid:101)O(c−1/2

µ

dΓ1/2

T T 1/2).

3. (βδ
T )2
cµ

d T
τ

scales as (cid:101)O(c−1/2

µ

dΓ1/2

T T 1/2).

Dividing by ∆ yields the announced result.

When ˆθt is in Θ it is also possible with a sliding window to obtain a usually better concentration result. This
discussion is not reported here, but can be easily adapted from Proposition 5.

D USEFUL RESULTS

D.1 Self-Concordant Properties

In this section we state the main properties and lemma that can be obtained with the self-concordance assumption.

Lemma 4 (Lemma 9 in Faury et al. (2020)). For any z1, z2 ∈ R, we have the following inequality

˙µ(z1)

1 − exp(−|z1 − z2|)
|z1 − z2|

≤

(cid:90) 1

0

˙µ(z1 + v(z2 − z1))dv ≤ ˙µ(z1)

exp(|z1 − z2|) − 1
|z1 − z2|

.

Furthermore,

(cid:90) 1

0

˙µ(z1 + v(z2 − z1))dv ≥ ˙µ(z1)(1 + |z1 − z2|)−1 .

Thanks to the self-concordance property we have an interesting relation between Gt(θ1, θ2) and Ht(θ1) or Ht(θ2)
when both θ1 and θ2 ∈ Θ. This relation is made explicit in the next lemma.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Lemma 5 (Self-concordance and sliding window). For all θ1, θ2 ∈ Θ, with Gt deﬁned in Equation (38) and Ht
deﬁned in Equation (34) the following inequalities hold

Gt(θ1, θ2) ≥ (1 + 2S)−1Ht(θ1) , Gt(θ1, θ2) ≥ (1 + 2S)−1Ht(θ2) .

Proof. Applying Lemma 4, for any θ1, θ2 ∈ Rd,

α(a, θ1, θ2) ≥

˙µ(a(cid:62)θ1)
1 + |a(cid:62)(θ1 − θ2)|

and α(a, θ1, θ2) ≥

˙µ(a(cid:62)θ2)
1 + |a(cid:62)(θ1 − θ2)|

.

Furthermore, if θ1 and θ2 ∈ Θ, then

|a(cid:62)(θ1 − θ2)| ≤ 2S .

Lemma 6 (Self-concordance and discount factors). For all θ1, θ2 ∈ Θ, with (cid:101)Ht(θ1) deﬁned in Equation (16) and
(cid:101)Gt(θ1, θ2) deﬁned in Equation (22) the following inequalities hold:

(cid:101)Gt(θ1, θ2) ≥ (1 + 2S)−1 (cid:101)Ht(θ1) ,

(cid:101)Gt(θ1, θ2) ≥ (1 + 2S)−1 (cid:101)Ht(θ2) .

Proof. Same arguments than for Lemma 5

D.2 Determinant Inequalities

Proposition 8 (Determinant inequality). Let (λt)t be a deterministic sequence of regularization parameters. Let
Ht = (cid:80)t−1

s + λt−1Id. Under the Assumption 1 and ∀t, σ2

t ≤ kµ, the following holds

s asa(cid:62)

sσ2

s=1 w2

Proof.

det(Ht) =

≤

≤

(cid:32)

det(Ht) ≤

λt−1 +

kµ

(cid:80)t

s=1 w2
s
d

(cid:33)d

.

d
(cid:89)

i=1
(cid:18) 1
d

li

(li are the eigenvalues) ≤

(cid:33)d

(cid:32)

1
d

d
(cid:88)

i=1

li

(AM-GM inequality)

(cid:19)d

trace(Ht)

(cid:32)

≤

1
d

t−1
(cid:88)

s=1

w2

sσ2
s

trace(asa(cid:62)

s ) + λt−1

(cid:33)d

(cid:32)

1
d

t−1
(cid:88)

s=1

w2

sσ2

s (cid:107)as(cid:107)2

2 + λt−1

(cid:33)d

(cid:32)

≤

λt−1 +

(cid:33)d

w2
s

.

kµ
d

t−1
(cid:88)

s=1

Corollary 7. In the speciﬁc case where the weights are given by wt = γ−t with 0 < γ < 1, under the same
assumptions than Proposition 8, with (cid:101)Ht = (cid:80)t−1

s + λId, one has

γ2(t−1−s)σ2

s asa(cid:62)

s=t−t0

(cid:18)

det( (cid:101)Ht) ≤

λ +

kµ(1 − γ2t0)
d(1 − γ2)

(cid:19)d

.

Corollary 8. In the speciﬁc case where the weights are given by wt = γ−t with 0 < γ < 1, under Assumption 1
with Vt = (cid:80)t−1

s + λId, one has

s=1 γt−1−sasa(cid:62)

(cid:18)

det(Vt) ≤

λ +

1 − γt−1
d(1 − γ)

(cid:19)d

.

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

Corollary 9. In the speciﬁc case where the weights are given by wt = 1 when t ≥ t − τ and 0 before. With
Ht = (cid:80)t−1

s + λId, one has

s=max(1,t−τ ) σ2

s asa(cid:62)

(cid:18)

det(Ht) ≤

λ +

kµ min(t, τ )
d

(cid:19)d

.

D.3 Elliptical Lemma

The following lemma is a version of the Elliptical Lemma when discount factors are used. It comes from Proposition
4 in (Russac et al., 2019) and is stated here for the sake of completeness.

Lemma 7 (Elliptical potential with discount factors (based on Proposition 4 in Russac et al. (2019))). Let
s=1 a sequence in Rd such that (cid:107)as(cid:107)2 ≤ 1 for all s ∈ N, and let λ be a non-negative scalar. For t ≥ 1 deﬁne
{as}∞
Vt = (cid:80)t−1

s + λId, the following inequality holds

s=1 γt−1−sasa(cid:62)

T
(cid:88)

t=1

(cid:107)at(cid:107)2

V−1
t

(cid:18)

≤ 2 max

1,

(cid:19)

1
λ

log

(cid:18) det(VT +1)
λdγdT

(cid:19)

.

Proof. In the proof we introduce the matrix Wt = (cid:80)t−1
have,

s=1 γ−sasa(cid:62)

s + γ−(t−1)λId such that Vt = γt−1Wt. We

Wt =

t−1
(cid:88)

s=1

γ−sasa(cid:62)

s + γ−(t−1)λId

= γ−(t−1)at−1a(cid:62)

t−1 +

t−2
(cid:88)

s=1

γ−sasa(cid:62)

s + γ−(t−2)λId + γ−(t−1)λId − γ−(t−2)λId

= γ−(t−1)at−1a(cid:62)
≥ γ−(t−1)at−1a(cid:62)

t−1 + γ−(t−1)(1 − γ)λId + Wt−1
t−1 + Wt−1 ≥ W1/2

t−1(Id + γ−(t−1)W−1/2

t−1 at−1a(cid:62)

t−1W−1/2

t−1 )W1/2
t−1 .

This implies,

det(Wt+1) ≥ det(Wt) det

(cid:16)

≥ det(Wt)

(cid:16)

1 + γ−t(cid:107)at(cid:107)2

W−1
t

This in turn gives,

Id + (γ−t/2W−1/2

at)(γ−t/2W−1/2

t

at)(cid:62)(cid:17)

t
(cid:17)

(det(Id + xx(cid:62)) = 1 + (cid:107)x(cid:107)2

2) .

det(WT +1)
det(W1)

=

T
(cid:89)

t=1

det(Wt+1)
det(Wt)

≥

T
(cid:89)

(cid:16)

t=1

1 + γ−t(cid:107)at(cid:107)2

W−1
t

(cid:17)

.

Taking the logarithm on both sides gives:

log

(cid:18) det(WT +1)
λd

(cid:19)

≥

≥

T
(cid:88)

t=1

T
(cid:88)

t=1

log(1 + γ−t(cid:107)at(cid:107)2

W−1
t

log(1 + γ−(t−1)(cid:107)at(cid:107)2

)

W−1
t

T
(cid:88)

) ≥



log

1 +

γ−(t−1)(cid:107)at(cid:107)2
W−1
t
(cid:1)
max (cid:0)1, 1

λ

t=1


 .

Next, by using Wt ≥ γ−(t−1)λId, we see that

γ−(t−1)(cid:107)at(cid:107)2

W−1
t

≤

1
λ

.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Which ensures that

Finally, with log(1 + x) ≥ x/2 valid when 0 ≤ x ≤ 1. We get,

0 ≤

γ−(t−1)(cid:107)at(cid:107)2
W−1
t
max (cid:0)1, 1
(cid:1)

λ

≤ 1 .

log

(cid:18) det(WT +1)
λd

(cid:19)

≥

1
2 max (cid:0)1, 1

λ

(cid:1)

T
(cid:88)

t=1

γ−(t−1)(cid:107)at(cid:107)2

W−1
t

.

The following lemma is a version of the Elliptical Lemma when a sliding window is used and can be extracted
from (Russac et al., 2019, Proposition 9). The proof is included here for the sake of completeness.
Lemma 8 (Elliptical potential with sliding window (Proposition 9 in Russac et al. (2019))). Let {as}∞
s=1
a sequence in Rd such that (cid:107)as(cid:107)2 ≤ 1 for all s ∈ N, and let λ be a non-negative scalar. For t ≥ 1 deﬁne
Vt = (cid:80)t−1

s + λId. The following inequality holds:

s=max(1,t−τ ) asa(cid:62)

T
(cid:88)

t=1

(cid:107)at(cid:107)2

V−1
t

(cid:18)

≤ 2d max

1,

(cid:19)

1
λ

(cid:100)T /τ (cid:101) log

(cid:16)

1 +

(cid:17)

τ
λd

.

Proof. We start by rewriting the sum as follows.

T
(cid:88)

t=1

(cid:107)at(cid:107)2

V−1
t

(cid:100)T /τ (cid:101)−1
(cid:88)

(k+1)τ
(cid:88)

=

k=0

t=kτ +1

(cid:107)at(cid:107)2

V−1
t

.

For the k-th block of length τ we deﬁne the matrix W(k)
1)τ ]], Vt ≥ W(k)
as every term in W(k)
deﬁnite matrices.

t

t

s + λId. We also have ∀t ∈ [[kτ + 1, (k +
is contained in Vt and the extra-terms in Vt correspond to positive

s=kτ +1 asa(cid:62)

t = (cid:80)t−1

Furthermore, ∀t ∈ [[kτ + 1, (k + 1)τ ]] we have,

k=0

t=kτ +1

(cid:100)T /τ (cid:101)−1
(cid:88)

(k+1)τ
(cid:88)

(cid:107)at(cid:107)2

V−1
t

(cid:100)T /τ (cid:101)−1
(cid:88)

(k+1)τ
(cid:88)

≤

k=0

t=kτ +1

(cid:107)at(cid:107)2

(W(k)
t

)−1 .

With positive deﬁnitive matrices whose determinants are strictly positive, this implies that

det(W(k)

t+1) = det(W(k)

t

(cid:16)

)

1 + (cid:107)at(cid:107)2

(W(k)
t

(cid:17)

.

)−1

det(W(k)

(k+1)τ +1)
kτ +1)

det(W(k)

(k+1)τ
(cid:89)

=

t=kτ +1

det(W(k)
t+1)
det(W(k)
)

t

=

(k+1)τ
(cid:89)

(cid:16)

t=kτ +1

1 + (cid:107)at(cid:107)2

(W(k)
t

(cid:17)

.

)−1

By deﬁnition we have W(k)

t + λId = λId.

kτ +1 = (cid:80)kτ
(cid:16)

t=kτ +1 ata(cid:62)
(cid:17)



det

W(k)



log



(k+1)τ +1
λd

 =

≥

(k+1)τ
(cid:88)

t=kτ +1

(k+1)τ
(cid:88)

t=kτ +1

log

(cid:16)
1 + (cid:107)at(cid:107)2

(W(k)
t

(cid:17)

)−1

(cid:18)

log

1 +

1
max(1, 1/λ)

(cid:107)at(cid:107)2

(W(k)
t

)−1

(cid:19)

.

In the next step we use, ∀0 ≤ x ≤ 1, log(1 + x) ≥ x/2.



det

(cid:16)

log



W(k)

(k+1)τ +1
λd

(cid:17)



 ≥

1
2 max(1, 1/λ)

(k+1)τ
(cid:88)

t=kτ +1

(cid:107)at(cid:107)2

(W(k)
t

)−1 .

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

By summing, over the diﬀerent blocks, we obtain

(cid:100)T /τ (cid:101)−1
(cid:88)

(k+1)τ
(cid:88)

k=0

t=kτ +1

(cid:107)at(cid:107)2

V−1
t

(cid:100)T /τ (cid:101)−1
(cid:88)

(k+1)τ
(cid:88)

≤

k=0

t=kτ +1

(cid:107)at(cid:107)2

(W(k)
t

)−1

≤ 2 max(1, 1/λ)

(cid:100)T /τ (cid:101)−1
(cid:88)

k=0



det

(cid:16)

log



W(k)

(k+1)τ +1
λd

(cid:17)



 .

Then, we upper bound det(W(k)

(k+1)τ +1) using similar arguments than for Corollary 9,

τ
d
Applying the logarithm function on both sides concludes the proof.

(k+1)τ +1) ≤

det(W(k)

λ +

(cid:16)

(cid:17)d

.

D.4 Link Between ∆t and the Instantaneous Regret

For any optimistic algorithm, even in a non-stationary environment the instantaneous regret can be directly
related to ∆t(a, θ) deﬁned as

∆t(a, θ) = |µ(a(cid:62)θ) − µ(a(cid:62)θ(cid:63)

t )| .

Proposition 9 (Based on Lemma 14 in Faury et al. (2020)). Consider any optimistic algorithm in a possibly
non-stationary environment such that the exploration bonus for action a at time t is deﬁned by βt(a). Let θt be
the estimator used at time t by the algorithm to compute the UCB, i.e. U CBt(a) = µ(a(cid:62)θt) + βt(a). Under the
assumption ∆t(a, θt) ≤ βt(a), the following inequality holds

rt ≤ 2βt(at) .

Proof. Let at,(cid:63) = arg maxa∈At µ(a(cid:62)θ(cid:63)
t )
t ) ≤ |µ(a(cid:62)
t,(cid:63)θ(cid:63)

t ) − µ(a(cid:62)

rt = µ(a(cid:62)

t θ(cid:63)

t,(cid:63)θ(cid:63)
= ∆t(at, θt) + ∆t(at,(cid:63), θt) + µ(a(cid:62)
= ∆t(at, θt) + ∆t(at,(cid:63), θt) + µ(a(cid:62)

t ) − µ(a(cid:62)
t,(cid:63)θt) − µ(a(cid:62)
t,(cid:63)θt) + βt(a(cid:63)

t θt)
t ) − µ(a(cid:62)

t,(cid:63)θt)| + µ(a(cid:62)

t,(cid:63)θt) − µ(a(cid:62)

t θt) + |µ(a(cid:62)

t θt) − µ(a(cid:62)

t θ(cid:63)

t )|

t θt) − βt(at) + βt(at) − βt(a(cid:63)

t ) .

For any optimistic algorithm with an exploration bonus of βt(.) and such that the upper conﬁdence bound of the
action a at time t is given by µ(a(cid:62)θt) + βt(a), by deﬁnition for all a ∈ At

In particular, this is also true for the action at,(cid:63). Therefore, plugging this inequality in the expression of the
instantaneous regret gives

µ(a(cid:62)θt) + βt(a) ≤ µ(a(cid:62)

t θt) + βt(at) .

Under the additional assumption that ∆t(a, θ) ≤ βt(a), we obtain the announced result.

rt ≤ ∆t(at, θt) + ∆t(at,(cid:63), θt) + β(at) − β(a(cid:63)

t ) .

This proposition shows that any improvement in an upper bound of ∆t(a, θt) will result in an improvement of the
regret, as long as the exploration bonus satisﬁes the assumption stated in the proposition.

E ON THE WORST CASE REGRET IN THE K-ARM SETTING

In this section, we build upon the analysis from Garivier and Moulines (2011) to provide a worst case regret
bound for the sliding window policy in the K-arm setting. Even if a proper lower bound is missing, the results
we provide here suggest that in some cases sliding window policies can suﬀer a regret of order O(Γ1/3
T T 2/3) in the
simpler K-arm setting. In particular, this would mean that the T 2/3 dependency is not a sub-optimality from our
setting but can already be seen for forgetting policies in the non-contextual setting. Worst-case regret bounds
(i.e. gap independent) for forgetting policies in non-stationary environments have seen little treatment in the
literature.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

Setting. The setting considered in this section is the one from Garivier and Moulines (2011). At each time t,
the player chooses an arm It ∈ {1, ..., K} based on the previous rewards and actions. Upon selecting It a reward
Xt(It) is observed. We consider abruptly changing environments as in other sections, where the distribution of
the rewards remains constant during phases and changes at unknown time instants. At time t, the arm i has a
mean reward µt(i). As before, ΓT denote the number of abrupt changes in the reward distributions before time
T . Following the notation from Trovo et al. (2020), we denote the ΓT breakpoints B = {b1, ..., bΓT }. We can
associate ΓT stationary phases {φ1, ..., φΓT } with these breakpoints, where φi = {t ∈ {1, ..., T } s.t bi−1 ≤ t < bi}
and b0 = 1. It is further assumed that for all arms and all time instants the means of the reward distributions lie
in [0, B]. In this section the focus is on the forgetting policy using a sliding window but the same arguments can
be used with exponentially increasing weights.

Improving the problem dependent bound.
times the arm i is played before time T while being sub-optimal is upper bounded in expectation as

In (Garivier and Moulines, 2011, Theorem 2), the number of

E [NT (i)] ≤

C(τ )
(∆µT (i))2

T log(τ )
τ

+ τ ΓT + log2(τ ) ,

(46)

where

∆µT (i) = min{µt(i(cid:63)

t ) − µt(i) : t ∈ {1, ..., T }, µt(i) < µt(i(cid:63)

t )} .

This result has a worst case ﬂavor in the sense that ∆µT (i) is the minimum distance between the mean of the
optimal arm and the mean of the i-th arm when i is sub-optimal over the entire time horizon. We obtain a less
pessimistic bound by decomposing the regret into the ΓT diﬀerent stationary phases and upper-bounding the
number of times a sub-optimal arm is drawn in each of these phases φ. The upper-bound naturally depends on
∆φ
, the diﬀerence between the mean of the optimal arm and the i-th arm in the φ-th stationary phase rather
i
than ∆µT (i). This is of utmost importance as for some phases ∆φ
i
During the φ-th stationary phase, let µφ
i
the arm i is selected. The regret can be decomposed as follows:

denote the mean of the i-th arm and N φ
i

can be signiﬁcantly larger than ∆µT (i).

denote the number of times

E [RT ] =

T
(cid:88)

(µ(cid:63)

t − µt(it)) =

t=1

K
(cid:88)

ΓT(cid:88)

i=1

φ=1

∆φ
i

E[N φ

i ] .

(47)

A worst-case bound. The bound from Equation (46) is problem dependent and depends explicitly on the
minimum gap. It is interesting to study the worst case regret. In particular when ∆µT (i) goes to 0 the upper
bound from Equation (46) becomes uninformative. At the same time, with a small gap ∆φ
the cost of selecting
i
the i-th arm rather than the optimal one diminishes. The trade-oﬀ between these two opposite eﬀects is made
explicit in the following result.

Theorem 7. The worst case regret of the sliding window policy from (Garivier and Moulines, 2011), can be
upper-bounded by

E[RT ] ≤ C1

K

√

T
√
τ

√

+ C2

Kτ ΓT + C3K

,

T
τ

with C1, C2 and C3 universal constants that depends only on the logarithm of τ .
In particular, setting τ = T 2/3

yields:

K1/3Γ2/3

T

Proof.

E[RT ] =

K
(cid:88)

ΓT(cid:88)

i=1

φ=1

E[RT ] = (cid:101)O(K 2/3Γ1/3

T T 2/3) .

∆φ
i

E[N φ

i ] =

(cid:88)

∆φ
i

E[N φ

i ] +

(cid:88)

∆φ
i

E[N φ
i ]

(cid:88)

≤

∆φ
i

E[N φ

i ] + ∆

i,φ:∆φ

i >∆
K
(cid:88)

ΓT(cid:88)

i,φ:∆φ

i ≤∆

E[N φ

i ] ≤

(cid:88)

∆φ
i

E[N φ

i ] + ∆T .

i,φ:∆φ

i >∆

i=1

φ=1

i,φ:∆φ

i >∆

Self-Concordant Analysis of Generalized Linear Bandits with Forgetting

The next step consists in upper bounding the expected number of times the arm i is selected in the φ-th phase.
We recall that N φ
i

is deﬁned as

N φ

i =

(cid:88)

t∈φ

1(It = i (cid:54)= i(cid:63)

t ) =

bφ
(cid:88)

t=bφ−1

1(It = i (cid:54)= i(cid:63)

t ) .

We introduce Nt(τ, i) = (cid:80)t
t. We have the following:

s=t−τ +1

1(Is = i), the number of times the arm i was selected in the τ steps preceding

N φ

i =

bφ−1+τ −1
(cid:88)

t=bφ−1

1(It = i (cid:54)= i(cid:63)

t ) +

bφ
(cid:88)

t=bφ−1+τ

1(It = i (cid:54)= i(cid:63)

t ) ≤ τ +

bφ
(cid:88)

t=bφ−1+τ

1(It = i (cid:54)= i(cid:63)
t )

bφ
(cid:88)

≤ τ +

t=bφ−1+τ

1(It = i (cid:54)= i(cid:63)

t , Nt(τ, i) ≤ Aφ

i ) +

bφ
(cid:88)

t=bφ−1+τ

1(It = i (cid:54)= i(cid:63)

t , Nt(τ, i) > Aφ

i ) .

The ﬁrst term can be bounded using (Garivier and Moulines, 2011, Lemma 1) that is restated here.

Lemma 9 (Lemma 1 in (Garivier and Moulines, 2011)). Let i ∈ {1, ..., K}. For any positive integer τ and any
positive m,

T
(cid:88)

t=K+1

1(It = i, Nt(τ, i) ≤ m) ≤ (cid:100)T /τ (cid:101)m .

Lemma 9 can be adapted to our setting and by introducing T φ the length of the φ-th stationary phase, one has:

bφ
(cid:88)

t=bφ−1+τ

This in turn gives,

1(It = i (cid:54)= i(cid:63)

t , Nt(τ, i) ≤ Aφ

i ) ≤ (cid:100)T φ/τ (cid:101)Aφ
i .

N φ

i ≤ τ + (cid:100)T φ/τ (cid:101)Aφ

i +

bφ
(cid:88)

t=bφ−1+τ

1(It = i (cid:54)= i(cid:63)

t , Nt(τ, i) > Aφ

i ) .

We recall that the upper conﬁdence bound for the sliding-window strategy has the following form in the K arm
setting (Garivier and Moulines, 2011):

U CBi(t) = ¯Xt(τ, i) + ct(τ, i) ,

with

¯Xt(τ, i) =

1
Nt(τ, i)

t
(cid:88)

s=t−τ +1

Xs(i)1(Is = i) and

ct(τ, i) = B

(cid:115)

ξ log(min(t, τ ))
Nt(τ, i)

.

Following the same arguments than Garivier and Moulines (2011) when the event {It = i (cid:54)= i(cid:63)
holds, at least one of the three following events E1, E2, E3 must be true where:

t , Nt(τ, i) > Aφ
i }

E1 = { ¯Xt(τ, i) > µt(i) + ct(τ, i)} the case where µt(i) is over-estimated.

E2 = { ¯Xt(τ, i(cid:63)

t ) < µ(cid:63)

t − ct(τ, i(cid:63)

t )} the case where the best arm at time t is under-estimated.

E3 = {µ(cid:63)
From now on, we set

t − µt(i) ≤ 2ct(τ, i), Nt(τ, i) > Aφ

i } the case where the means are too close to each others.

Aφ

i =

4B2ξ log(τ )
(∆φ
i )2

.

Yoan Russac*, Louis Faury*, Olivier Cappé, Aurélien Garivier

In doing so, on the event E3 the following holds:

(cid:115)

ct(τ, i) = B

ξ log(min(t, τ ))
Nt(τ, i)

< B

(cid:115)

ξ log(min(t, τ ))
Aφ
i

<

∆φ
i
2

(cid:115)

log(min(t, τ ))
log(τ )

<

∆φ
i
2

.

Therefore, this choice of Aφ
ensures that the event E3 never occurs. Bounding the probability of the events E1
i
and E2 can be done with the concentration inequality established in (Garivier and Moulines, 2011). For any
η > 0, by selecting a speciﬁc value of ξ one can obtain,

Consequently we have,

P(E1) ≤

(cid:109)

(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

and P(E2) ≤

(cid:109)

(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

.

E[N φ

i ] ≤ τ + (cid:100)T φ/τ (cid:101)

4B2ξ log(τ )
(∆φ
i )2

+ 2

bφ
(cid:88)

t=bφ−1+τ

(cid:109)

(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

.

Plugging this in the regret’s upper bound gives:

E[RT ] ≤

(cid:88)

i,φ:∆φ

i >∆


τ + (cid:100)T φ/τ (cid:101)

∆φ
i

4B2ξ log(τ )
(∆φ
i )2

+ 2

(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

(cid:109)



 + ∆T

bφ
(cid:88)

t=bφ−1+τ



bφ
(cid:88)

(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

(cid:109)



 + ∆T

(cid:88)

≤

i,φ:∆φ

i >∆

4B2ξ log(τ )
∆φ
i

(cid:100)T φ/τ (cid:101) +

(cid:88)

∆φ
i

τ + 2

i,φ:∆φ

i >∆

≤

4B2ξ log(τ )K
∆

T
τ

+ τ KΓT B + 2KB

ΓT(cid:88)

bφ
(cid:88)

φ=1

t=bφ−1+τ

t=bφ−1+τ
(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

(cid:109)

+ ∆T .

In the last inequality we have used ∆φ

i ≤ B coming from µi(t) ∈ [0, B] for all i and all t ≤ T . Furthermore,

ΓT(cid:88)

bφ
(cid:88)

φ=1

t=bφ−1+τ

(cid:109)

(cid:108) log(min(t,τ ))
log(1+η)
min(t, τ )

≤

T
(cid:88)

t=τ

log(min(t,τ ))

log(1+η) + 1
min(t, τ )

=

T
τ

(cid:18) log(τ )

log(1 + η)

(cid:19)

+ 1

.

Hence,

E[RT ] ≤

4B2ξ log(τ )K
∆

T
τ

+ ∆T + τ KΓT B + 2KB

(cid:18) log(τ )

log(1 + η)

+ 1

(cid:19) T
τ

.

By diﬀerentiating with respect to ∆, the right hand side is maximized when setting ∆ = 2B
this value of ∆,

(cid:113) ξ log(τ )K
τ

. With

E[RT ] ≤ 4B(cid:112)ξ log(τ )

√

K

T
√
τ

+ BKτ ΓT + 2BK log(τ )

T
τ

.

Now by selecting τ = T 2/3

, we obtain the announced scaling.

K1/3Γ2/3

T
√

Remark 4. The term T /
τ that can be seen in the worst case bound proposed in Theorem 7 also appears in
the gap independent bound of SC-SW-GLUCB (Theorem 5). When focusing on gap dependent bounds, there is also
a strong similarity. In the K-arm setting, Equation (46) has a T /τ dependency. This term can also be seen
in the GLB setting in Theorem 6 using an analogous assumption on the gap. This analogy explains why the
upper-bounds have the same scaling in the K-arm and in the GLB setting. Going from T /
τ to T /τ when adding
√
the assumption on the gaps is the key step allowing a scaling of the regret of order (cid:101)O(

T ΓT ).

√

