Generalized Chronicles for Temporal Sequence
Classification
Thomas Guyet, Yann Dauxais

To cite this version:

Thomas Guyet, Yann Dauxais. Generalized Chronicles for Temporal Sequence Classification. AALTD
2020 - International Workshop on Advanced Analytics and Learning on Temporal Data, Sep 2020,
Ghent, Belgium. pp.30-45, ￿10.1007/978-3-030-65742-0_3￿. ￿hal-03090211￿

HAL Id: hal-03090211

https://inria.hal.science/hal-03090211

Submitted on 29 Dec 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Generalized chronicles for
temporal sequence classiﬁcation

Yann Dauxais1 and Thomas Guyet2

1 KU Leuven, Celestijnenlaan 200a, Leuven, Belgium
yann.dauxais@kuleuven.be
2 Institut Agro/IRISA UMR6074
thomas.guyet@irisa.fr

Abstract. Discriminant chronicle mining (DCM) [6] tackles temporal
sequence classiﬁcation by combining machine learning and chronicle min-
ing algorithms. A chronicle is a set of events related by temporal bound-
aries on the delay between event occurrences. Such temporal constraints
are poorly expressive and discriminant chronicles may lack of accuracy.
This article generalizes discriminant chronicle mining by modeling com-
plex temporal constraints. We present the generalized model and we
instantiate diﬀerent generalized chronicle models. The accuracy of these
models are compared with each other on simulated and real datasets.

Keywords: Temporal patterns, discriminant patterns, sequence classi-
ﬁcation

1

Introduction

Temporal sequences, i.e., sequences of timestamped events, are broadly encoun-
tered in various applications. They may represent customer purchases, logs of
monitoring systems, or patient care pathways and their analysis is highly valu-
able to support experts 1) to better understand underlying processes and 2)
to decide future actions. Face to the large amount of such data, sequence min-
ing techniques have been proposed to extract interesting behaviors. While most
sequence mining approaches are dedicated to the extraction of frequent behav-
iors, few pattern mining approaches deal with discriminant patterns. Discrim-
inant patterns address the task of sequence classiﬁcation. In a set of labeled
sequences, a discriminant pattern associated to a label L occurs more likely in
sequence labeled with L than in the other sequences. Discriminant patterns de-
scribe the classes of sequences but they can also be used to predict labels of new
sequences.

In this work, we assume that temporal information is an important feature
to accurately discriminate behaviors. For instance, knowing the delay between
two successive visits on a commercial web site may distinguish loyal customers
from the others. The sequence of visited pages may be the same, it is the delay
between the visit that witnesses the customer loyalty.

2

Y. Dauxais and T. Guyet

Dauxais et al. [6] introduced chronicles to discriminate temporal behaviors
in temporal sequences. A chronicle is a set of events linked by temporal rela-
tions imposing numerical bounds on delays between events. We showed that the
temporal information captured by chronicles improves the accuracy of sequence
labeling.

Mining discriminant chronicles is similar to a regular classiﬁcation problem.
It consists in ﬁnding suitable boundaries on the temporal delay to accurately
discriminate classes. But, chronicles express very simple boundaries, i.e., delays
belonging to an interval.

In this article, we extend the expressiveness of the temporal constraints dis-
covered in chronicles to study the discriminatory power of diﬀerent types of
temporal constraints. The main contribution is the proposal of the generalized
discriminant chronicles (GDC). GDC is a meta-model that enables to represent
diﬀerent types of patterns, characterized by their modeling of temporal relations
between events. Our framework includes a uniﬁed GDC mining procedure in-
spired by the DCM algorithm [6] and a uniﬁed decision procedure to label new
sequences. The experiments compare the accuracy of four instances of GDC on
simulated and real datasets.

2 Related Works

Sequential patterns have been studied since the early stage of the ﬁeld of pattern
mining [18]. Mabroukeh et al. [13] review the most eﬃcient sequential pattern
approaches. All of them are based on the anti-monotonicity property of the
pattern support which states that larger patterns occur fewer times in sequences.
In temporal sequences, events are timestamped and our assumption is that
the temporal dimension is a key dimension to accurately characterize interesting
behaviors. While sequential patterns capture only information about the order
of occurrences of events, temporal patterns capture a more expressive temporal
information. Diﬀerent proposals have been made to enrich sequential patterns
with more complex temporal information. Mannila et al. proposed episodes [14]
as a pattern type which could combine parallel or serial events. Hoeppner et
al. [11] introduced Allen’s temporal logic to specify the temporal relations be-
tween interval events. Two events are not necessarily sequentially ordered, they
could “overlap” or “be covered”. The temporal relations that are discovered are
qualitative. In temporally annotated sequences (TAS) [10] the successive events
are constrained by numerical duration extracted by combining a density clus-
tering technique. The chronicle model [5] is at a crossroad between episode and
TAS. It is a partial temporal order applied on pattern events constrained by nu-
merical temporal intervals. This pattern model is more general than sequential
patterns, TAS and episodes.

Finally, quantitative episodes [15] are tree-based patterns that are graphi-
cally similar to chronicles but formally more similar to sets of TAS. Indeed, a
quantitative episode represents a set of TAS that are all specifying the same

Generalized chronicles for temporal sequence classiﬁcation

3

sequential pattern. This set of TAS is represented by a tree rooted on the ﬁrst
event of the sequential pattern for which each path leading to a leaf is a TAS.

Sequence classiﬁcation has been addressed with statistical approaches such
as HMM but also with machine-learning approaches such as recurrent neural
networks (LSTM) [12].

Bringmann et al. [3] reviewed “pattern-based classiﬁcation” that combines
pattern mining algorithms and machine learning algorithms to classify structured
data, such as sequences. This problem is quite similar to the subgroup discovery
task [2]. The main diﬀerence between both approaches is that subgroup discovery
is meant in a descriptive way whereas pattern-based classiﬁcation is meant in a
predictive way.

The main steps of pattern-based classiﬁcation are the following (1) a pattern
mining step building a vector representation of sequences based on the presence
or absence of some extracted patterns; and (2) a machine learning algorithm
building a classiﬁer based on the vector representations of labeled sequences.
The use of a ﬁnal classiﬁer makes the results diﬃcult to interpret. For this
reason, we focus our interest on the extraction of discriminant patterns, i.e.,
patterns that can be interpreted by their own as a discriminant behavior.

The proposed approaches are based on interestingness measures diﬀerent
from frequency and capturing the diﬀerences between occurrences with subsets
of sequences. The most-used measures are growth rate [7] and disproportion-
ality [1]. The BIDE-D algorithm [9] extracts discriminant sequential patterns
instead of frequent ones. This technique allows to use a smaller pattern set than
the frequent one with a similar accuracy. Recently, the DCM algorithm [6] ex-
tended the discriminant sequential pattern mining with chronicles. But temporal
constraints of chronicles (i.e., inter-event duration, so-called time gap, within an
interval) is maybe too simple to capture complex temporal relationships, and
mining patterns with more complex temporal constraints may improve classiﬁ-
cation accuracy.

3 Discriminant Chronicle Mining

Let E be a set of event types totally ordered by ≤E. An event is a pair (e, t) such
that e ∈ E and t ∈ R. A sequence is a tuple (cid:104)SID, (cid:104)(e1, t1), (e2, t2), . . . , (en, tn)(cid:105), L(cid:105)
where SID is the sequence index, (cid:104)(e1, t1), (e2, t2), . . . , (en, tn)(cid:105) a ﬁnite sequence
of events and L ∈ L where L is a label set. Sequence events are ordered by
timestamps and by labels if equality.

Table 1 represents a set of six sequences containing ﬁve event types (A, B,
C, D and E) and labeled with two diﬀerent labels L = {+, −}. In such case ≤E
is the lexicographic order.

A chronicle is a couple (E, T ) such that: E = {{e1 . . . en}}, ei ∈ E and ei ≤E ej
for all 1 ≤ i < j ≤ n. E is a multiset, i.e. E can contain several occurrences of a
same event type. T is a set of temporal constraints, i.e. expressions of the form
(ei, i)[t−, t+](ej, j) such that i, j ∈ [n], i < j and t−, t+ ∈ R ∪ {−∞, +∞}}. A

4

Y. Dauxais and T. Guyet

Label

SID
Sequence
s1 (A, 1), (B, 3), (A, 4), (C, 5), (C, 6), (D, 7) +
s2
+
(B, 2), (D, 4), (A, 5), (C, 7)
(A, 1), (B, 4), (C, 5), (B, 6), (C, 8),(D, 9) +
s3
−
s4
−
s5
−
s6

(B, 4), (A, 6), (E, 8), (C, 9)
(B, 1), (A, 3), (C, 4)
(C, 4), (B, 5), (A, 6), (C, 7), (D, 10)
Table 1. Sequences labeled with two classes {+, −}.

1 , 3 ]

[ -

A

[-3,5]

[4,5]

[1,3]

B

C

[
-
2
,
2
]

D

C

[ 2 , 3 ]

A

[4,5]

B

C

[
-
2
,
2
]

A

[-3,1]

[2,4]

C

C

Fig. 1. Examples of three chronicles occurring in Table 1 (detailed in the text). From
left to right, the chronicles C, C1 and C2.

temporal constraint speciﬁes acceptable delays between the occurrences of the
multiset events.

1, . . . , e(cid:48)

A chronicle C = (E = {{e(cid:48)

m}}, T ) occurs in a sequence s = (cid:104)(e1, t1), . . . ,
(en, tn)(cid:105), denoted C ∈ s, iﬀ there exists an injective function f : [m] (cid:55)→ [n]
such that 1) ˜s = (cid:104)(ef (1), tf (1)), . . . , (ef (m), tf (m))(cid:105) is a subsequence of s, 2)
∀i, e(cid:48)
i = ef (i) and 3) ∀i, j, tf (j) − tf (i) ∈ [t−, t+] where ef (i)[t−, t+]ef (j) ∈ T .
An occurrence of C in s is a list of timestamps O = (cid:104)o1, . . . , om(cid:105) where ∀i ∈
[m], oi = tf (i) ∈ R.

The support of a chronicle C in a set of sequences S is the number of sequences

in which C occurs:

supp(C, S) = |{s ∈ S | C ∈ s}|.

Fig. 1 illustrates three chronicles represented by graphs. Chronicle C =
(E, T ) where E = {{A, B, C, C, D}} and T = {(A, 1)[−1, 3](B, 2), (A, 1)[−3, 5](C, 3),
(B, 2)[−2, 2](C, 3), (B, 2)[4, 5](D, 5), (C, 3)[1, 3](C, 4)} is illustrated at the top left.
Chronicle C (see Fig. 1 on the left), occurs in sequences s1, s3 and s6 of Ta-
ble 1. We notice there are two occurrences of C in sequence s1. Nonetheless, its
support is supp(C, S) = 3. The two other chronicles, denoted C1 and C2, occur
respectively in sequences s1 and s3; and in sequence s6. Their supports are
supp(C1, S) = 2 and supp(C2, S) = 1.

Frequent chronicle mining consists in extracting all chronicles C in a dataset
S such that supp(C, S) ≥ σmin [5]. The DCM algorithm extracts discriminant
chronicle [6]. A discriminant chronicle occurs at least gmin times more in the
set of positive sequences, i.e. sequences labeled with +, than in the set of nega-
tive sequences (labeled with −). Then, it can be represented as a classiﬁcation
rule C ⇒ + specifying that sequences in which C occurs more likely belongs to

Generalized chronicles for temporal sequence classiﬁcation

5

class +. In this mining task, the user has to specify two thresholds: the minimum
frequency threshold σmin and the minimal growth rate gmin ≥ 1.

4 Generalized Discriminant Chronicles (GDC)

We now introduce the generalized discriminant chronicle (GDC) meta-model.
The GDC meta-model deﬁnes an abstract pattern model of temporal behaviors.
Next section instantiates diﬀerent concrete approaches within a uniﬁed frame-
work of generalized discriminant chronicle, i.e. a GDC model and a mining
algorithm (see Sect. 4.2).

Let L be a set of labels and E be a set of event types, a generalized discrim-
inant chronicle (GDC) is a couple (E, µ), where E is a multiset of event types
and µ : R|E| (cid:55)→ [0, 1]|L| is an occurrence assessment function.

The occurrence assessment function intuitively gives the conﬁdence measure
that a multiset witnesses each label. For some occurrence O ∈ R|E| of multiset
E in a sequence, µ (O) = [p1, p2, . . . , p|L|] where ∀i, µi (O) = pi ∈ [0, 1] gives the
conﬁdence measure that O belongs to the i-th class. In case it sum to 1, this
vector can be interpreted as a probability distribution.

GDC generalizes the previous deﬁnition of chronicles in two manners: 1) the
occurrence assessment function is a generalization of the temporal constraints,
2) the weighted vector of decisions [0, 1]|L| is the generalization of the association
of a chronicle to a label (C ⇒ L, L ∈ L).

In particular, it is possible to encode the discriminant chronicle (E, T ) ⇒ Ll,
where Ll ∈ L is the l-th sequence class, as a GDC using µT deﬁned such that
for some occurrence O = {oi}i∈[|E|] of E:

µT (O) =

(cid:26) 1l if ∀ei[a, b]ej ∈ T , a ≤ oj − oi ≤ b

(cid:126)0 otherwise

where 1l is a vector of zeros except at position l (value 1), and (cid:126)0 is a vector

of zeros. The size of these two vectors is |L|.

4.1 Taking Decisions with Generalized Discriminant Chronicles

This section describes how generalized discriminant chronicles are used to auto-
matically classify new sequences. Let C = (E, µ) be a GDC and s be a sequence
to classify such that there exists at least one occurrence O ∈ R|E| of multiset E.
Then, decision vector is given by µ(O). But the multiset E may occur several
times in s. All decisions have to be combined and the ﬁnal classiﬁcation decision
for sequence s, denoted dC(s) ∈ L, is the class label with the largest conﬁdence
value:

dC(s) = argmax

l∈L

max
O

(cid:16)

(cid:17)

µl(O)

(1)

6

Y. Dauxais and T. Guyet

Fig. 2. Examples of multiple occurrences of two chronicles C and C (cid:48) to be combined
to make the ﬁnal decision. A “rake” illustrates item positions in the sequence of one
occurrence of the multiset.

where maxO µl (O) denotes the maximum conﬁdence value obtained for label
l ∈ L for all occurrences O of the multiset. The function dC(s) enables to use a
GDC as a decision rule. It decides which class a sequence belongs to.

The class label can also be decided from a set of chronicles C = {Ci}1≤i≤n. In
this case, each chronicle yields its own decision, and they are merged into a ﬁnal
decision. The decision procedure we propose, denoted dC(s) – with a collection
of chronicles as subscript, is a linear combination of the number of occurrences
of a chronicle Ci in s labeled with lj ∈ L, more formally:

dC(s) = argmax

j∈L

(cid:33)

αi,jνj
Ci

(s) + βj

(cid:32) n
(cid:88)

i=1

(2)

where αi,j ∈ R and βi,j ∈ R are parameters, and νj
Ci

(s) is the number of
occurrences of chronicle Ci in sequence s that suggests classifying the sequence
in class j (i.e. dCi(s) = j):

νj
Ci

(s) =

(cid:26)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

O ∈ R|E|

(cid:12)
(cid:12)
argmax
(cid:12)
(cid:12)
l∈L

(µl(O)) = lj

(cid:27)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(3)

2 , oC(cid:48)

1 , oC(cid:48)

2 , oC(cid:48)

Fig. 2 illustrates a sequence s classiﬁed with a set of two chronicles C and
C (cid:48), and |L| = 3. Chronicle C occurs twice in s and C (cid:48) occurs thrice, O =
{oC

1 , oC
In this case, νC = [0, 0, 2] because the majority class in the two occurrences
of chronicle C is the third one, and νC(cid:48) = [0, 1, 2]. Assuming βj = 0 and αi,j =
1, ∀i, j, then the predicted class is dC(s) = 3 because (cid:80)n
(s) + β3 = 4
is the largest predicted value among possible classes.

3 }. The ﬁgure illustrates respective decision vectors.

i=1 αi,3ν3
Ci

The intuition is that the contribution to the ﬁnal decision of chronicle Ci
is more important if this chronicle appears several times in the sequence. Com-
bining the numbers of occurrences is preferred to the combination of conﬁdence
measures to prevent from bias due to chronicles with low recall (i.e. poorly
informative) but with possible high conﬁdence.

In practice, the αi,j and βj parameters are not set up manually but learned

from data as explain in next section.

Generalized chronicles for temporal sequence classiﬁcation

7

Fig. 3. Overall procedure of sequence classiﬁer learning.

4.2 Learning Generalized Discriminant Chronicles Classiﬁers

The overall procedure dedicated to learn the sequence classiﬁer is given in Fig. 3.
This procedure extracts both a set of γ discriminant chronicles, denoted C, and
parameters values of decision function (see Equation 2). First of all, the learning
dataset is split into two separated bunches of sequences.

One dataset is used to extract a set of discriminant chronicles C = {Ci}. A
subset of the γ most discriminant chronicles, denoted C, is selected from C. Ac-
cording to BIDE-D [9], reducing the set of chronicles prevents from overﬁtting.
The second dataset is used to learn the decision procedure. In case of a dataset
with two classes (L = {+, −}), equation 2 can be seen as a linear classiﬁcation
problem. Then, a linear-SVM classiﬁer learns the αi,j and βj parameters. In
practice, a linear-SVM classiﬁer is also used for a multi-class setting parame-
ters and its model serves as decision function that takes the ﬁnal classiﬁcation
decision.

We now come back to the mining of generalized discriminant chronicles. This
algorithm is based on the original DCM algorithm [6]. Algorithm 1 gives the
general principle of GDC mining from a dataset of labeled sequences S. The
two main parameters are σmin, a minimal support threshold used to prevent
from generating too much poorly-representative chronicles and gmin, a minimal
growth rate threshold. The overall principle from learning multiple-class chroni-
cles is the one class against all. For some class L, the minimal growth rate gmin
indicates that a GDC occurs at least gmin times more in sequences of class L
than in all other sequences.

Algorithm 1 extracts GDC for each class L in two main steps. It ﬁrstly
extracts M, the set of frequent multisets in the sequences of class L. Then,
ExtractDTC learns a µ function from the list of occurrences of a frequent
multiset. There is a unique µ per multiset. Its principle is ﬁrst to build a time-
gap table [19] from all occurrences of a multiset and, second, to learn a temporal
model from the time-gap table. Each time-gap occurrence is labeled by the label
of its sequence and any standard machine learning algorithm can learn the µ
function.

The DCM algorithm [6] is based on rule induction (e.g. Ripper [4]) to learn
temporal constraints of a chronicle (T ). It is a speciﬁc case of a µT function that
ﬁts the requirements of the original model of chronicle. Next section introduces
alternative classes of occurrence assessment functions.

8

Y. Dauxais and T. Guyet

Algorithm 1 Generalized discriminant chronicle mining
Require: S: labeled sequence sets, L: set of labels, σmin: minimal support threshold,

gmin: minimal growth threshold

(cid:46) C is the discriminant chronicle set

M ← ExtractMultiSet(S L, σmin)
for all ms ∈ M do

for all µ ∈ ExtractDTC(S, L, ms, gmin, σmin) do

C ← C ∪ {(ms, µ)}

(cid:46) Add a new GDC

1: C ← ∅
2: for all L ∈ L do
3:
4:
5:
6:
7: return C

5 Examples of GDC Instances

This section illustrates several types of patterns that can be represented by GDC:
discriminant sequential patterns, discriminant episodes, SVM-DC and DT-DC.
The ﬁrst two types of patterns illustrate the ability of GDC to model existing
patterns (less expressive than the original discriminant chronicles) and the last
two models illustrate meaningful generalizations of temporal constraints. In the
remaining of this section, we brieﬂy present each of these models as instances of
the GDC.

Discriminant episodes and sequences An episode is a set of events ordered tem-
porally by a partial order ≤E . If ≤E is a total order, the episode is a sequential
pattern. Such classical temporal patterns have been used for mining discrimi-
nant behaviors respectively by Fabr`egue et al. [8] and by Fradkin et al. [9]. A
discriminant episode is an episode associated to a label L ∈ L. Such discrimi-
nant patterns could be represented by a GDC model instance by the following
occurrence assessment function:

µT (o) =

(cid:26) 1L if ∀(i, j), i ≤E j ⇒ oi ≤ oj

(cid:126)0 otherwise

(4)

For example, a multiset E = {{A, B, C}} ordered by ≤E such that B ≤E A and
B ≤E C speciﬁes an episode representing sequences where B occurs before events
A and C, no matter the order between A and C. While associated to a label, it
becomes a discriminant episode. Expressed with chronicle temporal constraints,
we have T = {(A, 1)[−∞, 0](B, 2), (A, 1)[−∞, ∞](C, 3), (B, 2)[0, ∞](C, 3)}.

Decision Tree Discriminant chronicles (DT-DC) A discriminant chronicle is
characterized by temporal constraints on the time gaps (T ). A constraint (e, i)
[t−, t+](e(cid:48), j) enforces the time gap δ between some occurrences of events e and
e(cid:48) to belong to the interval [t−, t+]. But chronicle does not allow disjunctions
of constraints. For instance, it is not possible to specify that δ may belong to
[t−, t+] ∪ [t(cid:48)−, t(cid:48)+].

The DT-DC model replaces the conjunctive rule learning algorithm by a de-
cision tree, such as C4.5 [17]. For example, let’s consider a dataset of positive se-
quences matching temporal constraints (A, 1)[2, 3](B, 2) and (A, 1)[7, 9](B, 2) and

Generalized chronicles for temporal sequence classiﬁcation

9

Fig. 4. Illustration of temporal discrimination power of the diﬀerent instances of GDC.
Planes (x, y) represent a pair of temporal constraints for some sequences (A, t0)(B, t0 +
x)(C, t0 + x + y). Positive sequences are those with (x, y) values in the green region,
negative sequences have (x, y) values in the red region. The bold-green lines represent
the separation boundaries learned by a GDC depending on the type of occurrence
assessment function, µ. From left to right: discriminant episodes (temporal constraints
with shape [0, +∞]), chronicles (three chronicles with temporal constraints represented
by rectangles), DT-DC (a single shape combining several rectangles), linear-SVM (a
single chronicle, with generalized linear boundaries).

a dataset of negative sequences matching the temporal constraint (A, 1)[2, 9](B, 2).
In this case, two chronicles would be discriminant (one per interval, [2, 3] and
[7, 9]). On the opposite, a single DT-DC will capture the disjunction of intervals
in the same model. The expected beneﬁt of this model is a better generalization
power.

SVM Discriminant chronicles (SVM-DC) SVM Discriminant chronicles illus-
trate the case of a complex learnable occurrence assessment function µ, i.e. a
µ modeled by a multi-class SV M classiﬁer. Compared to the previous types
of patterns, SVM-DC is not limited to linear boundaries to separate examples
(time gaps of multiset occurrences) and is a good candidate for yielding accurate
patterns.

It is worth noticing that any machine learning model yields a new type of
discriminant temporal patterns based on the GDC. The above GDC instances
show the potential variety of temporal constraints that GDC can model. Fig. 4
illustrates the shape of boundaries deﬁned by occurrence assessment functions
of a chronicle learned from a synthetic dataset.

6 Experiments

In this part, we compare diﬀerent results in pattern-based classiﬁcation using
discriminant episodes, discriminant chronicles, DT-DC and SVM-DC. The goal
of these experiments is to highlight the impact of the GDC model choice on the
accuracy of decision functions presented in Sect. 4.1: dC(s) and dC(s). In the
experiments we analyze the classiﬁcation power of individual GDC (i.e. dC(s))
and of a set of GDC (i.e. dC(s)).

10

Y. Dauxais and T. Guyet

The DT-DC and SVM-DC mining algorithms are implemented in Python
using scikit-learn library [16]. The algorithm dedicated to discriminant chronicle
mining is implemented in C++.3

6.1 Experimental Setup

The diﬀerent experiments compare mean accuracy of diﬀerent GDC models ob-
tains by cross-validation on synthetic and real datasets.

A 5-cross-validation is performed on each dataset for the parameters σmin
and gmin of the mining step described in Sect. 3 and the parameter γ de-
scribed in Sect. 4.2. The domains used for σmin, gmin and γ are respectively
{0.2, 0.3, 0.4, 0.5, 0.6}, {1.4, 1.6, 1.8, 2, 3} and {90, +∞}. γ = +∞ means that
all discriminant chronicles are kept. To improve the computation time, a fourth
parameter is introduced for the mining step: the maximal size of extracted chron-
icles max size. This parameter constrains the maximal number of events that a
GDC, i.e. its multiset, can contain. The domain of this parameter is {3, 4, 5, 6}.

The real datasets are the UCI datasets presented in the BIDE-D experi-
ments [9]: asl-bu, asl-gt and blocks. These datasets are part of the standard
benchmark for pattern-based classiﬁcation approaches.
We generated two collections of synthetic datasets:

– A ﬁrst collection of datasets is based on the principle illustrated by Fig. 4.
Random sequences (cid:104)(A, 0)(B, x)(C, x + y)(cid:105) have been generated: the event
A occurs at time 0 in each sequence and the time gaps between A and B
and between B and C are randomly generated in the interval [0, 15]. The
label of the sequence is generated depending on the temporal constraints.
According to Fig. 4, positive examples having time gaps located in one of
the three green squares. Coordinates of the square corners are (1, 1), (6, 6);
(5, 5), (10, 10) and (9, 9), (14, 14). Each dataset contains 150 positive and 150
negative sequences.

– A second collection of datasets is based on random sequences with shape
(cid:104)(A, tA) (B, tA + x)(C, tC)(D, tC + k × x)(cid:105) where x ∈ [1, 9], tA = 15 and
tC ∈ [1, 29]. The two sequence classes are distinguished by the k factor:
k = 2 for positive sequences while k = 1 for negative ones. Each dataset
contains 100 positive and 100 negative sequences.

To ease the comparison between DT-DC and discriminant chronicles as indi-
vidual patterns, we choose to use each node of the extracted trees as discriminant
temporal constraint. This prevents from comparing the classiﬁcation power of
the decision-tree algorithm and the rule learning algorithm (Ripper ). Further-
more, decision trees produce more discriminant chronicles than Ripper because
tree nodes are more redundant.

3 All

software

sources

and

datasets

are

available

at

https://gitlab.inria.fr/ydauxais/GDC-PBC.

Generalized chronicles for temporal sequence classiﬁcation

11

σmin gmin

accuracy

support

number

0.2
0.2
0.2
0.2
0.2

1.6
2
1.8
3
1.4

0.92(±0.04) 33.15(±6.95) 5.2(±0.40)
0.88(±0.06) 36.77(±7.79) 4.4(±0.49)
0.86(±0.05) 35.41(±8.20) 4.4(±0.80)
0.85(±0.03) 36.57(±7.55) 4.2(±0.40)
0.83(±0.04) 36.67(±7.04) 4.8(±0.75)

Table 2. The ﬁve most accurate parameter sets for discriminant chronicles on the ﬁrst
synthetic dataset. The number attribute is the total number of chronicles extracted in
the 5 runs.

6.2 Results

Let us ﬁrst present results obtained by the GDC instances on the ﬁrst synthetic
datasets. For this experiment, we only consider the extracted chronicles with
multiset {{A, B, C}}. Thus, only one DT-DC and one SVM-DC are extracted for
each run, but the number of discriminant chronicles depends on the setting (see
Table 2). The three squares deﬁning the positive occurrences may be represented
by three discriminant chronicles but it is more diﬃcult to represent the nega-
tive occurrences because some of them are not included in a frequent rectangle
containing only negative occurrences.

The unique DT-DC represents almost perfectly the discriminant behavior
used to generate the dataset with a mean accuracy of 0.99(±0.02). This result
was expected because of the dataset structure (squares with boundaries orthog-
onal to the axis) ﬁts the discrimination capabilities of decision trees.

No SVM-DC are extracted for the default parameters of gmin. Our explana-
tion is that concavities in the shape containing positive occurrences disadvantage
linear SVM. Relaxing the constraint of gmin, SVM-DC reaches a mean accuracy
of 0.48(±0.05). This shows experimentally that DT-DC can be more accurate
than SVM-DC for some datasets. No discriminant episodes are extracted from
these datasets. It was expected from their design as each sequence only contains
the items A, B and C, and always in the same order. Some discriminant episodes
could be extracted from the negative sequences like the one representing A and
B occurring at the same time but these patterns are rare and thus they are not
extracted using the deﬁned parameters. It is an example of the need to generalize
such a simple model in order to catch more complex behaviors.

We compared these results with the discriminant chronicles obtained using
DCM. Among the discriminant chronicles extracted using DCM, discriminating
positive occurrences from negative ones generates three perfectly discriminant
chronicles representing the three squares used to generate the data with pa-
rameters σmin = 0.2, gmin = 3 and considering only the multiset {{A, B, C}}.
Discriminating the negative occurrences from the positive ones with DCM gen-
erates two perfectly discriminant chronicles representing the largest rectangles of
negative occurrences on the top left and on the bottom right of the Fig. 4. Then,
the mean accuracy of discriminant chronicles is 1 and, contrary to DT-DC, some
negative occurrences are not covered by these chronicles. This perfect accuracy

12

Y. Dauxais and T. Guyet

σmin

gmin

accuracy

support

number

0.3
0.2
0.2
0.6
0.6

3.0
1.8
1.6
1.6
1.4

1
0.95(±0.13)
0.90(±0.17)
0.79(±0.17)
0.72(±0.17)

10.5(±3.46)
8.19(±3.64)
12.9(±8.86)
18.8(±10.79)
20.4(±10.48)

16
37
31
11
10

Table 3. Five most accurate parameter sets for regular discriminant chronicles on
the second synthetic dataset. The attribute number is the total number of chronicles
extracted in the 5 runs.

is correlated to the strategy of Ripper that does not reuse covered occurrences to
build a new temporal constraint. The remaining occurrences are so considered
too few to be used for building a new constraint. The accuracy is better due to
the partial coverage of the dataset made by discriminant chronicles.

We present the same experiment on the second collection of datasets. These
datasets are generated to favor the SVM-DC model with boundaries that corre-
lates linearly the time gaps. Again, we only considered the extracted chronicles
with multiset {{A, B, C, D}}. For the simplest dataset, the single extracted SVM-
DC obtained the accuracy of 1 for the 5 runs. The extracted DT-DC obtained an
mean accuracy of 0.99(±0.02). Thus, SVM-DC accuracy is not better than DT-
DC, but, DT-DC builds a very large decision tree that overﬁts the boundaries,
which is not suitable in real applications.

Table 3 shows the results of regular discriminant chronicles. We observe that
the most accurate parameter sets extract chronicles with a small support and
the least accurate parameter sets extract chronicles with a bigger support. We
do not present results for discriminant episodes because not any discriminant
episodes are extracted. This shows the limit of a too simple model.

Let us now present the results obtained by the three GDC models as indi-
vidual patterns and as pattern sets on real datasets. An overview of the clas-
siﬁcation power of the individual patterns of DT-DC, discriminant chronicles
and discriminant episodes is given by Table 4. It shows that DT-DC patterns
are individually less accurate than discriminant chronicles obtained from the
same decision trees. Discriminant episodes are also individually more discrim-
inant than discriminant chronicles. The intuition behind these results is that
decision trees overﬁt more the datasets than temporal constraints or sequen-
tial orders. Temporal constraints and sequential orders gather only dense sets
of occurrences, represented as squares on Fig. 4, but decision trees generalize
examples and gather too dissimilar occurrences.

Conversely to the accuracy, the mean support is higher for DT-DC than
for discriminant chronicles and discriminant episodes. Each DT-DC covers more
examples than the two other types of patterns. Furthermore, the coverage of
discriminant episode is worse than DT-DC due to their poor expressiveness of
temporal behaviors.

Generalized chronicles for temporal sequence classiﬁcation

13

σmin gmin max size

accuracy

support

discriminant
episodes

discriminant
chronicles

3
4
3
4
3
3
4
3
3
4
5
5
3
3
4
Table 4. Comparison table of DT-DC, discriminant chronicles and discriminant
episodes for the 5 best parameter sets in terms of mean accuracy on asl-bu.

0.92(±0.14) 5.62(±8.60)
0.86(±0.19) 5.24(±7.65)
0.86(±0.20) 2.96(±5.54)
0.83(±0.18) 11.41(±9.71)
0.83(±0.22) 4.41(±5.94)
0.64(±0.37) 3.16(±2.91)
0.60(±0.40) 2.69(±2.68)
0.59(±0.33) 5.74(±7.01)
0.58(±0.38) 3.67(±3.28)
0.57(±0.34) 3.83(±3.37)
0.38(±0.28) 8.12(±6.57)
0.37(±0.25) 8.61(±6.34)
0.36(±0.34) 5.99(±5.68)
0.34(±0.21) 13.2(±8.19)
0.33(±0.23) 10.0(±7.40)

0.3
0.3
0.6
0.6
0.3
0.3
0.3
0.6
0.3
0.6
0.5
0.6
0.2
0.5
0.5

2.0
1.8
3.0
1.8
1.6
2.0
1.8
3.0
1.6
1.8
2.0
3.0
1.6
1.8
1.8

DT-DC

These accuracy results are extreme because we did not use the decision tree
parameter constraining a leaf to have a minimal support. For example, if this
parameter is set to fmin, a decision tree can be seen as a set of discriminant
chronicles for a unique multiset.

To illustrate the importance of this parameter, we can compare the two
accuracy distributions. Fig. 5 at top left shows the accuracy distribution of
DT-DC for the most accurate parameter set. The distribution at bottom right
is the accuracy distribution of discriminant chronicles for the most accurate
parameter set. The distribution at bottom left is the accuracy distribution of
DT-DC for the best discriminant chronicle parameter set with a mean accuracy
of 0.23(±0.25). The distribution at top right is the accuracy distribution of
discriminant chronicles for the best DT-DC parameter set with a mean accuracy
of 0.32(±0.39).

We ﬁrst notice that the accuracy distributions of discriminant chronicles and
DT-DC are almost similar. These histograms show three main peaks: patterns
that obtained the accuracy of 0, 0.5 and 1. It makes sense considering that both
types of patterns were extracted by the same algorithm. The diﬀerences between
the mean accuracy are mainly in the proportion of patterns with accuracy equals
to 0 and equals to 1. Proportionally to the number of 1-accuracy patterns, the
peak of 0-accuracy is higher for DT-DC than for discriminant chronicles. This
means that the proportion of patterns that always make wrong decisions is higher
for DT-DC than for discriminant chronicles and, thus, that DT-DC overﬁt more
the datasets than discriminant chronicles. The same behavior is observed in most
of the experiments.

Finally, Table 5 shows the accuracy of SVM-DC and discriminant chronicles
for real datasets: asl-bu, asl-gt and blocks. The parameters used for these results

14

Y. Dauxais and T. Guyet

Fig. 5. Accuracy distribution for DT-DC and discriminant chronicles with parameters
σmin = 0.5, gmin = 2 and max size = 5 for the ﬁrst row and σmin = 0.3, gmin = 2
and max size = 3 for the second one.

accuracy

SVM-DC

discriminant chronicles
CPU time (s)
accuracy
CPU time (s)
dataset
0.68(±0.06)
16.4(±0.37)
asl-bu 0.73(±0.05) 15.2(±0.34)
386(±8.37)
asl-gt 0.42(±0.02)
0.32(±0.01) 7.70(±0.22)
25.4(±2.58) 1.00(±0.00) 11.0(±0.13)
0.98(±0.05)
blocks

Table 5. Best accuracy results in SVM-DC-based and discriminant chronicle-based
classiﬁcation and computation times with σmin = 0.4, max size = 3 and gmin = 2.

were obtained through a grid search. The involved parameters are σmin, gmin
and γ but also the C parameter of the global linear SVM classiﬁer. Table 5
shows that SVM-DC produces patterns with better accuracy than discriminant
chronicles on asl-bu and asl-gt. For blocks, discriminant chronicles are not more
accurate than SVM-DC but the discriminant chronicles are discriminant enough
to describe such a simple dataset. A classiﬁer based on chronicles is perfect to
classify the blocks sequences.

These results show that combining decisions of discriminant chronicles makes
discriminant less competitive than SVM-DC, even if discriminant chronicles are
individually accurate. Thereby, we cannot conclude from previous results that
discriminant chronicles are the most accurate GDC. Indeed, chronicles do not
involve all the occurrences of a multiset and represent very speciﬁc discriminant
behaviors. But in a pattern-based classiﬁcation context, a set of very discriminant
chronicles is not suﬃcient to cover the whole dataset and so to obtain a good
accuracy. This leads to a typical overﬁtting situation.

Generalized chronicles for temporal sequence classiﬁcation

15

Finally, Table 5 also gives the mean computation times for both approaches.
These times are strongly related to the computing times of machine algorithms
which vary a lot depending on datasets. Discriminant chronicle mining (DCM)
is faster in most cases thanks to a particular implementation eﬀort for this
approach.

7 Conclusion and Perspectives

This article presents a generalization of the model of discriminant chronicles.
The model of generalized discriminant chronicles (GDC) proposes to combine
a multiset pattern and a decision function learned from the temporal duration
between occurrences of a multiset pattern. Initially, discriminant chronicles were
extracted using a rule learner and their temporal boundaries were intervals. Such
a representation may be too restrictive an assumption on how to discriminate
temporal sequences and, thus, had to be generalized.

We demonstrate the expressiveness of the framework by showing that it
can model classical patterns (episodes, sequential patterns and chronicles) and
episodes, sequential patterns and new types of patterns. DT-DC are based on
decision tree classiﬁers and SVM-DC are based on a SVM classiﬁer.

The experiments show that individual chronicles have good accuracy but
SVM-DC overtakes the combination of chronicles on real datasets. An interest-
ing perspective of this work is to blend diﬀerent types of chronicles within the
same combination. Furthermore, comparison in terms of interpretability between
several GDC instances would be interesting. Indeed, chronicles are attractive for
its interpretability, thanks to its graphical representation. However, new tem-
poral patterns like DT-DC or SVM-DC can not be graphically represented so
simply. Then it would be possible to suggest GDC instances that would oﬀer a
tradeoﬀ between prediction accuracy and interpretability.

Acknowledgements

This work has received funding from the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation program
(grant agreement No [694980] SYNTH: Synthesising Inductive Data Models).

References

1. Asker, L., Bostr¨om, H., Karlsson, I., Papapetrou, P., Zhao, J.: Mining candidates
for adverse drug interactions in electronic patient records. In: Proceedings of the
International Conference on PErvasive Technologies Related to Assistive Environ-
ments (PETRA). pp. 22:1–22:4 (2014)

2. Atzmueller, M.: Subgroup discovery. Wiley Interdisciplinary Reviews: Data Mining

and Knowledge Discovery 5(1), 35–49 (2015)

16

Y. Dauxais and T. Guyet

3. Bringmann, B., Nijssen, S., Zimmermann, A.: Pattern-based classiﬁcation: a uni-
fying perspective. In: Proceedings of the LeGo Workshop “From Local Patterns to
Global Models”. p. 10 (2009)

4. Cohen, W.W.: Fast eﬀective rule induction. In: Proceedings of the International

Conference on Machine Learning. pp. 115–123 (1995)

5. Cram, D., Mathern, B., Mille, A.: A complete chronicle discovery approach: appli-

cation to activity analysis. Expert Systems 29(4), 321–346 (2012)

6. Dauxais, Y., Guyet, T., Gross-Amblard, D., Happe, A.: Discriminant chronicles
mining - application to care pathways analytics. In: Proceedings of 16th Conference
on Artiﬁcial Intelligence in Medicine (AIME). pp. 234–244 (2017)

7. Dong, G., Li, J.: Eﬃcient mining of emerging patterns: Discovering trends and
diﬀerences. In: Proceedings of the International conference on Knowledge discovery
and data mining (KDD). pp. 43–52 (1999)

8. Fabr`egue, M., Braud, A., Bringay, S., Grac, C., Le Ber, F., Levet, D., Teisseire,
M.: Discriminant temporal patterns for linking physico-chemistry and biology in
hydro-ecosystem assessment. Ecological informatics 24, 210–221 (2014)

9. Fradkin, D., M¨orchen, F.: Mining sequential patterns for classiﬁcation. Knowledge

and Information Systems 45(3), 731–749 (2015)

10. Giannotti, F., Nanni, M., Pedreschi, D.: Eﬃcient mining of temporally anno-
tated sequences. In: Proceedings of the International Conference on Data Mining
(ICDM). pp. 348–359 (2006)

11. H¨oppner, F.: Discovery of temporal patterns. In: European Conference on Princi-

ples of Data Mining and Knowledge Discovery. pp. 192–203 (2001)

12. Lipton, Z.C., Berkowitz, J., Elkan, C.: A critical review of recurrent neural networks

for sequence learning. arXiv preprint arXiv:1506.00019 (2015)

13. Mabroukeh, N.R., Ezeife, C.I.: A taxonomy of sequential pattern mining algo-

rithms. ACM Computing Surveys (CSUR) 43(1), 3:1–3:41 (2010)

14. Mannila, H., Toivonen, H., Verkamo, A.I.: Discovery of frequent episodes in event

sequences. Data mining and knowledge discovery 1(3), 259–289 (1997)

15. Nanni, M., Rigotti, C.: Extracting trees of quantitative serial episodes. In: Inter-
national Workshop on Knowledge Discovery in Inductive Databases. pp. 170–188.
Springer (2006)

16. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine
learning in Python. Journal of Machine Learning Research 12, 2825–2830 (2011)
17. Quinlan, J.R.: Learning decision tree classiﬁers. ACM Computing Surveys (CSUR)

28(1), 71–72 (1996)

18. Srikant, R., Agrawal, R.: Mining sequential patterns: Generalizations and perfor-
mance improvements. Advances in Database Technology – EDBT pp. 1–17 (1996)
19. Yen, S.J., Lee, Y.S.: Mining non-redundant time-gap sequential patterns. Applied

Intelligence 39(4), 727–738 (2013)

