Refined Convergence and Topology Learning for
Decentralized SGD with Heterogeneous Data
Batiste Le Bars, Aurélien Bellet, Marc Tommasi, Erick Lavoie, Anne-Marie

Kermarrec

To cite this version:

Batiste Le Bars, Aurélien Bellet, Marc Tommasi, Erick Lavoie, Anne-Marie Kermarrec. Refined
Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data. Proceedings
of The 26th International Conference on Artificial Intelligence and Statistics (AISTATS 2023), 2023,
Valencia, Spain, Spain. ￿hal-03905091v2￿

HAL Id: hal-03905091

https://inria.hal.science/hal-03905091v2

Submitted on 23 Dec 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Reﬁned Convergence and Topology Learning for
Decentralized SGD with Heterogeneous Data

Batiste Le Bars

Aurélien Bellet
Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189, CRIStAL, F-59000 Lille

Marc Tommasi

2
2
0
2

t
c
O
1
2

]

G
L
.
s
c
[

3
v
2
5
4
4
0
.
4
0
2
2
:
v
i
X
r
a

Erick Lavoie
Université de Bâle, Bâle, Switzerland

Anne-Marie Kermarrec
EPFL, Lausanne, Switzerland

Abstract

One of the key challenges in decentralized and
federated learning is to design algorithms that efﬁ-
ciently deal with highly heterogeneous data distri-
butions across agents. In this paper, we revisit the
analysis of the popular Decentralized Stochastic
Gradient Descent algorithm (D-SGD) under data
heterogeneity. We exhibit the key role played by a
new quantity, called neighborhood heterogeneity,
on the convergence rate of D-SGD. By coupling
the communication topology and the heterogene-
ity, our analysis sheds light on the poorly under-
stood interplay between these two concepts. We
then argue that neighborhood heterogeneity pro-
vides a natural criterion to learn data-dependent
topologies that reduce (and can even eliminate)
the otherwise detrimental effect of data hetero-
geneity on the convergence time of D-SGD. For
the important case of classiﬁcation with label
skew, we formulate the problem of learning such a
good topology as a tractable optimization problem
that we solve with a Frank-Wolfe algorithm. As
illustrated over a set of simulated and real-world
experiments, our approach provides a principled
way to design a sparse topology that balances the
convergence speed and the per-iteration communi-
cation costs of D-SGD under data heterogeneity.

1

Introduction

Decentralized and federated learning methods allow training
from data stored locally by several agents (nodes) without
exchanging raw data, in line with the increasing demand for

Preprint. Under review.

more privacy-preserving algorithms (Kairouz et al., 2021).
One of the key challenges in decentralized learning is to
deal with data heterogeneity: as each agent collects its own
data, local datasets typically exhibit different distributions.
In this work, we study this challenge in the context of fully
decentralized learning algorithms, which provide a scalable
and robust alternative to server-based approaches (Colin
et al., 2016; Lian et al., 2017; Koloskova et al., 2019, 2020).
Fully decentralized optimization algorithms, such as the
celebrated Decentralized SGD (D-SGD) (Lian et al., 2017,
2018; Koloskova et al., 2020), operate on a graph represent-
ing the communication topology, i.e. which pairs of nodes
exchange information with each other. The connectivity of
the topology then rules a trade-off between the convergence
rate and the per-iteration communication complexity of fully
decentralized algorithms (Wang et al., 2019). Choosing a
good topology for fully decentralized machine learning is
therefore an important question, and remains a largely open
problem in the presence of data heterogeneity.

Until recently, the impact of the communication topology
on the convergence was believed to be mainly characterized
by its spectral gap: a large spectral gap indicating good
connectivity and thus faster convergence. Focusing solely
on the connectivity of the topology has however shown to
be insufﬁcient, even when we have identically distributed
data (Neglia et al., 2020; Vogels et al., 2022). In the hetero-
geneous setting, Bellet et al. (2022) notably observe that the
choice of topology has a large inﬂuence, beyond its spectral
gap, on the convergence speed of D-SGD. However, these
empirical observations are not supported by any theory.

In this work, we ﬁll the theoretical gap that currently exists
on these questions. We focus on D-SGD (Lian et al., 2017,
2018; Koloskova et al., 2020), which is arguably the most
popular decentralized optimization algorithm in the context
of machine learning due to its good properties inherited from
centralized SGD. In particular, D-SGD has been praised for
its computational scalability (Lin et al., 2021), its applica-
bility to training deep neural networks at scale (Ying et al.,

 
 
 
 
 
 
Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

2021; Kong et al., 2021), and the good generalization guar-
antees that it provides (Sun et al., 2021; Zhu et al., 2022).

Our ﬁrst contribution is a reﬁned convergence analysis of
D-SGD which introduces a new quantity, called neighbor-
hood heterogeneity, that couples the topology and the local
data distributions. Neighborhood heterogeneity essentially
measures the expected distance between the global gradient
and the aggregated gradients in the neighborhood of nodes.
Our results demonstrate that the impact of the topology on
the convergence rate of D-SGD, for both convex and non-
convex objectives, does not only depend on its connectivity
(i.e., spectral gap): it also depends on its capacity to com-
pensate the heterogeneity of local data distributions at the
neighborhood level. This new perspective allows to avoid
the restrictive assumption of bounded heterogeneity used in
previous work (Lian et al., 2017, 2018; Tang et al., 2018; As-
sran et al., 2019; Koloskova et al., 2020; Ying et al., 2021).

Our second contribution deals with the problem of learn-
ing a good data-dependent topology, going beyond prior
work which focused mainly on optimizing the spectral gap
(Boyd et al., 2004, 2006; Wang et al., 2019). We argue that
neighborhood heterogeneity provides a natural objective and
show that it can be effectively optimized in practice in the
important case of classiﬁcation with label distribution het-
erogeneity across nodes (label skew) (Kairouz et al., 2021;
Hsieh et al., 2020; Bellet et al., 2022). We solve the resulting
problem using a Frank-Wolfe algorithm (Frank and Wolfe,
1956; Jaggi, 2013), allowing us to track the quality of the
learned topology as new edges are added in a greedy man-
ner. Our results imply that we can approximately minimize
neighborhood heterogeneity up to a ﬁxed additive error with
a topology whose maximum degree is constant in the num-
ber of nodes. To the best of our knowledge, our work is the
ﬁrst to learn the graph topology for decentralized learning in
a way that (i) is data-dependent, (ii) controls communication
costs, and (iii) optimizes the convergence rate of D-SGD.
We illustrate the usefulness of our approach in simulated
and real data experiments with linear and deep models.

2 Related Work

Consensus vs personalized objectives. In this work, we
study the consensus problem which aims to learn a single
model that minimizes the average of the local objectives
(see Eq. 1). Another line of research tackles the problem
of heterogeneity in decentralized learning through personal-
ization (Koppel et al., 2017; Vanhaesebrouck et al., 2017;
Zantedeschi et al., 2020; Marfoq et al., 2021; Even et al.,
2022). In that setting, each agent aims to learn a personal-
ized model that minimizes its own (expected) local objective.
It is thus natural and desirable to connect nodes that have
similar data distributions. In contrast, our results show that
for the consensus problem, the topology should connect
nodes that are different so that local neighborhoods are rep-

resentative of the global distribution. We emphasize that
personalization and consensus are relevant to different use
cases and can be considered as orthogonal to each other.

Algorithmic improvements to decentralized SGD. Sig-
niﬁcant work has been devoted to extensions of D-SGD. We
can mention those based on momentum (Assran et al., 2019;
Gao and Huang, 2020; Lin et al., 2021; Yuan et al., 2021),
cross-gradient aggregations (Esfandiari et al., 2021), gra-
dient tracking (Koloskova et al., 2021) and bias correction
(or variance reduction) (Tang et al., 2018; Yuan et al., 2020;
Yuan and Alghunaim, 2021; Huang and Pu, 2021). Many of
these schemes are able to reduce the order of the term that de-
pends on data heterogeneity but remain impacted by strong
heterogeneous scenarios. We stress that the above line of
research is complementary to ours as it is based on modiﬁ-
cations of the D-SGD algorithm (which often requires addi-
tional computation and/or communication). In contrast, our
work does not modify the algorithm: we provide a reﬁned
analysis and a method to learn the topology. We believe
that our results can be combined with the above algorithmic
improvements, but leave such extensions for future work.

Good topologies for decentralized learning. There is a
long line of research on choosing a good topology (e.g.,
expanders or exponential graphs) (Chow et al., 2016; Nedi´c
et al., 2018; Ying et al., 2021), or learning it to maximize
the spectral gap (Boyd et al., 2004, 2006; Wang et al., 2019)
or network throughput (Marfoq et al., 2020). Unlike our
approach, these methods simply seek to optimize the con-
nectivity of the topology while respecting some communi-
cation constraints, but they do not take into account the data
distributions across nodes.

Until recently, Bellet et al. (2022) was the only approach
that leverages the distribution of data in the design of the
topology. Focusing on classiﬁcation under label skew, they
propose a heuristic approach that consists of inter-connected
cliques, where class proportions in each clique should be as
close as possible to the global proportions. Our approach is
more ﬂexible: it can learn more general topologies, and pro-
vides full control over their sparsity. Furthermore, our topol-
ogy learning criteria is theoretically justiﬁed, while the one
in Bellet et al. (2022) is only supported by empirical experi-
ments. We think however that the ideas of the present paper
could pave the way for a theoretical analysis of their work.

Concurrent to and independently from our work, Dandi et al.
(2022) provide a similar analysis of the convergence rate
of D-SGD using a quantity called “relative heterogeneity”.
However, our approaches differ greatly in how they learn
the topology. In fact, Dandi et al. (2022) do not learn the
topology itself (i.e., which nodes are connected) but only
the weights of a predeﬁned topology. In other words, the
set of edges is ﬁxed in advance. This severely limits the
ability to mitigate the effect of data heterogeneity unless
the predeﬁned topology is dense. In contrast, our approach
learns a sparse topology (both the edges and their associated

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

mixing weights) in order to balance the convergence rate
and the communication complexity of D-SGD.

3 Preliminaries

Problem setting.
In decentralized federated learning,
n ∈ N(cid:63) agents (nodes) with their own data distribution seek
to collaborate in order to solve a global consensus problem.
Formally, the agents aim to learn a single parameter θ ∈ Rd
so as to optimize the global objective (Lian et al., 2017):
(cid:2)f (θ) (cid:44) 1

f ∗ (cid:44) minθ∈Rd

i=1 fi(θ)(cid:3),

(cid:80)n

(1)

n

where fi(θ) (cid:44) EZi∼Di[Fi(θ; Zi)] is the local objective
function associated to node i. The random vector Zi is
drawn from the data distribution Di of agent i, having sup-
port over a space Ωi, and Fi : Rd × Ωi → R is its pointwise
loss function (differentiable in its ﬁrst argument). Note that
the distributions Di can be very different, which is common
in real applications (Kairouz et al., 2021). From an optimiza-
tion point of view, this means that a local optimum θ(cid:63)
i ∈
arg minθ fi(θ) can be far from a global optimum θ(cid:63) of (1).

To collaboratively solve (1) in a fully decentralized manner,
the agents communicate with each other over a directed
graph. The graph topology is represented by a matrix
W ∈ [0, 1]n×n, where Wij > 0 gives the weight that agent
i gives to messages received from agent j, while Wij = 0
(no edge) means that i does not receive messages from j.
The choice of topology W affects the trade-off between the
convergence rate of decentralized optimization algorithms
and the communication costs. Indeed, more edges imply
higher communication costs but often faster convergence.
Communication costs, or per-iteration complexity, are
often regarded as proportional to the maximum (in or out)-
degrees of nodes in the topology, representing the maximum
(incoming or outcoming) load of a node (Lian et al., 2017):

din
max(W ) = maxi
dout
max(W ) = maxi

(cid:80)n

j=1

(cid:80)n

j=1

I[Wij > 0],
I[Wji > 0].

(2)

From this perspective, the complete graph, and the star
topology induced by server-based federated learning, yield
high communication costs, as the maximum degree is n − 1.

Decentralized SGD. Decentralized Stochastic Gradient De-
scent (D-SGD) (Lian et al., 2017; Koloskova et al., 2020)
is a popular fully decentralized algorithm for solving prob-
lems of the form (1). As mentioned above, such algorithms
operate on a graph topology represented by the matrix W ∈
[0, 1]n×n. In particular, D-SGD requires that W is a mixing
matrix, i.e. doubly stochastic: W 1 = 1 and 1TW = 1T.

In the rest of the paper, we will use the terms topology and
mixing matrix interchangeably. For sake of generality, we
consider a setting where the mixing matrix may change
at each iteration (Koloskova et al., 2020). On the other
hand, we assume for simplicity that the mixing matrices are

Algorithm 1 Decentralized SGD (Lian et al., 2017)
Require: Initialize ∀i, θ(0)

i = θ(0) ∈ Rd, iterations T ,

stepsizes {ηt}T −1
for t = 0, . . . , T − 1 do

t=0 , mixing {W (t)}T −1
t=0 .

for each node i = 1, . . . , n (in parallel) do

i ∼ Di

Sample Z (t)
θ(t+ 1
2 )
i
θ(t+1)
i

← θ(t)
← (cid:80)n

i − ηt∇Fi(θ(t)
i
ij θ(t+ 1
2 )
j=1 W (t)

j

end for

, Z (t)
i )

end for

deterministic. All our results can however be extended to
random mixing matrices, see Appendix C.1 for details.

D-SGD is summarized in Algorithm 1. At iteration t,
each node i ﬁrst updates its local estimate θ(t)
based on
∇Fi(θ(t)
, Z (t)
i ), the stochastic gradient of Fi evaluated at
i
θ(t)
i with Z (t)
sampled from Di. Then, each node aggregates
its current parameter value with its neighbors according to
the mixing matrix W (t).

i

i

General assumptions. We recall some standard assump-
tions extensively considered in decentralized learning
(Bubeck, 2014; Nguyen et al., 2019; Lian et al., 2017; Tang
et al., 2018; Assran et al., 2019; Li et al., 2019; Kong et al.,
2021; Ying et al., 2021).

, there exists a constant σ2

Assumption 1. (L-smoothness) There exists a constant
L > 0 such that for any Z ∈ Ωi, θ, ˜θ ∈ Rd we have
(cid:107)∇Fi(θ, Z) − ∇Fi(˜θ, Z)(cid:107) ≤ L(cid:107)θ − ˜θ(cid:107) .
Assumption 2. (Bounded variance) For any node i ∈
i > 0 such that for any
1, . . . , n
(cid:3) ≤ σ2
(cid:74)
θ ∈ Rd, we have EZ∼Di
i .
Assumption 3. (Mixing parameter) There exists a mixing
parameter p ∈ [0, 1] such that for any matrix M ∈ Rd×n,
we have (cid:107)M W T − M (cid:107)2
F ≤ (1 − p)(cid:107)M − M (cid:107)2
F , where
(cid:107) · (cid:107)F denotes the Frobenius norm and M = M ( 1
n 11T).

(cid:2) (cid:107)∇Fi(θ, Z) − ∇fi(θ)(cid:107)2

(cid:75)

2

Assumption 3 measures how well an averaging step using a
mixing matrix W brings an arbitrary matrix M closer to M .
It is always veriﬁed for p = 1 − λ2(W TW ) with λ2(W TW )
the second largest eigenvalue of W TW (Boyd et al., 2006) .

4

Joint Effect of Topology and Data
Heterogeneity

In this section, we introduce a new quantity, called neigh-
borhood heterogeneity, and derive new convergence rates
for D-SGD that depend on this quantity. These rates have
several nice properties: (i) they hold under weaker assump-
tions than previous work (unbounded local heterogeneity),
(ii) they highlight the interplay between the topology and
the heterogeneous data distribution across nodes, and (iii)
they provide a criterion for choosing topologies not only

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

based on their mixing properties but also based on data.

3, 5). To see this, we ﬁrst show that our set of assumptions
is implied by the latter (proof in Appendix C).

4.1 Neighborhood Heterogeneity

Given a mixing matrix W , our notion of neighborhood
heterogeneity measures the expected distance between the
aggregated gradients in the neighborhood of a node (as
weighted by W ) and the global average of gradients. In our
analysis, we will assume this distance to be bounded.

Assumption 4 (Bounded neighborhood heterogeneity).
There exists a constant ¯τ 2 > 0 such that ∀θ ∈ Rd:

1
n

n
(cid:88)

i=1

E

(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

j=1

Wij∇Fj(θ, Zj)−

1
n

n
(cid:88)

j=1

∇Fj(θ, Zj)

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ¯τ 2.

(3)

To better understand Assumption 4, we can upper-bound
the left-hand term of the previous equation, denoted H(θ),
using a bias-variance decomposition. This leads to the fol-
lowing bound:

H(θ) ≤

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

Wij∇fj(θ) − ∇f (θ)

j=1

(cid:13)
2
(cid:13)
(cid:13)
2

(4)

+

σ2
max
n

(cid:107)W −

1
n

11T(cid:107)2

F ,

max = maxi σ2

with σ2
i . This upper bound contains two
terms. The ﬁrst one is a bias term, related to the hetero-
geneity of the problem. It essentially measures how the
gradients of local objectives differ from the gradient of the
global objective when they are aggregated at the neighbor-
hood level of the topology through W . The second one is
a variance term closely related to the mixing parameter p
of Assumption 3: we can show that it is upper bounded by
σ2
max(1 − p) and lower bounded by σ2
max(1 − p)/n, see
Proposition 3 in Appendix C.

Comparison to classic bounded heterogeneity assump-
tion. In our analysis, we use Assumption 4 in replacement
of the bounded local heterogeneity condition used in previ-
ous literature (Lian et al., 2017, 2018; Assran et al., 2019;
Koloskova et al., 2020; Ying et al., 2021). We recall it below.

Assumption 5 (Bounded local heterogeneity). There exists
a constant ¯ζ 2 > 0 such that 1
2 ≤
n
¯ζ 2, ∀θ ∈ Rd.

i=1 (cid:107)∇fi(θ)−∇f (θ)(cid:107)2

(cid:80)n

Assumption 5 has the same form as the bias term of in Equa-
tion (4) but considers W = I (i.e., it does not depend on
the topology). It requires that the local gradients should not
be too far from the global gradient: the more heterogeneous
the nodes’ distribution (and objectives), the bigger ¯ζ 2. In
contrast, neighborhood heterogeneity takes into account the
mixture of gradients in the neighborhoods deﬁned by W .
Crucially, Assumption 4 is more ﬂexible than Assumption 5.
More precisely, our set of assumptions (Assumptions 2-4) is
less restrictive than those in previous work (Assumptions 2,

Proposition 1. Let Assumptions 2-3 and 5 to be veriﬁed.
Then Assumption 4 is satisﬁed with ¯τ 2 = (1 − p) (cid:0)¯ζ 2 + ¯σ2(cid:1),
where ¯σ2 (cid:44) 1
n

i σ2
i .

(cid:80)

We now show that our set of assumptions (2-4) is strictly
more general than Assumptions 2, 3, 5 by identifying situ-
ations where Assumption 4 is veriﬁed while Assumption 5
is not. A trivial example is the complete graph W = 1
n 11T,
for which we have ¯τ 2 = 0, regardless of heterogeneity.
More interestingly, some combinations of sparse topologies
and data distributions can ensure that ¯τ 2 remains small
while ¯ζ 2 can be arbitrary large. We give a simple example
below (detailed derivations in Appendix A).

Example 1 (Two clusters and a ring topology). Let n be
an even number and assume Zi ∼ Di (cid:44) N (m, ˜σ2) if
i is odd and Zi ∼ Di (cid:44) N (−m, ˜σ2) if i is even. Let
˜σ2 < +∞ (necessary to have Assumption 2) and m > 0
potentially asymptotically large. We ﬁx Fi(θ, Zi) = (θ −
Zi)2 (mean estimation). Consider a ring topology that
alternates between one odd node and one even node, with
the diagonal and off-diagonal entries of W equal to 1/2 and
i = 4˜σ2 < +∞,
1/4 respectively. Then we have ¯τ 2 = σ2
while ¯ζ 2 = 4m2 can be arbitrarily large as m grows.

This illustrates that an appropriate topology, even as sparse
as a ring, can control ¯τ 2 and mitigate the underlying hetero-
geneity of the problem. In Section 5, we will show that we
can learn a sparse topology W that (approximately) min-
imizes the neighborhood heterogeneity bound ¯τ 2. Before
that, we validate the relevance of our new Assumption 4 by
deriving a novel convergence result for D-SGD.

4.2 Convergence Analysis

We now present the main theoretical result of this section:
two new non-asymptotic convergence results for D-SGD
under Assumption 4. The proof of this theorem is given in
Appendix B.

Theorem 1. Consider Algorithm 1 with mixing matrices
W (0), . . . , W (T −1) satisfying Assumptions 3 and 4. As-
sume further that Assumptions 1-2 are respected, and denote
¯θ(t) (cid:44) 1
. For any target accuracy ε > 0, there
n
exists a constant stepsize η ≤ ηmax = p
Convex case:

8L such that:

i=1 θ(t)

(cid:80)n

i

1
T +1

(cid:80)T

t=0

E(f (¯θ(t)) − f (cid:63)) ≤ ε as soon as
√

T ≥ O

(cid:16) ¯σ2
nε2 +

L¯τ
pε 3

2

+

(cid:17)

L
pε

r0 ,

(5)

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Non-convex case:

1
T +1

(cid:80)T

t=0

E(cid:107)∇f (¯θ(t)(cid:107)2

2 ≤ ε as soon as

(cid:17)

T ≥ O

(cid:16) L¯σ2
nε2 +

L¯τ
pε 3
where T is the number of iterations, r0 = (cid:107)θ(0) − θ(cid:63)(cid:107)2
2,
f0 = f (θ(0)) − f (cid:63) and O(·) hides the numerical constants
explicitly provided in the proof.

L
pε

f0 ,

(6)

+

2

trade-off between the convergence rate and the per-iteration
communication complexity given in Equation (2). However,
minimizing neighborhood heterogeneity in the general
setting appears to be challenging without further statistical
assumptions, as Equation (3) should hold for all θ ∈ Rd.
Below, we focus on classiﬁcation with label skew, and show
that Equation (3) simpliﬁes to a more tractable quantity.

5.1 Statistical Learning with Label Skew

Analysis and comparison to prior results. To put the
above theorem into perspective, recall that Centralized (Par-
allel) Stochastic Gradient Descent (C-PSGD) is equivalent
to D-SGD with the mixing matrix W = 1
n 11T (complete
graph). For this speciﬁc case, it has been shown that
in the convex scenario, an accuracy ε is achieved after
T ≥ O( ¯σ2
ε ) iterations (Dekel et al., 2012; Bottou et al.,
2018; Stich and Karimireddy, 2020). On the other hand,
existing results for D-SGD (under Assumption 5 instead of
Assumption 4) require T ≥ O( ¯σ2
+ L
pε )
nε2 +
iterations (Koloskova et al., 2020).

L(1−p)(¯ζ+¯σ
pε3/2

nε2 + L

√

p)

√

The ﬁrst thing to note is that rate (5) is consistent with the
above rates. When the complete graph topology W = 1
n 11T
is used at each iteration we have ¯τ = 0 and p = 1, which al-
lows us to recover the rate of the communication-inefﬁcient
C-PSGD. Furthermore, considering the classical Assump-
tion 5 and using Proposition 1 gives the looser bound
O( ¯σ2
pε ) which is equivalent to the
rate of D-SGD in Koloskova et al. (2020). Similarly, the
rate (6) obtained for non-convex objectives is also consistent
with Koloskova et al. (2020).

L(1−p)(¯ζ+¯σ)
pε3/2

nε2 +

+ L

√

√

Crucially, recall that in the heterogeneous setting ¯τ can be
1 − p(¯ζ + ¯σ) (see Section 4.1), which
much smaller than
makes our bounds sharper. This is because the topology now
inﬂuences the convergence rate in Theorem 1 via both the
mixing parameter p and ¯τ . This is of particular signiﬁcance
in situations where communication constraints are strong
so that the topology connectivity has to be low (i.e., p close
to 0). In that case, prior rates are heavily impacted by data
heterogeneity as p can no longer compensate for it.
In
contrast, we can expect that a well-chosen sparse topology
can achieve small ¯τ and thus mitigate the impact of data
heterogeneity. To highlight this, we can go back to Exam-
ple 1. For the chosen ring topology, we have p = Θ( 1
n2 ),
but the speciﬁc arrangement of nodes and the weights in W
still allow a small bound ¯τ 2 on neighborhood heterogeneity.

5 Learning the Topology

In the previous rates (5) and (6), the smaller the bound ¯τ 2 on
neighborhood heterogeneity, the fewer iterations needed to
reach an error ε. This motivates the idea of learning a sparse
topology W that approximately minimizes neighborhood
the
heterogeneity (Equation (3)),

in order to control

Label skew is an important type of data heterogeneity in fed-
erated classiﬁcation problems (Kairouz et al., 2021; Hsieh
et al., 2020; Bellet et al., 2022). In this setting, each agent
i is associated with a random variable Zi = (Xi, Yi) ∼ Di
where Xi ∈ Rq represents the feature vector and Yi ∈
1, . . . , K
the associated class label. The agents aim to
(cid:74)
learn a classiﬁer hθ : Rq →
1, . . . , K
parameterized by
(cid:74)
θ ∈ Rp such that hθ(Xi) is a good predictor of Yi for all
i. The heterogeneity of the distributions {Di}n
i=1 comes
only from a difference in the label distribution Pi(Y ) i.e.
Di = Pi(X, Y ) = P (X|Y )Pi(Y ). For simplicity, we as-
sume that all agents use the same pointwise loss function
(Fi = F for all i), which is typically the cross-entropy.

(cid:75)

(cid:75)

Under the above framework, we can derive a neighborhood
heterogeneity bound ¯τ 2 that can effectively be minimized
with respect to W .

Proposition 2 (Bounded neighborhood heterogeneity un-
der label skew). Consider the statistical framework de-
ﬁned above and assume there exists B > 0 such that
∀k = 1, . . . , K and ∀θ ∈ Rd, (cid:107)EX [∇F (θ; X, Y )|Y =
EX [∇F (θ; X, Y )|Y = k(cid:48)](cid:107)2
k] − 1
2 ≤ B. Then,
K
denoting πjk (cid:44) Pj(Y = k), Assumption 4 is satisﬁed with:

(cid:80)K

k(cid:48)=1

¯τ 2 =

KB
n

K
(cid:88)

n
(cid:88)

(cid:16) n
(cid:88)

Wijπjk −

k=1

i=1

j=1

1
n

n
(cid:88)

j=1

(cid:17)2

πjk

(7)

+

σ2
max
n

(cid:107)W −

1
n

11T(cid:107)2

F .

The proof is provided in Appendix C.Note that the condi-
tion involving B corresponds to a bounded heterogeneity
assumption at the class level (rather than at the agent level
as in Assumption 5).
The neighborhood heterogeneity bound ¯τ 2 in (7) is quadratic
in W and composed of two terms. The ﬁrst one is a bias term
due to the label skew: it will be minimal if neighborhood-
level class proportions (weighted by W ) match the global
class proportions. This is trivially achieved for any choice
of W if the class proportions are the same across nodes.
The second term is a variance term which is minimal when
W = 11T
n , the complete topology with uniform weights. As
a matter of fact, this topology is also the unique global min-
imizer of (7), which is equal to 0 in this case. However, as
already discussed, such a dense mixing matrix is impractical
as it yields huge communication costs. We will show how

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Algorithm 2 Sparse Topology Learning with Frank-Wolfe
(STL-FW)

Require: Initialization (cid:99)W (0) = In, class proportions Π ∈

[0, 1]n×K and hyperparameter λ > 0.
for l = 0, . . . , L do

P (l+1) = arg minP ∈A (cid:104)P, ∇g((cid:99)W (l))(cid:105)
γ(l+1) = arg minγ∈[0,1] g(cid:0)(1 − γ)(cid:99)W (l) + γP (l+1)(cid:1)
(cid:99)W (l+1) = (1 − γ(l+1))(cid:99)W (l) + γ(l+1)P (l+1)

end for

the per-iteration communication complexity of D-SGD can
be controlled while approximately minimizing ¯τ 2 in (7).

5.2 Optimization with the Frank-Wolfe Algorithm

In this section, we design an algorithm that ﬁnds a sparse
approximate minimizer of ¯τ 2 in (7). We focus on learning
a single mixing matrix W as a “pre-processing” step (i.e.,
before running D-SGD), and do so in a centralized manner.
Speciﬁcally, we assume that a single party (which may be
one of the agents, or a third-party) has access to the class
proportions πik = Pi(Y = k) for each agent i and each
class k. In practice, since each agent has access to its local
dataset, it can compute these local proportions locally and
share them without sharing the local data itself.

a

Our

objective

is
sparse mixing matrix W which

Optimization problem.
to
ap-
learn
proximately minimizes ¯τ 2
Denoting by
S (cid:44) (cid:8)W ∈ [0, 1]n×n : W 1 = 1, 1TW = 1T(cid:9) the
set of doubly stochastic matrices, the optimization problem
can be written as follows:

in (7).

(cid:110)

g(W ) (cid:44) 1
n

(cid:13)
(cid:13)W Π − 11T
(cid:13)

n Π

min
W ∈S

(cid:13)
2
(cid:13)
(cid:13)

F

+ λ
n

(cid:13)
(cid:13)W − 11T
(cid:13)

n

(cid:111)

,

(cid:13)
2
(cid:13)
(cid:13)

F

max

KB , but σ2

(8)
where Π ∈ [0, 1]n×K contains the class proportions {πik}
and λ > 0 is a hyperparameter. To exactly match (7), λ
should be equal to σ2
max and B are unknown in
Instead, we use λ to control the bias-variance
practice.
trade-off. As discussed in Section 4.1, the variance term is
an upper bound of 1 − p with p the mixing parameter of W .
Therefore, λ allows to tune a trade-off between the mini-
mization of the bias due to label skew and the maximization
of the mixing parameter of W .

Algorithm. We propose to ﬁnd sparse approximations of (8)
using a Frank-Wolfe (FW) algorithm, which is well-suited
to learn a sparse parameter over convex hulls of ﬁnite set
of atoms (Jaggi, 2013). In our case, S corresponds to the
convex hull of the set A of all permutation matrices (Lovász
and Plummer, 2009; Tewari et al., 2011; Valls et al., 2020).

The algorithm is summarized in Algorithm 2. Starting from
the identity matrix (cid:99)W (0) = In ∈ S, each iteration l ≥ 0
consists of moving towards a feasible point P (l+1) that

minimizes a linearization of g at the current iterate (cid:99)W (l).
As ﬁnding P (l+1) is a linear problem, solving it over S is
equivalent to solving it over A. Although A contains n!
elements, the linear program corresponds to the well-known
assignment problem (Burkard et al., 2012; Crouse, 2016)
and can be solved tractably with the Hungarian algorithm,
which has a worst-case complexity of O(n3) (Lovász and
Plummer, 2009).1 Note that the gradient ∇g(W ) needed
to solve the assignment problem is given by

2
n

(cid:80)K

k=1(W Π:,k − Π:,k1) · ΠT

:,k + 2

n λ

(cid:16)

W − 11T
n

(cid:17)

,

where Π:,k is the k-th column of Π. The next iterate (cid:99)W (l+1)
is then obtained as a convex combination of P (l+1) and
(cid:99)W (l), and is thus guaranteed to be in S. The optimal combin-
ing weight is computed by line-search, which has a closed-
form solution since g is quadratic (see Appendix C.2).

Crucially, Algorithm 2 allows to control the sparsity of the
ﬁnal solution: since a permutation matrix contains exactly
one non-zero entry in each row and each column, at most
one new incoming and one new outgoing edge per node
are added. As we start from the identity matrix (i.e., only
self-edges), this guarantees that at the end of the l-th itera-
tion, each node will have at most l in-neighbors and l out-
neighbors. The per-iteration communication complexity of
D-SGD induced by the learned topology can thus be directly
controlled by the number of iterations of our algorithm. The
trade-off with the quality of the solution is quantiﬁed by the
following theorem, which is derived from standard results
for FW (Jaggi, 2013) combined with a tight bound on the
smoothness of g in appropriate norm (see Appendix C).

Theorem 2. Consider the statistical setup presented in Sec-
tion 5.1 and let {(cid:99)W (l)}L
l=1 be the sequence of mixing ma-
trices generated by Algorithm 2. Then, at any iteration
l = 1, . . . , L, we have:

g((cid:99)W (l)) ≤

16
l + 2

(cid:0)λ+

(cid:13)
(cid:13)

1
n

K
(cid:88)

(Π:,k−Π:,k1)·ΠT
:,k

(cid:13)
(cid:63)
(cid:13)
2

(cid:1) , (9)

k=1

where (cid:107)·(cid:107)(cid:63)
singular values. Furthermore, we have din
and dout
bounded by l.

2 stands for the nuclear norm, i.e., the sum of
max((cid:99)W (l)) ≤ l
max((cid:99)W (l)) ≤ l, resulting in a per-iteration complexity

The above theorem shows that the objective g decreases at a
rate of O(1/l) as new connections between nodes are made.
In general, we can bound (9) less tightly by g((cid:99)W (l)) ≤
16
l+2 (λ + 1), which is independent of the number of nodes n.
Recall that with λ = σ2
max/KB, the value ¯τ 2 of Proposition
2 is exactly equal to KB ·g(W ). Therefore, the bound given
in Theorem 2 directly bounds neighborhood heterogeneity
and can thus be plugged in the rates of Theorem 1.

1The algorithm is quite fast in practice: for instance, the scipy

implementation runs in 0.3s on a regular laptop for n = 1000.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

(a) Topology learning

(b) D-SGD with dmax = 3

(c) D-SGD with dmax = 9

(cid:80)n

Figure 1: (a) Evolution of key quantities across the iterations of topology learning: in red the objective function g(W (l)), in green the bias
2 and in yellow the mixing parameter 1 − p = λ2(W (l)TW (l)). Here, λ = 0.5 and
term supθ
m = 5. (b, c) Error n−1(cid:107)θ(t) −θ(cid:63)(cid:107)2
2 (solid line) of D-SGD after 50 iterations, averaged over 10 runs, for increasing levels of heterogeneity
(measured by parameter m). The dashed lines show maxi(θ(t)

i − θ(cid:63))2, illustrating the variability across nodes.

i − θ(cid:63))2 and mini(θ(t)

ij ∇fj(θ) − ∇f (θ)(cid:107)2

i=1 (cid:107) (cid:80)n

j=1 W (l)

1
n

To summarize, our approach provides a principled way to
learn the topology so as to reduce neighborhood hetero-
geneity while controlling the per-iteration communication
complexity of D-SGD. Remarkably, the fact that g((cid:99)W (l)) is
independent of n implies that we can ﬁnd topologies that ap-
proximately optimize the convergence rate of D-SGD while
keeping the communication load per node constant, thereby
guaranteeing scalability to a large number of nodes even in
highly heterogeneous scenarios.

6 Experiments

This section shows the practical usefulness of our topology
learning method, referred to as Sparse Topology Learning
with Frank-Wolfe (STL-FW). We call communication
budget dmax = max{din
max} the maximal number of
neighbors a node can have in the used topologies, which
controls the per-iteration communication complexity
incurred by any node.

max, dout

6.1 Simulations on Synthetic Data

Statistical setup. We generalize the mean estimation objec-
tive of Example 1 with K = 10 clusters and n = 100 nodes,
with exactly 10 nodes associated to each cluster. Each
cluster is associated with a speciﬁc Gaussian distribution,
which corresponds to a class in the statistical framework de-
scribed in Section 5.1. The variance of the K distributions
is the same (˜σ2 = 1) but their means are evenly spread
over [−m, m]. Thus, m ≥ 0 controls the heterogeneity of
the problem (the bigger m, the more heterogeneous the
setup). We can analytically compute all numerical constants
introduced throughout the paper. Unless otherwise noted,
λ is set to σ2/KB where σ2 = 4˜σ2 and B = 4m2.

have exactly b neighbors, with uniform weights). We use
a ﬁxed step-size for D-SGD, which is tuned separately for
each topology in the interval [0.001, 1].

Results. We ﬁrst study the behavior of our topology learn-
ing algorithm. As seen in Figure 1(a), the objective function
g(W (l)) decreases quickly in the ﬁrst iterations with a clear
elbow at l = 9 iterations. This is because we have K = 10
“classes”, hence 9 neighbors are sufﬁcient to compensate
for label skew. We also see that decreasing g successfully
decreases the two key quantities that affect the convergence
of D-SGD and are upper bounded by g: the bias term in
Equation (4) (which does not depend on θ in this setup
and can therefore be computed exactly) and the mixing
parameter 1 − p (which continues to decrease beyond l = 9).

Figure 1(b, c) shows that the topology learned by STL-FW
indeed translates into faster convergence for D-SGD than
with the random (but well-connected) topology in data het-
erogeneous settings. This is especially striking when look-
ing at best and worst-case errors across nodes (dashed lines).
For a low budget (dmax = 3), D-SGD with our topology
remains slightly impacted by heterogeneity. But remarkably,
for dmax = 9, our topology makes D-SGD completely in-
sensitive to increasing data heterogeneity. This observation
is consistent with the elbow observed at l = dmax = 9 in
Figure 1(a). In Appendix D.2, we provide basic statistics
on the topologies obtained for the two budgets dmax = 3
and dmax = 9. We can see in particular that the topologies
obtained with STL-FW are dmax-regular (like the random
graph) but have much lower bias (i.e., the distribution of la-
bels in the neighborhood of nodes is closer to the global dis-
tribution). As expected, this bias is equal to 0 for dmax = 9.

6.2 Experiments on Real Datasets

Competitor. For a ﬁxed budget dmax, we compare the topol-
ogy learned by STL-FW to a random dmax-regular graph
with uniform weights
dmax+1 . This graph is independent of
the data but has good mixing parameter p (every node will

1

Setup. We follow the experimental setup in Bellet et al.
(2022) and consider two classiﬁcation tasks: a linear
model on MNIST (Deng, 2012) and a Group Normalized
LeNet (Hsieh et al., 2020) on CIFAR10 (Krizhevsky et al.,

Mixing Parameter020406080100Heterogeneity parameter m0255075100125150175Error after 50 iterationsSTL-FW (ours)Random020406080100Heterogeneity parameter m05101520253035Error after 50 iterationsSTL-FW (ours)RandomReﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

MNIST dmax = 2

MNIST dmax = 5

MNIST dmax = 10

CIFAR10 dmax = 2

CIFAR10 dmax = 5

CIFAR10 dmax = 10

Figure 2: Convergence of D-SGD with STL-FW (our approach) and alternative topologies on real datasets under different communication
budgets. The fully connected graph induces intractable communication costs but gives a performance upper bound, while the exponential
graph is shown for dmax = 10 but exceeds this budget.

2009). In both cases, we partition the dataset on 100 nodes
using the scheme introduced in McMahan et al. (2017),
i.e. on average, nodes will have examples of two classes,
but may have only 1 and up to 4. We re-use the hyper-
parameters from Bellet et al. (2022): a learning rate of 0.1
and batch size of 128 for MNIST, and a learning rate of
0.002 and batch size of 20 for CIFAR10. Using D-SGD,
we compare the topology learned with our approach STL-
FW to other ﬁxed topologies: (1) a fully-connected graph
(dmax = 99), which exhibits the fastest convergence speed
but is impractical, (2) a random graph with the same com-
munication budget as STL-FW, (3) D-Cliques (Bellet et al.,
2022), also with the same budget, and (4) a deterministic ex-
ponential graph promoted in recent work (Ying et al., 2021)
(dmax = 14). Note that all competing topologies are data-
independent, except D-Cliques. To have a fair comparison,
we use standard D-SGD without algorithmic modiﬁcations
like “clique-averaging” introduced by Bellet et al. (2022).
For all experiments with STL-FW, we use λ = 0.1 since, re-
markably, its value does not signiﬁcantly change the results
(see Figure 3 in Appendix D).

Results. Figure 2 shows our results for varying communi-
cation budget dmax: small (2), medium (5) and large (10).
On MNIST, STL-FW makes convergence faster than all
competitors and quickly matches the speed of the fully-
connected topology as the budget dmax increases. Remark-
ably, STL-FW is already showing good performance at
dmax = 2, which is a very small budget that the other topolo-
gies (except the random one) cannot handle. As expected,

the two data-dependent topologies (D-Cliques and STL-FW)
outperform the random topologies, including the exponen-
tial graph which has better connectivity (dmax = 14) but
does not compensate for the heterogeneity. The fact that
STL-FW improves over D-Cliques can be explained by the
fact that D-Cliques only compensate the heterogeneity (the
bias term in Equation (4)) without consideration for the over-
all connectivity (the variance term in Equation (4)). This is
illustrated in the tables of Appendix D.2.

On CIFAR10, we see that dmax = 2 is not sufﬁcient to reach
good performance. This can be explained by the increased
complexity of the problem (non-convex objective with a
deep model), requiring larger communication budgets. This
is in line with empirical results in prior work (Kong et al.,
2021). However, with slightly larger budgets i.e. dmax = 5
and 10 (dmax = 3 in Fig. 5, App. D), performance improves
and the results are consistent with those on MNIST: STL-
FW outperforms other sparse topologies and comes close to
the performance of the fully connected topology for dmax =
10. Overall, STL-FW provides better convergence speed
than all tractable alternatives, with the additional ability to
operate in low communication regimes (unlike D-Cliques
and the exponential graph).

7 Conclusion

This paper addressed two important open problems in decen-
tralized learning. First, thanks to our new notion of neigh-
borhood heterogeneity, we characterized the joint effect of

020406080100Epochs020406080Test Accuracy (%)Fully-ConnectedSTL-FWRandom020406080100Epochs878889909192Test Accuracy (%)Fully-ConnectedSTL-FWRandomD-Cliques020406080100Epochs878889909192Test Accuracy (%)Fully-ConnectedSTL-FWRandomD-CliquesExponential dmax=14020406080100Epochs010203040506070Test Accuracy (%)Fully-ConnectedSTL-FWRandom020406080100Epochs010203040506070Test Accuracy (%)Fully-ConnectedSTL-FWRandomD-Cliques020406080100Epochs010203040506070Test Accuracy (%)Fully-ConnectedSTL-FWRandomD-CliquesExponential dmax=14Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

the topology and the data heterogeneity in the convergence
rate of D-SGD. Our results show that, if chosen appropri-
ately, the topology can compensate for the heterogeneity
and speed up convergence. Second, we tackled the problem
of learning a good topology under data heterogeneity. To the
best of our knowledge, our work is the ﬁrst to provide a prin-
cipled and data-dependent approach, with explicit control on
the trade-off between the communication costs and the con-
vergence speed of D-SGD. We believe that our work paves
the way for the design of other data-dependent topology
learning techniques. One may for instance investigate dif-
ferent types of heterogeneity (beyond label skew), different
knowledge assumptions (e.g., not knowing the proportions),
and dynamic learning of the topology. We can also envision
fully decentralized and privacy-preserving versions.

References

Assran, M., Loizou, N., Ballas, N., and Rabbat, M. (2019).
Stochastic gradient push for distributed deep learning. In
ICML.

Bellet, A., Kermarrec, A.-M., and Lavoie, E. (2022). D-
Cliques: Compensating for Data Heterogeneity with
Topology in Decentralized Federated Learning. In SRDS.

Bottou, L., Curtis, F. E., and Nocedal, J. (2018). Opti-
mization methods for large-scale machine learning. Siam
Review, 60(2):223–311.

Boyd, S., Diaconis, P., and Xiao, L. (2004). Fastest mixing
markov chain on a graph. SIAM review, 46(4):667–689.

Boyd, S., Ghosh, A., Prabhakar, B., and Shah, D. (2006).
Randomized gossip algorithms. IEEE Transactions on
Information Theory, 52(6):2508–2530.

Bubeck, S. (2014). Convex optimization: Algorithms and

complexity. arXiv:1405.4980.

Burkard, R., Dell’Amico, M., and Martello, S. (2012). As-

signment problems: revised reprint. SIAM.

Chow, Y.-T., Shi, W., Wu, T., and Yin, W. (2016). Expander
graph and communication-efﬁcient decentralized opti-
mization. In 2016 50th Asilomar Conference on Signals,
Systems and Computers, pages 1715–1720. IEEE.

Colin, I., Bellet, A., Salmon, J., and Clémençon, S. (2016).
Gossip dual averaging for decentralized optimization of
pairwise functions. In ICML.

Crouse, D. F. (2016). On implementing 2d rectangular
assignment algorithms. IEEE Transactions on Aerospace
and Electronic Systems, 52(4):1679–1696.

Dandi, Y., Koloskova, A., Jaggi, M., and Stich, S. U. (2022).
Data-heterogeneity-aware mixing for decentralized learn-
ing. arXiv preprint arXiv:2204.06477.

Deng, L. (2012). The mnist database of handwritten digit
images for machine learning research. IEEE Signal Pro-
cessing Magazine, 29(6):141–142. MNIST is distributed
under Creative Commons Attribution-Share Alike 3.0
license.

Esfandiari, Y., Tan, S. Y., Jiang, Z., Balu, A., Herron, E.,
Hegde, C., and Sarkar, S. (2021). Cross-Gradient Ag-
gregation for Decentralized Learning from Non-IID data.
Technical report, arXiv:2103.02051.

Even, M., Massoulié, L., and Scaman, K. (2022). Sam-
ple Optimality and All-for-all Strategies in Personalized
Federated and Collaborative Learning. Technical report,
arXiv:2201.13097.

Frank, M. and Wolfe, P. (1956). An algorithm for quadratic
programming. Naval Research Logistics Quarterly, 3:95–
110.

Gao, H. and Huang, H. (2020). Periodic stochastic gradi-
ent descent with momentum for decentralized training.
arXiv:2008.10435.

Hsieh, K., Phanishayee, A., Mutlu, O., and Gibbons, P. B.
(2020). The Non-IID Data Quagmire of Decentralized
Machine Learning. In ICML.

Huang, K. and Pu, S. (2021).

Improving the tran-
sient times for distributed stochastic gradient methods.
arXiv:2105.04851.

Jaggi, M. (2013). Revisiting frank-wolfe: Projection-free

sparse convex optimization. In ICML.

Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., et al.
(2021). Advances and open problems in federated learn-
ing. Foundations and Trends® in Machine Learning,
14(1–2):1–210.

Koloskova, A., Lin, T., and Stich, S. U. (2021). An improved
analysis of gradient tracking for decentralized machine
learning. NeurIPS, 34.

Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich,
S. U. (2020). A uniﬁed theory of decentralized sgd with
changing topology and local updates. In ICML.

Koloskova, A., Stich, S., and Jaggi, M. (2019). Decentral-
ized stochastic optimization and gossip algorithms with
compressed communication. In ICML.

Kong, L., Lin, T., Koloskova, A., Jaggi, M., and Stich, S.
(2021). Consensus control for decentralized deep learning.
In ICML.

Koppel, A., Sadler, B. M., and Ribeiro, A. (2017). Proxim-
ity without consensus in online multiagent optimization.
IEEE Transactions on Signal Processing, 65(12):3062–
3077.

Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L.
(2012). Optimal distributed online prediction using mini-
batches. Journal of Machine Learning Research, 13(1).

Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple
layers of features from tiny images. CIFAR is distributed
under MIT license.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Li, X., Yang, W., Wang, S., and Zhang, Z. (2019).
Communication-efﬁcient local decentralized sgd meth-
ods. arXiv:1910.09126.

Vanhaesebrouck, P., Bellet, A., and Tommasi, M. (2017).
Decentralized collaborative learning of personalized mod-
els over networks. In AISTATS.

Vogels, T., Hendrikx, H., and Jaggi, M. (2022). Beyond
spectral gap: The role of the topology in decentralized
learning. arXiv preprint arXiv:2206.03093.

Wang, J., Sahu, A. K., Yang, Z., Joshi, G., and Kar, S.
(2019). Matcha: Speeding up decentralized sgd via match-
ing decomposition sampling. In ICC.

Ying, B., Yuan, K., Chen, Y., Hu, H., Pan, P., and Yin,
W. (2021). Exponential graph is provably efﬁcient for
decentralized deep training. NeurIPS, 34.

Yuan, K. and Alghunaim, S. A. (2021). Removing data
heterogeneity inﬂuence enhances network topology de-
pendence of decentralized sgd. arXiv:2105.08023.

Yuan, K., Alghunaim, S. A., Ying, B., and Sayed, A. H.
(2020). On the inﬂuence of bias-correction on distributed
stochastic optimization. IEEE Transactions on Signal
Processing, 68:4352–4367.

Yuan, K., Chen, Y., Huang, X., Zhang, Y., Pan, P., Xu, Y.,
and Yin, W. (2021). Decentlam: Decentralized momen-
tum sgd for large-batch deep training. In ICCV.

Zantedeschi, V., Bellet, A., and Tommasi, M. (2020). Fully
decentralized joint learning of personalized models and
collaboration graphs. In AISTATS.

Zhu, T., He, F., Zhang, L., Niu, Z., Song, M., and Tao, D.
(2022). Topology-aware generalization of decentralized
sgd. In International Conference on Machine Learning,
pages 27479–27503. PMLR.

Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and
Liu, J. (2017). Can decentralized algorithms outperform
centralized algorithms? a case study for decentralized
parallel stochastic gradient descent. NIPS.

Lian, X., Zhang, W., Zhang, C., and Liu, J. (2018). Asyn-
chronous Decentralized Parallel Stochastic Gradient De-
scent. In ICML.

Lin, T., Karimireddy, S. P., Stich, S., and Jaggi, M. (2021).
Quasi-global momentum: Accelerating decentralized
deep learning on heterogeneous data. In ICML.

Lovász, L. and Plummer, M. D. (2009). Matching theory,

volume 367. American Mathematical Soc.

Marfoq, O., Neglia, G., Bellet, A., Kameni, L., and Vidal, R.
(2021). Federated Multi-Task Learning under a Mixture
of Distributions. In NeurIPS.

Marfoq, O., Xu, C., Neglia, G., and Vidal, R. (2020).
Throughput-optimal topology design for cross-silo feder-
ated learning. NeurIPS.

McMahan, B., Moore, E., Ramage, D., Hampson, S., and
y Arcas, B. A. (2017). Communication-efﬁcient learning
of deep networks from decentralized data. In AISTATS.
Nedi´c, A., Olshevsky, A., and Rabbat, M. G. (2018). Net-
work topology and communication-computation tradeoffs
in decentralized optimization. Proceedings of the IEEE,
106(5):953–976.

Neglia, G., Xu, C., Towsley, D., and Calbi, G. (2020). De-
centralized gradient methods: does topology matter? In
AISTATS.

Nguyen, L. M., Nguyen, P. H., Richtárik, P., Scheinberg, K.,
Takác, M., and van Dijk, M. (2019). New convergence
aspects of stochastic gradient algorithms. Journal of
Machine Learning Research, 20:176–1.

Stich, S. U. and Karimireddy, S. P. (2020). The error-
feedback framework: Better rates for sgd with delayed
gradients and compressed updates. Journal of Machine
Learning Research, 21:1–36.

Sun, T., Li, D., and Wang, B. (2021). Stability and gener-
alization of decentralized stochastic gradient descent. In
Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 35, pages 9756–9764.

Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J. (2018).
D²: Decentralized training over decentralized data. In
ICML.

Tewari, A., Ravikumar, P., and Dhillon, I. (2011). Greedy
algorithms for structurally constrained high dimensional
problems. NIPS.

Valls, V., Iosiﬁdis, G., and Tassiulas, L. (2020). Birkhoff’s
decomposition revisited: Sparse scheduling for high-
speed circuit switches. arXiv:2011.02752.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Appendix

A Details on Example 1

In this section, we provide more details on Example 1 (Section 4.1) by giving the exact parametrization. Recall that we want
to ﬁnd an example where Assumption 5 is not veriﬁed while Assumption 4 is.
Let us consider n nodes with n an even number. For all i = 1, . . . , n, assume Zi ∼ N (m, ˜σ2) if i is odd and Zi ∼
N (−m, ˜σ2) if i is even. Assume further that ˜σ2 < +∞ but m > 0 can be asymptotically large. For all i = 1, . . . , n we ﬁx
Fi(θ, Zi) = (θ − Zi)2, which corresponds to a simple mean estimation objective.

Consider a ﬁxed mixing matrix W associated with a ring topology that alternates between the two distributions. Speciﬁcally,
for i = 2, . . . , n − 1 and j = 1, . . . , n, we ﬁx the weights as follows:

Wij =






1
2
1
4
0

if j = i ,
if j = i + 1 or j = i − 1 ,
otherwise .

Moreover, we ﬁx W11 = Wnn = 1
With such parametrization we have ∇Fi(θ, Zi) = 2(θ − Zi) and therefore ∇fi(θ) = 2(θ − m) if i is odd and ∇fi(θ) =
2(θ + m) if i is even. Moreover, the gradient of the global objective is ∇f (θ) = 1
i ∇fi(θ) = 2θ and the neighborhood
n
averaging (cid:80)

2 and W1n = Wn1 = 1
4 .

j Wij∇fj(θ) = 2θ for all i.

(cid:80)

We ﬁrst verify that Assumptions 2 is satisﬁed:

E

(∇Fi(θ, Zi) − ∇fi(θ))2(cid:105)
(cid:104)

= E

(cid:104)

4 (Zi − EZi)2(cid:105)

= 4˜σ2 < ∞ .

Let us now ﬁnd a bound ¯τ 2 on the neighborhood heterogeneity. Using a bias-variance decomposition, we have:

H(θ) =

=

=

1
n

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:32) n
(cid:88)

E

j=1

Wij∇Fj(θ) −

(cid:33)2

∇Fj(θ)

1
n

n
(cid:88)

j=1

(cid:16) n
(cid:88)

j=1

Wij∇fj(θ) −

1
n

n
(cid:88)

j=1

(cid:17)2

∇fj(θ)

+

1
n

n
(cid:88)

(cid:16) n
(cid:88)

E

i=1

j=1

(Wij −

1
n

)(∇fj(θ) − ∇Fj(θ))

(cid:17)2

(2θ − 2θ)2 +

1
n

n
(cid:88)

n
(cid:88)

(Wij −

i=1

j=1

1
n

)2E(∇fj(θ) − ∇Fj(θ))2

= 0 + 4˜σ2 1
n

n
(cid:88)

n
(cid:88)

i=1

j=1

(Wij −

1
n

)2 ≤ 4˜σ2 .

The third equality was obtained thanks to the fact that E[∇fj(θ) − ∇Fj(θ)] = 0. This result shows that Assumption 4 is
veriﬁed with ¯τ 2 = 4˜σ2 < ∞.

On the contrary, since m can be arbitrary large, Assumption 5 is not veriﬁed. Indeed:



∇fi(θ) −

1
n

n
(cid:88)

i=1

1
n

n
(cid:88)

j=1



2

∇fj(θ)



=

=

n
(cid:88)

(2m)2

1
n

i=1
4m2
n

(cid:44) ¯ζ 2 −→
m→∞

+∞ .

Remark. At ﬁrst sight, one may wonder why the local variance term ˜σ2 appears in ¯τ 2 but not in ¯ζ 2. This is because we
chose to deﬁne neighborhood heterogeneity in expectation with respect to the pointwise loss functions F1, . . . , Fn, resulting
in a bias-variance decomposition (see Eq. 4) which is the relevant quantity to optimize when learning the topology in

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Section 5. In contrast, following the convention used in previous work, local heterogeneity is deﬁned with respect to the local
objectives f1, . . . , fn and thus only measures a bias term, while the variance term is accounted separately by Assumption 2.
Since the variance terms are the same in both settings, the difference is in how the bias term is measured (at the node level
or at the neighborhood level): in the example above, it is equal to 4m2
n for local heterogeneity while it is equal to 0 for
neighborhood heterogeneity (see the above calculation of H(θ)).

B Proof of Theorem 1

B.1 Notations and Overview

We start by re-writing the updates of D-SGD (Algorithm 1) in matrix form.

(cid:16)

(cid:17)

Let Θ(t) (cid:44)

1 , . . . , θ(t)
θ(t)
(cid:16)

n
1 ), . . . , ∇Fn(θ(t)
∇F1(θ(t)
∇F (Θ(t), Z (t)) (cid:44)
The D-SGD update at time t can then be written as:

∈ Rd×n be the matrix that contains the parameter vectors of all nodes at time t. Denote by
1 , Z (t)

∈ Rd×n the matrix containing all stochastic gradients at time t.

n , Z (t)
n )

(cid:17)

Θ(t+1) =

(cid:16)

Θ(t) − ηt∇F (Θ(t), Z (t))

(cid:17)

W (t)T .

In the following, we denote Θ

(t) (cid:44) (cid:0)¯θ(t), . . . , ¯θ(t)(cid:1) = Θ(t) · 1

n 11T.

The proof follows the classical steps found in the literature (see e.g. Koloskova et al. (2020); Neglia et al. (2020)). The
main difference resides in how the consensus term (cid:107)Θ(t) − Θ
(cid:107)2
F is controlled across iterations (Lemma 3). The proof is
organized as follows.

(t)

Convex case.

1. Lemma 1 provides a descent recursion that allows to control the decreasing of the term (cid:13)

(cid:13)¯θ(t) − θ(cid:63)(cid:13)
2
(cid:13)

. The proof closely

follows the one of Koloskova et al. (2020); Neglia et al. (2020).

2. In Lemma 3, the consensus term (cid:107)Θ(t) − Θ

(t)

(cid:107)2
F , which appears in the result of Lemma 1, is upper-bounded. The

resulting upper-bound exhibits our new quantity ¯τ 2 (an upper bound on neighborhood heterogeneity).

3. Corollary 1 uses the previous lemma to bound 1

T +1

(cid:80)T

t=0 (cid:107)Θ(t) − Θ

(t)

(cid:107)2
F .

4. Lemma 4 provides an upper-bound on the error term with the following form:

1
T + 1

T
(cid:88)

t=0

E(f (¯θ(t)) − f (cid:63)) ≤ 2

(cid:19) 1

2

(cid:18) br0
T + 1

+ 2e

1
3

(cid:18) r0

(cid:19) 2

3

T + 1

+

dr0
T + 1

,

where b = ¯σ2

n , e = 36L¯τ 2

p2

, d = 8L

p and r0 = (cid:107)θ(0) − θ(cid:63)(cid:107)2
2.

5. To get the ﬁnal rate of Theorem 1, it sufﬁces to ﬁnd T such that each term in the right-hand side of the previous

equation in bounded by ε
3 .

• 2

• 2e 1

3

2

(cid:17) 1

(cid:16) br0
T +1
(cid:16) r0
T +1

(cid:17) 2

3

≤ ε

• dr0

T +1 ≤ ε

3 ⇐⇒ 3dr0

≤ ε

3 ⇐⇒ 36br0

ε2 ≤ T + 1 ⇐⇒ 36¯σ2r0

3
1
2 r0
2 6
3
2

3 ⇐⇒ e
ε ≤ T + 1 ⇐⇒ 24Lr0

ε

pε
pε ≤ T + 1.

≤ T + 1 ⇐⇒ 6

nε2 ≤ T + 1,
√

5
2

L¯τ r0
3
2

≤ T + 1,

In particular, it sufﬁces to take

T ≥

36¯σ2r0
nε2 +

√

89

L¯τ r0

pε 3

2

+

24Lr0
pε

= O

(cid:32)

¯σ2
nε2 +

in order to have all three terms bounded by ε

3 , and obtain the ﬁnal result.

√

L¯τ
pε 3

2

(cid:33)

+

L
pε

r0 ,

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Non-convex case. The proof is similar to the convex one: it only differs in the descent lemmas that are used.

1. Lemma 2 provides the descent lemma for the non-convex scenario.

2. The consensus term is bounded using the same results as in the convex case, i.e., with Lemma 3 and Corollary 1.

3. Lemma 5 provides an upper-bound on the error term with the following form:

1
T + 1

T
(cid:88)

t=0

(cid:13)
(cid:13)
2
(cid:13)∇f (¯θ(t))
E
(cid:13)
(cid:13)
(cid:13)
2

≤ 2

(cid:19) 1

2

(cid:18) 4bf0
T + 1

+ 2e

1
3

(cid:19) 2

3

(cid:18) 4f0
T + 1

+

4df0
T + 1

,

where b = 2L¯σ2

n , e = 96L2 ¯τ 2

p2
4. We bound in each term of the previous equation by ε

, d = 8L

p and f0 = f (θ(0)) − f (cid:63).

3 and get the sufﬁcient condition:

T ≥

288L¯σ2f0
nε2

+

576L¯τ f0
pε 3

2

+

96Lf0
pε

= O

(cid:18) L¯σ2

nε2 +

L¯τ
pε 3

2

+

L
pε

(cid:19)

f0 .

B.2 Preliminaries and Useful Results

Property 1 (Averaging preservation). Let W ∈ Rn×n be a mixing matrix and Θ be any matrix in Rd×n. Then, W preserves
averaging:

Property 2 (Implications of L-smoothness and convexity).

(ΘW )

11T
n

= Θ

11T
n

= Θ

• If we assume convexity, we have for all i ∈

1, . . . , n

:

(cid:74)
(cid:104)∇fi(˜θ), ˜θ − θ(cid:105) ≥ fi(˜θ) − fi(θ).

(cid:75)

• Under Assumption 1 (L-smoothness), it holds for all i ∈

1, . . . , n

(cid:74)

:

(cid:75)

L
2
Taking the expectation of the previous equation, we also have:

Fi(θ, Z) ≤ Fi(˜θ, Z) + (cid:104)∇Fi(˜θ, Z), θ − ˜θ(cid:105) +

(cid:107)θ − ˜θ(cid:107)2
2,

∀θ, ˜θ ∈ Rd, Z ∈ θi.

fi(θ) ≤ fi(˜θ) + (cid:104)∇F (˜θ), θ − ˜θ(cid:105) +

L
2

(cid:107)θ − ˜θ(cid:107)2
2,

∀θ, ˜θ ∈ Rd.

• If we further assume that the Fi’s are convex, Assumption 1 also implies ∀θ, ˜θ ∈ Rd,Z ∈ θi:

(cid:107)∇fi(θ) − ∇fi(˜θ)(cid:107)2 ≤ L(cid:107)θ − ˜θ(cid:107)2,
(cid:107)∇fi(θ) − ∇fi(˜θ)(cid:107)2

2 ≤ 2L

(cid:107)∇Fi(θ, Z) − ∇Fi(˜θ, Z)(cid:107)2

2 ≤ 2L

fi(θ) − fi(˜θ) − (cid:10)∇fi(˜θ), θ − ˜θ(cid:11)(cid:17)
(cid:16)
Fi(θ, Z) − Fi(˜θ, Z) − (cid:10)∇Fi(˜θ, Z), θ − ˜θ(cid:11)(cid:17)
(cid:16)

,

.

These results can be found in many convex optimization books and papers, e.g. in Bubeck (2014).
Property 3 (Norm inequalities).

• For a set of vectors {ai}n

i=1 such that ai ∈ Rd,

(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

ai

(cid:13)
2
(cid:13)
(cid:13)
2

≤ n

n
(cid:88)

i=1

(cid:107)ai(cid:107)2
2 .

• For two vectors a, b ∈ Rd,

• For two vectors a, b ∈ Rd,

(cid:107)a + b(cid:107)2

2 ≤ (1 + α)(cid:107)a(cid:107)2

2 + (1 + α−1)(cid:107)b(cid:107)2
2,

∀α > 0.

2(cid:104)a, b(cid:105) ≤ α(cid:107)a(cid:107)2

2 + α−1(cid:107)b(cid:107)2
2,

∀α > 0.

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

B.3 Needed Lemmas

In the following we denote by Ft = σ(Z (k)|k ≤ t) the natural ﬁltration with respect to Z (t) = (Z (t)
that ∀i = 1, . . . , n the iterates θ(t+1)
Lemma 1 (Descent Lemma - Convex case). Consider the setting of Theorem 1 and let ηt ≤ 1

and ¯θ(t+1) are in particular Ft-measurable.

i

1 , . . . , Z (t)

n ). Remark

4L , then we almost surely have:

E

Z(t)|Ft−1

¯θ(t+1) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

¯θ(t) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

η2
t ¯σ2
n

(cid:16)

f (¯θ(t)) − f (cid:63)(cid:17)

+

− ηt

3L
2n

ηt

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

,

(20)

where E

Z(t)|Ft−1 stands for the conditional expectation E

Z(t) [·|Ft−1].

Proof. The proof closely follows the one in Koloskova et al. (2020). Using the recursion of D-SGD and since all mixing
matrices are doubly stochastic and preserve the average (Proposition 1) we have:

(cid:107)¯θ(t+1) − θ(cid:63)(cid:107)2 =

(cid:13)
(cid:13)
¯θ(t) −
(cid:13)
(cid:13)
(cid:13)

ηt
n

n
(cid:88)

i=1

∇Fi(θ(t)
i

, Z (t)

i ) − θ(cid:63)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

∇fi(θ(t)

i ) −

ηt
n

n
(cid:88)

i=1

∇Fi(θ(t)
i

, Z (t)
i )

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

=

(cid:13)
(cid:13)
¯θ(t) − θ(cid:63) −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
¯θ(t) − θ(cid:63) −
(cid:13)
(cid:13)
(cid:13)

ηt
n

ηt
n

n
(cid:88)

i=1

n
(cid:88)

i=1

∇fi(θ(t)

i ) +

ηt
n

∇fi(θ(t)
i )

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

+ η2
t

n
(cid:88)

∇fi(θ(t)

i ) −

1
n

n
(cid:88)

i=1

∇Fi(θ(t)
i

, Z (t)
i )

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:43)

(cid:42)

+ 2

¯θ(t) − θ(cid:63) −

ηt
n

n
(cid:88)

i=1

∇fi(θ(t)

i ),

ηt
n

i=1
n
(cid:88)

i=1

∇fi(θ(t)

i ) −

ηt
n

n
(cid:88)

i=1

∇Fi(θ(t)
i

, Z (t)
i )

.

Passing to the conditional expectation, the last term (the inner product) is equal to 0. This comes from the fact that
E

i ). We therefore need to bound the ﬁrst two terms in the conditional expectation.

Z(t)
i

[∇Fi(θ(t)
i

i )] = ∇fi(θ(t)
The second one can easily be bounded using Assumption 2:

, Z (t)

|Ft−1

η2
t

E

Z(t)|Ft−1

(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

∇fi(θ(t)

i ) −

1
n

n
(cid:88)

∇Fi(θ(t)
i

i=1

(cid:13)
2
, Z (t)
(cid:13)
i )
(cid:13)

=

=

η2
t
n2

η2
t
n2

(A.2)
≤

E

Z(t)|Ft−1

(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

(∇fi(θ(t)

i ) − ∇Fi(θ(t)

i

, Z (t)

i ))

(cid:13)
2
(cid:13)
(cid:13)

i=1
(cid:13)
(cid:13)∇fi(θ(t)
(cid:13)

Z(t)
i

|Ft−1

i ) − ∇Fi(θ(t)

i

(cid:13)
2
, Z (t)
(cid:13)
i )
(cid:13)

n
(cid:88)

E

i=1
η2
t ¯σ2
n

,

where the second equality was obtained using the identity E (cid:107)(cid:80)

i Yi(cid:107)2

2 = (cid:80)

i

E(cid:107)Yi(cid:107)2

Now that the second term is bounded, we can move to the ﬁrst one. Because θ(t)

i

2 when Yi are independent and EYi = 0.
and ¯θ(t) are Ft−1-measurable, we have

E

Z(t)|Ft−1

(cid:13)
(cid:13)
¯θ(t) − θ(cid:63) −
(cid:13)
(cid:13)
(cid:13)

ηt
n

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
∇fi(θ(t)
(cid:13)
i )
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
¯θ(t) − θ(cid:63) −
(cid:13)
(cid:13)
(cid:13)

ηt
n

n
(cid:88)

i=1

∇fi(θ(t)
i )

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

¯θ(t) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ η2
t

1
n

n
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:13)
2
(cid:13)
∇fi(θ(t)
(cid:13)
i )
(cid:13)
(cid:13)
(cid:125)

(cid:123)(cid:122)
T1

(cid:42)

− 2ηt

¯θ(t) − θ(cid:63),

(cid:124)

n
(cid:88)

i=1

1
n
(cid:123)(cid:122)
T2

(cid:43)

∇fi(θ(t)
i )

.

(cid:125)

In order to bound T1, recall that by deﬁnition 1
n

(cid:80)

i ∇fi(θ(cid:63)) = 0, therefore:

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

(∇fi(θ(t)

(cid:13)
2
(cid:13)
i ) − ∇fi(¯θ(t)) + ∇fi(¯θ(t)) − ∇fi(θ(cid:63)))
(cid:13)
(cid:13)
(cid:13)

(∇fi(θ(t)

i ) − ∇fi(¯θ(t)))

+ 2

i=1
(cid:13)
(cid:13)∇fi(θ(t)
(cid:13)

(cid:13)
2
i ) − ∇fi(¯θ(t))
(cid:13)
(cid:13)

+

2
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

(∇fi(¯θ(t)) − ∇fi(θ(cid:63)))

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)∇fi(¯θ(t)) − ∇fi(θ(cid:63))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
n
(cid:88)

i=1

T1 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

(17)
≤ 2

n
(cid:88)

i=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

n
(cid:88)

(17)
≤

2
n

(14)(15)
≤

i=1
2L2
n

n
(cid:88)

i=1

(cid:13)
(cid:13)θ(t)
(cid:13)

i − ¯θ(t)(cid:13)
2
(cid:13)
(cid:13)

+

4L
n

n
(cid:88)

(cid:16)

i=1

fi(¯θ(t)) − fi(θ(cid:63)) −

∇fi(θ(cid:63)), ¯θ(t) − θ(cid:63)(cid:69)(cid:17)
(cid:68)

=

2L2
n

n
(cid:88)

i=1

(cid:13)
(cid:13)θ(t)
(cid:13)

i − ¯θ(t)(cid:13)
2
(cid:13)
(cid:13)

+

4L
n

n
(cid:88)

(cid:16)

i=1

fi(¯θ(t)) − fi(θ(cid:63))

(cid:17)

− 4L

n
(cid:88)

i=1

(cid:42)

1
n
(cid:124)

∇fi(θ(cid:63))

, ¯θ(t) − θ(cid:63)

(cid:43)

(cid:123)(cid:122)
=0

(cid:125)

=

2L2
n

n
(cid:88)

i=1

(cid:13)
(cid:13)θ(t)
(cid:13)

i − ¯θ(t)(cid:13)
2
(cid:13)
(cid:13)

+ 4L

(cid:16)

f (¯θ(t)) − f (cid:63)(cid:17)

.

Finally, we have to bound T2:

−T2 = −

= −

2ηt
n

2ηt
n

n
(cid:88)

i=1
n
(cid:88)

(cid:68)¯θ(t) − θ(cid:63), ∇fi(θ(t)
i )

(cid:69)

(cid:104)(cid:68)¯θ(t) − θ(t)

i

, ∇fi(θ(t)
i )

(cid:69)

(cid:68)
i − θ(cid:63), ∇fi(θ(t)
θ(t)
i )

(cid:69)(cid:105)

+

(13)(11)

≤ −

i=1
2ηt
n

n
(cid:88)

i=1

(cid:20)
fi(¯θ(t)) − fi(θ(t)

i ) −

L
2

(cid:107)¯θ(t) − θ(t)

i (cid:107)2

2 + fi(θ(t)

(cid:21)
i ) − fi(θ(cid:63))

(cid:16)

= −2ηt

f (¯θ(t)) − f (θ(cid:63))

(cid:17)

+

Lηt
n

n
(cid:88)

i=1

(cid:107)¯θ(t) − θ(t)

i (cid:107)2
2

= −2ηt

(cid:16)

f (¯θ(t)) − f (cid:63)(cid:17)

+

Lηt
n

(t)

(cid:107)Θ

− Θ(t)(cid:107)2
F .

Combining all previous results, we get:

E

Z(t)|Ft−1

¯θ(t+1) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

¯θ(t) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

η2
t ¯σ2
n

+

Lηt
n

(2Lηt + 1) (cid:107)Θ
(cid:16)

+ 2ηt (2Lηt − 1)

(t)

− Θ(t)(cid:107)2
F
f (¯θ(t)) − f (cid:63)(cid:17)

.

Since, by hypothesis, ηt ≤ 1

4L , we have 2Lηt + 1 ≤ 3

2 and 2Lηt − 1 ≤ − 1

2 , which concludes the proof.

Lemma 2 (Descent Lemma - Non-convex case). Consider the setting of Theorem 1 and let ηt ≤ 1
have:

4L , then we almost surely

Z(t)|Ft−1 f (¯θ(t+1)) − f (cid:63) ≤ f (¯θ(t)) − f (cid:63) −
E

ηt
4

(cid:107)∇f (¯θ(t))(cid:107)2

2 +

L2
n

ηt

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

+

L¯σ2
2n

η2
t .

(21)

Proof. The proof adapts the one of Lemma 10 in Koloskova et al. (2020) to our setting.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Z(t)|Ft−1 f (¯θ(t+1)) = E
E

Z(t)|Ft−1f

(cid:16)¯θ(t) −

ηt
n

n
(cid:88)

i=1

∇Fi(θ(t)
i

(cid:17)
, Z (t)
i )

(13)
≤ f (¯θ(t)) − E

Z(t)|Ft−1

(cid:68)
∇f (¯θ(t)),

ηt
n

n
(cid:88)

i=1

∇Fi(θ(t)
i

, Z (t)
i )

(cid:69)

= f (¯θ(t)) −

(cid:68)
∇f (¯θ(t)),

(cid:124)

+

L
2

Lη2
t
2

E

Z(t)|Ft−1

(cid:13)
(cid:13)
(cid:13)

ηt
n

E

Z(t)|Ft−1

(cid:13)
(cid:13)
(cid:13)

1
n

(cid:69)

∇fi(θ(t)
i )

+

(cid:125)

(cid:124)

n
(cid:88)

i=1
n
(cid:88)

∇Fi(θ(t)
i

, Z (t)
i )

∇Fi(θ(t)
i

, Z (t)
i )

i=1

(cid:123)(cid:122)
(cid:44)T5

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
2
(cid:13)
(cid:13)
2
(cid:125)

n
(cid:88)

i=1

ηt
n
(cid:123)(cid:122)
(cid:44)T4

Adding and subtracting ηt∇f (¯θ(t)) in T4, we have

T4 = −ηt

(cid:13)
(cid:13)
2
(cid:13)∇f (¯θ(t))
(cid:13)
(cid:13)
(cid:13)
2

+

ηt
n

(19),α=1

≤ −ηt

(cid:13)
(cid:13)∇f (¯θ(t))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

n
(cid:88)

(cid:68)
∇f (¯θ(t)), ∇fi(¯θ(t)) − ∇fi(θ(t)
i )

(cid:69)

i=1
ηt
2

+

(cid:13)
(cid:13)∇f (¯θ(t))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

+

ηt
2n

n
(cid:88)

i=1

(cid:13)
(cid:13)
2
(cid:13)∇fi(¯θ(t)) − ∇fi(θ(t)
(cid:13)
(cid:13)
i )
(cid:13)
2

(14)
≤ −

ηt
2

(cid:13)
(cid:13)
2
(cid:13)∇f (¯θ(t))
(cid:13)
(cid:13)
(cid:13)
2

+

L2ηt
2n

n
(cid:88)

i=1

(cid:13)
¯θ(t) − θ(t)
(cid:13)
(cid:13)

i

(cid:13)
2
(cid:13)
(cid:13)
2

.

Let us now bound the term T5:

T5 = E

Z(t)|Ft−1

(cid:13)
(cid:13)
(cid:13)

1
n

=

1
n2

E

Z(t)|Ft−1

i=1
n
(cid:13)
(cid:88)
(cid:13)
(cid:13)

i=1

n
(cid:88)

∇Fi(θ(t)
i

, Z (t)

i ) −

n
(cid:88)

∇fi(θ(t)

i ) +

1
n

i=1
n
(cid:88)

i=1

1
n

n
(cid:88)

i=1

∇fi(θ(t)
i )

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
2
(cid:13)
(cid:13)
2

+

(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

∇fi(θ(t)
i )

(cid:13)
2
(cid:13)
(cid:13)
2

∇Fi(θ(t)
i

, Z (t)

i ) −

∇fi(θ(t)
i )

(A.2)
=

¯σ2
n

+

(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

∇fi(θ(t)

i ) − ∇f (¯θ(t)) + ∇f (¯θ(t))

(cid:13)
2
(cid:13)
(cid:13)
2

i=1
n
(cid:88)

∇fi(θ(t)

i ) − ∇f (¯θ(t))

+ 2

(cid:13)
(cid:13)
(cid:13)

1
n

(cid:13)
2
(cid:13)
(cid:13)
2

+ 2

(cid:13)
(cid:13)
2
(cid:13)∇f (¯θ(t))
(cid:13)
(cid:13)
(cid:13)
2

i=1
(cid:13)
(cid:13)∇fi(θ(t)
(cid:13)

2
n

n
(cid:88)

i=1

i ) − ∇fi(¯θ(t))

(cid:13)
2
(cid:13)
(cid:13)
2

+ 2

(cid:13)
(cid:13)∇f (¯θ(t))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

2L2
n

n
(cid:88)

i=1

(cid:13)
(cid:13)θ(t)
(cid:13)

i − ¯θ(t)(cid:13)
2
(cid:13)
(cid:13)
2

+ 2

(cid:13)
(cid:13)∇f (¯θ(t))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

.

(17)
≤

(17)
≤

(14)
≤

¯σ2
n

¯σ2
n

¯σ2
n

+

+

Next, plugging T4 and T5 into the ﬁrst inequality, we have:

Z(t)|Ft−1f (¯θ(t+1))
E
(cid:16) 1
2

≤ f (¯θ(t)) − ηt

− Lηt

(cid:13)
(cid:17)(cid:13)
2
(cid:13)∇f (¯θ(t))
(cid:13)
(cid:13)
(cid:13)
2

Since by hypothesis ηt ≤ 1

4L , we have 1

2 − Lηt ≥ 1

4 and L2ηt

n , we therefore get

+

+

(cid:16) L2ηt
2n
2n + L3η2

L3η2
t
n
n ≤ L2ηt

t

(cid:17) (cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

+

L¯σ2
2n

η2
t .

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Z(t)|Ft−1f (¯θ(t+1)) ≤ f (¯θ(t)) −
E

ηt
4

(cid:13)
(cid:13)
2
(cid:13)∇f (¯θ(t))
(cid:13)
(cid:13)
(cid:13)
2

+

L2ηt
n

Subtracting each side of the equation by f (cid:63), we obtain the ﬁnal result.

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

+

L¯σ2
2n

η2
t .

Lemma 3 (Consensus Control). Consider the setting of Theorem 1 and let ηt ≤ p

8L , then:

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

≤ (1 −

p
4

)E

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

+

6n¯τ 2
p

η2
t−1.

(22)

Proof. For any α > 0, we have:

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= E

(cid:16)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

= E

(cid:13)
(cid:13)
Θ(t)
(cid:13)
(cid:13)

(cid:18)

I −

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

Θ(t−1) − ηt−1∇F (Θ(t−1), Z (t−1))

(cid:17)

W (t−1)T

(cid:18)

I −

(10)
= E

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Θ(t−1) − ηt−1∇F (Θ(t−1), Z (t−1))

(cid:17) (cid:18)

W (t−1)T −

(18)
≤ (1 + α)E

(cid:13)
(cid:13)
Θ(t−1)
(cid:13)
(cid:13)

(cid:18)

W (t−1)T −

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

+(1 + α−1)η2

t−1

E

(cid:124)

(cid:13)
(cid:13)
∇F (Θ(t−1), Z (t−1))
(cid:13)
(cid:13)

(cid:18)

W (t−1)T −

(cid:123)(cid:122)
T3

11T
n

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F
(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F
(cid:125)

(A.3)
≤ (1 + α)(1 − p)E

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

+ (1 + α−1)η2

t−1T3.

We now bound T3 by relying on Assumption 4:

(cid:13)
(cid:13)
T3 = E
(cid:13)
(cid:13)
(cid:13)

(cid:16)

∇F (Θ(t−1), Z (t−1)) − ∇F (Θ

(t−1)

, Z (t−1)) + ∇F (Θ

(t−1)

(cid:17)

, Z (t−1))

·

(cid:18)

·

W (t−1)T −

11T
n

(cid:19) (cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(17)
≤ 2E

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇F (Θ(t−1), Z (t−1)) − ∇F (Θ

(t−1)

(cid:17) (cid:18)

, Z (t−1))

W (t−1)T −

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

11T
n
(cid:18)

+2E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇F (Θ

(t−1)

, Z (t−1))

W (t−1)T −

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

= 2E

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇F (Θ(t−1), Z (t−1)) − ∇F (Θ

(t−1)

(cid:17) (cid:18)

, Z (t−1))

W (t−1)T −

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

n
(cid:88)

E

+2

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

j=1

W (t−1)

ij ∇Fj(¯θ(t−1), Z (t−1)

j

) −

1
n

n
(cid:88)

j=1

∇Fj(¯θ(t−1), Z (t−1)

j

)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(3)
≤ 2E

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇F (Θ(t−1), Z (t−1)) − ∇F (Θ

(t−1)

(cid:17) (cid:18)

, Z (t−1))

W (t−1)T −

11T
n

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

+ 2n¯τ 2.

For conciseness, we will denote Fi(θ(t−1)

i

, Z (t−1)
j

) by Fi(θ(t−1)

i

) and ∇F (Θ, Z (t−1)) by ∇F (Θ). Using Assumption 3,

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

we can bound the ﬁrst term of the previous equation by:

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:34)

E

2(1 − p)E

(17)
≤ 4(1 − p)

= 4(1 − p)×

∇F (Θ(t−1)) − ∇F (Θ

(t−1)

(cid:17)
)

−

(cid:18)

∇F (Θ(t−1)) − ∇F (Θ

(t−1)

(cid:19)(cid:13)
2
(cid:13)
)
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)∇F (Θ(t−1)) − ∇F (Θ
(cid:13)

(t−1)

(cid:13)
2
(cid:13)
)
(cid:13)

F

+ E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇F (Θ(t−1)) − ∇F (Θ

(t−1)

(cid:13)
2
(cid:13)
)
(cid:13)
(cid:13)

F

(cid:35)





n
(cid:88)

i=1

(cid:32)

E

(cid:13)
(cid:13)∇Fi(θ(t−1)
(cid:13)

i

(cid:13)
2
) − ∇Fi(¯θ(t−1))
(cid:13)
(cid:13)
2

(cid:13)
+ E
(cid:13)
(cid:13)

1
n

n
(cid:88)

(cid:16)

j=1

∇Fj(θ(t−1)
j

) − ∇Fj(¯θ(t−1))

(cid:33)


(cid:17) (cid:13)
2
(cid:13)
(cid:13)
2

×

(A.1)

≤ 4(1 − p)


L2

n
(cid:88)

i=1

E

(cid:13)
(cid:13)θ(t−1)
(cid:13)

i

− ¯θ(t−1)(cid:13)
2
(cid:13)
(cid:13)
2

+

n
n2

E

(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

(cid:16)

j=1

∇Fj(θ(t−1)
j

) − ∇Fj(¯θ(t−1))





(cid:17) (cid:13)
2
(cid:13)
(cid:13)
2

(17)
≤ 4(1 − p)


L2

n
(cid:88)

i=1

E

(cid:13)
(cid:13)θ(t−1)
(cid:13)

i

− ¯θ(t−1)(cid:13)
2
(cid:13)
(cid:13)
2

+

n
(cid:88)

j=1

E

(cid:13)
(cid:13)∇Fj(θ(t−1)
(cid:13)

j

(cid:13)
2
) − ∇Fj(¯θ(t−1))
(cid:13)
(cid:13)
2





(A.1)

≤ 4(1 − p)


L2

n
(cid:88)

i=1

E

(cid:13)
(cid:13)θ(t−1)
(cid:13)

i

− ¯θ(t−1)(cid:13)
2
(cid:13)
(cid:13)
2

+ L2

n
(cid:88)

j=1

E

(cid:13)
(cid:13)θ(t−1)
(cid:13)

j

− ¯θ(t−1)(cid:13)
2
(cid:13)
(cid:13)
2





= 8(1 − p)L2E

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

.

Combining all previous results and setting α = p

2 , we get:

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

≤ (1 + α)(1 − p)E

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

+ 2(1 + α−1)η2

t−1n¯τ 2

+ 8(1 + α−1)(1 − p)L2η2

t−1

E

)(1 − p)

E

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)
(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

≤ (1 +

(cid:124)

p
2
(cid:123)(cid:122)
≤1− p
2

(cid:125)

2
p
(cid:123)(cid:122)
≤ 16
p

+ 8(1 +

(cid:124)

)(1 − p)

L2η2

t−1

E

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(t−1)(cid:13)
2
(cid:13)
(cid:13)

F

+ 2(1 +

2
p

)

t−1n¯τ 2.
η2

(cid:125)

(cid:124)

(cid:123)(cid:122)
≤ 6
p

(cid:125)

Since by hypothesis we have ηt−1 ≤ p

8L , we can bound the second term and get:

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

(cid:16)

(cid:16)

≤

=

1 −

1 −

p
2
p
4

+

(cid:17)

E

(cid:17)

E

p
4
(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)

(cid:13)
(cid:13)Θ(t−1) − Θ
(cid:13)
(t−1)(cid:13)
2
(cid:13)
(cid:13)

(t−1)(cid:13)
2
(cid:13)
(cid:13)

+

F

+

F
6n¯τ 2
p

6n¯τ 2
p

η2
t−1

η2
t−1.

Corollary 1 (Consensus recursion). Consider the setting of Theorem 1 and ﬁx ηt = η ≤ p

8L , we have:

1
T + 1

T
(cid:88)

E

t=0

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

≤

24η2n¯τ 2
p2

.

(23)

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Proof. Unrolling the expression (22) in Lemma 3 up to t = 0, we have for all t > 0:

+

6n¯τ 2
p

η2

t−1
(cid:88)

(cid:16)

j=0

1 −

(cid:17)j

p
4

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

(cid:16)

≤

1 −

p
4

(0)(cid:13)
2
(cid:13)
(cid:13)

F
(cid:125)

(cid:17)t (cid:13)
(cid:13)Θ(0) − Θ
(cid:13)
(cid:124)
(cid:123)(cid:122)
=0
1 − (cid:0)1 − p
1 − (cid:0)1 − p
4
p

×

4

4

(cid:1)t
(cid:1)

=

≤

=

η2 ×

6n¯τ 2
p
6η2n¯τ 2
p
24η2n¯τ 2
p2

Summing and dividing by T + 1, we get the ﬁnal result.

Lemma 4 (Convergence rate with T - Convex case). Consider the setting of Theorem 1 in the convex case. There exists a
constant stepsize η ≤ ηmax = p

8L such that
T
(cid:88)

1
T + 1

t=0

E(f (¯θ(t)) − f (cid:63)) ≤ 2

(cid:19) 1

2

(cid:18) br0
T + 1

+ 2e

1
3

(cid:18) r0

(cid:19) 2

3

T + 1

+

dr0
T + 1

,

(24)

where b = ¯σ2

n , e = 36L¯τ 2

p2

, d = 8L

p and r0 = (cid:107)θ(0) − θ(cid:63)(cid:107)2
2.

Proof. Thanks to the descent lemma (Lemma 1), we almost surely have:

f (¯θ(t)) − f (cid:63) ≤

(cid:16) (cid:13)
¯θ(t) − θ(cid:63)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
η

− E

Z(t)|Ft−1

(cid:13)
¯θ(t+1) − θ(cid:63)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

η2 ¯σ2
n

+

3L
2n

η

where all terms are Ft−1-measurable. Therefore,

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

(cid:17)

,

E(f (¯θ(t)) − f (cid:63)) ≤

(cid:16)

E

¯θ(t) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
η

− E

¯θ(t+1) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

η2 ¯σ2
n

+

3L
2n

ηE

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

(cid:17)

,

and summing up we get:

1
T + 1

T
(cid:88)

t=0

E(f (¯θ(t)) − f (cid:63))

1
η(T + 1)

T
(cid:88)

(cid:18)

E

t=0

¯θ(t) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

− E

¯θ(t+1) − θ(cid:63)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

η2 ¯σ2
n

+

3L
2n

ηE

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

(cid:19)

≤

≤

1
η(T + 1)

(23)
≤

1
η(T + 1)

(cid:13)
(cid:13)

2
(cid:13)
(cid:13)

(cid:13)θ(0) − θ(cid:63)(cid:13)
(cid:13)θ(0) − θ(cid:63)(cid:13)
(cid:16) r0

(cid:17) 1

(cid:13)
(cid:13)

3

e(T +1)

+

η¯σ2
n

+

3L
2n

1
T + 1

T
(cid:88)

t=0

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

2
(cid:13)
(cid:13)

+

¯σ2
n

η +

36L¯τ 2
p2

η2.

Fixing η = min

(cid:26)(cid:16) r0

b(T +1)

(cid:17) 1

2

,

(cid:27)

with b = ¯σ2

n , e = 36L¯τ 2

p2

, d = 8L

p and r0 = (cid:107)θ(0) − θ(cid:63)(cid:107)2

2, then applying

, 1
d

Lemma 6 that is recalled after, we obtain the ﬁnal result.

Lemma 5 (Convergence rate with T - Non convex case). Consider the setting of Theorem 1 in the non-convex case. There
exists a constant stepsize η ≤ ηmax = p

8L such that

1
T + 1

T
(cid:88)

t=0

(cid:13)
(cid:13)∇f (¯θ(t))
E
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

≤ 2

(cid:19) 1

2

(cid:18) 4bf0
T + 1

+ 2e

1
3

(cid:19) 2

3

(cid:18) 4f0
T + 1

+

4df0
T + 1

,

(25)

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

where b = 2L¯σ2

n , e = 96L2 ¯τ 2

p2

, d = 8L

p and f0 = f (θ(0)) − f (cid:63).

Proof. Similarly to Lemma 4 for the convex case, we can use the descent Lemma 2 and obtain

(cid:13)
(cid:13)
2
(cid:13)∇f (¯θ(t))
E
(cid:13)
(cid:13)
(cid:13)
2

≤

(cid:16)

4
η

Eft − Eft+1 +

L2η
n

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

+

L¯σ2
2n

η2(cid:17)

,

where for all t ≥ 0, ft (cid:44) f (¯θ(t)) − f (cid:63). Then summing up and dividing by T + 1 we get:

1
T + 1

T
(cid:88)

t=0

(cid:13)
(cid:13)∇f (¯θ(t))
E
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

≤

4f0
η(T + 1)

+

4L2
n

1
T + 1

T
(cid:88)

E

(cid:13)
(cid:13)Θ(t) − Θ
(cid:13)

(t)(cid:13)
2
(cid:13)
(cid:13)

F

+

2L¯σ2
n

η

(23)
≤

=

4f0
η(T + 1)
4f0
η(T + 1)

+

+

4L2
n
96L2 ¯τ 2
p2

t=0
24η2n¯τ 2
p2

+

2L¯σ2
n

η

η2 +

2L¯σ2
n

η .

Fixing η = min

(cid:26)(cid:16) 4f0

b(T +1)

(cid:17) 1

2

,

(cid:16) 4f0

e(T +1)

(cid:17) 1

3

, 1
d

(cid:27)

Lemma 6 and obtain the ﬁnal result.

with b = 2L¯σ2

n , e = 96L2 ¯τ 2

p2

, d = 8L

p and f0 = f (θ(0)) − f (cid:63), we can apply

Lemma 6 (Tuning stepsize (Koloskova et al., 2020)). For any parameter r0, b, e, d ≥ 0, T ∈ N, we can ﬁx

η = min

(cid:40)(cid:18)

r0
b(T + 1)

(cid:19) 1

2

(cid:18)

,

r0
e(T + 1)

(cid:19) 1

3

(cid:41)

,

1
d

≤

1
d

,

and get

r0
η(T + 1)

+ bη + eη2 ≤ 2

(cid:19) 1

2

(cid:18) br0
T + 1

+ 2e

1
3

(cid:18) r0

(cid:19) 2

3

T + 1

+

dr0
T + 1

.

Proof. The proof of this lemma can be found in the supplementary materials of Koloskova et al. (2020) (Lemma 15).

C Additional Results and Proofs

Proposition 1. Let Assumptions 2-3 and 5 to be veriﬁed. Then Assumption 4 is satisﬁed with ¯τ 2 = (1 − p) (cid:0)¯ζ 2 + ¯σ2(cid:1),
where ¯σ2 (cid:44) 1
n

i σ2
i .

(cid:80)

Proof. Denoting ∇F (θ) = (∇F1(θ, Z1), . . . , ∇Fn(θ, Zn)) ∈ Rd×n, and using the relation

E(cid:107)Y (cid:107)2

2 = (cid:107)EY (cid:107)2

2 + E(cid:107)Y − EY (cid:107)2
2,

(26)

we have:

H (t) =

E(cid:107)∇F (θ)W (t) − ∇F (θ)(cid:107)2
F

1
n
1 − p
n
1 − p
n

(A.3)
≤

=

E(cid:107)∇F (θ) − ∇F (θ)(cid:107)2
F

n
(cid:88)

i=1

(cid:13)
E
(cid:13)
(cid:13)∇Fi(θ, Zi) −

1
n

n
(cid:88)

j=1

∇Fj(θ, Zj)

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:32)

n
(cid:88)

i=1
(cid:32)

(26)=

1 − p
n

(cid:13)
(cid:13)
(cid:13)∇fi(θ) −

1
n

(A.5)

≤ (1 − p)

¯ζ 2 +

1
n

n
(cid:88)

i=1

E

(cid:13)
(cid:13)
(cid:13)

∇fj(θ)

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
+ E
(cid:13)
(cid:13)

n
(cid:88)

j=1

(cid:0)1{j=i} −

1
n

)(cid:0)∇Fj(θ, Zj) − ∇fj(θ))

(cid:33)

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:0)1{j=i} −

1
n

(cid:1)(∇Fj(θ, Zj) − ∇fj(θ))

(cid:33)
.

(cid:13)
2
(cid:13)
(cid:13)
2

n
(cid:88)

j=1

n
(cid:88)

j=1

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Since all terms j in the norm are independent and with expectation 0, the expectation of the sum is equal to the sum of
expectations and

H (t) ≤ (1 − p)


¯ζ 2 +

(cid:32)

= (1 − p)

¯ζ 2 +

1
n

n
(cid:88)

n
(cid:88)

i=1

j=1

(cid:0)1{j=i} −

1
n

(cid:1)2E (cid:107)∇Fj(θ, Zj) − ∇fj(θ)(cid:107)2

2

1
n

n
(cid:88)

j=1

E (cid:107)∇Fj(θ, Zj) − ∇fj(θ)(cid:107)2
2

n
(cid:88)

i=1
(cid:124)

(cid:0)1{j=i} −

(cid:123)(cid:122)
= n−1
n

(cid:1)2

1
n

(cid:125)





(cid:33)

(A.2)

≤ (1 − p)

(cid:18)

¯ζ 2 +

(cid:19)

n − 1
n

¯σ2

≤ (1 − p) (cid:0)¯ζ 2 + ¯σ2(cid:1) ,

which concludes the proof.

(Bounded neighborhood heterogeneity under label skew) Consider the statistical framework de-
Proposition 2.
ﬁned above and assume there exists B > 0 such that ∀k = 1, . . . , K and ∀θ ∈ Rd, (cid:107)EX [∇F (θ; X, Y )|Y =
k] − 1
K

2 ≤ B. Then, denoting πjk (cid:44) Pj(Y = k), Assumption 4 is satisﬁed with:

EX [∇F (θ; X, Y )|Y = k(cid:48)](cid:107)2

(cid:80)K

k(cid:48)=1

¯τ 2 =

KB
n

K
(cid:88)

n
(cid:88)

(cid:16) n
(cid:88)

k=1

i=1

j=1

Wijπjk −

1
n

n
(cid:88)

j=1

(cid:17)2

+

πjk

σ2
max
n

(cid:107)W −

1
n

11T(cid:107)2

F .

Proof. First, observe that the local objective functions can be re-written

fj(θ) = E(X,Y )∼Dj [F (θ; X, Y )]

=

=

K
(cid:88)

k=1

K
(cid:88)

k=1

Pj(Y = k)EX [F (θ; X, Y )|Y = k]

πjkEX [F (θ; X, Y )|Y = k] .

From (4), we have the bias-variance decomposition

H(θ) ≤

=

=

1
n

1
n

1
n

n
(cid:88)

i=1

n
(cid:88)

i=1

n
(cid:88)

i=1

n
(cid:88)

j=1

n
(cid:88)

j=1

n
(cid:88)

j=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

Wij∇fj(θ) − ∇f (θ)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

+

σ2
max
n

(cid:107)W −

1
n

11T(cid:107)2
F

(Wij −

1
n

)∇fj(θ)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

+

σ2
max
n

(cid:107)W −

1
n

11T(cid:107)2
F

(Wij −

1
n

)

K
(cid:88)

k=1

(cid:13)
2
(cid:13)
(cid:13)
πjkEX [∇F (θ; X, Y )|Y = k]
(cid:13)
(cid:13)
(cid:13)
2
(cid:125)

(cid:123)(cid:122)
T4

+

σ2
max
n

(cid:107)W −

1
n

11T(cid:107)2

F .

Then, observing that (cid:80)n

j=1(Wij − 1

n ) = 0 and (cid:80)K

k=1 πjk = 1 imply

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

n
(cid:88)

j=1

(Wij −

1
n

)

K
(cid:88)

k=1

πjk

1
K

n
(cid:88)

k(cid:48)=1

EX [∇F (θ; X, Y )|Y = k(cid:48)] = 0 ,

we can add this in the norm of the term T4 deﬁned above and get

T4 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

(Wij −

j=1

1
n

)

K
(cid:88)

k=1

(cid:16)

πjk

EX [∇F (θ; X, Y )|Y = k] −

1
K

K
(cid:88)

k(cid:48)=1

(cid:17)
EX [∇F (θ; X, Y )|Y = k(cid:48)]

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

(cid:16)

k=1

EX [∇F (θ; X, Y )|Y = k] −

1
K

K
(cid:88)

k(cid:48)=1

(17)
≤ K

= K

(cid:16)

K
(cid:88)

EX [∇F (θ; X, Y )|Y = k] −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
k=1
(cid:13)
(cid:13)
EX [∇F (θ; X, Y )|Y = k] −
(cid:13)
(cid:13)
(cid:13)
(cid:124)

1
K

K
(cid:88)

k=1

1
n

)πjk

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

EX [∇F (θ; X, Y )|Y = k(cid:48)]

(Wij −

(cid:17) n
(cid:88)

EX [∇F (θ; X, Y )|Y = k(cid:48)]

j=1
(cid:17) n
(cid:88)

j=1

1
K

K
(cid:88)

k(cid:48)=1

(Wij −

1
n

)πjk

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

K
(cid:88)

k(cid:48)=1
(cid:123)(cid:122)
≤B

(cid:13)
2
(cid:13)
EX [∇F (θ; X, Y )|Y = k(cid:48)]
(cid:13)
(cid:13)
(cid:13)
2
(cid:125)

(cid:16) n
(cid:88)

×

j=1

(Wij −

(cid:17)2

)πjk

1
n

≤ KB

K
(cid:88)

(cid:32) n
(cid:88)

k=1

j=1

Wijπjk −

(cid:33)2

πjk

.

1
n

n
(cid:88)

j=1

Finally, plugging this into the upper-bound on H(θ) found above, we get the ﬁnal result.

Theorem 2 Consider the statistical setup presented in Section 5.1 and let {(cid:99)W (l)}L
generated by Algorithm 2. Then, at any iteration l = 1, . . . , L, we have:

l=1 be the sequence of mixing matrices

g((cid:99)W (l)) ≤ 16
l+2

(cid:0)λ + 1

n

(cid:13)
(cid:13)

(cid:80)K

k=1(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:63)
(cid:13)
2

(cid:1) ,

where (cid:107)·(cid:107)(cid:63)
can obtain the looser bound

2 stands for the nuclear norm, i.e., the sum of singular values. Bounding the second term in the parenthesis, we

g((cid:99)W (l)) ≤ 16

l+2 (λ + 1) .

Furthermore, we have din

max((cid:99)W (l)) ≤ l and dout

max((cid:99)W (l)) ≤ l, resulting in a per-iteration complexity bounded by l.

Proof. The proof of this theorem is directly derived from Theorem 3 given below, applied with the parameters of our
problem. To prove the ﬁrst inequality, we ﬁrst need to ﬁnd a bound on the diameter of the set of doubly stochastic matrices,
denoted diam(cid:107)·(cid:107)(S), for a certain (matrix) norm (cid:107) · (cid:107). We ﬁx this norm to be the operator norm induced by the (cid:96)2-norm,
denoted (cid:107)·(cid:107)2, which is simply the maximum singular value of the matrix.
For all W, P ∈ S, we have

(cid:107)W − P (cid:107)2 ≤ (cid:107)W (cid:107)2 + (cid:107)P (cid:107)2

= 1 + 1 = 2 ,

which comes from the fact that W and P are doubly stochastic, i.e., their largest eigenvalue is 1. This shows that
diam(cid:107)·(cid:107)2(S) ≤ 2.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Let us now ﬁnd the Lipschitz constant associated to the gradient of the objective:

∇g(W ) =

2
n

K
(cid:88)

(W Π:,k − Π:,k1) · ΠT

:,k +

k=1

(cid:18)

λ

W −

2
n

(cid:19)

.

11T
n

Recall that the dual norm (cid:107)·(cid:107)(cid:63)

1 of (cid:107)·(cid:107)1 is the nuclear norm, i.e., the sum of the singular values.

For any W, P ∈ S, we have

(cid:107)∇g(W ) − ∇g(P )(cid:107)(cid:63)

2 =

≤

2
n

2
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(W − P )

(cid:32)

λI +

K
(cid:88)

Π:,kΠT
:,k

(cid:33)(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:107)λ(W − P )I(cid:107)(cid:63)

2 +

(W − P )

k=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
n

K
(cid:88)

k=1

Π:,kΠT
:,k

≤

2λ
n

(cid:107)W − P (cid:107)2 (cid:107)I(cid:107)(cid:63)

2 +

2
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(W − P )

K
(cid:88)

k=1

Π:,kΠT
:,k

,

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

where the last inequality is obtained using the fact that for any real matrices A and B, (cid:107)AB(cid:107)(cid:63) ≤ (cid:107)AT(cid:107)(cid:107)B(cid:107)(cid:63).

Before bounding the second term, we must observe that because W and P are doubly stochastic, (W − P )1 = 0 and
therefore, for any matrix A ∈ Rn×n, (W − P )A = (W − P )(A − 11T

n A).

Now, the second term can be re-written and bounded as follows:

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(W − P )

2
n

K
(cid:88)

k=1

Π:,kΠT
:,k

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(W − P )

(cid:107)W − P (cid:107)2

(cid:107)W − P (cid:107)2

=

≤

=

2
n

2
n

2
n

(cid:32) K
(cid:88)

Π:,kΠT

:,k −

k=1

K
(cid:88)

Π:,kΠT

:,k −

11T
n

11T
n

K
(cid:88)

k=1

K
(cid:88)

k=1

Π:,kΠT
:,k

Π:,kΠT
:,k

(cid:33)(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

K
(cid:88)

(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

k=1

Plugging the previous result into the bound obtained above, and since (cid:107)I(cid:107)(cid:63)

2 = n, we get

(cid:107)∇g(W ) − ∇g(P )(cid:107)(cid:63)

2 ≤ 2

λ +

(cid:32)

1
n

K
(cid:88)

(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

(cid:33)

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:107)W − P (cid:107)2 .

We can now apply Theorem 3 with the found Lipschitz constant and diameter, which gives:

g((cid:99)W (l)) − g(W (cid:63)) ≤

16
l + 2

(cid:32)

λ +

1
n

K
(cid:88)

(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

(cid:33)

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

,

where W (cid:63) is the optimal solution of the problem. Since we known that W (cid:63) = 11T
inequality in Theorem 2.

n with g(W (cid:63)) = 0, we obtain the ﬁrst

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

To prove the second inequality, it sufﬁces to show that

(cid:80)K

(cid:13)
(cid:13)
(cid:13)

k=1(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:63)
(cid:13)
(cid:13)
2

≤ n. We have:

K
(cid:88)

(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

=

≤

=

≤

(cid:19) K
(cid:88)

Π:,kΠT
:,k

Π:,kΠT
:,k

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

I −

11T
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)

I −

11T
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

Π:,kΠT
:,k

K
(cid:88)

k=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
k=1
(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

K
(cid:88)

k=1

(cid:13)
(cid:13)Π:,kΠT
:,k

(cid:13)
(cid:63)
(cid:13)
2

.

Because for any k = 1, . . . , K, Π:,kΠT

:,k is a rank-1 matrix, its unique eigenvalue is ΠT

:,kΠ:,k and therefore

K
(cid:88)

(Π:,k − Π:,k1) · ΠT
:,k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

(cid:13)
(cid:63)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

≤

=

=

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:13)
(cid:13)Π:,kΠT
:,k

(cid:13)
(cid:63)
(cid:13)
2

ΠT

:,kΠ:,k

K
(cid:88)

n
(cid:88)

k=1

i=1

π2
ik

Holder
≤

n
(cid:88)

i=1

max
k

{πik}

≤

n
(cid:88)

i=1

1 = n ,

K
(cid:88)

πik

k=1
(cid:124) (cid:123)(cid:122) (cid:125)
=1

which concludes the proof of the second inequality in Theorem 2.

The last statement of the theorem follows directly from the structure of permutation matrices and the greedy nature of the
algorithm.

Theorem 3. (Frank-Wolfe Convergence (Jaggi, 2013; Bubeck, 2014)) Let the gradient of the objective function g : x → g(x)
be L-smooth with respect to a norm (cid:107) · (cid:107) and its dual norm (cid:107) · (cid:107)(cid:63):

(cid:107)∇g(x) − ∇g(y)(cid:107)(cid:63) ≤ L(cid:107)x − y(cid:107) .

If g is minimized over S using Frank-Wolfe algorithm, then for each l ≥ 1, the iterates x(l) satisfy

g(x(l)) − g(x(cid:63)) ≤

2Ldiam(cid:107)·(cid:107)(S)2
l + 2

,

where x(cid:63) ∈ S is an optimal solution of the problem and diam(cid:107)·(cid:107)(S) stands for the diameter of S with respect to the norm
(cid:107) · (cid:107).

Proof. The proof of this theorem is a direct combination of Theorem 1 and Lemma 7 in Jaggi (2013), both proved in the
paper.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

Proposition 3. (Relation between p and (cid:107)W − 11T

n (cid:107)2

F ) Let W be a mixing matrix satisfying Assumption 3. Then,

(1 − p) ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

W −

11T
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

≤ (n − 1)(1 − p) .

Proof. The upper-bound is a direct application of Assumption 3 with M = I, the identity matrix of size n:

(cid:13)
(cid:13)
W T −
(cid:13)
(cid:13)

11T
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

=

(cid:13)
(cid:13)
IW T − I
(cid:13)
(cid:13)

11T
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

(A.3)

≤ (1 − p)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

I −

11T
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

= (1 − p)(n − 1) .

To show the lower-bound, denote by s1(M ), . . . , sn(M ) the (decreasing) singular values of any square matrix M ∈ Rn×n.
Denote similarly λ1(M ), . . . , λn(M ) the eigenvalues of any symmetric square matrix M ∈ Rn×n.

(cid:13)
(cid:13)
(cid:13)
(cid:13)

W −

11T
n

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

=

(cid:18)

n
(cid:88)

i=1

s2
i

W −

(cid:19)

11T
n

(cid:18)

W −

≥ s2
1

(cid:19)

11T
n

(cid:18)

= λ1

(W −

11T
n

)T(W −

(cid:19)

11T
n

)

(cid:18)

W TW −

= λ1

(cid:19)

11T
n

= λ2(W TW ) ≥ 1 − p .

The last equality is obtained by noticing that W TW is a symmetric doubly stochastic matrix. It therefore admits an
n 1. This makes W TW − 11T
eigenvalue decomposition where the largest eigenvalue 1 is associated with the eigenvector 1√
n
n 1 and the largest eigenvalue of W TW − 11T
having the eigenvalue 0 associated to the vector 1√
n becomes the second-largest
eigenvalue of W TW . The ﬁnal inequality comes from the fact that Assumption 3 is always true with p = 1 − λ2(W TW )
which implies that the best p satisfying Assumption 3 in necessarily greater or equal to 1 − λ2(W TW ).

C.1 Extension to Random Mixing Matrices

As mentioned in Section 3, all our theoretical results can easily be extended to random mixing matrices. In that framework,
at each iteration t of the D-SGD algorithm, the matrix W (t) is sampled from a doubly stochastic matrix distribution denoted
W (t), independent of the iterates of parameters θ(t), and possibly time-varying.

To obtain the convergence result, we slightly modify Assumption 3 and Assumption 4 by adding an expectation with respect
to W in front of the equations. For instance, Assumption 3 becomes EW ∼W (cid:107)M W T − M (cid:107)2
F . Then,
the statement of Theorem 1 is also slightly modiﬁed by assuming that it is the distributions W (0), . . . , W (T −1) that must
now respect Assumptions 3 and 4.

F ≤ (1 − p)(cid:107)M − M (cid:107)2

By appropriately conditioning with respect to the random mixing matrices or with respect to the iterates, the proof of the
theorem remains the same.

C.2 Closed-Form for the Line-Search

In this section, we give the closed-form solution of the line-search problem found in the Frank-Wolfe algorithm 2. Recall
that we seek to solve:

with

γ(cid:63) = arg min
γ∈[0,1]

(cid:110)

˜g(γ) (cid:44) g ((1 − γ)W + γP )

,

(cid:111)

g(W ) =

(cid:13)
(cid:13)
(cid:13)W Π −

1
n

11T
n

Π

(cid:13)
2
(cid:13)
(cid:13)

F

+

(cid:13)
(cid:13)
(cid:13)W −

λ
n

11T
n

(cid:13)
2
(cid:13)
(cid:13)

F

.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

The function g being quadratic, the objective ˜g(γ) is also quadratic with respect to γ. Hence, it sufﬁces to put the derivative
˜g(cid:48) of ˜g equal to 0, and we get the closed-form solution:

(cid:80)K

k=1(Π:,k1 − W Π:,k)T(P − W ) · Π:,k − λ · tr

(cid:18)(cid:16)

(cid:17)T

W − 11T
n

(P − W )

(cid:107)(P − W )Π(cid:107)2

F + λ(cid:107)P − W (cid:107)2
F

(cid:19)

.

γ(cid:63) =

D Additional Experiments

In this section, we provide additional details on our experimental setup, as well as additional results to complement the main
results in the paper.

D.1 Detailed Experimental Setup

Our main goal is to provide a fair comparison of the convergence speed across different topologies in order to show the
beneﬁts of the principled approach to topology learning provided by STL-FW. We essentially follow the experimental setup
in Bellet et al. (2022), which we recall below.

In our study, we focus our investigation on the convergence speed, rather than the ﬁnal accuracy after a ﬁxed number of
iterations. Indeed, depending on when training is stopped, the relative difference in ﬁnal accuracy across different algorithms
may vary signiﬁcantly and lead to different conclusions. Instead of relying on somewhat arbitrary stopping points, we
show the convergence curves of generalization performance (i.e., the accuracy on the test set throughout training), up to a
point where it is clear that the different approaches have converged, will not make signiﬁcantly more progress, or behave
essentially the same.

Datasets. We experiment with two datasets: MNIST (Deng, 2012) and CIFAR10 (Krizhevsky et al., 2009), which both
have K = 10 classes. For MNIST, we use 50k and 10k examples from the original set for training and testing respectively.
For CIFAR10, we used 50k images of the original training set for training and 10k examples of the test set for measuring
prediction accuracy.

For both MNIST and CIFAR10, we use the heterogeneous data partitioning scheme proposed by McMahan et al. (2017) in
their seminal FL work: we sort all training examples by class, then split the list into shards of equal size, and randomly
assign two shards to each node. When the number of examples of one class does not divide evenly in shards, as is the case
for MNIST, some shards may have examples of more than one class and therefore nodes may have examples of up to 4
classes. However, most nodes will have examples of 2 classes.

Models. We use a logistic regression classiﬁer for MNIST, which provides up to 92.5% accuracy in the centralized setting.
For CIFAR10, we use a Group-Normalized variant of LeNet (Hsieh et al., 2020), a deep convolutional network which
achieves an accuracy of 74.15% in the centralized setting. These models are thus reasonably accurate (which is sufﬁcient to
study the effect of the topology) while being sufﬁciently fast to train in a fully decentralized setting and simple enough
to conﬁgure and analyze. Regarding hyper-parameters, we use the learning rate and mini-batch size found in Bellet et al.
(2022) after cross-validation for n = 100 nodes, respectively 0.1 and 128 for MNIST and 0.002 and 20 for CIFAR10.

Metrics. We evaluate a network of n = 100 nodes, creating multiple models in memory and simulating the exchange of
messages between nodes. To ignore the impact of distributed execution strategies and system optimization techniques, we
report the test accuracy of all nodes (min, max, average) as a function of the number of times each example of the dataset has
been sampled by a node, i.e. an epoch. This is equivalent to the classic case of a single node sampling the full distribution.
All our results were obtained on a custom version of the non-iid topology simulator made available online by the authors
of Bellet et al. (2022),2 which provides deterministic and fully replicable experiments on top of Pytorch and ensures all
topologies were used in the same algorithm implementation and used exactly the same inputs.

Baselines We compare our results against an ideal baseline: a fully-connected network topology with the same number of
nodes. All other things being equal, any other topology using less edges will converge at the same speed or slower: this
is therefore the most difﬁcult and general baseline to compare against. This baseline is also essentially equivalent to a
centralized (single) IID node using a batch size n times bigger, where n is the number of nodes. Both a fully-connected

2https://gitlab.epfl.ch/sacs/distributed-ml/non-iid-topology-simulator

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

dmax = 3

dmax = 9

Topology
STL-FW (ours)
Random d-regular
STL-FW (ours)
Random d-regular

In-degree Out-degree Classes in neighborhood
3.0 ± 0.0
3.0 ± 0.0
9.0 ± 0.0
9.0 ± 0.0

4.0 ± 0.0
3.88 ± 0.38
10.0 ± 0.0
9.65 ± 0.65

3.0 ± 0.0
3.0 ± 0.0
9.0 ± 0.0
9.0 ± 0.0

Bias
0.15 ± 0.0
0.28 ± 0.1
0.0 ± 0.0
0.09 ± 0.04

1 − p
0.85
0.89
0.41
0.39

Table 1: Statistics of the topologies used in the synthetic data experiments of Section 6.1.

network and a single IID node effectively optimize a single model and sample uniformly from the global distribution: both
thus remove entirely the effect of label distribution skew and of the network topology on the optimization. In practice, we
prefer a fully-connected network because it converges slightly faster and obtains slightly better ﬁnal accuracy than a single
node sampling randomly from the global distribution.

We also provide comparisons against popular sparse topologies, such as random graphs and exponential graphs (Ying et al.,
2021). For the random graph, we use a similar number of edges (dmax) per node to determine whether a simple sparse
topology could work equally well. For the exponential graph, we follow the deterministic construction of Ying et al. (2021)
and consider edges to be undirected, resulting in dmax = 14 for n = 100.

We ﬁnally compare against D-Cliques (Bellet et al., 2022), the only competitor which takes into account the data heterogeneity
in the choice of topology. D-Cliques constructs a topology around sparsely inter-connected cliques such that the union of
local datasets within a clique is representative of the global distribution, i.e. it minimizes the ﬁrst term in our objective
function (Eq. 8) within each clique.

D.2 Statistics of the Used Topologies

In this section, we provide tables containing important statistics about the topologies used in the experiments. In each table,
a row corresponds to a speciﬁc topology having at most dmax in and out-neighbors per node. The columns are as follows:

• In-degree (respectively Out-degree): average and standard deviation of the number of incoming (respectively outgoing)

edges per node.

• Classes in neighborhood: average and standard deviation of the number of different classes in the direct neighborhood
of a node. Recall that each node individually observes examples only from a subset of the 10 different classes (1 for the
synthetic dataset, 2 for MNIST, 2-4 for CIFAR10).

• Bias: average and standard deviation of ((cid:80)n

j=1 Wijπjk − 1
j=1 πjk)2 across each node i. In other words, it measures
n
the neighborhood heterogeneity in terms of class proportions, which (up to a constant factor) corresponds to the bias
term in (7). According to our theory, the smaller the bias term, the better the topology.

(cid:80)n

• 1 − p: the mixing parameter of the topology (see Assumption 3). Recall that for a topology W , 1 − p = λ2(W TW ).
According to our theory (and prior work, see e.g., Koloskova et al., 2020), the smaller 1 − p, the better the topology.

Interestingly, all tables show that our algorithm STL-FW outputs topologies that are dmax-regular. Therefore, the com-
munication burden is the same for all nodes. This is a desirable property for scalability, that the star topology induced by
server-based federated learning does not satisfy.

Table 1 provides the statistics of the topologies used in the synthetic data experiment. We observe that the mixing parameter
p are similar for both our topologies (STL-FW) and the random d-regular graph. However, STL-FW achieves much smaller
bias, resulting in more classes being represented in the neighborhood of each node. This explains the faster convergence of
D-SGD with our topology, in line with our theoretical results.

Table 2 (MNIST) and Table 3 (CIFAR10) provide the statistics of the topologies used in the real data experiments. The same
conclusions made regarding the synthetic experiments hold here regarding the comparison of STL-FW with the random
d-regular graphs. D-Cliques (Bellet et al., 2022), the only topology that is also constructed in a data-dependent fashion,
achieves rather low bias (albeit slightly larger than our STL-FW topology) but has rather bad mixing properties (large
1 − p). This conﬁrms our claim that D-Cliques reduces the bias without ensuring good mixing (due to the constrained
arrangements of nodes in sparsely interconnected cliques). This explains the superior performance of our topology. Last
but not least, looking at dmax = 5, we notice that D-Cliques is unable to satisfy the constraints that the maximum degree

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

dmax = 2

dmax = 5

dmax = 10

Topology
STL-FW (ours)
Random d-regular
STL-FW (ours)
Random d-regular
D-cliques
STL-FW (ours)
Random d-regular
D-cliques
Exponential

In-degree
2.0 ± 0.0
2.0 ± 0.0
5.0 ± 0.0
5.0 ± 0.0
5.82 ± 0.38
10.0 ± 0.0
10.0 ± 0.0
9.9 ± 0.3
14.0 ± 0.0

Out-degree
2.0 ± 0.0
2.0 ± 0.0
5.0 ± 0.0
5.0 ± 0.0
5.82 ± 0.38
10.0 ± 0.0
10.0 ± 0.0
9.9 ± 0.3
14.0 ± 0.0

Classes in neighborhood
6.03 ± 0.62
4.94 ± 0.89
9.99 ± 0.1
7.53 ± 1.07
9.81 ± 0.39
10.0 ± 0.0
9.31 ± 0.76
10.0 ± 0.0
9.72 ± 0.51

Bias
0.08 ± 0.02
0.14 ± 0.06
0.007 ± 0.004
0.07 ± 0.03
0.02 ± 0.01
0.001 ± 0.001
0.03 ± 0.02
0.005 ± 0.002
0.02 ± 0.01

Table 2: Statistics of the topologies used on the MNIST experiments of Section 6.2.

dmax = 2

dmax = 5

dmax = 10

Topology
STL-FW (ours)
Random d-regular
STL-FW (ours)
Random d-regular
D-cliques
STL-FW (ours)
Random d-regular
D-cliques
Exponential

In-degree
2.0 ± 0.0
2.0 ± 0.0
5.0 ± 0.0
5.0 ± 0.0
5.82 ± 0.38
10.0 ± 0.0
10.0 ± 0.0
9.9 ± 0.3
14.0 ± 0.0

Out-degree
2.0 ± 0.0
2.0 ± 0.0
5.0 ± 0.0
5.0 ± 0.0
5.82 ± 0.38
10.0 ± 0.0
10.0 ± 0.0
9.9 ± 0.3
14.0 ± 0.0

Classes in neighborhood
5.79 ± 0.45
4.86 ± 0.81
9.98 ± 0.14
7.4 ± 0.97
9.71 ± 0.55
10.0 ± 0.0
9.26 ± 0.82
10.0 ± 0.0
9.68 ± 0.58

Bias
0.08 ± 0.02
0.14 ± 0.06
0.008 ± 0.005
0.07 ± 0.03
0.022 ± 0.012
0.001 ± 0.001
0.033 ± 0.016
0.004 ± 0.002
0.024 ± 0.012

Table 3: Statistics of the topologies used in the CIFAR10 experiments of Section 6.2.

1 − p
0.88
0.99
0.55
0.68
0.99
0.35
0.39
0.84
0.54

1 − p
0.99
0.99
0.64
0.68
0.99
0.45
0.39
0.84
0.54

should not exceed 5. This also illustrates the greater ﬂexibility of STL-FW when it comes to controlling the per-iteration
communication complexity.

D.3

Impact of λ in STL-FW

Figure 3 shows the impact of λ, which rules the bias-variance trade-off in our objective function for learning the topology.
We present results for two extreme values, respectively 0.0001 and 1000 as well as middle ground of 0.1. For both datasets,
λ has little effect on convergence speed. From a practical perspective, this is an advantage as it removes the need for tuning
λ (one can simply set it to a default positive value). This behavior may be explained by the fact that reducing the bias term
alone also leads to a reduction of variance. Hence, the variance term becomes useful only when the bias term has been
“erased” (or made very small), which can happen only after a certain number of STL-FW iterations, i.e., for a potentially
large dmax. For all other experiments, we used λ = 0.1.

D.4

Impact of dmax on STL-FW

Figure 4 shows on a single plot the impact of the communication budget dmax of STL-FW on the convergence speed of
D-SGD. The communication budget has a strong impact in both cases, with STL-FW providing the same convergence speed
as fully-connected when dmax = 99, but with some residual variance because some nodes end up wth less than 99 edges.
Most of the beneﬁts of STL-FW are obtained with the ﬁrst 10 edges, with additional edges providing only marginal beneﬁts
compared to fully-connected. We thus chose to show all experiments of the main text with three budgets, a small dmax = 2,
a medium dmax = 5, and a large budget dmax = 10.

Finally, for a small budget dmax = 2, we had seen in Figure 2 in the main text that STL-FW did not provide signiﬁcant
beneﬁts compared to a random graph on CIFAR10. Figure 5 shows that as soon as dmax = 3, STL-FW starts providing
beneﬁts compared to a random topology on CIFAR10.

Reﬁned Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data

MNIST

CIFAR10

Figure 3: Effect of the hyperparameter λ of STL-FW on the convergence speed of D-SGD with 100 nodes, dmax = 10.

(a) MNIST

(b) CIFAR10

Figure 4: Effect of communication budget (dmax) of STL-FW on the convergence speed of D-SGD with 100 nodes, λ = 0.1.

(a) MNIST

(b) CIFAR10

Figure 5: Convergence speed of D-SGD with our STL-FW topology and a random topology under small communication budget
dmax = 3.

020406080100Epochs878889909192Validation Accuracy (%)STL-FW =0.0001STL-FW =0.1STL-FW =1000020406080100Epochs40455055606570Validation Accuracy (%)STL-FW =0.0001STL-FW =0.1STL-FW =1000020406080100Epochs8586878889909192Validation Accuracy (%)Fully-Connected dmax=99STL-FW dmax=99STL-FW dmax=10STL-FW dmax=4STL-FW dmax=2020406080100Epochs010203040506070Validation Accuracy (%)Fully-Connected dmax=99STL-FW dmax=99STL-FW dmax=10STL-FW dmax=4STL-FW dmax=2020406080100Epochs70.072.575.077.580.082.585.087.590.092.5Validation Accuracy (%)Fully-connected dmax=99STL-FWRandom020406080100Epochs010203040506070Validation Accuracy (%)Fully-Connected dmax=99STL-FWRandom