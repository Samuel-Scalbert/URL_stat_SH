MaxHedge: Maximising a Maximum Online
Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

To cite this version:

Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster. MaxHedge: Maximising
a Maximum Online. International Conference on Artificial Intelligence and Statistics, Apr 2019, Naha,
Okinawa, Japan. ￿hal-02376987￿

HAL Id: hal-02376987

https://inria.hal.science/hal-02376987

Submitted on 22 Nov 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

MaxHedge: Maximising a Maximum Online

Stephen Pasteris
University College London
London, UK
s.pasteris@cs.ucl.ac.uk

Fabio Vitale
Sapienza University
Italy & INRIA Lille, France
fabio.vitale@inria.fr

Kevin Chan
Army Research Lab
Adelphi, MD, USA
kevin.s.chan.civ@mail.mil

Shiqiang Wang
IBM Research
Yorktown Heights, NY, USA
wangshiq@us.ibm.com

Mark Herbster
University College London
London, UK
m.herbster@cs.ucl.ac.uk

Abstract

1 Introduction

We introduce a new online learning framework
where, at each trial, the learner is required to
select a subset of actions from a given known
action set. Each action is associated with an
energy value, a reward and a cost. The sum
of the energies of the actions selected cannot
exceed a given energy budget. The goal is
to maximise the cumulative proﬁt, where the
proﬁt obtained on a single trial is deﬁned as
the diﬀerence between the maximum reward
among the selected actions and the sum of
their costs. Action energy values and the bud-
get are known and ﬁxed. All rewards and
costs associated with each action change over
time and are revealed at each trial only af-
ter the learner’s selection of actions. Our
framework encompasses several online learn-
ing problems where the environment changes
over time; and the solution trades-oﬀ between
minimising the costs and maximising the max-
imum reward of the selected subset of actions,
while being constrained to an action energy
budget. The algorithm that we propose is
eﬃcient and general that may be specialised
to multiple natural online combinatorial prob-
lems.

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

In this paper we propose a novel online framework
where learning proceeds in a sequence of trials and
the goal is to select, at each trial, a subset of actions
maximising a proﬁt while taking into account a certain
constraint. More precisely, we are given a ﬁnite set
of actions enumerated from 1 to n. Each action is
associated with three values: (i) a cost, (ii) an amount
of energy (both of which are required to perform the
given action), as well as (iii) a reward. Both the cost
and the reward associated with each action may change
over time, are unknown at the beginning of each trial,
and their values are all revealed after the learner’s
selection. The energy associated with each action is
instead known by the learner and does not change over
time. At each trial, the learner is required to select a
subset of actions such that the sum of their energies
does not exceed a ﬁxed energy budget. The goal of
the learner is to maximise the cumulative proﬁt, where
the proﬁt obtained on a single trial is deﬁned as the
diﬀerence between the maximum reward among the
selected actions and the sum of their costs. We denote
by T the total number of trials.

Our framework is general and ﬂexible in the sense that
it encompasses several online learning problems. In the
general case, the main challenge lies in the fact that the
rewards are not known at the beginning of each trial,
and the learner’s proﬁt depends only on the maximum
reward among the selected subset of actions, instead of
the sum of all their rewards. In particular, it is worth
mentioning three diﬀerent problems which can be seen
as special cases of our online learning framework; these
are variants of the Facility Location problem, the 0-1
Knapsack problem, and the Knapsack Median problem.

When all action energy values are equal to 0, we obtain

MaxHedge: Maximising a Maximum Online

an online learning variant of the Facility Location prob-
lem (see, e.g., [4], [21], [18]). The goal of this speciﬁc
problem may be viewed as selecting and opening a sub-
set of facilities at each given trial, to service a sequence
of users which arrive one at a time. At any given trial
t, each action’s cost may be interpreted as the cost for
opening a facility for the t-th user. The reward asso-
ciated with a given facility represents what the user
potentially gains when it is opened. More speciﬁcally,
the rewards can be seen as quantities dependent on the
distance between the user and the facilities in some
metric space. In this context, it is reasonable to assume
that the proﬁt obtained on a single trial depends only
on the maximum reward among the opened facilities
taking into account the facility costs. This represents
a natural setting for the Facility Location problem,
because in practical scenarios users may arrive sequen-
tially and each arrival requires a connection to an open
facility. Online versions of the Facility Location prob-
lem have been studied in several contexts (see, e.g.,
[5], [20]). However, as far as we are aware, our work
is the ﬁrst study of this speciﬁc online setting for the
Facility Location problem. Moreover, this dynamic
model is natural and interesting within this context1,
because in practice the location of the next user, which
in turn determines the reward associated with each
facility, is often unknown to the learner. At each trial,
after the user’s location is known, the rewards are then
revealed, because disclosing the user’s location enables
to compute all distances between the facilities and the
user, which are previously unknown. In fact, this cor-
responds to assuming all rewards are revealed at the
end of the trial.

When, instead, all rewards are equal to 0 and all costs
are negative, the problem can be seen as an online
learning variant of the 0-1 Knapsack problem (see, e.g.,
[6], [19]). In this case, each action corresponds to an
item whose weight is equal to the associated energy, the
energy budget represents the knapsack capacity, and
the absolute value of each action cost can be viewed as
the corresponding item value. Our formulation makes
the problem challenging especially because the item
values are revealed only after the learner’s selection.

Finally, when all costs are equal to 0, we obtain an on-
line learning variant of the Knapsack Median problem
([1], [15]). In this problem (which is a generalization
of the k-median problem – see, e.g., [2], [10]), we are
given a set of clients and facilities, as well as a distance
metric. The goal is to open a subset of facilities such
that the total connection cost (distance to nearest open
facility) of all clients is minimized, while the sum of the
open facility weights is limited by a budget threshold.

1Similar arguments hold also for motivating the other

two special cases mentioned in this section.

In our framework, at each trial the rewards can express
the closeness of each facility, and the action energy
budget represents therefore the threshold of sum of the
opened facility weights.

This problem has practical applications in multiple do-
mains. In computer networks, network administrators
may want to understand where to place network moni-
tors or intrusion detection systems. Network packets
or malicious attacks are related to the events playing a
crucial role in this scenario, and a limited amount of
network resources are available to detect or observe net-
work behavior. Another class of application examples
include municipal emergency services. A service center
needs to deploy responders (police, paramedics, ﬁre
rescue), and with limited resources, personnel must be
deployed sparingly. A further application is related to
deploying program instances in a distributed computing
environment (e.g. distributed cloud). These systems
must respond to user requests and are subject to con-
straints. Both costs and rewards may be unknown in
practical real-world scenarios at the beginning of each
trial.

For our general problem we propose and rigorously
analyse a very scalable algorithm called MaxHedge
based on the complex interplay between satisfying the
energy budget constraint and bounding the proﬁt by a
concave function, which in turn is related to the online
gradient descent algorithm. Moreover, the total time
required per trial by our learning strategy is quasi-linear
in n. We measure the performance of proposed solution
with respect to the diﬀerence between its cumulative
proﬁt and a discounted cumulative proﬁt of the best
ﬁxed subset of actions.

In summary, our framework captures several real-world
problems, where the environment changes over time
and the solutions trade-oﬀ between minimising the
costs and maximising the maximum reward among the
selected subset of actions, while being constrained to an
action energy budget. The framework is very general
and the proposed algorithm is very eﬃcient and may
be specialised to several natural online combinatorial
problems. Finally we provide a guarantee on the re-
wards achieved and costs incurred as compared to the
best ﬁxed subset of actions.

1.1 Related Work

The closest work to our online learning framework is
perhaps addressed in [13], where the authors describe
an online learning algorithm for structured concepts
that are formed by components. Each component and
each concept can be respectively seen as an action and
a feasible subset of actions. Despite several similarities,
the algorithm they proposed cannot be used when

Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

the rewards are non-zero, because we focus on the
maximum reward among the selected subsets of actions,
whereas in [13] the proﬁt corresponds to the sum of the
rewards of all selected components/actions. When all
rewards are instead equal to 0, then we have the online
variant of the 0-1 Knapsack problem described above
as one of the three special cases of our general problem.
In Appendix C we prove that if the algorithm presented
in [13] could handle the Knapsack problem, then the
classical version of the Knapsack problem could be
solved in polynomial time, which therefore implies that
it cannot address this problem unless P = N P .

The Hedge Algorithm described in [7] obtains a regret
linear in n. However, as we have exponentially many
possible subsets of selected actions, a vanilla application
of Hedge would require an exponential amount of time
and space to solve our problem.

Another class of problems and algorithms that are
not far from ours is represented by online decision
problems where eﬃcient strategies use, as a subroutine,
an approximation algorithm for choosing a concept
to maximise an inner product ([8], [12], [11]). Again,
these learning strategies cannot handle the case of non-
zero reward. They also have a signiﬁcantly higher
time complexity than MaxHedge in the case of zero
reward.

In [3] , [9], and [22], the authors address the problem
of online maximisation of non-negative monotone sub-
modular functions. Although our proﬁt is submodular,
it is not necessarily either non-negative or monotone.
However, as we shall show in Appendix D, we could,
for the facility location special case, combine much of
the mechanics of our paper with the second algorithm
of [3], essentially doing gradient ascent with the exact
expected proﬁt instead of an approximate expected
proﬁt (as is done in MaxHedge). Even though this
new algorithm could be as eﬃcient as the one presented
in our paper, its theoretical guarantees are worse. Note
that in the Knapsack and Knapsack Median special
cases, the proﬁt is indeed monotone and non-negative.
For these special cases, the approach presented in [22]
comes close to solving the problem, but only guaran-
tees that the expectation of the total energy does not
exceed the budget rather than the actual total energy.
In addition, for the Knapsack Median problem, [22]
uses quadratic time and space. As far as we are aware,
there does not exist any trivial reduction to use [3] or
[9] for these special cases.

Finally, in [16] the authors address a problem of online
minimisation of submodular function. Our paper max-
imises, instead of minimising, a submodular function
(the proﬁt). This is a very diﬀerent problem.

Some of the techniques behind the development of

MaxHedge were inspired by [25].

2 Preliminaries and Problem Setup

2.1 Preliminaries

Vectors in Rn will be indicated in bold. Given a vector
v ∈ Rn we deﬁne vi to be its i-th component. Given
vectors v, x ∈ Rn deﬁne hv, xi := Pn
i=1 vixi. Deﬁne
P to be the set of n-dimensional real vectors in which
every component is non-negative. Given a closed convex
set C ⊆ Rn and a vector x ∈ Rn, we deﬁne ΠC(x) as
the projection (under the Euclidean norm) of x onto
C. Let N be the set of the positive integers. For l ∈ N
deﬁne [l] = {1, 2, 3, ..., l}. Given an event E let ¬E
be the event that E does not occur. Let P (E) be the
probability that event E occurs. For a random variable
Y , let E (Y ) be the expected value of Y . Given a
diﬀerentiable function h : Rn → R and a vector x ∈ Rn
let ∇h(x) be the derivative of h evaluated at x. Let
∂ih(x) be the i-th component of ∇h(x).

2.2 Problem Setup

In this section we formally deﬁne our problem. We
have a set of n actions enumerated from 1 to n. Each
action i has an energy zi ∈ [0, β] for some β < 1,
i ∈ R
and on each trial t each action i has a cost ct
i ∈ R+. The
(which can be negative) and a reward rt
learner knows z, but ct and rt are revealed to the
learner only at the end of trial t. On each trial t
the learner has to select a set X t ⊆ [n] of actions,
such that the total energy P
i∈X t zi of the selected
actions is no greater than 1. In selecting the set X t,
the learner pays a cost equal to P
i. On each
trial t the learner then receives the maximum reward
maxi∈X t rt
i, over all actions selected (deﬁned as equal
to zero if X t is empty). Hence, the proﬁt obtained by
the learner on trial t is equal to maxi∈X t rt
i∈X t ct
i.

i∈X t ct

i − P

Formally, this online problem can be deﬁned as
follows: We have a vector z ∈ P known to the learner.
On trial t:

1. Nature selects vectors ct ∈ Rn and rt ∈ P (but

does not reveal these vectors to learner)

2. Learner selects a set X t ⊆ [n] with P

i∈X t zi ≤ 1

3. Learner obtains proﬁt:

µt(X t) := max
i∈X t

rt
i −

X

ct
i

i∈X t

4. ct and rt are revealed to the learner.

MaxHedge: Maximising a Maximum Online

In this paper we write, for a trial t, the cost vector
ct as the sum ct+ + ct− where ct+
i} and
i
ct−
i}, i.e. ct+ and ct− are the positive and
i
negative parts of the cost vector, respectively.

:= max{0, ct

:= min{0, ct

In order to bound the cumulative proﬁt of our algorithm
we, for some α, δ ∈ [0, 1], some set S ⊆ [n] and some
trial t, deﬁne the (α, δ)-discounted proﬁt ˆµt

α,δ(S) as:

ˆµt

α,δ(S) := α max
i∈S

rt
i − α

ct−
i − δ

X

i∈S

ct+
i

X

i∈S

which would be the proﬁt obtained on trial t if we
selected the subset of actions S, and all the rewards
and negative costs were multiplied by α and all positive
costs were multiplied by δ.

In this paper we provide a randomised quasi-linear
time (per trial) algorithm MaxHedge that, for any
set S ⊆ [n] with P
i∈S zi ≤ 1, obtains an expected
cumulative proﬁt bounded below by:

!

µt(X t)

≥

  T

X

E

t=1

T
X

t=1

√

ˆµt

α,δ(S) − n

2T δ(ˆr + ˆc)

√

where δ := (cid:0)1 −
ˆr := maxt∈[T ],i∈[n] rt

, α := 1 − exp

β(cid:1)2
− (cid:0)1 −
i and ˆc := maxt∈[T ],i∈[n] |ct
i|.

(cid:16)

√

β(cid:1)2(cid:17)

,

This paper is structured as follows. In Section 3 we
introduce the algorithms that deﬁne MaxHedge. In
Section 4 we prove that the sets of actions selected by
MaxHedge are feasible. In Section 5 we prove the
above bound on the cumulative proﬁt. In Section 6 we
give special cases of the general problem.

3 Algorithms

We now present our learning strategy MaxHedge,
describing the two subroutines “Algorithm 1” and “Al-
gorithm 2” (see the pseudocode below). MaxHedge
maintains a vector ω ∈ C where C := {x ∈ [0, 1]n :
hx, zi ≤ 1}. We deﬁne ωt to be the vector ω at the
start of trial t. We initialise ω1 ← 0. On trial t
MaxHedge (randomly) constructs X t from ωt using
Algorithm 1. After receiving rt and ct MaxHedge
updates ω (from ωt to ωt+1) using Algorithm 2. Algo-
rithm 2 also uses a “learning rate” ˆηt which is deﬁned
from ˆηt−1. We deﬁne ˆη0 := ∞.

Algorithm 1 operates with a partition of all possible
actions and, for each set in this partition, Algorithm
1 draws a certain subset of actions from it. Given
a set in the partition, the number of actions drawn
from it and the probability distribution governing the
draws depends on β and ωt. The subset, X t, of actions
selected by Algorithm 1 satisﬁes the following three
crucial properties (proved in sections 4 and 5):

• The total energy of all actions selected is no greater

than 1.

• Given an arbitrary set Z ⊆ [n], the probability
that X t and Z intersect is lower bounded by 1 −
exp (cid:0)−δ P

(cid:1).

i∈Z ωt
i

• Given an action i, the probability that it is selected

on trial t is upper bounded by δωt
i .

In the analysis we shall construct, from rt and ct, a
convex function ht : C → R. Using the second and
third properties (given above) of Algorithm 1, we show
that ht(ωt) is an upper bound on the negative of the
expected proﬁt on trial t. Algorithm 2 computes the
gradient gt := ∇ht(ωt) and updates ω using online
gradient descent on C.

The last line of Algorithm 2 requires us to project
(with Euclidean distance) the vector yt onto the set
C, i.e. we must compute the x that minimises the
value kx − ytk subject to x ∈ C. Note that minimis-
ing kx − ytk is equivalent to minimising kx − ytk2 =
hx, xi − 2hyt, xi + hyt, yti, which is in turn equivalent
to minimising hx, xi − 2hyt, xi. The constraints deﬁn-
ing the set C then imply that this projection is a case
of the continuous bounded quadratic knapsack problem
which can be solved in linear time (see, e.g., [14]).

The bottleneck of the algorithms is hence the ordering
step in Algorithm 2 which takes a time of O(n log(n))

Algorithm 1 Constructing X t
1: C ← {x ∈ [0, 1]n : hx, zi ≤ 1}
2: β ← maxi∈[n] zi
3: τ ← 1 −
4: δ ← (cid:0)1 −
5: Γ ← {q ∈ N : ∃i ∈ [n] with τ qβ < zi ≤ τ q−1β}
6: For all q ∈ Γ set Ωq ← {i ∈ [n] : τ qβ < zi ≤

√
β
√
β(cid:1)2

.

τ q−1β}.

7: Input: ωt ∈ C

8: For all q ∈ Γ set πt
9: For all q ∈ Γ set ζ t
10: For all q ∈ Γ and for all k ≤ ζ t

q ← P
q ← (cid:4)δπt

i∈Ωq
(cid:5)

q

ωt
i

from Ωq such that ξt
11: For all q ∈ Γ and for k := ζ t

q draw ξt
q,k ← i with probability ωt
q +1 draw ξt

q,k randomly
i /πt
q
q,k randomly
from Ωq ∪ {0} such that, for i ∈ Ωq we have
qc)ωt
ξt
q,k ← i with probability (δπt
i /πt
q, and
qc + 1 − δπt
we have ξt
q.
NB: In the case that πt

q − bδπt
q,k ← 0 with probability bδπt

q = 0 we deﬁne 0/0 = 0

12: Output: X t ← {ξt

q,k : q ∈ Γ, k ≤ ζ t

q + 1} \ {0}

Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

Algorithm 2 Computing ωt+1
1: C ← {x ∈ [0, 1]n : hx, zi ≤ 1}
2: β ← maxi∈[n] zi
β(cid:1)2
3: δ ← (cid:0)1 −

√

5 Bounding the Cumulative Proﬁt

In this section we bound the cumulative proﬁt of
MaxHedge.

4: Input: ωt ∈ C, ct ∈ Rn rt ∈ P

5.1 The Probability of Intersection

5: Order [n] as [n] = {σ(t, 1), σ(t, 2), ...σ(t, n)} such

that rt

σ(t,j) ≥ rt

6: For all j ∈ [n] set (cid:15)t
7: For all j ∈ [n] set:

σ(t,j+1) for all j ∈ [n − 1]
(cid:16)
k=1 ωt

j ← exp

−δ Pj

(cid:17)

σ(t,k)

j ← rt
λt

σ(t,n)(cid:15)t

n + Pn−1

k=j

(cid:16)

rt
σ(t,k) − rt

σ(t,k+1)

(cid:17)

(cid:15)t
k

8: For all j ∈ [n] set:

(cid:16)

σ(t,j) + ct−
ct+
gt
σ(t,j) ← δ
√
9: ˆηt ← min (cid:8)ˆηt−1,
10: ηt ← ˆηt/
11: yt ← ωt − ηtgt

2t

√

n/kgtk(cid:9)

σ(t,j) exp

(cid:16)

−δωt

σ(t,j)

(cid:17)

(cid:17)

− λt
j

12: Output: ωt+1 ← ΠC(yt)

4 The Feasibility of X t

In this section we show that the the total energy of the
actions selected by Algorithm 1 is no greater than 1,
as required in our problem deﬁnition (see Section 2.2).
We ﬁrst introduce the sets and quantities used in the
selection of the actions.
Deﬁnition 4.1. We deﬁne the following:

• τ := 1 −

√

β and δ := (cid:0)1 −

√

β(cid:1)2

• For all q ∈ N we deﬁne Ωq := {i ∈ [n] : τ qβ <

zi ≤ τ q−1β}

• Γ := {q ∈ N : Ωq 6= ∅}. Note that {Ωq : q ∈ Γ} is

a partition of [n]

• On trial t, for all q ∈ Γ deﬁne πt
(cid:5).

q := (cid:4)δπt

and ζ t

q

q := P

ωt
i

i∈Ωq

Algorithm 1 works by drawing actions {ξt
q,k : q ∈
Γ, k ∈ [ζ t
q + 1]} randomly, where the number of
actions (including a “null action” 0) drawn (with
replacement) from Ωq is equal to ζ t
q + 1 and the
probability distribution of the draws is dependent on
ωt.

The following theorem ensures
the choice
of X t made by our method satisﬁes our problem’s
energy constraint.
Theorem 4.2. On trial t we have P

that

i∈X t zi ≤ 1.

Proof. See Appendix A

In this subsection we ﬁrst bound below the probability
that, on a trial t, an arbitrary set Z intersects with X t.
We start with the following lemma:

Lemma 5.1. Given a set D of independent draws
from |D|-many probability distributions, such that the
probability of an event E happening on draw d ∈ D is
ρd, then the probability of event E happening on either
of the draws is lower bounded by:

1 − exp

−

!

X

d∈D

ρd

Proof. See Appendix A

We now bound the probability of intersection:

Theorem 5.2. For any trial t and any subset Z ⊆ [n]
we have P (Z ∩ X t 6= ∅) ≥ 1 − exp (cid:0)−δ P

(cid:1).

i∈Z ωt
i

Proof. See Appendix A

We now bound the probability that some arbitrary
action is selected on trial t:

Theorem 5.3. Given some action i ∈ [n] and some
i ) ≤ P (i ∈ X t) ≤ δωt
trial t ∈ [T ] we have 1−exp(−δωt
i .

Proof. See Appendix A

5.2 Approximating the Expected Proﬁt

In this subsection we deﬁne a convex function ht :
C → R and show that the expected proﬁt on trial t is
bounded below by −ht(ωt).

Deﬁnition 5.4. For each trial t we order [n] as [n] =
{σ(t, 1), σ(t, 2), . . . , σ(t, n)} where rt
σ(t,j+1)
for all j ∈ [n − 1]. We also deﬁne σ(t, n + 1) := 0
and rt

σ(t,j) ≥ rt

0 := 0.

Deﬁnition 5.5. Given a trial t and a number j ∈ [n]
we deﬁne the function f t

j : P → R by:

(cid:16)

f t
j (γ) :=

σ(t,j) − rt
rt

σ(t,j+1)

(cid:17)

1 − exp

−δ

!!

γσ(t,k)

j
X

k=1

We also deﬁne the function ht : P → R as:

ht(γ) = hct+, γi +

n
X

i=1

ct−
i (1 − exp(−δγi)) −

n
X

j=1

f t
j (γ)

 
 
 
MaxHedge: Maximising a Maximum Online

Theorem 5.6. For all t ∈ [T ], the function ht is
convex.

Proof. See Appendix A

The rest of this subsection proves that the expected
proﬁt on trial t is bounded below by −ht(ωt).
Lemma 5.7. On trial t we have maxi∈X t rt
(cid:17)
Pn
I (∃k ≤ j : σ(t, k) ∈ X t).

i =

(cid:16)

rt
σ(t,j) − rt

σ(t,j+1)

j=1

Proof. See Appendix A

Lemma 5.8. On trial t we have E (maxi∈X t rt
(cid:17)
Pn
P (∃k ≤ j : σ(t, k) ∈ X t).

(cid:16)

σ(t,j) − rt
rt

σ(t,j+1)

j=1

i) =

Proof. Direct from lemma 5.7 using linearity of expec-
tation.

Theorem 5.9. On trial t we have E (µt(X t)) ≥
−ht(ωt).

Proof. See Appendix A

5.3 The Gradient

In this subsection we show how to construct the gradi-
ent of ht and bound its magnitude. We start with the
following deﬁnitions.

Deﬁnition 5.10. On any trial t and for any j ∈ [n]
we deﬁne:

• (cid:15)t

j := exp

(cid:16)

−δ Pj

k=1 ωt

σ(t,k)

(cid:17)

• λt

j := Pn

k=j

(cid:16)

σ(t,k) − rt
rt

σ(t,k+1)

(cid:17)

(cid:15)t
k

• gt

σ(t,j) := δ

(cid:16)

σ(t,j) + ct−
ct+

σ(t,j) exp

(cid:16)

−δωt

σ(t,j)

(cid:17)

(cid:17)

− λt
j

We ﬁrst show that gt is the gradient of ht evaluated at
ωt.
Theorem 5.11. On any trial t we have gt = ∇ht(ωt).

Proof. See Appendix A

We now bound the magnitude of the gradient.
Lemma 5.12. For any trial t we have kgtk2 ≤ nδ2(ˆr +
ˆc)2.

j ≤ δrt

σ(t,k) ≥ 0 for all k ∈ [n] we have (cid:15)t

Proof. Since ωt
j ∈
[0, 1] for all j ∈ [n]. This gives us, for all j ∈ [n], that
δλt
σ(t,j) ≤
δˆr. Since also λt
i ≤ δˆr + δˆc
and that gt
i )2 ≤ (δˆr + δˆc)2. This then
implies the result.

σ(t,k) −rt
j ≥ 0 this implies that −gt

σ(t,n) +δ Pn−1

σ(t,k+1)) = δrt

i ≤ δˆc so (gt

k=j (rt

5.4 Online Gradient Descent

In this subsection we show that Algorithm 2 corre-
sponds to the use of online gradient descent over C
with convex functions {ht : t ∈ [T ]} and we use the
standard analysis of online gradient descent to derive
a lower bound on the cumulative proﬁt.

From here on we compare the performance of
our algorithm against any ﬁxed set S of actions such
that P
i∈S zi ≤ 1. We deﬁne φ as the vector in Rn
such that, for all i ∈ [n], we have φi := 0 if i /∈ S and
φi := 1 if i ∈ S. It is clear that φ ∈ C.
Deﬁnition 5.13. Our learning rates are deﬁned as
follows:

• ˆηt := mint0≤t(

√

n/kgt0

k)

• ηt := ˆηt/

√

2t

The next result follows from the standard analysis of
online gradient descent.

Theorem 5.14. We have:

T
X

(ht(ωt) − ht(φ)) ≤

t=1

R2
2ηT +

1
2

T
X

t=1

ηtkgtk2

where R := maxx,y∈C kx − yk.

Proof. For all trials t: From Theorem 5.6 we have that
ht is a convex function. From Theorem 5.11 we have
that gt = ∇ht(ωt). We also have that ηt+1 ≤ ηt so
since, by Algorithm 2, we have ωt+1 = ΠC(ωt − ηtgt)
and φ ∈ C, the standard analysis of online gradient
descent (see, e.g., [24]) gives us the result.

We now bound the right hand side of the equation in
Theorem 5.14.
Deﬁnition 5.15. Deﬁne s := maxt∈[T ] kgtk2/n.
Lemma 5.16. For any trial t, ηtkgtk2 ≤ nps/(2t).

√

√

n/kgt0
√
2t ≤

n/kgtk).
√
2t)
2t which, by deﬁnition of s, is

n/(kgtk

k) ≤
√

Proof. We have ˆηt = mint0≤t(
This implies that that ηt = ˆηt/
√
so ηtkgtk2 ≤
bounded above by nps/(2t).
√

nkgtk/

Lemma 5.17. R2/ηT ≤ n

2sT

√

√

√

√

√

2T ≤ 1/

Proof. We have ˆηT = mint∈[T ]
s so
ηT := ˆηT /
2sT . Since C ⊆ [0, 1]n, if
x, y ∈ C then each component of x − y has mag-
nitude bounded above by 1 which implies that R2 ≤ n.
Putting together gives the result.

n/kgtk = 1/

Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

Theorem 5.18. We have:

6 Special Cases

(ht(ωt) − ht(φ)) ≤ n

√

2T δ(ˆr + ˆc)

T
X

t=1

The following online variants of classic computer science
problems are special cases of the general problem.

Proof. See Appendix A

6.1 Facility Location Problem

The next result bounds ht(φ).
Lemma 5.19. On trial t we have ˆµt
where α := 1 − e−δ.

α,δ(S) ≤ −ht(φ)

The (inverted) facility location problem is deﬁned by a
vector c ∈ P and vectors r1, r2, · · · , rT ∈ P. A feasible
solution is any X ⊆ [n]. The aim is to maximise the
objective function:

(cid:17)

(cid:16)

Proof. Let j0 = argmaxj∈[n]:σ(t,j)∈S rt
σ(t,j) which is
equal to the minimum j such that σ(t, j) ∈ S. Note
that for all j ≥ j0 we have Pj
k=1 φσ(t,j) ≥ 1 and hence
(1−e−δ) so Pn
rt
f t
j=1 f t
σ(t,j) − rt
j (φ) ≥
j (φ) ≥
Pn
j (φ) ≥ Pn
j=j0 f t
(1 − e−δ) =
j=j0
σ(t,j+1)
σ(t,j0)(1 − e−δ) = (1 − e−δ) maxi∈S rt
rt
i.
Also note that δhct+, φi = δ P
i∈S ct+
and that
i (1 − exp(−δφi)) = P
Pn
i∈S ct−
i (1 − e−δ). Com-

σ(t,j+1)
(cid:16)
σ(t,j) − rt
rt

i=1 ct−

(cid:17)

i

bining with the above gives us the result.

Putting together we obtain the main result:

Theorem 5.20. We have:

!

µt(X t)

≥

  T

X

E

t=1

T
X

t=1

√

ˆµt

α,δ(S) − n

2T δ(ˆr + ˆc)

√

where δ := (cid:0)1 −
ˆr := maxt∈[T ],i∈[n] rt

, α := 1 − exp

β(cid:1)2
− (cid:0)1 −
i and ˆc := maxt∈[T ],i∈[n] |ct
i|.

(cid:16)

√

β(cid:1)2(cid:17)

,

Proof. Let α := 1 − e−δ. By Theorem 5.9 we have, for
all t ∈ [T ], that E (µt(X t)) ≥ −ht(ωt). By Lemma 5.19
α,δ(S) ≤ −ht(φ). Hence
we have, for all t ∈ [T ], that ˆµt
α,δ(S) − E (µt(X t)) ≤ ht(ωt) − ht(φ).
we have that ˆµt
By Theorem 5.18 we than have:

T
X

t=1

(cid:0)ˆµt

α,δ(S) − E (cid:0)µt(X t)(cid:1)(cid:1)

T
X

(ht(ωt) − ht(φ))

≤

t=1
√

≤n

2T δ(ˆr + ˆc)

Rearranging gives us:

T
X

t=1

E (cid:0)µt(X t)(cid:1) ≥

T
X

t=1

√

ˆµt

α,δ(S) − n

2T δ(ˆr + ˆc)

T
X

t=1

max
i∈X

rt
i −

X

i∈X

ci

An example of the problem is as follows. There are
n sites and T users, all located in some metric space.
We have to choose a set X of sites to open a facility
on. Opening a facility on site i costs us ci. Each user
pays us a reward based on how near it is to the closest
open facility. If the nearest open facility to user t is
at site i then user t rewards us rt
i. The objective is to
maximise the total proﬁt.

In our online variant of the (inverted) facility location
problem, learning proceeds in trials. On trial t:

1. For all sites i, the cost, ct

i of opening a facility on

site i is revealed to the learner.

2. The learner chooses a set X t of sites in which to

open facilities on.

3. User t requests the use of a facility, revealing rt

to the learner.

4. Learner incurs proﬁt: maxi∈X t rt

i − P

i∈X t ct
i

The objective is to maximise the cumulative proﬁt.
Note that this is the special case of our problem when,
for all i ∈ [n] and t ∈ [T ] we have zi = 0 and ct
i ≥ 0.
Given some set S the expected cumulative proﬁt of
MaxHedge is then bounded below by:

T
X

t=1

(1 − 1/e) max
i∈S

rt
i −

ct
i

X

i∈S

!

√

− n

2T (ˆr + ˆc)

6.2 Knapsack Median Problem

The (inverted) knapsack median problem is deﬁned by a
vector z ∈ P and vectors r1, r2, · · · , rT ∈ P. A feasible
solution is any X ⊆ [n] with P
i∈X zi ≤ 1. The aim is
to maximise the objective function PT

t=1 maxi∈X rt
i.

An example of the problem is as follows. There are n
sites and T users, all located in some metric space. We

 
MaxHedge: Maximising a Maximum Online

have to choose a set X of sites to open a facility on.
Opening a facility on site i has a fee of zi and we have
a budget of 1 to spend on opening facilities. Each user
pays us a reward based on how near it is to the closest
open facility. If the nearest open facility to user t is
at site i then user t rewards us rt
i. The objective is to
maximise the total reward.

In our online variant of the (inverted) knapsack median
problem, learning proceeds in trials. The learner has
knowledge of the fee zi for every site i. On trial t:

1. The learner chooses a set X t of sites in which to
i∈X t zi, can’t

open facilities on. The total fee, P
exceed 1.

2. User t requests the use of a facility, revealing rt

to the learner.

3. Learner incurs reward: maxi∈X t rt
i

The objective is to maximise the cumulative reward.
Note that this is the special case of our problem when,
for all i ∈ [n] and t ∈ [T ] we have ct
i = 0. Given some
set S with P
i∈S zi ≤ 1 the expected cumulative reward
of MaxHedge is then bounded below by:

(1 − exp (−δ))

T
X

t=1

max
i∈S

√

rt
i − δn

2T ˆr

where δ := (1 −

√

maxi∈[n] zi)2

6.3

0-1 Knapsack Problem

The knapsack problem is deﬁned by a vector z ∈ P and
a vector v ∈ P. A feasible solution is any X ⊆ [n] with
P
i∈X zi ≤ 1. The aim is to maximise the objective
function P

i∈X vi.

An example of the problem is as follows. We have n
items.
Item i has a value vi and a weight zi. The
objective is to place a set X ⊆ [n] of items in the
knapsack that maximises the total value of all items
in the knapsack subject to their total weight being no
greater than 1.

In our online variant of the knapsack problem, learning
proceeds in trials. The learner has knowledge of the
weight zi for every item i. On trial t:

1. The learner chooses a set X t of items to place in
i∈X t zi, can’t

the knapsack. The total weight, P
exceed 1.

2. For each item i, the value vt
is revealed to the learner
3. Learner incurs proﬁt: P

i∈X t vt
i

i , of item i on this trial

The objective is to maximise the cumulative proﬁt.
Note that this is the special case of our problem when,
for all i ∈ [n] and t ∈ [T ] we have rt
i ≤ 0 (not-
ing that ct
i∈S zi ≤ 1
the expected cumulative proﬁt of MaxHedge is then
bounded below by:

i = 0 and ct
i ). Given some set S with P

i = −vt

(1 − exp (−δ))

T
X

X

t=1

i∈S

√

vt
i − δn

2T ˆc

where δ := (1 −

√

maxi∈[n] zi)2

7 Conclusions and Ongoing Research

We presented and investigated in depth a novel on-
line framework, capable of encompassing several online
learning problems and capturing many practical prob-
lems in the real-world. The main challenge of the
general version of this problem lies in the fact that the
learner’s proﬁt depends on the maximum reward of the
selected actions, instead of the sum of all their rewards.
We proposed and rigorously analysed a very scalable
and eﬃcient learning strategy MaxHedge.

Current ongoing research includes:

• Deriving a lower bound on the achievable proﬁt.

• Complementing our results with a set of experi-

ments on synthetic and real-world datasets.

• Several real systems usually have a switching cost
for turning on/oﬀ services, which translates in
our framework to the cost incurred whenever an
action selected at any given trial is not selected in
the preceding one. This represents an interesting
direction for further research, which is certainly
motivated by practical problems.

Acknowledgements. This research was sponsored by
the U.S. Army Research Laboratory and the U.K. Min-
istry of Defence under Agreement Number W911NF-
16-3-0001. The views and conclusions contained in
this document are those of the authors and should
not be interpreted as representing the oﬃcial poli-
cies, either expressed or implied, of the U.S. Army
Research Laboratory, the U.S. Government, the U.K.
Ministry of Defence or the U.K. Government. The
U.S. and U.K. Governments are authorized to repro-
duce and distribute reprints for Government purposes
notwithstanding any copyright notation hereon. Fabio
Vitale acknowledges support from the ERC Starting
Grant “DMAP 680153”, the Google Focused Award
“ALL4AI”, and grant “Dipartimenti di Eccellenza 2018-
2022”, awarded to the Department of Computer Science
of Sapienza University.

Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

References

[1] M. Charikar, S. Guha, E. Tardos, D.B. Shmoys. A
constant–factor approximation algorithm for the k–
median problem. In ACM Symposium on Theory
of Computing (STOC), pp. 1–10, ACM (1999).

[2] M. Charikar and S. Guha. Improved combinatorial
algorithms for the facility location and k–median
problems. In IEEE Foundations of Computer Sci-
ence, pages 378–388, 1999.

[3] L. Chen, H. Hassani, A. Karbasi. Online Continu-
ous Submodular Maximization. In International
Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2018.

[4] G. Cornuejols, G. L. Nemhauser, and L. A. Wolsey.
The uncapacitated facility location problem. In
Pitu B. Mirchandani and Richard L. Francis, ed-
itors, Discrete Location Theory, pages 119–171.
John Wiley and Son, Inc., New York, 1990.

[5] M. Cygan, A. Czumaj, M. Mucha, P. Sankowski.
Online Facility Location with Deletions. In Annual
European Symposium on Algorithms, ESA 2018.

[6] G.B. Dantzig. Discrete variable extremum prob-
lems, In Operations Research 5 (1957) 266–277.

[7] Y. Freund and R. E. Schapire. A decision-theoretic
generalization of on-line learning and an applica-
tion to boosting. In Journal of Computer and
System Sciences, 55:119–139, 1997.

[8] T. Fujita, K. Hatano, E Takimoto Combinatorial
Online Prediction via Metarounding. In Algorith-
mic Learning Theory (2013), 68-82,

[9] D. Golovin, A. Krause, M. Streeter. Online
Submodular Maximization under a Matroid Con-
straint with Application to Learning Assignments
In Technical report, arXiv, 2014.

[10] K. Jain and V. Vazirani. Approximation algo-
rithms for metric facility location and k–median
problems using the primal–dual schema and La-
grangian relaxation. In J. ACM, 48(2):274–296,
2001.

[11] A. Kalai and S. Vempala. Eﬃcient algorithms for
online decision problems, In Journal of Computer
and System Sciences, vol. 71, no. 3, pp. 291–307,
2005.

[12] S. Kakade, A. Kalai, and K. Ligett. Playing games
with approximation algorithms. In ACM Sympo-
sium on the Theory of Computing, pages 546-555,
STOC 2007.

[13] W.M. Koolen, M.K. Warmuth, J. Kivinen. Hedg-
ing structured concepts. In Conference on Learn-
ing Theory, Omnipress, 2010, pp. 239-254.

[14] K. C. Kiwiel. Breakpoint searching algorithms for
the continuous quadratic knapsack problem. In
Math. Program, 112(2): 473-491 (2008).

[15] A. Kumar. Constant factor approximation algo-
rithm for the knapsack median problem. In ACM–
SIAM Symposium on Discrete Algorithms (SODA),
pp. 824–832, SIAM (2012).

[16] E. Hazan, and S. Kale. Online submodular min-
imization. In Journal of Machine Learning Re-
search (JMLR), 2012.

[17] D. P. Helmbold and S. V. N. Vishwanathan. On-
line Learning of Combinatorial Objects via Ex-
tended Formulation. In International Conference
on Algorithmic Learning Theory, ALT 2018.

[18] N. Laoutaris, G. Smaragdakis, K. Oikonomou, I.
Stavrakakis, A. Bestavros. Distributed Placement
of Service Facilities in Large–Scale Networks. In
IEEE INFOCOM, pages 2144–2152, 2007.

[19] S. Martello, P. Toth. An upper bound for the zero–
one knapsack problem and a branch and bound
algorithm, In European Journal of Operational
Research 1 (1977) 169–175.

[20] A. Meyerson. Online Facility Location. In IEEE
Symposium on Foundations of Computer Science,
FOCS 2001.

[21] D. Shmoys, E. Tardos and K. Aardal. Approxima-
tion algorithms for facility location problems. In
ACM Symposium on Theory of Computing (STOC
1997), pages 265–274, ACM Press, 1997.

[22] M. Streeter and D. Golovin. An Online Algorithm
for Maximizing Submodular Functions. In Confer-
ence on Neural Information Processing Systems,
NIPS 2008.

[23] H. Yu, M. J. Neely, X. Wei. Online Convex Opti-
mization with Stochastic Constraints. In Confer-
ence on Neural Information Processing Systems,
NIPS 2017.

[24] M. Zinkevich. Online convex programming and
generalized inﬁnitesimal gradient ascent. In Inter-
national Conference on Machine Learning, 2003.

[25] S. Pasteris, S. Wang, M. Herbster, T. He. Service
Placement with Provable Guarantees in Hetero-
geneous Edge Computing Systems. To appear in
IEEE INFOCOM, 2019.

MaxHedge: Maximising a Maximum Online

A Missing Proofs

Proof of Theorem 4.2

Proof. Deﬁne z0 := 0. We have X t = {ξt
ζ t
q + 1} \ {0} so:

q,k : q ∈ Γ, k ≤

X

i∈X t

zi ≤

=

≤

=

≤

X

q+1

q∈Γ,k≤ζt
X
X

q∈Γ
X

q+1

k≤ζt
X

zξt

q,k

zξt

q,k

τ q−1β

k≤ζt

q+1

(ζ t

q + 1)τ q−1β

(δπt

q + 1)τ q−1β

q∈Γ
X

q∈Γ
X

q∈Γ

X

q∈Γ
∞
X

= β

≤ β

q=1
β
1 − τ

β
1 − τ

β
1 − τ

β
1 − τ

β
1 − τ

β
1 − τ

=

=

=

≤

≤

≤

τ q−1 + βδ

τ q−1 + βδ

qτ q−1
πt

qτ q−1
πt

X

q∈Γ

X

q∈Γ

+ βδ

X

q∈Γ

qτ q−1
πt

X

X

+ βδ

i τ q−1
ωt

q∈Γ

i∈Ωq

X

X

+ δ

i τ −1(τ qβ)
ωt

q∈Γ

i∈Ωq

X

X

q∈Γ

i∈Ωq

X

X

i τ −1zi
ωt

ωt

i zi

q∈Γ

i∈Ωq

X

i∈[n]

ωt

i zi

+ δ

+

+

δ
τ

δ
τ

δ
τ

β
1 − τ

+
≤
= pβ + (1 − pβ)
= 1

|D| = 0 as then P (ED) = 0 = 1 − 1 = 1 − exp(0) =
1 − exp (cid:0)− P

(cid:1).

d∈D ρd

Now suppose the inductive hypothesis holds for |D| = l
for some l. We now show that it holds for |D| = l + 1
which will complete the proof. To show this choose
some d ∈ D and set D0 = D \ {d} We have:

P (¬ED) = P (¬ED0 ∧ ¬Ed)
= P (¬ED0) P (¬Ed)
= P (¬ED0) (1 − P (Ed))
= P (¬ED0) (1 − ρd)
≤ P (¬ED0) exp(−ρd)
≤ (1 − P (ED0)) exp(−ρd)

(1)

≤ exp

−

X

d∈D0

!

ρd

exp(−ρd)

(2)

= exp

−

!

X

d∈D

ρd

Where Equation 1 is due to the independence of the
draws and Equation 2 is from the inductive hypothesis
(noting |D0| = l). We now have

P (ED) = 1 − P (¬ED)

≥ 1 − exp

−

!

X

d∈D

ρd

(3)

(4)

which proves the inductive hypothesis.

Proof of Theorem 5.2

Proof. Note that the event Z ∩X t 6= ∅ happens if either
q} are in Z. For every q ∈ N and
of {ξt
k ∈ [ζ t

q,k : q ∈ N, k ≤ ζ t
q] we have:

P (cid:0)ξt

q,k ∈ Z(cid:1) =

X

P (cid:0)ξt

q,k = i(cid:1)

i∈Z∩Ωq

X

=

i∈Z∩Ωq

ωt
i
πt
q

and for k = ζ t

q + 1 we similarly have:

P (cid:0)ξt

q,k ∈ Z(cid:1) = (δπt

q − bδπt

qc)

X

i∈Z∩Ωq

ωt
i
πt
q

Proof of Lemma 5.1

Proof. Given some d ∈ D let Ed be the event that
E happens on draw d. Given some set D0 ⊆ D let
ED0 be the probability that E happens on either of
the draws in D0. We prove the result by induction
on |D|. The inductive hypothesis clearly holds for

So letting ρ(q,k) := P

(cid:16)

ξt
q,k ∈ Z

(cid:17)

we have:

ζt
q+1
X

k=1
ζt
q
X

ρ(q,k)

X

k=1

i∈Z∩Ωq

=

ωt
i
πt
q

+ (δπt

q − bδπt

qc)

X

i∈Z∩Ωq

ωt
i
πt
q

 
 
 
Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

X

=ζ t
q

i∈Z∩Ωq

ωt
i
πt
q

+ (δπt

q − bδπt

qc)

=(ζ t

q + δπt

q − bδπt

qc)

X

i∈Z∩Ωq

ωt
i
πt
q

ωt
i
πt
q

X

=δπt
q

=δ

i∈Z∩Ωq
X
ωt
i

i∈Z∩Ωq

X

i∈Z∩Ωq

ωt
i
πt
q

i=1 ct−

Pn
i (1 − exp(−δγi)) is convex. Since also the func-
tion ct+ · γ is convex we then have that ht is a positive
sum of convex functions and is therefore convex.

Proof of Lemma 5.6

Proof. Let l := min{j ∈ [n] : σ(t, j) ∈ X t}. Note that
rt
σ(t,l) = maxi∈X t rt
i.
From the deﬁnition of l we have:

So, plugging into Lemma 5.1 with D := {(q, k) : q ∈
N, k ∈ [ζ t

q + 1]}, we get:

• For all j < l, I (∃k ≤ j : σ(t, k) ∈ X t) = 0

• For all j ≥ l, I (∃k ≤ j : σ(t, k) ∈ X t) = 1

P (cid:0)Z ∩ X t 6= ∅(cid:1) = P (cid:0)∃i ∈ Z : i ∈ X t(cid:1)
!

≥ 1 − exp

−



= 1 − exp

−

X

ρd

d∈D

∞
X

ζt
q+1
X

q=1

k=1



ρ(q,k)





= 1 − exp

−δ

= 1 − exp

−δ



ωt
i



∞
X

q=1

X

i∈Z

X

i∈Z∩Ωq
!

ωt
i

Proof of Theorem 5.3

Proof. From Theorem 5.2 we have P (i ∈ X t) =
i ). Choosing q ∈ N
P ({i} ∩ X t 6= ∅) ≥ 1 − exp(−δωt
such that i ∈ Ωq we also have:

P (cid:0)i ∈ X t(cid:1) ≤

ζt
q+1
X

P (cid:0)ξt

q,k = i(cid:1)

q + (δπt

q − bδπt

qc)ωt

i /πt
q

k=1
i /πt
qωt
= ζ t
i /πt
qωt
= δπt
q
= δωt
i

Proof of Theorem 5.7

Proof. For all j ∈ [n − 1] the function δ Pj
k=1 γσ(t,k)
is concave and the function (1 − exp(−x)) is concave
and monotonic increasing which implies their combina-
−δ Pj
k=1 γσ(t,k)
), is concave. Hence,
tion, (1 − exp
(cid:17)
≥ 0, f t
as
j is concave. Similarly,
as all components of ct− are negative, we have that

σ(t,j) − rt
rt

σ(t,j+1)

(cid:16)

(cid:17)

(cid:16)

This implies that:

n
X

(cid:16)

j=1
n
X

(cid:16)

j=l

=

rt
σ(t,j) − rt

σ(t,j+1)

σ(t,j) − rt
rt

σ(t,j+1)

(cid:17)

(cid:17)

I (cid:0)∃k ≤ j : σ(t, k) ∈ X t(cid:1)

=rt

σ(t,l)
= max
i∈X t

rt
i

Proof of Theorem 5.9

Proof. For all j ∈ [n], Theorem 5.2 with Z := {σ(t, k) :
k ≤ j} implies that:

P (cid:0)∃k ≤ j : σ(t, k) ∈ X t(cid:1)
=P (cid:0){σ(t, k) : k ≤ j} ∩ X t 6= ∅(cid:1)

≥1 − exp

−δ

!

ωt

σ(t,k)

j
X

k=1

Lemma 5.8 then gives us:

(cid:19)

(cid:18)

E

max
i∈X t

rt
i

≥

=

n
X

(cid:16)

σ(t,j) − rt
rt

σ(t,j+1)

(cid:17)

1 − exp

−δ

j=1
n
X

j=1

f t
j (ωt)

!!

j
X

k=1

ωt

σ(t,k)

By Theorem 5.3 we also have:

X

P (cid:0)i ∈ X t(cid:1)

ct
i

≤

i<0

i∈[n]:ct
X

i∈[n]:ct

i<0

i(1 − exp(−δωt
ct

i ))

 
 
 
 
 
MaxHedge: Maximising a Maximum Online

and:

X

=

i∈[n]

ct−
i (1 − exp(−δωt

i ))

X

P (cid:0)i ∈ X t(cid:1)

ct
i

i>0

i∈[n]:ct
X

iδωt
ct
i

i∈[n]:ct
X

i>0
ct+
i δωt
i

i∈[n]

≤

=

=δhct+, ωti

so:

n
X

i=1

P (cid:0)i ∈ X t(cid:1) ≤ δhct+, ωti+

ct
i

X

i∈[n]

ct−
i (1−exp(−δωt

i ))

Hence, we have:

E (cid:0)µt(X t)(cid:1)

!

X

ct
i

i∈X t
n
X

i I (cid:0)i ∈ X t(cid:1)
ct

!

=E

max
i∈X t

rt
i −

=E

max
i∈X t

rt
i −

(cid:18)

(cid:18)

=E

=E

max
i∈X t

rt
i

max
i∈X t

rt
i

(cid:19)

(cid:19)

i=1
n
X

−

i=1
n
X

−

j (ωt) −
f t

i=1
n
X

ct
i

i=1

P (cid:0)i ∈ X t(cid:1)

≥

≥

n
X

j=1
n
X

j=1

E (cid:0)I (cid:0)i ∈ X t(cid:1)(cid:1)

(5)

P (cid:0)i ∈ X t(cid:1)

ct
i

ct
i

j (ωt) − δhct+, ωti −
f t

X

i∈[n]

ct−
i (1 − exp(−δωt

i ))

= − ht(ωt)

where Equation 5 is due to linearity of expectation.

Proof of Theorem 5.11

Proof. Suppose we have some l ∈ [n]. For j < l we
j (ωt) = 0 and for j ≥ l we have
have ∂σ(t,l)f t

∂σ(t,l)f t

j (ωt)

(cid:16)

(cid:16)

=δ

=δ

σ(t,j) − rt
rt

σ(t,j+1)

σ(t,j) − rt
rt

σ(t,j+1)

(cid:17)

(cid:17)

exp

−δ

!

ωt

σ(t,k)

j
X

k=1

(cid:15)t
j

This implies that:

∂σ(t,l)

n
X

j=1

j (ωt) = δ
f t

n
X

(cid:16)

j=l

σ(t,j) − rt
rt

σ(t,j+1)

(cid:17)

(cid:15)t
j

= δλt
l

which gives us

∂σ(t,l)ht(ωt)

σ(t,j) + δct−

σ(t,j) exp(−δωt

σ(t,j)) − δλt
l

=δct+
=gt

σ(t,l)

Proof of Theorem 5.18

Proof. From Lemma 5.16 we have:

T
X

t=1

ηtkgtk2 ≤

nps/(2t)

T
X

t=1

= nps/2

T
X

t=1

1
√
t

≤ nps/2

1 +

Z T

t=1

!

1
√
t

√

T − 1)

= nps/2(2
≤ 2npsT /2
So using Lemma 5.17 and plugging into Theorem 5.14
we have:
T
X

√

2sT + npsT /2

(ht(ωt) − ht(φ)) ≤

1
2

n

t=1

= 2npsT /2
√
2sT
= n

By Deﬁnition 5.15 and Lemma 5.12 we have s ≤ δ2(ˆr +
ˆc)2. Plugging this into the above gives us the result.

B When maxi∈[n] zi is large

When maxi∈[n] zi is large we construct X t from ωt
the following way:

i in

1. Deﬁne A = {i ∈ [n] : zi ∈ [1/2, 1]}
2. Deﬁne at := P

i∈A ωt

i /4

3. Flip a biased coin with probability of heads equal

to at

4. If heads sample i from A with probability ωt

i /(4at)

and set X t = {i}

5. If tails deﬁne β := 1/2 and run Algorithm 1.

We defer the analysis.

 
 
 
 
Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster

continuous submodular. We can use a slight modiﬁca-
tion of our method of computing the gradient of ht to
compute the gradient of ˆht in quasi-linear time. With
this in hand we can then plug the gradient into one of
the algorithms of [3]:

• The ﬁrst algorithm requires the submodular func-
tion to be monotone and non-negative whereas
ours, in general, is neither. Even if our expected
proﬁt was monotone and non-negative, for their
T their
algorithm to have a regret linear in
√
per-trial time complexity bound becomes Ω(n
T )
which is much larger than ours.

√

• In the gradient ascent based second algorithm,
the submodular function also needs to be mono-
tone and non-negative but the cost vector can be
separated from the reward vector in the analysis
allowing us to use the expected maximum reward
which is monotone non-negative. The result of this
analysis is similar to ours expect that for them
α = 1/2 instead of the better α = 1 − e−1 that
we have (NB: α is the discount on the comparator
selection S).

C On the Knapsack Problem and

Component Hedge

In this section we show that Component Hedge can-
not solve the online knapsack problem special case in
polynomial time unless P = N P .

The Component Hedge algorithm described in [13]
requires a set of concepts X ⊆ {0, 1}n. In the knapsack
problem, X is the subset of all x that ﬁt in the knapsack,
i.e. those x ∈ {0, 1}d with hx, zi ≤ 1. For Component
Hedge, the convex hull H of X must have a number
of constraints polynomial in n and that any y ∈ H
can, in polynomial time, be decomposed into a convex
combination of n + 1 concepts. We now show that if
this is true then we can solve the knapsack problem in
polynomial time:

Let c be the vector deﬁning the objective function
of the knapsack problem (i.e. we seek the x ∈ X
that maximises hc, xi). Now, since H has polynomial
constraints we can eﬃciently choose the maximiser
of hc, yi (for y ∈ H) via linear programming. Let
this maximiser be y. Now decompose y into a convex
combination, Pn+1
i=1 (mixi), of n + 1 concepts xi (where
Pn+1
i=1 mi = 1). Now suppose, for contradiction, that
hc, xji < hc, yi for some j with mj > 0. Without loss
of generality let j = n + 1.

Then, since for all x ∈ X with have hc, xi ≤ hc, yi
(as X ⊂ H) we have: hc, yi = hc, (Pn+1
i=1 mixi)i =
Pn+1
i=1 mihc, xii + mn+1hc, xn+1i <
Pn
i=1 mihc, yi +
mn+1hc, yi = Pn+1
i=1 mihc, yi = hc, yi which is a con-
tradiction.

i=1 mihc, xii = Pn
i=1 mihc, xii + mn+1hc, yi ≤ Pn

Hence, for all j with mj > 0 we have hc, xji = hc, yi
so the maximising x is found in polynomial time. This
contradicts the assumption that P 6= N P .

D Using Submodular Maximisation

We consider the facility location special case (i.e, z := 0
and all costs are positive). In this case it is possible to
obtain our bound by selecting, on a trial t, each action i
with probability ωt
i . We can then use some deﬁnitions
in our paper to deﬁne the following functions on a
trial t:

• For all j ∈ [n] deﬁne ˆf t

(cid:16)

σ(t,j) − rt
rt

σ(t,j+1)

(cid:17) (cid:16)

j (γ) :=
1 − Qj

k=1

(cid:0)1 − γσ(t,k)

(cid:1)(cid:17)

• Deﬁne ˆht(γ) = Pn

j=1

ˆf t
j (γ) − hct, γi

We can then use the arguments in our paper to write
the expected proﬁt as: E (µt(X t)) = ˆht(ωt) which is

