s-LIME: Reconciling Locality and Fidelity in Linear
Explanations
Romaric Gaudel, Luis Galárraga, Julien Delaunay, Laurence Rozé, Vaishnavi

Bhargava

To cite this version:

s-LIME:
Romaric Gaudel, Luis Galárraga, Julien Delaunay, Laurence Rozé, Vaishnavi Bhargava.
Reconciling Locality and Fidelity in Linear Explanations. IDA 2022 - Symposium on Intelligent Data
Analysis, Apr 2022, Rennes, France. pp.1-13. ￿hal-03741042￿

HAL Id: hal-03741042

https://inria.hal.science/hal-03741042

Submitted on 2 Aug 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

S-LIME: Reconciling Locality and Fidelity in Linear
Explanations

Romaric Gaudel1, Luis Gal´arraga2, Julien Delaunay2, Laurence Roz´e3, Vaishnavi
Bhargava4

1 (corresponding author) Univ. Rennes, Ensai, CNRS, CREST, Rennes, France
romaric.gaudel@ensai.fr
2 Univ. Rennes, Inria, Irisa, France
{julien.delaunay,luis.galarraga}@inria.fr
3 Univ. Rennes, Insa, Inria, Irisa, Rennes, France laurence.roze@insa-rennes.fr
4 (during research) Inria/Irisa, Rennes, France vaishnavi.bhargava2605@gmail.com

Abstract. The beneﬁt of locality is one of the major premises of LIME, one of
the most prominent methods to explain black-box machine learning models. This
emphasis relies on the postulate that the more locally we look at the vicinity of an
instance, the simpler the black-box model becomes, and the more accurately we
can mimic it with a linear surrogate. As logical as this seems, our ﬁndings suggest
that, with the current design of LIME, the surrogate model may degenerate when
the explanation is too local, namely, when the bandwidth parameter σ tends to
zero. Based on this observation, the contribution of this paper is twofold. Firstly,
we study the impact of both the bandwidth and the training vicinity on the ﬁdelity
and semantics of LIME explanations. Secondly, and based on our ﬁndings, we
propose S-LIME, an extension of LIME that reconciles ﬁdelity and locality.

Keywords: Explainable AI · Interpretability

1

Introduction

The pervasiveness of complex automatic decision-making nowadays has raised multiple
concerns about the implications of AI for the values of fairness, trust, transparency, and
privacy [2, 4, 13]. These concerns have propelled a plethora of work in explainable AI, a
domain concerned with the design of models that can provide high-level comprehensive
explanations for their answers. These models can be either explainable-by-design, or
rely on external modules that compute explanations a posteriori. This need for post-
hoc explainability is particularly compelling for sophisticated machine learning models,
e.g., neural networks, whose logic is perceived as a black box by lay users.

One of the most prominent modules to compute post-hoc explanations for black-
box supervised ML models is LIME [15]. This approach builds upon the notion of local
feature attribution via a linear surrogate. Feature attribution means that the explanation
quantiﬁes the contribution of a set of features to the black box’s answer. This allows
users to build a ranking of the features that play the biggest role in the model’s logic. We
say the explanation is local because it only holds for a target instance and its vicinity.

2

Gaudel et al.

By focusing on a region of the feature space, LIME reduces the complexity of the black
box and can approximate it using a surrogate sparse linear function whose coefﬁcients
constitute the feature attribution scores of the explanation. To learn this surrogate, LIME
constructs a training set by generating artiﬁcial instances – called neighbors – around
the target instance, and labeling them using the black box. The neighbors may not lie in
the original feature space, but rather on a surrogate space that is meaningful to humans,
e.g., image segments instead of pixels for images. The neighbors are weighted using an
exponential kernel that depends on the distance to the target and a bandwidth parameter
σ ∈ R+. The weighting process controls the level of locality of the explanation: the
smaller σ is, the more local the explanation becomes as closer neighbors are weighted
higher than farther ones. More locality also implies focusing on a smaller region where
the black box is presumably easier to approximate.

As logical as this sounds, our experiments suggest that small values of σ can yield
unfaithful or even trivially empty explanations. This counter-intuitive result has thus
motivated this work, which brings two contributions: (a) A study of the impact of the
bandwidth and the training vicinity on the ﬁdelity and semantics of LIME, namely the
meaning of the feature attribution scores5; and (b) S-LIME, an extension of LIME that
can solve the locality-ﬁdelity paradox.

This paper is structured as follows. In Section 2 we introduce some background
concepts and notations. We elaborate on our contributions in Sections 3 and 4. Section 5
presents an experimental evaluation of S-LIME. In Section 6 we survey the state of the
art. Section 7 concludes the paper.

2 Preliminaries

Black Boxes and Linear Surrogates. We assume our black box is a classiﬁcation func-
tion f : Rd → R (d ∈ Z+) that predicts the probability that a target instance x ∈ Rd
belongs to a given class. We denote by x[i] the i-th feature of x. Conversely, the ex-
planation g : R ˆd → R ( ˆd ∈ Z+) is a linear surrogate function that approximates f in
the locality of x, i.e., g(ˆx) = ˆα0 + (cid:80)
1≤i≤ ˆd ˆαi ˆx[i]. Note that g may be deﬁned on a
surrogate space different from f ’s. This implies the existence of a conversion function
ηx : R ˆd → Rd from the surrogate to the original space.

LIME. In [15], the authors propose a model-agnostic method to compute local explana-
tions for ML models in the form of sparse linear surrogates. LIME learns an explanation
g for a black box f and an instance x by solving the following minimization problem:

g = argmin

g∈G: (cid:107) ˆα(cid:107)0(cid:54)k

Lx(f, g)

(1)

In other words, the surrogate g is chosen such that it minimizes the error Lx w.r.t. the
answers of f on a neighborhood X around a target instance x. To keep the explanation
meaningful to humans, LIME restricts itself to surrogate functions g with less than k
non-zero parameters, where k is a user-conﬁgurable hyper-parameter set by default to
6. LIME does not assume access to the training data of the black box6, therefore the

5 By semantics of LIME, we mean the information carried by the feature attribution scores.
6 The exception to this rule is its implementation for tabular data.

S-LIME: Reconciling Locality and Fidelity in Linear Explanations

3

neighbors z ∈ X take the form z = ηx(ˆz) where ˆz ∈ ˆX ⊆ {0, 1} ˆd is a synthetic in-
stance that lies on a binary space. This space is interpretated as the presence or absence
of features of the target x, so that x = ηx(ˆx) with ˆx = 1 ˆd. The neighbors in ˆX are ob-
tained by toggling off bits in x’s binary representation ˆx. When a bit is set to zero in the
surrogate space, the conversion function ηx must map the resulting vector to the orig-
inal space. For images, this can be achieved by replacing the toggled-off super-pixels
with a baseline monochrome segment or with a patch from another image [16]. LIME
weighs the neighbors in ˆX according to a kernel function πσ
x (based on a distance D
and a bandwidth hyper-parameter σ ∈ R+) on the surrogate space, that is,

Lx(f, g) =

(cid:88)

ˆz∈ ˆX

x (ˆz)(f (ηx(ˆz)) − g(ˆz))2, with πσ
πσ

x (ˆz) = exp (cid:0)−D(ˆx,ˆz)2/σ2(cid:1) .

The hyper-parameter σ controls the locality of the explanation so that smaller values
give more weight to the instances that lie close to ˆx, i.e., those instances with fewer
toggled-off bits. LIME does not make any assumptions about the inner-workings of f ,
however the distance D and the conversion functions ηx depend on f ’s original space,
which at the same time depends on the instances’ data type.

Quality Metrics. The quality of the local surrogate g is evaluated in terms of its ﬁdelity,
which can be measured via the surrogate’s adherence to the black box f in the vicinity
of x. Adherence is usually measured via the coefﬁcient of determination R2 [5, 17,
20]. The R2 score measures the similarity between the predictions of both functions,
compared to the variance of the black-box prediction. This coefﬁcient lies in (−∞, 1],
where R2 = 1 means g ﬁts f perfectly and R2 = 0 (respectively R2 < 0) implies that
g is as accurate as (resp. less accurate than) the best constant model.
When a gold standard set Ff (x) of important features is available, we can also calculate
ﬁdelity as the agreement between the explanation and the gold standard. This can be
quantiﬁed via metrics such as recall [15], precision, or coverage [8]. Assuming the
surrogate and the original feature spaces are identical, if the explanation g for the target
instance x reports features Fg(x) as the most important, the recall and precision of g
are respectively |Ff (x)∩Fg(x)|
. Coverage can be used for data types
where segments, i.e., conglomerates of contiguous features, are more meaningful to
humans than individual features. Examples are time series and images. For those cases,
the coverage is the proportion of the gold standard regions that overlap with the regions
reported by the surrogate. Further specialized metrics have been proposed to measure
the ﬁdelity of pixel attribution explanations for image classiﬁers [9].

and |Ff (x)∩Fg(x)|

|Ff (x)|

|Fg(x)|

3 Locality vs. Fidelity

In this section we study the impact of two important elements of LIME on the ﬁdelity
and semantics of explanations, namely the bandwidth σ and the neighborhood ˆX .

4

Gaudel et al.

(a) σ = 0.1

(b) σ = 0.75

(c) σ = 100

Fig. 1: LIME explanations for three different bandwidths on the same instance of the
wine dataset (k = 4).

(a) R2 vs. σ

(b) σ = 0.1

(c) σ = 100

Fig. 2: Left: Impact of the bandwidth σ on the R2 score of LIME for two instances of
the wine dataset. Right: Distribution of the neighborhood weights for instance 2.

3.1 The Paradox of Small Bandwidth

We illustrate the impact of σ on the output of the tabular variant of LIME7, which
we use to explain a random forest classiﬁer trained on the UCI wine dataset8. Tabular
LIME sets σ = 0.75 with no further explanation. Changing σ can, however, drastically
change the resulting explanation as depicted in Figure 1. In particular, LIME computes
null attribution coefﬁcients when σ = 0.1. Changing σ from 0.75 to 100 rearranges the
attribution ranking of the features.

To investigate the cause of this instability, we measure the adherence of the surro-
gate in ˆX as σ varies for all the test instances of the dataset. We plot the results for two
instances in Figure 2a, where instance 2 is the example explained in Figure 1.

We recall that the R2 score is calculated as 1 − vr(g)/v(f ), where vr(g) is the residual
sum of squares of the surrogate g and vr(f ) is the total sum of squares of f ’s answers.
This means that the surrogate accounts for no more than 60% of the variability of the
black box in ˆX . The dashed regions of the curves indicate that the surrogate model has
degenerated into a set of zero weights. This points out a counter-intuitive phenomenon:
higher locality – achieved by making σ small – yields poor explanations. We also ob-
serve that the R2 may not increase monotonically with σ. Based on these observations,
we devise two research questions that drive our contribution: (i) Why do seem locality
and ﬁdelity in opposition?, and (ii) what makes a good LIME explanation?

7 The discretization is off, hence the classiﬁer and the explanation operate in the same space.
8 https://archive.ics.uci.edu/ml/datasets/wine

S-LIME: Reconciling Locality and Fidelity in Linear Explanations

5

(a) Logistic Regression

(b) σ = 0.5

(c) σ = 1.0

Fig. 3: Left: A logistic regression classiﬁer and a neighborhood (denoted by + marks)
generated on a 2D discrete surrogate space. Center and right: Two LIME explanations.
The gradient of each of these functions at the target example (denoted by the * mark)
is orthogonal to the border between white area and black area. The explanation in the
middle captures the black box’s gradient more faithfully.

3.2 Why do Seem Locality and Fidelity in Opposition?

We investigate the cause of this paradox by means of Figures 2b and 2c that depict the
distribution of weights for the neighbors of instance 2 for σ = 0.1 and σ = 100. In the
ﬁrst case, the LIME surrogate is a degenerated model that predicts a constant as hinted
by Figure 2a and its corresponding explanation in Figure 1a. Figure 2b tells us that the
bulk of the weights is concentrated on the target instance. Such a phenomenon leads to a
trivial training set. Even though locality is deﬁned in terms of the entire set of instances
in ˆX , almost all of them are dispensable because they do not have any inﬂuence when
learning the surrogate. The situation is less skewed for σ = 100 (Figure 2c), which
yields the non-trivial explanation in Figure 1c.

From this analysis we conclude that the selection of σ and the construction of ˆX

must go in hand. We thus propose a strategy to jointly select them in Section 4.

3.3 What Makes a Good LIME Explanation?

The human aspects of interpretability are beyond the scope of this paper; instead this
study is concerned with the quality and meaningfulness of explanations from a mathe-
matical point of view. As suggested by [6], LIME computes a scaled version of the gra-
dient ∇f for linear black boxes f . The scaling arises because the surrogate is learned
on a ﬁnite number of neighbors in a discrete space, and the scaling factor depends on x,
σ, ηx, and ˆX . We argue that in the absence of a reference instance (as in [12, 18, 19]),
explanations based on instantaneous gradients are meaningful and desirable because
their semantics are well-deﬁned: the surrogate gradient ˆ∇f (x) is the contribution of
each surrogate feature to f ’s change rate at point x. That said, LIME does not always
estimate ˆ∇f accurately as suggested by Figure 3. The ﬁgures show that the weights
associated to the neighbors may yield an estimation that differs largely from the black
box’s actual gradient in Figure 3a.

6

Gaudel et al.

Algorithm 1 S-LIME

applied to black-box function f at target instance x

Require: Conversion function ηx, distribution νσ on the surrogate space
Require: Number k of features in the explanation, number n of local examples
1: ˆX ←
2: return argming∈G: (cid:107) ˆα(cid:107)0(cid:54)k

, where ˆz(i) ∼ νσ for i = 1, . . . , n
(cid:80)

ˆz∈ ˆX (f (ηx(ˆz)), g(ˆz))2

ˆz(i) : i = 1, . . . , n

(cid:110)

(cid:111)

4

S-LIME

To tackle the locality-ﬁdelity paradox explained in Section 3.1, we introduce an exten-
sion of LIME, called S-LIME (Smoothed LIME), that we detailed in the following.

4.1 Generic Algorithm

LIME may compute degenerated explanations due to two main factors: (i) the discrete-
ness of the surrogate space, and (ii) the fact that instance generation and weighting are
decoupled. Indeed, LIME ﬁrst generates a discrete neighborhood ˆX (independently of
σ), and then weighs the instances in ˆX using πσ
x . In the extreme cases when σ tends to
zero, the weighting is concentrated on ˆx.

To prevent this skewed concentration of weights, we control the locality of the ex-
planation in a single step (see Algorithm 1). Hence, we deﬁne the neighbors in the
continuous space [0, 1] ˆd and populate ˆX with examples ˆz whose distance D to ˆx is of
the same magnitude as σ. Concretely, the neighborhood ˆX = {ˆz(1), . . . , ˆz(n)} consists
of n equally-weighted instances drawn independently from a distribution νσ. Such a
design decision enables g to approximate ˆ∇f when σ tends to zero, without hinder-
ing interpretability: g still combines the contributions of the surrogate features linearly,
and we can still confer an interpretable meaning to the neighbors as later explained in
Section 4.4. Moreover, this allows controlling locality via the bandwidth of the neigh-
borhood distribution, and not anymore through an a-posteriori weighting.

Note that S-LIME also requires the deﬁnition of new conversion functions ηx as ˆX
is now a subset of the continuous space [0, 1] ˆd instead of the discrete space {0, 1} ˆd. In
Section 4.4 we provide examples of proper distributions νσ and functions ηx for images,
time series, and tabular data.

4.2

S-LIME Subsumes LIME

Lemma 1. Let f be a function and x a target instance. There is a distribution νσ over
[0, 1] ˆd such that LIME and S-LIME are minimizing the same expected loss function.

Proof. LIME outputs a function g that minimizes the loss Lx(f, g) which is the residual
sum of squares of the examples drawn from a distribution ν. The expectation of this loss
function w.r.t. to a random neighborhood is Eˆz∼ν
. Re-
mark that ν is a distribution on the ﬁnite space {0, 1} ˆd, then ν = (cid:80)
where δ(ˆz) is the Dirac distribution at point ˆz, and wν(ˆz) is a positive real number.

x (ˆz) (f (ηx(ˆz)) − g(ˆz))2(cid:105)
πσ

ˆz∈{0,1} ˆd wν(ˆz)δ(ˆz),

(cid:104)

S-LIME: Reconciling Locality and Fidelity in Linear Explanations

7

Similarly, S-LIME returns the linear surrogate g that minimizes a loss with expecta-
(f (ηx(ˆz)) − g(ˆz))2(cid:105)
(cid:104)
. Let Z be (cid:80)
tion Eˆz∼νσ
x (ˆz)wν(ˆz). If we consider
S-LIME with generating distribution νσ = 1/Z (cid:80)
x (ˆz)wν(ˆz)δ(ˆz), then

ˆz∈{0,1} ˆd πσ
ˆz∈{0,1} ˆd πσ

Eˆz∼νσ

(cid:104)

(f (ηx(ˆz)) − g(ˆz))2(cid:105)

=

(cid:88)

ˆz∈{0,1} ˆd
1
Z

Eˆz∼ν

=

πσ
x (ˆz)wν(ˆz)
Z

(f (ηx(ˆz)) − g(ˆz))2

x (ˆz) (f (ηx(ˆz)) − g(ˆz))2(cid:105)
(cid:104)
πσ

,

which concludes the proof.

Remark 1. It follows from Lemma 1 that S-LIME may be used as a placeholder for
LIME. Still, the proposed distribution νσ is practical only when d is small, or when
νσ corresponds to a well-known distribution. Otherwise, storing the 2 ˆd coefﬁcients
πσ
x (ˆz)wν(ˆz) is unpractical. Anyway, we demonstrate in Section 5 that S-LIME with
a continuous distribution is more faithful than LIME.

4.3

S-LIME and the Gradient of the Black-Box Function

Let us assume the surrogate function f ◦ ηx to be differentiable at ˆx. Let us also denote
by ˆα the weights of the linear model returned by S-LIME when we drop the sparseness
constraint. Then for any family of continuous distributions νσ on [0, 1] ˆd, such that their
mass concentrates on ˆx when σ tends to zero, ˆα tends to the gradient ˆ∇f (x) of f ◦ ηx at
point ˆx. An example of such family of distributions is the set {N (cid:0)ˆx, σ2III(cid:1) , σ ∈ R+} of
Gaussian distributions centered at ˆx with variance σ2III, where III is the identity matrix.
This property has two main implications. First, while LIME degenerates as σ ap-
proaches zero, S-LIME remains well-deﬁned for any value of σ. Secondly, we know
what S-LIME is targeting when we look locally at ˆx: ˆ∇f (x).

Remark 2. There are settings for which surrogate gradients are meaningless: piece-wise
constant functions such as random forests. In such a scenario, S-LIME outputs a zero
gradient as soon as the bandwidth of the generating distribution is small enough. While
the weights returned by S-LIME are mathematically consistent for such kinds of mod-
els, they are useless as they carry on information that is too local. If that is the case,
users may pick a higher value for σ, or resort to a rule-based surrogate [16].

4.4

S-LIME Implementations

Let us now discuss examples of concrete distributions νσ and functions ηx. The gener-
ating distribution νσ is the same for image and time series datasets: the uniform distri-
bution on [1 − σ, 1] ˆd, with σ ∈ (0, 1]. As needed, this distribution concentrates around
the surrogate target ˆx = 1 ˆd when σ tends to zero.

In regards to the conversion function ηx, we recall that for both images [15] and
time series [8], LIME splits the original instance into ˆd contiguous regions, namely
super-pixels for images or fragments of ﬁxed size for time series. Those regions deﬁne

8

Gaudel et al.

the features of the surrogate space. Given a neighbor ˆz ∈ ˆX and a surrogate feature j,
we can project ˆz back to the original space by interpolating the original features of the
target x with a baseline x0, i.e., ηx(ˆz)[i] = (1 − ˆz[j])x0 + ˆz[j]x[i] for all the original
features i, i.e., pixels or time measures, covered by segment j. We set x0 = 0 in our
experiments, i.e., the interpolation is done w.r.t. a black image and a null time series.

Finally, for tabular data we consider one surrogate feature per original feature.
Therefore, the generating distribution νσ is the centered multivariate Gaussian distri-
bution with covariance σ2III, and the function ηx(ˆz) = x + ˆz.

Remark 3. The design of a proper distribution νσ and a proper function ηx requires the
black-box model to handle examples living in a continuous space. As a consequence,
S-LIME cannot be deﬁned for text data.

5 Experiments

We now show-case the impact of the bandwidth σ on the ﬁdelity of LIME and S-LIME
explanations. We ﬁrst detail our experimental setup and then elaborate on our ﬁndings.

5.1 Experimental Settings

Datasets and Black Boxes. We conduct our experiments on a variety of datasets, com-
prising Cifar10 [10] and MNIST [11] for image data, the FordA and StarlightCurves
time series datasets from the UEA & UCR Time Series Classiﬁcation Repository, and
the Compas and Diabetes datasets from the UCI Machine Learning Repository for tab-
ular data. We also consider a selection of black-box models, which may be smooth or
piece-wise constant, simple or complex, interpretable or not.

Protocol and Metrics. For each combination of dataset, model, and explanation mod-
ule, we compute the average value of the experimental metrics for different values of σ
on the test instances of the dataset. The experimental metrics were introduced in Sec-
tion 2: the R2 score for all models, and the precision/recall or the coverage for the
interpretable models, i.e., those for which a ground truth is available. All these metrics
take values either in (−∞, 1] or in [0, 1], and higher values denote higher ﬁdelity.

5.2

Impact of σ

To study the impact of σ on the ﬁdelity of the LIME and S-LIME explanations, we plot
the surrogate’s adherence on the StarlightCurves dataset for several black-box mod-
els all using 100 random shapelets as input features. The models include Learning
Shapelets (LS) [7], RESNET [21], Fast Shapelets (FS) [14], and a sparse logistic re-
gression (LR, with L1-regularization to enforce at most 10 features). The results are
depicted in Figure 4. We set k = 6 for the number of features in explanations [15].

We observe that very local S-LIME neighborhoods lead to higher adherence and
coverage, except for FS. This translates into more faithful explanations as σ approaches
zero, where LIME cannot deliver proper explanations. In contrast, LIME achieves higher

S-LIME: Reconciling Locality and Fidelity in Linear Explanations

9

(a) S-LIME on LS

(b) S-LIME on RES.

(c) S-LIME on LR

(d) S-LIME on FS

(e) LIME on LS

(f) LIME on RESNET

(g) LIME on LR

(h) LIME on FS

Fig. 4: R2 and coverage vs. σ on the StarlightCurves dataset. Each subplot corresponds
to a couple (explainer, dataset). The plotted results are averaged on the instances of the
test dataset. Recall that for S-LIME σ is deﬁned in (0, 1].

(a) S-LIME on RESNET

(b) S-LIME on LR

Fig. 5: R2 and coverage vs. σ on the StarlightCurves dataset. Each subplot corresponds
to a couple (explainer, dataset). Each curve corresponds to one target instance.

adherence and coverage for FS, because this model is a decision tree. Hence, the deci-
sion function is piece-wise constant and its gradient is zero almost every-where. When
σ is small enough, S-LIME recovers this gradient and returns an explanation with null
coefﬁcients, which has little practical value. That said, a wider locality can still yield a
more informative explanation.

We also remark that, for complex models, the best value for σ may depend on the
target instance. This is corroborated by Figure 5 that shows the disaggregated results
for 3 instances on RESNET, a deep neural network. We can observe that the adherence
is maximal when σ is equal 10−4, 3 × 10−3, and 2 × 10−2 respectively. On the other
hand, the same values of σ are optimal for all examples on a simpler LR model.

Finally, we highlight that the coverage peaks when the adherence is maximal both at
the instance (Figure 5b) and dataset level (Figures 4(cdgh)). This shows the pertinence
of the R2 score as metric to select the right level of locality.

5.3 Fidelity Analysis

Tables 1 and 2 show the average scores obtained by S-LIME and LIME when σ is se-
lected to maximize the aggregated adherence (R2 score) in the test instances of the ex-

1041021000.00.51.0R21041021000.00.51.0R21041021000.00.51.0R2 or coverageR2Coverage1041021000.00.51.0R2 or coverage1021011040.00.51.0R21021011040.00.51.0R21021011040.00.51.0R2 or coverage1021011040.00.51.0R2 or coverage1041031021011000.000.250.500.751.00R2T. i. 1T. i. 2T. i. 31041031021011000.000.250.500.751.00R2 or coverageT. i. 1T. i. 2T. i. 3R2Coverage10

Gaudel et al.

Table 1: Best average recall and precision, or coverage (std. in parentheses) on different
datasets and interpretable black-box classiﬁers.
Data type Dataset Model

S-LIME

LIME

Rec. or Cov. Precision Rec. or Cov. Precision

Timeseries FordA

LR on shapelets 0.87 (0.15)
0.51 (0.30)
Fast Shapelets
Starlight- LR on shapelets 0.81 (0.17)
0.68 (0.19)
Curves

Fast Shapelets

- (-)
- (-)
- (-)
- (-)

0.73 (0.17)
0.49 (0.27)
0.75 (0.17)
0.45 (0.15)

- (-)
- (-)
- (-)
- (-)

Tabular data Diabetes Logistic Reg.

Dec. Tree

Compas Logistic Reg.

Dec. Tree

1.00 (0.00) 1.00 (0.00) 0.88 (0.12) 0.88 (0.12)
0.95 (0.13) 0.81 (0.20) 0.94 (0.14) 0.80 (0.20)
1.00 (0.00) 1.00 (0.00) 0.52 (0.21) 0.52 (0.21)
0.66 (0.33) 0.25 (0.00) 0.65 (0.33) 0.33 (0.00)

Table 2: Best average R2 (std. in parentheses) on different datasets and black-box classi-
ﬁers. MLP stands for a neural network with one hidden layer composed of 100 neurons
and logistic sigmoid activation function. Column Int. indicates interpretable black-box
models ((cid:88)). FS, DT and RF are put aside as they are piecewise constant models.
Int. k S-LIME

Data type Model

k S-LIME

LIME

LIME

Images

MNIST

Cifar10

Alexnet
VGG16

10 0.80 (0.28) 0.58 (0.20) 10 0.84 (0.10) 0.55 (0.25)
10 0.56 (0.43) 0.57 (0.21) 10 0.69 (0.13) 0.50 (0.27)

Timeseries

FordA

StarlightCurves

Learning Shapelets
RESNET
LR on Shapelets

Fast Shapelets

6 0.84 (0.08) 0.57 (0.15) 6 0.92 (0.07) 0.70 (0.07)
6 0.73 (0.20) 0.10 (1.05) 6 0.87 (0.15) 0.44 (0.15)
(cid:88) 6 1.00 (0.01) 0.56 (0.13) 6 0.99 (0.02) 0.58 (0.12)
(cid:88) 6 0.15 (0.18) 0.19 (0.14) 6 0.25 (0.13) 0.19 (0.16)

Tabular data

Diabetes
Logistic Regression (cid:88) 4 1.00 (0.00) 0.99 (0.01) 11 1.00 (0.00) 0.42 (0.23)
4 0.97 (0.03) 0.72 (0.13) 6 0.79 (0.01) 0.31 (0.16)
MLP
(cid:88) 3 0.46 (0.09) 0.46 (0.10) 3 0.34 (0.00) 0.36 (0.00)
4 0.62 (0.03) 0.58 (0.12) 6 0.30 (0.01) 0.30 (0.02)

Decision Tree
Random Forest

Compas

perimental datasets. Table 1 shows recall, precision, and coverage for the interpretable
models, whereas Table 2 provides the R2 score for all models.

Firstly, we remark that S-LIME’s explanations are strictly more faithful than LIME’s
except for piecewise constant models (FS, DT, and RF). That said, this does not prevent
S-LIME from achieving higher adherence for such models on some datasets when we
look at a larger vicinity.

Secondly, the R2 score is a good proxy to predict the best neighborhood in terms of
recall, precision, or coverage. This is a strong result from an application point of view.

S-LIME: Reconciling Locality and Fidelity in Linear Explanations

11

Practitioners are mostly interested by the features that are actually used by the black-
box model. For cases where those actual features are unknown, the R2 score enables
the computation of faithful linear explanations that can identify the important features.

6 Related Work

Feature-attribution explanations. Methods such as DeepLIFT [18], Integrated Gradi-
ents (IG) [19], SHAP [12], or LIME [15] compute importance local attribution scores
for the features of a black-box ML model. Among those, SHAP and LIME are model-
agnostic and compute linear surrogates learned from artiﬁcial neighbors. Despite these
similarities, the semantics of their explanations are different as conﬁrmed by existing
studies [1]. While LIME approximates – often coarsely – the instantaneous gradient of
the black box w.r.t. the input features [6], SHAP computes – or rather approximates –
the Shapley values [12], which quantify the feature contributions to the difference be-
tween the model’s answer on a baseline instance and the target. The baseline depends
on the use case, e.g., a single-color image (represented by the vector 0 ˆd in the surrogate
space). This makes SHAP and LIME complementary methods rather than competitors.

LIME Extensions. An important body of literature has studied the impact of the differ-
ent components and parameters of LIME on the quality of the explanations. This has
led to multiple extensions of the original LIME algorithm. As opposed to this work,
some extensions [17, 22, 20] tackle the instability of LIME, i.e., the fact that two execu-
tions of the algorithm with the same input may not deliver the same explanation. This
instability originates from the randomness in the different steps of the approach, e.g.,
sampling in the surrogate space, non-deterministic conversion functions, etc. On those
grounds, the techniques to tackle instability are diverse. ALIME [17], for example, re-
sorts to a denoising auto-encoder to create a surrogate space that characterizes the data
manifold more accurately. DLIME [22], in contrast, applies hierarchical agglomerative
clustering on the training instances to identify the closest neighbors of the target and use
them to learn the surrogate. In another line of thought, the authors of OptiLIME [20]
study the relationship between the bandwidth σ, the adherence, and the instability of
LIME. Similar to our work, the authors highlight the importance of choosing the right
σ in a per-instance basis. Moreover, they show an inverse relationship between σ and
explanation instability. This observation constitutes the basis of a method to select the
bandwidth σ that yields the best trade-off between adherence and instability. We high-
light that all these approaches have been proposed only for tabular data, and that none
of them takes into account recall, precision, or coverage ﬁdelity.

Other extensions of LIME have focused entirely on improving ﬁdelity. ILIME [5]
proposes the use of inﬂuence functions in order to up-weight the neighbors that play a
higher role in the linear ﬁt of the surrogate. QLIME-A [3] proposes to extend the local
surrogate to report quadratic relationships for cases where a linear surrogate is still inac-
curate. While quadratic functions do exhibit higher ﬁt capabilities, their interpretability
in general settings is debatable.

12

Gaudel et al.

7 Conclusion

In this paper we have introduced S-LIME, an extension of LIME that reconciles local-
ity and ﬁdelity for linear explanations. We argue that LIME can produce degenerated
explanations as locality – controlled through the bandwidth σ – increases. We solve
this paradox by means of a new neighbor generation process on a continuous surrogate
space. Our experiments on image, time series, and tabular data suggest that this strat-
egy can provide even more faithful linear explanations with gradient-compliant seman-
tics that are not affected by high locality. As a future work, we envision to investigate
the ﬁdelity of S-LIME explanations with other generating distributions and conversion
functions, as well as to study the impact on the stability of the explanations.

Acknowledgements This research was partially supported by the Inria Project Lab
“Hybrid Approaches for Interpretable AI” (HyAIAI), the project “Framework for Auto-
matic Interpretability in Machine Learning” ﬁnanced by the French National Research
Agency (ANR JCJC FAbLe), and the network on the foundations of trustworthy AI,
integrating learning, optimisation, and reasoning (TAILOR) ﬁnanced by the EU’s Hori-
zon 2020 research and innovation program under agreement 952215.

References

1. Amparore, E., Perotti, A., Bajardi, P.: To Trust or not to Trust an Explanation: Us-
ing LEAF to Evaluate Local Linear XAI Methods. PeerJ Computer Science 7 (2021).
https://doi.org/10.7717/peerj-cs.479, http://dx.doi.org/10.7717/peerj-cs.479

2. Bodria, F., Giannotti, F., Guidotti, R., Naretto, F., Pedreschi, D., Rinzivillo, S.: Benchmark-
ing and Survey of Explanation Methods for Black Box Models. CoRR abs/2102.13076
(2021)

3. Bramhall, S., Horn, H., Tieu, M., Lohia, N.: QLIME-A: Quadratic Local Interpretable

Model-Agnostic Explanation Approach. SMU Data Science Rev 3 (2020)

4. Doshi-Velez, F., Kortz, M., Budish, R., Bavitz, C., Gershman, S., O’Brien, D., Schieber, S.,
Waldo, J., Weinberger, D., Wood, A.: Accountability of AI Under the Law: The Role of
Explanation. CoRR abs/1711.01134 (2017), http://arxiv.org/abs/1711.01134

5. ElShawi, R., Sherif, Y., Al-Mallah, M., Sakr, S.: ILIME: Local and Global Interpretable

Model-Agnostic Explainer of Black-Box Decision. In: ADBIS (2019)

6. Garreau, D., von Luxburg, U.: Explaining the Explainer: A First Theoretical Analysis of

LIME. In: AISTATS (2020)

7. Grabocka, J., Schilling, N., Wistuba, M., Schmidt-Thieme, L.: Learning Time-Series

Shapelets. In: KDD (2014)

8. Guillem´e, M., Masson, V., Roz´e, L., Termier, A.: Agnostic Local Explanation for Time Series

Classiﬁcation. In: ICTAI (2019)

9. Jia, Y., Frank, E., Pfahringer, B., Bifet, A., Lim, N.: Studying and Exploiting the Relationship

Between Model Accuracy and Explanation Quality. In: ECML/PKDD (2021)

10. Krizhevsky, A.: Learning Multiple Layers of Features from Tiny Images. Tech. rep., Cana-

dian Institute for Advanced Research (2009)

11. LeCun,

Y.,

Cortes,

C.:

MNIST

Handwritten

Digit

Database.

http://yann.lecun.com/exdb/mnist/ (2010)

S-LIME: Reconciling Locality and Fidelity in Linear Explanations

13

12. Lundberg, S.M., Lee, S.: A Uniﬁed Approach to Interpreting Model Predictions. In: NeurIPS

(2017)

13. Merrer, E.L., Tr´edan, G.: The Bouncer Problem: Challenges to Remote Explainability. CoRR

abs/1910.01432 (2019), http://arxiv.org/abs/1910.01432

14. Rakthanmanon, T., Keogh, E.: Fast Shapelets: A Scalable Algorithm for Discovering Time

Series Shapelets. In: SDM (2013)

15. Ribeiro, M.T., Singh, S., Guestrin, C.: Why should I trust you?: Explaining the Predictions

of Any Classiﬁer. In: KDD (2016)
M.T.,

16. Ribeiro,
Precision
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982

Singh,
Model-Agnostic

Explanations.

Guestrin,

C.:
In:

S.,

Anchors:
AAAI

High-
(2018),

17. Shankaranarayana, S.M., Runje, D.: ALIME: Autoencoder Based Approach for Local Inter-

pretability. CoRR abs/1909.02437 (2019), http://arxiv.org/abs/1909.02437
P.,

Kundaje,

A.:

18. Shrikumar,
tures
http://proceedings.mlr.press/v70/shrikumar17a.html

Greenside,
Propagating

A.,
Through

Activation

Differences.

Learning
In:

Important
ICML

Fea-
(2017),

19. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic Attribution for Deep Networks. CoRR

abs/1703.01365 (2017)

20. Visani, G., Bagli, E., Chesani, F.: OptiLIME: Optimized LIME Explanations for
Diagnostic Computer Algorithms. In: AIMLAI@CIKM (2020), http://ceur-ws.org/Vol-
2699/paper03.pdf

21. Wang, Z., Yan, W., Oates, T.: Time Series Classiﬁcation from Scratch with Deep Neural Net-

works: A Strong Baseline. CoRR abs/1611.06455 (2016), http://arxiv.org/abs/1611.06455

22. Zafar, M.R., Khan, N.M.: DLIME: A Deterministic Local Interpretable Model-Agnostic
Explanations Approach for Computer-Aided Diagnosis Systems. CoRR abs/1906.10263
(2019), http://arxiv.org/abs/1906.10263

