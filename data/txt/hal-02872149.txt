Bandits manchots avec échantillonnage de Thompson
pour des recommandations multiples suivant un modèle
fondé sur les positions
Camille-Sovanneary Gauthier, Romaric Gaudel, Elisa Fromont

To cite this version:

Camille-Sovanneary Gauthier, Romaric Gaudel, Elisa Fromont. Bandits manchots avec échantillon-
nage de Thompson pour des recommandations multiples suivant un modèle fondé sur les positions.
CAp 2020 - Conférence sur l’Apprentissage automatique, Jun 2020, Vannes, France. pp.1-2.
￿hal-
02872149￿

HAL Id: hal-02872149

https://hal.science/hal-02872149

Submitted on 17 Jun 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Bandits manchots avec ´echantillonnage de Thompson
pour des recommandations multiples
suivant un mod`ele fond´e sur les positions

Camille-Sovanneary Gauthier ∗

Romaric Gaudel †

Elisa Fromont ‡

R´esum´e

Les syst`emes de recommandation en ligne ont pour
but de proposer les produits les plus int´eressants
aux positions ad´equates sur une page internet. Nous
pr´esentons un nouvel algorithme, PB-MHB, perme-
ttant de faire des recommandations multiples en
ligne en suivant un mod`ele fond´e sur les posi-
tions. Cet algorithme s’appuie sur le principe des
bandits manchots et utilise un ´echantillonnage de
Thompson coupl´e avec un algorithme de Metropolis-
Hastings pour tirer les param`etres des lois proba-
bilistes utilis´ees, ce qui n’avait jamais ´et´e fait dans le
contexte d’un mod`ele bas´e positions. Notre m´ethode
ne n´ecessite pas d’avoir en param`etre les probabilit´es
de vue des utilisateurs sur chaque position de la page
Web, comme cela est usuellement le cas pour les algo-
rithmes r´epondant `a ce type d’interaction. Celles-ci
sont d’ailleurs en pratique diﬃcile `a obtenir a priori.
Les exp´eriences faites sur des donn´ees simul´ees et sur
des donn´ees issues de bases de donn´ees r´eelles (KDD-
CUP2012 et Yandex) montrent que notre m´ethode,
avec moins d’information, fournit de meilleurs recom-
mandations que l’´etat de l’art.

References

Shipra Agrawal and Navin Goyal. 2013. Thompson
Sampling for Contextual Bandits with Linear Payoﬀs.
In proc. of the 30th Int. Conf. on Machine Learning
(ICML ’13).

∗Louis Vuitton - Univ. Rennes
†Univ. Rennes, ENSAI, CNRS, CREST - UMR 919
‡Univ. Rennes, IRISA/INRIA rba, IUF

Shipra Agrawal and Navin Goyal. 2017. Near-Optimal
Regret Bounds for Thompson Sampling. Jour. of the
ACM, JACM 64, 5, Article 30 2017,

Maarten de Rijke Aleksandr Chuklin, Ilya Markov. 2015.
Click Models for Web Search. In Click Models for Web
Search.

Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. 2002.
Finite-time Analysis of the Multiarmed Bandit Prob-
lem. Machine Learning 47, 2 (01 May 2002), 235–256.
Olivier Chapelle and Lihong Li. 2011. An Empirical Eval-
uation of Thompson Sampling. In Advances in Neural
Information Processing Systems, NIPS’11.

Wei Chen, Yajun Wang, and Yang Yuan. 2013. Combina-
torial multi-armed bandit: General framework and ap-
plications. In proc. of the Int. Conf. on Machine Learn-
ing (ICML ’13).

Wang Chi Cheung, Vincent Tan, and Zixin Zhong. 2019.
A Thompson Sampling Algorithm for Cascading Ban-
dits. In proc. of the 22nd Int. Conf. on Artiﬁcial Intel-
ligence and Statistics, AISTATS’19.

Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke.

2015. Click Models for Web Search.

Richard Combes,

Stefan Magureanu, Alexandre
Proutiere, and Cyrille Laroche. 2015. Learning to
Rank: Regret Lower Bounds and Eﬃcient Algorithms.
In proc. of the ACM Int. Conf. on Measurement and
Modeling of Computer Systems.

Nick Craswell, Onno Zoeter, Michael Taylor, and Bill
Ramsey. 2008. An Experimental Comparison of Click
Position-bias Models. In proc. of the Int. Conf. on Web
Search and Data Mining (WSDM ’08).

Bianca Dumitrascu, Karen Feng, and Barbara Engel-
hardt. 2018. PG-TS: Improved Thompson Sampling
for Logistic Contextual Bandits. In Advances in Neu-
ral Information Processing Systems 31, NIPS’18.

Sumeet Katariya, Branislav Kveton, Csaba Szepesvari,

1

port CRG-TR-93-1. University of Zurich, Department
of Informatics.

Matthew Richardson, Ewa Dominowska, and Robert
Ragno. 2007. Predicting Clicks: Estimating the Click-
Through Rate for New Ads. In proc. of the 16th Inter-
national World Wide Web Conference (WWW ’07).
Carlos Riquelme, George Tucker, and Jasper Snoek.
[n.d.]. Deep Bayesian Bandits Showdown: An Empiri-
cal Comparison of Bayesian Deep Networks for Thomp-
son Sampling. In proc. of the Int. Conf. on Learning
Representations, ICLR’18.

William R. Thompson. 1933. On the Likelihood that One
Unknown Probability Exceeds Another in View of the
Evidence of Two Samples. Biometrika 25, 3/4 (1933),
285–294.

John von Neumann. 1951. Various Techniques Used
In Monte Carlo

in Connection with Random Digits.
Method, 12, 36–38.

Zoghi,

Tomas

Yandex. 2013. Yandex personalized web search challenge.
Mohammad
Masrour
Ghavamzadeh, Branislav Kveton, Csaba Szepes-
vari, and Zheng Wen. 2017. Online Learning to Rank
in Stochastic Click Models. In proc. of the 34th Int.
Conf. on Machine Learning, ICML’17

Tunys,

Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng
Wen, and Branislav Kveton. 2016. Cascading Bandits
for Large-scale recommandation Problems. In proc. of
the 32nd Conference on Uncertainty in Artiﬁcial Intel-
ligence (UAI ’16).

Claire Vernade, and Zheng Wen. 2017a. Stochastic
Rank-1 Bandits. In proc. of the Int. Conf. on Artiﬁ-
cial Intelligence and Statistics, AISTATS’17.

Sumeet Katariya, Branislav Kveton, Csaba Szepesv´ari,
Claire Vernade, and Zheng Wen. 2017b.Bernoulli
Rank-1 Bandits for Click Feedback. In proc. of the In-
ternational Joint Conference on Artiﬁcial Intelligence,
IJCAI’17.

Jaya Kawale, Hung H Bui, Branislav Kveton, Long Tran-
Thanh, and Sanjay Chawla. 2015. Eﬃcient Thompson
Sampling for Online Matrix Factorization recomman-
dation. In Advances in Neural Information Processing
Systems 28, NIPS’15.

Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa.
2015. Optimal Regret Analysis of Thompson Sampling
in Stochastic Multi-armed Bandit Problem with Mul-
tiple Plays. In proc. of the 32nd Int. Conf. on Machine
Learning (ICML ’15).

Junpei Komiyama, Junya Honda, and Akiko Takeda.
2017. Position-based Multiple-play Bandit Problem
with Unknown Position Bias. In Advances in Neural
Information Processing Systems 30, NIPS’17.

Branislav Kveton, Chang Li, Tor Lattimore, Ilya Markov,
Maarten de Rijke, Csaba Szepesv´ari, and Masrour
Zoghi. 2018. BubbleRank: Safe Online Learning to
Rerank. CoRR abs/1806.05819 (2018).

Branislav Kveton, Csaba Szepesv´ari, Zheng Wen, and
Azin Ashkan. 2015a. Cascading Bandits: Learning to
Rank in the Cascade Model. In proc. of the Int. Conf.
on Machine Learning (ICML ’15).

Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba
Szepesv´ari. 2015b. Combinatorial Cascading Bandits.
In Advances in Neural Information Processing Systems
28 (NIPS ’15).

Paul Lagr´ee, Claire Vernade, and Olivier Capp´e. 2016.
Multiple-play Bandits in the Position-based Model. In
Advances in Neural Information Processing Systems 30
(NIPS ’16).

Tor Lattimore, Branislav Kveton, Shuai Li, and Csaba
Szepesvari. 2018. TopRank: A practical algorithm for
online stochastic ranking. In NIPS’18.

Shuai Li, Baoxiang Wang, Shengyu Zhang, and Wei Chen.
2016. Contextual Combinatorial Cascading Bandits. In
proc. of Int. Conf. on Machine Learning, ICML’16.
Eric Mazumdar, Aldo Pacchiano, Yi-An Ma, Peter L.
Bartlett, and Michael I. Jordan. 2020. On Thomp-
CoRR
son Sampling with Langevin Algorithms.
abs/2002.10002 (2020).

Radford M. Neal. 1993. Probabilistic Inference Using
Markov Chain Monte Carlo Methods. Technical Re-

2

