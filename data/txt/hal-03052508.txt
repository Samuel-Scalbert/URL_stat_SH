Model Interpretability through the Lens of
Computational Complexity
Pablo Barceló, Mikaël Monet, Jorge A. Perez, Bernardo Subercaseaux

To cite this version:

Pablo Barceló, Mikaël Monet, Jorge A. Perez, Bernardo Subercaseaux. Model Interpretability through
￿hal-
the Lens of Computational Complexity. NeurIPS 2020, Dec 2020, Held online, United States.
03052508￿

HAL Id: hal-03052508

https://inria.hal.science/hal-03052508

Submitted on 10 Dec 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

0
2
0
2

v
o
N
2
1

]
I

A
.
s
c
[

2
v
5
6
2
2
1
.
0
1
0
2
:
v
i
X
r
a

Model Interpretability through the Lens of
Computational Complexity

Pablo Barceló1,4, Mikaël Monet2, Jorge Pérez3,4, Bernardo Subercaseaux3,4

1 Institute for Mathematical and Computational Engineering, PUC-Chile
2 Inria Lille, France
3 Department of Computer Science, Universidad de Chile
4 Millennium Institute for Foundational Research on Data, Chile
pbarcelo@ing.puc.cl, mikael.monet@inria.fr, [jperez,bsuberca]@dcc.uchile.cl

Abstract

In spite of several claims stating that some models are more interpretable than
others – e.g., “linear models are more interpretable than deep neural networks” – we
still lack a principled notion of interpretability to formally compare among different
classes of models. We make a step towards such a notion by studying whether folk-
lore interpretability claims have a correlate in terms of computational complexity
theory. We focus on local post-hoc explainability queries that, intuitively, attempt
to answer why individual inputs are classiﬁed in a certain way by a given model.
In a nutshell, we say that a class C1 of models is more interpretable than another
class C2, if the computational complexity of answering post-hoc queries for models
in C2 is higher than for those in C1. We prove that this notion provides a good theo-
retical counterpart to current beliefs on the interpretability of models; in particular,
we show that under our deﬁnition and assuming standard complexity-theoretical
assumptions (such as P (cid:54)= NP), both linear and tree-based models are strictly more
interpretable than neural networks. Our complexity analysis, however, does not
provide a clear-cut difference between linear and tree-based models, as we obtain
different results depending on the particular post-hoc explanations considered. Fi-
nally, by applying a ﬁner complexity analysis based on parameterized complexity,
we are able to prove a theoretical result suggesting that shallow neural networks
are more interpretable than deeper ones.

1

Introduction

Assume a dystopian future in which the increasing number of submissions has forced journal editors
to use machine-learning systems for automatically accepting or rejecting papers. Someone sends
his/her work to the journal and the answer is a reject, so the person demands an explanation for the
decision. The following are examples of three alternative ways in which the editor could provide an
explanation for the rejection given by the system:

1. In order to accept the submitted paper it would be enough to include a better motivation

and to delete at least two mathematical formulas.

2. Regardless of the content and the other features of this paper, it was rejected because it has

more than 10 pages and a font size of less than 11pt.

3. We only accept 1 out of 20 papers that do not cite any other paper from our own journal. In

order to increase your chances next time, please add more references.

 
 
 
 
 
 
These are examples of so called local post-hoc explanations [3, 19, 23, 26, 27]. Here, the term “local”
refers to explaining the verdict of the system for a particular input [19, 27], and the term “post-hoc”
refers to interpreting the system after it has been trained [23, 26]. Each one of the above explanations
can be seen as a query asked about a system and an input for it. We call them explainability queries.
The ﬁrst query is related with the minimum change required to obtain a desired outcome (“what is the
minimum change we must make to the article for it to be accepted by the system?”). The second one
is known as a sufﬁcient reason [32], and intuitively asks for a subset of the features of the given input
that sufﬁces to obtain the current verdict. The third one, that we call counting completions, relates to
the probability of obtaining a particular output given the values in a subset of the features of the input.

In this paper we use explainability queries to formally compare the interpretability of machine-
learning models. We do this by relating the interpretability of a class of models (e.g., decision trees)
to the computational complexity of answering queries for models in that class. Intuitively the lower
the complexity of such queries is, the more interpretable the class is. We study whether this intuition
provides an appropriate correlate to folklore wisdom on the interpretability of models [20, 23, 28].

Our contributions. We formalize the framework described above (Section 2) and use it to perform a
theoretical study of the computational complexity of three important types of explainability queries
over three classes of models. We focus on models often mentioned in the literature as extreme points
in the interpretability spectrum: decision trees, linear models, and deep neural networks. In particular,
we consider the class of free binary decision diagrams (FBDDs), that generalize decision trees, the
class of perceptrons, and the class of multilayer perceptrons (MLPs) with ReLU activation functions.
The instantiation of our framework for these classes is presented in Section 3.

We show that, under standard complexity assumptions, the computational problems associated to our
interpretability queries are strictly less complex for FBDDs than they are for MLPs. For instance, we
show that for FBDDs, the queries minimum-change-required and counting-completions can be solved
in polynomial time, while for MLPs these queries are, respectively, NP-complete and #P-complete
(where #P is the prototypical intractable complexity class for counting problems). These results,
together with results for other explainability queries, show that under our deﬁnition for comparing the
interpretability of classes of models, FBDDs are indeed more interpretable than MLPs. This correlates
with the folklore statement that tree-based models are more interpretable than deep neural networks.
We prove similar results for perceptrons: most explainability queries that we consider are strictly
less complex to answer for perceptrons than they are for MLPs. Since perceptrons are a realization
of a linear model, our results give theoretical evidence for another folklore claim stating that linear
models are more interpretable than deep neural networks. On the other hand, the comparison between
perceptrons and FBDDs is not deﬁnitive and depends on the particular explainability query. We
establish all our computational complexity results in Section 4.

Then, we observe that standard complexity classes are not enough to differentiate the interpretability
of shallow and deep MLPs. To present a meaningful comparison, we then use the machinery of
parameterized complexity [12, 16], a theory that allows the classiﬁcation of hard computational
problems on a ﬁner scale. Using this theory, we are able to prove that there are explainability queries
that are more difﬁcult to solve for deeper MLPs compared to shallow ones, thus giving theoretical
evidence that shallow MLPs are more interpretable. This is the most technically involved result of
the paper, that we think provides new insights on the complexity of interpreting deep neural networks.
We present the necessary concepts and assumptions as well as a precise statement of this result
in Section 5.

Most deﬁnitions of interpretability in the literature are directly related to humans in a subjective
manner [5, 10, 25]. In this respect we do not claim that our complexity-based notion of interpretability
is the right notion of interpretability, and thus our results should be taken as a study of the correlation
between a formal notion and the folklore wisdom regarding a subjective concept. We discuss this and
other limitations of our results in Section 6. We only present a few sketches for proofs in the body of
the paper and refer the reader to the appendix for detailed proofs of all our claims.

2 A framework to compare interpretability

In this section we explain the key abstract components of our framework. The idea is to introduce the
necessary terminology to formalize our notion of being more interpretable in terms of complexity.

2

Models and instances. We consider an abstract deﬁnition of a model M simply as a Boolean
function M : {0, 1}n → {0, 1}. That is, we focus on binary classiﬁers with Boolean input features.
Restricting inputs and outputs to be Booleans makes our setting cleaner while still covering several
relevant practical scenarios. A class of models is just a way of grouping models together. An instance
is a vector in {0, 1}n and represents a possible input for a model. A partial instance is a vector
in {0, 1, ⊥}n, with ⊥ intuitively representing “undeﬁned” components. A partial instance x ∈
{0, 1, ⊥}n represents, in a compact way, the set of all instances in {0, 1}n that can be obtained by
replacing undeﬁned components in x with values in {0, 1}. We call these the completions of x.

Explainability queries. An explainability query is a question that we ask about a model M and a
(possibly partial) instance x, and refers to what the model M does on instance x. We assume all
queries to be stated either as decision problems (that is, YES/NO queries) or as counting problems
(queries that ask, for example, how many completions of a partial instance satisfy a given property).
Thus, for now we can think of queries simply as functions having models and instances as inputs. We
will formally deﬁne some speciﬁc queries in the next section, when we instantiate our framework.

Complexity classes. We assume some familiarity with the most common computational complexity
classes of polynomial time (PTIME) and nondeterministic polynomial time (NP), and with the notion
of hardness and completeness for complexity classes under polynomial time reductions. In the
paper we also consider the class Σp
2, consisting of those problems that can be solved in NP if we
further grant access to an oracle that solves NP queries in constant time. It is strongly believed that
PTIME (cid:40) NP (cid:40) Σp
2 [2], where for complexity classes K1 and K2 we have that K1 (cid:40) K2 means
the following: problems in K1 can be solved in K2, but complete problems for K2 cannot be solved
in K1.

While for studying the complexity of our decision problems the above classes sufﬁce, for counting
problems we will need another one. This will be the class #P, which corresponds to problems that
can be deﬁned as counting the number of accepting paths of a polynomial-time nondeterministic
Turing machine [2]. Intuitively, #P is the counting class associated to NP: while the prototypical
NP-complete problem is checking if a propositional formula is satisﬁable (SAT), the prototypical #P-
complete problem is counting how many truth assignments satisfy a propositional formula (#SAT).
It is widely believed that #P is “harder” than Σp

2, which we write as Σp

(cid:40) #P.1

2

Complexity-based interpretability of models. Given an explainability query Q and a class C of
models, we denote by Q(C) the computational problem deﬁned by Q restricted to models in C. We
deﬁne next the most important notion for our framework: that of being more interpretable in terms of
complexity (c-interpretable for short). We will use this notion to compare among classes of models.
Deﬁnition 1. Let Q be an explainability query, and C1 and C2 be two classes of models. We say
that C1 is strictly more c-interpretable than C2 with respect to Q, if the problem Q(C1) is in the
complexity class K1, the problem Q(C2) is hard for complexity class K2, and K1 (cid:40) K2.

For instance, in the above deﬁnition one could take K1 to be the PTIME class and K2 to be the NP
class, or K1 = NP and K2 = Σp
2.

3

Instantiating the framework and main results

Here we instantiate our framework on three important classes of Boolean models and explainability
queries, and then present our main theorems comparing such models in terms of c-interpretability.

3.1 Speciﬁc models

Binary decision diagrams. A binary decision diagram (BDD [35]) is a rooted directed acyclic
graph M with labels on edges and nodes, verifying: (i) each leaf is labeled with true or with false;
(ii) each internal node (a node that is not a leaf) is labeled with an element of {1, . . . , n}; and

1One has to be careful with this notation, however, as Σp

2 and #P are complexity classes for problems of
different sort: the former being for decision problems, and the latter for counting problems. Although this issue
can be solved by considering the class PP, we skip these technical details as they are not fundamental for the
paper and can be found in most complexity theory textbooks, such as that of Arora and Barak [2].

3

(iii) each internal node has an outgoing edge labeled 1 and another one labeled 0. Every instance
x = (x1, . . . , xn) ∈ {0, 1}n deﬁnes a unique path πx from the root to a leaf in M, which satisﬁes
the following condition: for every non-leaf node u in πx, if i is the label of u, then the path πx goes
through the edge that is labeled with xi. The instance x is positive, i.e., M(x) := 1, if the label of
the leaf in the path πx is true, and negative otherwise. The size |M| of M is its number of edges. A
binary decision diagram M is free (FBDD) if for every path from the root to a leaf, no two nodes on
that path have the same label. A decision tree is simply an FBDD whose underlying graph is a tree.

Multilayer perceptron (MLP). A multilayer perceptron M with k layers is deﬁned by a se-
quence of weight matrices W (1), . . . , W (k), bias vectors b(1), . . . , b(k), and activation func-
tions f (1), . . . , f (k). Given an instance x, we inductively deﬁne

h(i) := f (i)(h(i−1)W (i) + b(i))

(i ∈ {1, . . . , k}),

(1)

assuming that h(0) := x. The output of M on x is deﬁned as M(x) := h(k). In this paper we
assume all weights and biases to be rational numbers. That is, we assume that there exists a sequence
of positive integers d0, d1, . . . , dk such that W (i) ∈ Qdi−1×di and b(i) ∈ Qdi . The integer d0 is
called the input size of M, and dk the output size. Given that we are interested in binary classiﬁers,
we assume that dk = 1. We say that an MLP as deﬁned above has (k − 1) hidden layers. The size of
an MLP M, denoted by |M|, is the total size of its weights and biases, in which the size of a rational
number p/q is log2(p) + log2(q) (with the convention that log2(0) = 1).
We focus on MLPs in which all internal functions f (1), . . . , f (k−1) are the ReLU function relu(x) :=
max(0, x). Usually, MLP binary classiﬁers are trained using the sigmoid as the output function f (k).
Nevertheless, when an MLP classiﬁes an input (after training), it takes decisions by simply using
the pre activations, also called logits. Based on this and on the fact that we only consider already
trained MLPs, we can assume without loss of generality that the output function f (k) is the binary
step function, deﬁned as step(x) := 0 if x < 0, and step(x) := 1 if x ≥ 0.

Perceptron. A perceptron is an MLP with no hidden layers (i.e., k = 1). That is, a perceptron M is
deﬁned by a pair (W , b) such that W ∈ Qd×1 and b ∈ Q, and the output is M(x) = step(xW +b).
Because of its particular structure, a perceptron is usually deﬁned as a pair (w, b) with w a rational
vector and b a rational number. The output of M(x) is then 1 if and only if (cid:104)x, w(cid:105) + b ≥ 0,
where (cid:104)x, w(cid:105) denotes the dot product between x and w.

3.2 Speciﬁc queries

Given instances x and y, we deﬁne d(x, y) := (cid:80)n
i=1 |xi − yi| as the number of components in
which x and y differ. We now formalize the minimum-change-required problem, which checks if the
output of the model can be changed by ﬂipping the value of at most k components in the input.

Problem: MINIMUMCHANGEREQUIRED (MCR)

Input: Model M, instance x, and k ∈ N

Output: YES, if there exists an instance y with d(x, y) ≤ k
and M(x) (cid:54)= M(y), and NO otherwise

Notice that, in the above deﬁnition, instead of “ﬁnding” the minimum change we state the problem
as a YES/NO query (a decision problem) by adding an additional input k ∈ N and then asking for a
change of size at most k. This is a standard way of stating a problem to analyze its complexity [2].
Moreover, in our results, when we are able to solve the problem in PTIME then we can also output a
minimum change, and it is clear that if the decision problem is hard then the optimization problem is
also hard. Hence, we can indeed state our problems as decision problems without loss of generality.
To introduce our next query, recall that a partial instance is a vector y = (y1, . . . , yn) ∈ {0, 1, ⊥}n,
and a completion of it is an instance x = (x1, . . . , xn) ∈ {0, 1}n such that for every i where yi ∈
{0, 1} it holds that xi = yi. That is, x coincides with y on all the components of y that are
not ⊥. Given an instance x and a model M, a sufﬁcient reason for x with respect to M [32]
is a partial instance y, such that x is a completion of y and every possible completion x(cid:48) of y
satisﬁes M(x(cid:48)) = M(x). That is, knowing the value of the components that are deﬁned in y is

4

enough to determine the output M(x). Observe that an instance x is always a sufﬁcient reason for
itself, and that x could have multiple (other) sufﬁcient reasons. However, given an instance x, the
sufﬁcient reasons of x that are most interesting are those having the least possible number of deﬁned
components; indeed, it is clear that the less deﬁned components a sufﬁcient reason has, the more
information it provides about the decision of M on x. For a partial instance y, let us write (cid:107)y(cid:107)
for its number of components that are not ⊥. The previous observations then motivate our next
interpretability query.

Problem: MINIMUMSUFFICIENTREASON (MSR)

Input: Model M, instance x, and k ∈ N

Output: YES, if there exists a sufﬁcient reason y for x wrt. M with (cid:107)y(cid:107) ≤ k,

and NO otherwise

As for the case of MCR, notice that we have formalized this interpretability query as a decision
problem. The last query that we will consider refers to counting the number of positive completions
for a given partial instance.

Problem: COUNTCOMPLETIONS (CC)
Input: Model M, partial instance y

Output: The number of completions x of y such that M(x) = 1

Intuitively, this query informs us on the proportion of inputs that are accepted by the model, given
that some particular features have been ﬁxed; or, equivalently, on the probability that such an instance
is accepted, assuming the other features to be uniformly and independently distributed.

3.3 Main interpretability theorems

We can now state our main theorems, which are illustrated in Figure 1. In all these theorems we
use CMLP to denote the class of all models (functions from {0, 1}n to {0, 1}) that are deﬁned by MLPs,
and similarly for CFBDD and CPerceptron. The proofs for all these results will follow as corollaries from
the detailed complexity analysis that we present in Section 4. We start by stating a strong separation
between FBDDs and MLPs, which holds for all the queries presented above.

Theorem 2. CFBDD is strictly more c-interpretable than CMLP with respect to MCR, MSR, and CC.

For the comparison between perceptrons and MLPs, we can establish a strict separation for MCR
and MSR , but not for CC. In fact, CC has the same complexity for both classes of models, which
means that none of these classes strictly “dominates” the other in terms of c-interpretability for CC.

Theorem 3. CPerceptron is strictly more c-interpretable than CMLP with respect to MCR and MSR. In
turn, the problems CC(CPerceptron) and CC(CMLP) are both complete for the same complexity class.

The next result shows that, in terms of c-interpretability, the relationship between FBDDs and
perceptrons is not clear, as each one of them is strictly more c-interpretable than the other for some
explainability query.

Theorem 4. The problems MCR(CFBDD) and MCR(CPerceptrons) are both in PTIME. How-
ever, CPerceptron is strictly more c-interpretable than CFBDD with respect to MSR, while CFBDD is
strictly more c-interpretable than CPerceptron with respect to CC.

We prove these results in the next section, where for each query Q and class of models C we pinpoint
the exact complexity of the problem Q(C).

5

MLPs

MCR, MSR, CC

MCR, MSR

FBDDs

CC

MSR

Perceptrons

Figure 1: Illustration of the main interpretability results. Arrows depict that the pointed class of
models is harder with respect to the query that labels the edge. We omit labels (or arrows) when a
problem is complete for the same complexity class for two classes of models.

4 The complexity of explainability queries

FBDDs

Perceptrons

MLPs

MINIMUMCHANGEREQUIRED
MINIMUMSUFFICIENTREASON NP-complete
CHECKSUFFICIENTREASON
COUNTCOMPLETIONS

PTIME
PTIME

PTIME

PTIME
PTIME
PTIME
#P-complete

NP-complete
Σp
2-complete
coNP-complete
#P-complete

Table 1: Summary of our complexity results.

In this section we present our technical complexity results proving Theorems 2, 3, and 4. We divide
our results in terms of the queries that we consider. We also present a few other complexity results that
we ﬁnd interesting on their own. A summary of the results is shown in Table 1. With the exception of
Proposition 6, items (1) and (3), the proofs for this section are relatively routine, were already known
or follow from known techniques. As mentioned in the introduction, we only present the main ideas
of some of the proofs in the body of the paper, and a detailed exposition of each result can be found
in the appendix.

4.1 The complexity of MINIMUMCHANGEREQUIRED

In what follows we determine the complexity of the MINIMUMCHANGEREQUIRED problem for the
three classes of models that we consider.
Proposition 5. The MINIMUMCHANGEREQUIRED query is (1) in PTIME for FBDDs, (2) in PTIME
for perceptrons, and (3) NP-complete for MLPs.

Proof sketch. This query has been shown to be solvable in PTIME for ordered binary decision
diagrams (OBDDs, a restricted form of FBDDs) by Shih et al. [31, Theorem 6] (the query is called
robusteness in the work of Shih et al. [31]). We show that the same proof applies to FBDDs. Recall
that in an FBDD every internal node is labeled with a feature index in {1, . . . , n}. The main idea is to
compute a quantity mcru(x) ∈ N ∪ {∞} for every node u of the FBDD M. This quantity represents
the minimum number of features that we need to ﬂip in x to modify the classiﬁcation M(x) if we are
only allowed to change features associated with the paths from u to some leaf in the FBDD. One can
easily compute these values by processing the FBDD bottom-up. Then the minimum change required
for x is the value mcrr(x) where r is the root of M, and thus we simply return YES if mcrr(x) ≤ k,
and NO otherwise.

For the case of a perceptron M = (w, b) and of an instance x, let us assume without loss of generality
that M(x) = 1. We ﬁrst deﬁne the importance s(i) ∈ Q of every input feature at position i as
follows: if xi = 1 then s(i) := wi, and if xi = 0 then s(i) := −wi. Consider now the set S that
contains the top k most important input features for which s(i) > 0. We can easily show that it is
enough to check whether ﬂipping every feature in S changes the classiﬁcation of x, in which case we
return YES, and return NO otherwise.

Finally, NP membership of MCR for MLPs is clear: guess a partial instance y with d(x, y) ≤ k and
check in polynomial time that M(x) (cid:54)= M(y). We prove hardness with a simple reduction from
the VERTEXCOVER problem for graphs, which is known to be NP-complete.

6

Notice that this result immediately yields Theorems 2, 3, and 4 for the case of MCR.

4.2 The complexity of MINIMUMSUFFICIENTREASON

We now study the complexity of MINIMUMSUFFICIENTREASON. The following result yields
Theorems 2, 3, and 4 for the case of MSR.
Proposition 6. The MINIMUMSUFFICIENTREASON query is (1) NP-complete for FBDDs (and
hardness holds already for decision trees), (2) in PTIME for perceptrons, and (3) Σp
2-complete for
MLPs.

Proof sketch. Membership of the problem in the respective classes is easy. We show NP-completeness
of the problem for FBDDs by a nontrivial reduction from the NP-complete problem of determining
whether a directed acyclic graph has a dominating set of size at most k [22]. For a perceptron M =
(w, b) and an instance x, assume without loss of generality that M(x) = 1. As in the proof of
Proposition 5, we consider the importance of every component of x, and prove that it is enough to
check whether the k most important features of x are a sufﬁcient reason for it, in which case we
return YES, and simply return NO otherwise. Finally, the Σp
2-completeness for MLPs is obtained
again using a technical reduction from the problem called SHORTEST IMPLICANT CORE, deﬁned
and shown to be Σp

2-complete by Umans [34].

To reﬁne our analysis, we also consider the natural problem of checking if a given partial instance is a
sufﬁcient reason for an instance.

Problem: CHECKSUFFICIENTREASON (CSR)

Input: Model M, instance x and a partial instance y

Output: YES, if y is a sufﬁcient reason for x wrt. M, and NO otherwise

We obtain the following (easy) result.
Proposition 7. The query CHECKSUFFICIENTREASON is (1) in PTIME for FBDDs, (2) in PTIME
for perceptrons, and (3) co-NP-complete for MLPs.

We note that this result for FBDDs already appears in [9] (under the name of implicant check).
Interestingly, we observe that this new query maintains the comparisons in terms of c-interpretability,
in the sense that CFBDD and CPerceptron are strictly more c-interpretable than CMLP with respect to CSR.

4.3 The complexity of COUNTCOMPLETIONS

What follows is our main complexity result regarding the query COUNTCOMPLETIONS, which yields
Theorems 2, 3, and 4 for the case of CC.
Proposition 8. The query COUNTCOMPLETIONS is (1) in PTIME for FBDDs, (2) #P-complete for
perceptrons, and (3) #P-complete for MLPs.

Proof sketch. Claim (1) is a a well-known fact that is a direct consequence of the deﬁnition of FBDDs;
indeed, we can easily compute by bottom-up induction of the FBDD a quantity representing for each
node the number of positive completions of the sub-FBDD rooted at that node (e.g., see [9, 35]). We
prove (2) by showing a reduction from the #P-complete problem #KNAPSACK, i.e., counting the
number of solutions to a 0/1 knapsack input.2 For the last claim, we show that MLPs with ReLU
activations can simulate arbitrary Boolean formulas, which allows us to directly conclude (3) since
counting the number of satisfying assignments of a Boolean formula is #P-complete.

Comparing perceptrons and MLPs. Although the query COUNTCOMPLETIONS is #P-complete
for perceptrons, we can still show that the complexity goes down to PTIME if we assume the weights
and biases to be integers given in unary; this is commonly called pseudo-polynomial time.
Proposition 9. The query COUNTCOMPLETIONS can be solved in pseudo-polynomial time for
perceptrons (assuming the weights and biases to be integers given in unary).

2Recall that such an input consists of natural numbers (given in binary) s1, . . . , sn, k ∈ N, and a solution to

it is a set S ⊆ {1, . . . , n} with (cid:80)

i∈S si ≤ k.

7

Proof sketch. This is proved by ﬁrst reducing the problem to #KNAPSACK, and then using a classical
dynamic programming algorithm to solve #KNAPSACK in pseudo-polynomial time.

This result establishes a difference between perceptrons and MLPs in terms of CC, as this query
remains #P-complete for the latter even if weights and biases are given as integers in unary. Another
difference is established by the fact that COUNTCOMPLETIONS for perceptrons can be efﬁciently
approximated, while this is not the case for MLPs. To present this idea, we brieﬂy recall the notion of
fully polynomial randomized approximation scheme (FPRAS [21]), which is heavily used to reﬁne
the analysis of the complexity of #P-hard problems. Intuitively, an FPRAS is a polynomial time
algorithm that computes with high probability a (1 − (cid:15))-multiplicative approximation of the exact
solution, for (cid:15) > 0, in polynomial time in the size of the input and in the parameter 1/(cid:15). We show:
Proposition 10. The problem COUNTCOMPLETIONS restricted to perceptrons admits an FPRAS
(and the use of randomness is not even needed in this case). This is not the case for MLPs, on the
other hand, at least under standard complexity assumptions.

5 Parameterized results for MLPs in terms of number of layers

In Section 4.1 we proved that the query MINIMUMCHANGEREQUIRED is NP-complete for MLPs.
Moreover, a careful inspection of the proof reveals that MCR is already NP-hard for MLPs with only
a few layers. This is not something speciﬁc to MCR: in fact, all lower bounds for the queries studied
in the paper in terms of MLPs hold for a small, ﬁxed number of layers. Hence, we cannot differentiate
the interpretability of shallow and deep MLPs with the complexity classes that we have used so far.

In this section, we show how to construct a gap between the (complexity-based) interpretability of
shallow and deep MLPs by considering reﬁned complexity classes in our c-interpretability framework.
In particular, we use parameterized complexity [12, 16], a branch of complexity theory that studies
the difﬁculty of a problem in terms of multiple input parameters. To the best of our knowledge, the
idea of using parameterized complexity theory to establish a gap in the complexity of interpreting
shallow and deep networks is new.

We ﬁrst introduce the main underlying idea of parameterized complexity in terms of two classical
graph problems: VERTEXCOVER and CLIQUE. In both problems the input is a pair (G, k) with G a
graph and k an integer. In VERTEXCOVER we verify if there exists a set of nodes of size at most k
that includes at least one endpoint for every edge in G. In CLIQUE we check if there exists a set of
nodes of size at most k such that all nodes in the set are adjacent to each other. Both problems are
known to be NP-complete. However, this analysis treats G and k at the same level, which might not
be fair in some practical situations in which k is much smaller than the size of G. Parameterized
complexity then studies how the complexity of the problems behaves when the input is only G, and k
is regarded as a small parameter.

It happens to be the case that VERTEXCOVER and CLIQUE, while both NP-complete, have a
different status in terms of parameterized complexity. Indeed, VERTEXCOVER can be solved in
time O(2k · |G|), which is polynomial in the size of the input G – with the exponent not depending
on k – and, thus, it is called ﬁxed-parameter tractable [12]. In turn, it is widely believed that there is
no algorithm for CLIQUE with time complexity O(f (k) · poly(G)) – with f being any computable
function, that depends only on k – and thus it is ﬁxed-parameter intractable [12]. To study the notion
of ﬁxed-parameter intractability, researchers on parameterized complexity have introduced the W[t]
complexity classes (with t ≥ 1), which form the so called W-hierarchy. For instance CLIQUE
is W[1]-complete [12]. A core assumption in parameterized complexity is that W[t] (cid:40) W[t + 1], for
every t ≥ 1.

In this paper we will use a related hierarchy, called the W(Maj)-hierarchy [14]. We defer the formal
deﬁnitions of these two hierachies to the appendix. We simply mention here that both classes, W[t]
and W(Maj)[t], are closely related to logical circuits of depth t. The circuits that deﬁne the W-
hierarchy use gates AND, OR and NOT, while circuits for W(Maj) use only the MAJORITY gate
(which outputs a 1 if more than half of its inputs are 1). Our result below applies to a special class of
MLPs that we call restricted-MLPs (rMLPs for short), where we assume that the number of digits
of each weight and bias in the MLP is at most logarithmic in the number of neurons in the MLP (a
detailed exposition of this restriction can be found in the appendix). We can now formally state the
main result of this section.

8

Proposition 11. For every t ≥ 1 the MINIMUMCHANGEREQUIRED query over rMLPs with 3t + 3
layers is W(Maj)[t]-hard and is contained in W(Maj)[3t + 7].

By assuming that the W(Maj)-hierarchy is strict, we can use Proposition 11 to provide separations
for rMLPs with different numbers of layers. For instance, instantiating the above result with t = 1
we obtain that for rMLPs with 6 layers, the MCR problem is in W(Maj)[3t + 7] = W(Maj)[10].
Moreover, instantiating it with t = 11 we obtain that for rMLPs with 36 layers, the MCR problem
is W(Maj)[11]-hard. Thus, assuming that W(Maj)[10] (cid:40) W(Maj)[11] we obtain that rMLPs with 6
layers are strictly more c-interpretable than rMLPs with 36 layers. We generalize this observation in
the following result.

Proposition 12. Assume that the W(Maj)-hierarchy is strict. Then for every t ≥ 1 we have that
rMLPs with 3t + 3 layers are strictly more c-interpretable than rMLPs with 9t + 27 layers wrt. MCR.

6 Discussion and concluding remarks

Related work. The need for model interpretability in machine learning has been heavily advocated
during the last few years, with works covering theoretical and practical issues [3, 19, 23, 26, 27].
Nevertheless, a formal deﬁnition of interpretability has remained elusive [23]. In parallel, a related
notion of interpretability has emerged from the ﬁeld of knowledge compilation [9, 30, 31, 32,
33]. The intuition here is to construct a simpler and more interpretable model from a complex
one. One can then study the simpler model to understand how the initial one makes predictions.
Motivated by this, Darwiche and Hirth [8] use variations of the notion of sufﬁcient reason to
explore the interpretability of Ordered BDDs (OBDDs). The FBDDs that we consider in our
work generalize OBDDs, and thus, our results for sufﬁcient reasons over FBDDs can be seen as
generalizations of the results in [8]. We consider FBDDs instead of OBDDs as FBDDs subsume
decision trees, while OBDDs do not. We point out here that the notion of sufﬁcient reason for a
Boolean classiﬁer is the same as the notion of implicant for a Boolean function, and that minimal
sufﬁcient reasons (with minimailty refering to subset-inclusion of the deﬁned components) correspond
to prime implicants [9]. We did not incorporate a study of minimal sufﬁcient reasons (also called
PI-explanations) to our work due to space constraints. In a contemporaneous work [24], Marques-
Silva et al. study the task of enumerating the minimal sufﬁcient reasons of naïve Bayes and linear
classiﬁers. The queries COUNTCOMPLETIONS and CHECKSUFFICIENTREASON have already been
studied for FBDDs in [9] (CHECKSUFFICIENTREASON under the name of implicant check). The
query MINIMUMCHANGEREQUIRED is studied in [31] for OBDDs, where it is called robustness.
Finally, there are papers exploring queries beyond the ones presented here [30, 31, 32], such as
monotonicity, unateness, bias detection, minimum cardinality explanations, etc.

Limitations. Our framework provides a formal way of studying interpretability for classes of
models, but still can be improved in several respects. One of them is the use of a more sophisticated
complexity analysis that is not so much focused on the worst case complexity study propose here,
but on identifying relevant parameters that characterize more precisely how difﬁcult it is to interpret
a particular class of models in practice. Also, in this paper we have focused on studying the local
interpretability of models (why did the model make a certain prediction on a given input?), but
one could also study their global interpretability, that is, making sense of the general relationships
that a model has learned from the training data [27]. Our framework can easily be extended to the
global setting by considering queries about models, independent of the input it receives. In order to
avoid the difﬁculties of deﬁning a general notion of interpretability [23], we have used explainability
queries and their complexity as a formal proxy. Nonetheless, we do not claim that our notion of
complexity-based interpretability is the deﬁnitive notion of interpretability. Indeed, most deﬁnitions
of interpretability are directly related to humans in a subjective manner [5, 10, 25]. Our work is
thus to be taken as a study of the correlation between a formal notion of interpretability and the
folk wisdom regarding a subjective concept. Finally, even though the notion of complexity-based
interpretability gives a precise way to compare models, our results show that it is still dependent on
the particular set of queries that one picks. To achieve a more robust formalization of interpretability,
one would then need to propose a more general approach that prescinds of speciﬁc queries. This is a
challenging problem for future research.

9

7 Broader impact

Although interpretability as a subject may have a broad practical impact, our results in this paper are
mostly theoretic, so we think that this work does not present any foreseeable societal consequences.

Acknowledgments and Disclosure of Funding

Barceló and Pérez are funded by Fondecyt grant 1200967.

10

References

[1] E. Allender. A note on the power of threshold circuits.

In 30th Annual Symposium on

Foundations of Computer Science. IEEE, 1989.

[2] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge University

Press, 2009.

[3] A. B. Arrieta, N. Díaz-Rodríguez, J. D. Ser, A. Bennetot, S. Tabik, A. Barbado, S. Garcia,
S. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila, and F. Herrera. Explainable artiﬁcial
intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.
Information Fusion, 58:82–115, 2020.

[4] G. Berbeglia and G. Hahn. Counting feasible solutions of the traveling salesman problem with
pickups and deliveries is# P-complete. Discrete Applied Mathematics, 157(11):2541–2547,
2009.

[5] O. Biran and C. V. Cotton. Explanation and Justiﬁcation in Machine Learning : A Survey. 2017.

[6] J. F. Buss and T. Islam. Simplifying the weft hierarchy. Theoretical Computer Science,

351(3):303–313, 2006.

[7] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Third

Edition. The MIT Press, 3rd edition, 2009.

[8] A. Darwiche and A. Hirth. On the reasons behind decisions. arXiv preprint arXiv:2002.09284,

2020.

[9] A. Darwiche and P. Marquis. A knowledge compilation map. Journal of Artiﬁcial Intelligence

Research, 17:229–264, 2002.

[10] F. Doshi-Velez and B. Kim. A Roadmap for a Rigorous Science of Interpretability. CoRR,

abs/1702.08608, 2017.

[11] R. G. Downey and M. R. Fellows. Fixed-Parameter Tractability and Completeness I: Basic

Results. SIAM Journal on Computing, 24(4):873–921, Aug. 1995.

[12] R. G. Downey and M. R. Fellows. Fundamentals of parameterized complexity, volume 4.

Springer, 2013.

[13] R. G. Downey, M. R. Fellows, and K. W. Regan. Parameterized circuit complexity and the W

hierarchy. Theoretical Computer Science, 191(1-2):97–115, Jan. 1998.

[14] M. Fellows, D. Hermelin, M. Müller, and F. Rosamond. A purely democratic characterization of
W[1]. In Parameterized and Exact Computation, pages 103–114. Springer Berlin Heidelberg.

[15] M. R. Fellows, J. Flum, D. Hermelin, M. Müller, and F. A. Rosamond. Combinatorial circuits

and the W-hierarchy. 2007.

[16] J. Flum and M. Grohe. Parameterized complexity theory. Texts in Theoretical Computer

Science. An EATCS Series. Springer, 2006.

[17] M. Goldmann and M. Karpinski. Simulating threshold circuits by majority circuits. SIAM

Journal on Computing, 27(1):230–246, 1998.

[18] P. Gopalan, A. Klivans, R. Meka, D. Štefankovic, S. Vempala, and E. Vigoda. An FPTAS
for #knapsack and related counting problems. In 2011 IEEE 52nd Annual Symposium on
Foundations of Computer Science, pages 817–826. IEEE, 2011.

[19] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of

methods for explaining black box models. ACM Comput. Surv., 51(5).

[20] D. Gunning and D. Aha. DARPA’s explainable artiﬁcial intelligence (XAI) program. AI

Magazine, 40(2):44–58, 2019.

[21] M. R. Jerrum, L. G. Valiant, and V. V. Vazirani. Random generation of combinatorial structures

from a uniform distribution. TCS, 43:169–188, 1986.

[22] J. A. King. Approximation algorithms for guarding 1.5 dimensional terrains. PhD thesis, 2005.

[23] Z. C. Lipton. The mythos of model interpretability. Queue, 16(3):31–57, 2018.

11

[24] J. Marques-Silva, T. Gerspacher, M. C. Cooper, A. Ignatiev, and N. Narodytska. Explaining
Naive Bayes and Other Linear Classiﬁers with Polynomial Time and Delay. arXiv preprint
arXiv:2008.05803, 2020.

[25] T. Miller. Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial

Intelligence, 267:1–38, Feb. 2019.

[26] C. Molnar.

Interpretable machine learning. 2019. https://christophm.github.io/

interpretable-ml-book/.

[27] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu. Deﬁnitions, methods, and
applications in interpretable machine learning. Proceedings of the National Academy of Sciences,
116(44):22071–22080, 2019.

[28] T. D. Nguyen, K. E. Kasmarik, and H. A. Abbass. Towards interpretable deep neural networks:
An exact transformation to multi-class multivariate decision trees. arXiv, pages arXiv–2003,
2020.

[29] R. Rizzi and A. I. Tomescu. Faster FPTASes for counting and random generation of Knapsack

solutions. Information and Computation, 267:135–144, 2019.

[30] W. Shi, A. Shih, A. Darwiche, and A. Choi. On tractable representations of binary neural

networks. arXiv preprint arXiv:2004.02082, 2020.

[31] A. Shih, A. Choi, and A. Darwiche. Formal veriﬁcation of Bayesian network classiﬁers. In

International Conference on Probabilistic Graphical Models, pages 427–438, 2018.

[32] A. Shih, A. Choi, and A. Darwiche. A symbolic approach to explaining Bayesian network

classiﬁers. arXiv preprint arXiv:1805.03364, 2018.

[33] A. Shih, A. Darwiche, and A. Choi. Verifying binarized neural networks by Angluin-style
learning. In International Conference on Theory and Applications of Satisﬁability Testing, pages
354–370. Springer, 2019.

[34] C. Umans. The minimum equivalent DNF problem and shortest implicants. Journal of Computer

and System Sciences, 63(4):597–611, 2001.

[35] I. Wegener. BDDs—design, analysis, complexity, and applications. Discrete Applied Mathe-

matics, 138(1-2):229–251, 2004.

12

Appendix

The appendix contains the proofs for all the results presented in the main document. It is organized
as follows:

Appendix A shows how MLPs can simulate Boolean circuits, which will be used in order to

prove several propositions.

Appendix B contains a proof of Proposition 5.
Appendix C contains a proof of Proposition 6.
Appendix D contains a proof of Proposition 7.
Appendix E contains a proof of Proposition 8.
Appendix F contains a proof of Proposition 9.
Appendix G contains a proof of Proposition 10.
Appendix H contains a more detailed description of the parameterized complexity frame-

work.

Appendix I contains a proof of Proposition 11.
Appendix J contains a proof of Proposition 12.

Appendix A. Simulating Boolean formulas/circuits with MLPs

In this section we show that multilayer perceptrons can efﬁciently simulate arbitrary Boolean formulas.
We will often use this result throughout the appendix to prove the hardness of our explainability
queries over MLPs. In fact, and this will make the proof cleaner, we will show a slightly more general
result: that MLPs can simulate arbitrary Boolean circuits. Formally, we show:
Lemma 13. Given as input a Boolean circuit C, we can build in polynomial time an MLP MC that
is equivalent to C as a Boolean function.

Proof. We will proceed in three steps. The ﬁrst step is to build from C another equivalent circuit C (cid:48)
that uses only what we call relu gates. A relu gate is a gate that, on input x = (x1, . . . , xm) ∈
Rm, outputs relu((cid:104)w, x(cid:105) + b), for some rationals w1, . . . , wm, b. Observe that these gates do not
necessarily output 0 or 1, and so the circuit C (cid:48) might not be Boolean. However, we will ensure in the
construction that the output of every relu gate in C (cid:48), when given Boolean inputs (i.e., x ∈ {0, 1}m),
is Boolean. This will imply that the circuit C (cid:48) is Boolean as well. To this end, it is enough to show
how to simulate each original type of internal gate (NOT, OR, AND) by relu gates. We do so as
follows:

• NOT-gate: simulated with a relu gate with only one weight of value −1 and a bias of 1.

Indeed, it is clear that for x ∈ {0, 1}, we have that relu(−x + 1) =

(cid:26)1
0

if x = 0
if x = 1

.

• AND-gate of in-degree n: simulated with a relu gate with n weights, each of value 1,
Indeed, it is clear that for x ∈ {0, 1}n, we have that
if (cid:86)n
otherwise

and a bias of value −(n − 1).
(cid:26)1
relu((cid:80)n
0

i=1 xi − (n − 1)) =

i=1 xi = 1

.

• OR-gate of in-degree n: we ﬁrst rewrite the OR-gate with NOT- and AND-gates using De

Morgan’s laws, and then we use the last two items.

The second step is to build a circuit C (cid:48)(cid:48), again using only relu gates, that is equivalent to C (cid:48) and
that is what we call layerized. This means that there exists a leveling function l : C (cid:48)(cid:48) → N that

13

m) − l(g(cid:48)

1) ≤ . . . ≤ l(g(cid:48)

i) identity gates in between g(cid:48)

assigns to every gate of C (cid:48) a level such that (i) every variable gate is assigned level 0, and (ii) for
any wire g → g(cid:48) (meaning that g is an input to g(cid:48)) in C (cid:48)(cid:48) we have that l(g(cid:48)) = l(g) + 1. To this end,
let us call a relu gate that has a single input and weight 1 and bias 0 an identity gate, and observe
then that the value of an identity gate is the same as the value of its only input, when this input is
in {0, 1}. We will obtain C (cid:48)(cid:48) from C (cid:48) by inserting identity gates in between the gates of C (cid:48), which
clearly does not change the Boolean function being computed. We can do so naïvely as follows. First,
we initialize l(g) to 0 for all the variable gates g of C (cid:48). We then iterate the following process: select a
gate g such that l(g) is undeﬁned and such that l(g(cid:48)) is deﬁned for every input g(cid:48) of g. Let g(cid:48)
1, . . . , g(cid:48)
m
be the inputs of g, and assume that l(g(cid:48)
m). For every 1 ≤ i ≤ m, we insert a line
of l(g(cid:48)
m) + 1, and we set the
levels of the identity gates that we have inserted appropriately. It is clear that this construction can be
done in polynomial time and that the resulting circuit C (cid:48)(cid:48) is layerized.
Finally, the last step is to transform C (cid:48)(cid:48) into an MLP MC using only relu for the internal activation
functions and the step function for the output layer (i.e., what we simply call “an MLP” in the paper),
and that respects the structure given by our deﬁnition in Section 3.1 (i.e., where all neurons of a given
layer are connected to all the neurons of the preceding layer). We ﬁrst deal with having a step gate
instead of a relu gate for the output. To achieve this, we create a fresh identity gate g0, we set the
output of C (cid:48)(cid:48) to be an input of g0, and we set g0 to be the new output gate of C (cid:48)(cid:48) (this does not change
the Boolean function computed). We then replace g0 by a step gate (which, we recall, on input x ∈ R
outputs 0 if x < 0 and 1 otherwise) with a weight of 2 and bias of −1, which again does not change
if x = 1
if x = 0

the Boolean function computed; indeed, for x ∈ {0, 1}, we have that step(2x − 1) =

i and g, and we set l(g) := l(g(cid:48)

(cid:26)1
0

.

The level of g0 is one plus the level of the previous output gate of C (cid:48)(cid:48). Therefore, to make C (cid:48)(cid:48) become
a valid MLP, it is enough to do the following: for every gate g of level i and gate g(cid:48) of level i + 1, if g
and g(cid:48) are not connected in C (cid:48)(cid:48), we make g be an input of g(cid:48) and we set the corresponding weight
to 0. This clearly does not change the function computed, and the obtained circuit can directly be
regarded as an equivalent MLP MC. Since the whole construction can be performed in polynomial
time, this concludes the proof.

Appendix B. Proof of Proposition 5

In this section we prove Proposition 5. We recall its statement for the reader’s convenience:

Proposition 5. The MINIMUMCHANGEREQUIRED query is (1) in PTIME for FBDDs, (2) in PTIME
for perceptrons, and (3) NP-complete for MLPs.

We prove each item separately.

Lemma 14. The MINIMUMCHANGEREQUIRED query can be solved in linear time for FBDDs.

Proof. Let (M, x, k) be an instance of MINIMUMCHANGEREQUIRED, where M is an FBDD.
For every node u in M we deﬁne Mu to be the FBDD obtained by restricting M to the nodes
that are (forward-)reachable from u; in other words, Mu is the sub-FBDD rooted at u. Then, we
deﬁne mcru(x) to be the minimum change required on x to obtain a classiﬁcation under Mu that
differs from M(x). More formally,

mcru(x) = min{k(cid:48) | there exists an instance y such that d(x, y) = k(cid:48) and Mu(y) (cid:54)= M(x)},

with the convention that min ∅ = ∞. Observe that, (†) for an instance y minimizing k(cid:48) in this
equality, since the FBDD Mu does not depend on the features associated to any node u(cid:48) from the
root of M to u excluded, we have that for any such node yu(cid:48) = xu(cid:48) holds (otherwise k(cid:48) would not
be minimized).3 Let r be the root of M. Then, by deﬁnition we have that (M, x, k) is a positive
instance of MINIMUMCHANGEREQUIRED if and only mcrr(x) ≤ k. We now explain how we can
compute all the values mcru(x) for every node u of M in linear time.

3We slightly abuse notation and write xu for the value of the feature of x that is indexed by the label of u.

14

By deﬁnition, if u is a leaf labeled with true we have that Mu(y) = 1 for every y, and thus
if M(x) = 0 we get mcru(x) = 0, while if M(x) = 1 we get that mcru(x) = ∞. Analogously,
if u is a leaf labeled with false, then mcru(x) is equal to 0 if M(x) = 1 and to ∞ otherwise.

For the recursive case, we consider a non-leaf node u. Let u1 be the node going along the edge labeled
with 1 from u, and u0 analogously. Using the notation [xu = a] to mean 1 if the feature of x indexed
by the label of node u has value a ∈ {0, 1}, and 0 otherwise, and the convention that ∞ + 1 = ∞,
we claim that:

mcru(x) = min

(cid:16)

[xu = 1] + mcru0(x), [xu = 0] + mcru1 (x)

(cid:17)

Indeed, consider by inductive hypothesis that mcru0(x) and mcru1(x) have been properly calculated,
and let us show that this equality holds. We prove both inequalities in turn:

(cid:16)

(cid:17)

• We show that mcru(x) ≤ min

[xu = 1] + mcru0(x), [xu = 0] + mcru1(x)

. It is enough
to show that both mcru(x) ≤ [xu = 1] + mcru0 (x) and mcru(x) ≤ [xu = 0] + mcru1(x)
hold. We only show the ﬁrst inequality, as the other one is similar. If mcru0 (x) = ∞
then clearly the inequality holds, hence let us assume that mcru0(x) = k(cid:48) ∈ N. This
means that there is an instance y(cid:48) such that d(x, y(cid:48)) = k(cid:48) and such that Mu0(y(cid:48)) (cid:54)= M(x).
Furthermore, by the observation (†) we have that for any node u(cid:48) from the root of M
to u (included), we have yu(cid:48) = xu(cid:48). Therefore, the instance y that is equal to y(cid:48) but has
value yu = 0 differs from x in exactly k(cid:48)(cid:48) = [xu = 1] + k(cid:48), which implies that mcru(x) ≤
[xu = 1] + mcru0 (x). Hence, the ﬁrst inequality is proven.

(cid:16)

(cid:17)

• We show that mcru(x) ≥ min

[xu = 1] + mcru0 (x), [xu = 0] + mcru1 (x)

. First,
assume that both mcru0 (x) and mcru1 (x) are equal to ∞. This means that every path
in both Mu0 and Mu1 leads to a leaf with the same classiﬁcation as M(x). Then, as
every path from u goes either through u0 or through u1, it must be that every path from u
leads to a leaf with the same classiﬁcation as M(x), and thus mcru(x) = ∞, and so
the inequality holds. Therefore, we can assume that one of mcru0 (x) or mcru1(x) is
[xu = 1] + mcru0(x), [xu =
ﬁnite. Let us assume without loss of generality that ((cid:63)) min
= [xu = 1] + mcru0 (x) ∈ N (the other case being similar). Let us now
0] + mcru1 (x)
assume, by way of contradiction, that the inequality does not hold, that is, we have that (††)
mcru(x) < [xu = 1] + mcru0(x), and let y be an instance such that Mu(y) (cid:54)= Mu(x)
and d(x, y) = mcru(x). Thanks to ((cid:63)), we can assume wlog that yu = 0. But then we
would have that mcru0(x) ≤ mcru(x) − [xu = 1], which contradicts (††). Hence, the
second inequality is proven.

(cid:17)

(cid:16)

It is clear that the recursive function mcr can be computed bottom-up in linear time, thus concluding
the proof.

Lemma 15. The MINIMUMCHANGEREQUIRED query can be solved in linear time for perceptrons.

Proof. Let (M = (w, b), x, k) be an instance of the problem, and let us assume without loss of
generality that M(x) = 1, as the other case is analogous. For each feature i of x we deﬁne its
importance s(i) as wi if xi = 1 and −wi otherwise. Intuitively, s represents how good it is to
keep a certain feature in order to maintain the verdict of the model. We now assume that x and w
have been sorted in decreasing order of score s (paying the cost of a sorting procedure) . For
example, if originally w = (3, −5, −2) and x = (1, 0, 1), then after the sorting procedure we have
w = (−5, 3, −2) and x = (0, 1, 1). This sorting procedure has cost O(|M|) as it is a classical
problem of sorting strings whose total length add up to M and can be carried with a variant of
Bucketsort [7]. As a result, for every pair 1 ≤ i < j ≤ n we have that s(i) ≥ s(j). Let k(cid:48)
be the largest integer no greater than k such that s(k(cid:48)) > 0 and then deﬁne x(cid:48) as the instance
that differs from x exactly on the ﬁrst k(cid:48) features. We claim that M(x(cid:48)) (cid:54)= M(x) if and only
if (M, x, k) is a positive instance of MINIMUMCHANGEREQUIRED. The forward direction follows
from the fact that k(cid:48) ≤ k. For the backward direction, assume that (M, x, k) is a positive instance
of MINIMUMCHANGEREQUIRED. This implies that there is an instance y that differs from x in at

15

most k features, and for which M(y) = 0. If y = x(cid:48), then we are immediately done, so we can
safely assume this is not the case.

We then deﬁne, for any instance y of M the function v(y) = (cid:104)w, y(cid:105). Note that an instance y of M
is positive if and only if v(y) ≥ −b. Then, since we have that M(y) = 0, it holds that v(y) < −b.
We now claim that v(x(cid:48)) ≤ v(y):

Claim 16. For every instance y such that d(y, x) ≤ k and M(y) (cid:54)= M(x), it must hold
that v(x(cid:48)) ≤ v(y).

Proof. For an instance z, let us write Cz for the set of features for which z differs from x. We then
have on the one hand
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(1 − xi)wi +

(1 − xi)wi +

xiwi +

xiwi

v(x(cid:48)) =

i∈Cx(cid:48) \Cy

i∈Cy∩Cx(cid:48)

i(cid:54)∈Cx(cid:48) ∪Cy

i∈Cy\Cx(cid:48)

and on the other hand
(cid:88)

v(y) =

(1 − xi)wi +

(cid:88)

(1 − xi)wi +

(cid:88)

xiwi +

(cid:88)

xiwi

i∈Cy\Cx(cid:48)

i∈Cy∩Cx(cid:48)

i(cid:54)∈Cx(cid:48) ∪Cy

i∈Cx(cid:48) \Cy

As the two middle terms are shared, we only need to prove that

(cid:88)

(1 − xi)wi +

(cid:88)

xiwi ≤

(cid:88)

(1 − xi)wi +

(cid:88)

xiwi

i∈Cx(cid:48) \Cy

i∈Cy\Cx(cid:48)

i∈Cy\Cx(cid:48)

i∈Cx(cid:48) \Cy

which is equivalent to proving that

(cid:88)

wi +

(cid:88)

wi ≤

(cid:88)

wi +

(cid:88)

wi

i∈Cx(cid:48) \Cy,xi=0

i∈Cy\Cx(cid:48) ,xi=1

i∈Cy\Cx(cid:48) ,xi=0

i∈Cx(cid:48) \Cy,xi=1

and by using the deﬁnition of importance, equivalent to

(cid:88)

−s(i) +

(cid:88)

s(i) ≤

(cid:88)

−s(i) +

(cid:88)

s(i)

i∈Cx(cid:48) \Cy,xi=0

i∈Cy\Cx(cid:48) ,xi=1

i∈Cy\Cx(cid:48) ,xi=0

i∈Cx(cid:48) \Cy,xi=1

which can be rearranged into

(cid:88)

s(i) ≤

(cid:88)

s(i)

But this inequality must hold as C (cid:48)
maximizes (cid:80)
i∈C s(i).

i∈Cy\Cx(cid:48)
x is by deﬁnition the set C of features of size at most k that

i∈Cx(cid:48) \Cy

Because of the claim, and the fact that v(y) < −b we conclude that v(x(cid:48)) < −b, and
thus M(x(cid:48)) (cid:54)= M(x). This concludes the backward direction, and thus, the fact that checking
whether M(x(cid:48)) (cid:54)= M(x) is enough to solve the entire problem. Since checking this can be done in
linear time, constructing x(cid:48) is the most expensive part of the process, which can effectively be done
in time O(|M|). This concludes the proof of the lemma.

Lemma 17. The MINIMUMCHANGEREQUIRED query is NP-complete for MLPs.

Proof. Membership is easy to see, it is enough to non-deterministically guess an instance y and
check that d(x, y) ≤ k and M(x) (cid:54)= M(y).

In order to prove hardness, we reduce from VERTEX COVER. Given an undirected graph G = (V, E)
and an integer k, the VERTEX COVER problem consists in deciding whether there is a subset S ⊆ V
of at most k vertices such that every edge of G touches a vertex in S. Let (G = (V, E), k) be an
instance of VERTEX COVER, and let n denote |V |. Based on G, we build a formula ϕG, where
propositional variables correspond to vertices of G.

ϕG =

(cid:94)

(xu ∨ xv)

(u,v)∈E

16

It is clear that the satisfying assignments of ϕG correspond to the vertex covers of G, and furthermore,
that a satisfying assignment of Hamming weight k (number of variables assigned to 1) corresponds
to a vertex cover of size k.

Moreover, we can safely assume that there is at least 1 edge in G, as otherwise the instance would be
trivial, and a constant size positive instance of MCR would ﬁnish the reduction. This implies in turn,
that we can assume that assigning every variable to 0 does not satisfy ϕG.
We now build an MLP Mϕ from ϕG, using Lemma 13. We claim that the instance (Mϕ, 0n, k) is
a positive instance of MINIMUMCHANGEREQUIRED if and only if (G, k) is a positive instance of
VERTEX COVER.
Indeed, 0n is a negative instance of Mϕ, as assigning every variable to 0 does not satisfy ϕG.
Moreover a positive instance of weight k for Mϕ corresponds to a satisfying assignment of weight k
for ϕG, which in turn corresponds to a vertex cover of size k for G. This is enough to conclude
conclude the proof, recalling that both the construction of ϕG and Mϕ take polynomial time.

Appendix C. Proof of Proposition 6

In this section we prove Proposition 6, whose statement we recall here:
Proposition 6. The MINIMUMSUFFICIENTREASON query is (1) NP-complete for FBDDs (and
hardness holds already for decision trees), (2) in PTIME for perceptrons, and (3) Σp
2-complete for
MLPs.

Again, we prove each claim separately.
Lemma 18. The MINIMUMSUFFICIENTREASON query is NP-complete for FBDDs, and hardness
holds already for decision trees.

Proof. Membership in NP is clear, it sufﬁces to guess the instance y and check both that it has
less than k deﬁned components and that is a sufﬁcient reason for x, which can be done thanks to
Lemma 23. We will prove that hardness holds already for the particular case of decision trees, and
when the input instance x is positive. Hardness of this particular setting implies of course the hardness
of the general problem. In order to do so, we will reduce from the problem of determining whether a
directed acyclic graph has a dominating set of size at most k, which we abbreviate as DOM-DAG.
Recall that in a directed graph G = (V, E), a subset of vertices D ⊆ V is said to be dominating
if every vertex in V \ D has an incoming edge from a vertex in D. The problem of DOM-DAG is
shown to be NP-complete in [22].

An illustration of the reduction is presented in Figure 2. Let (G = (V, E), k) be an instance of
DOM-DAG, and let us deﬁne n := |V |. We start by computing in polynomial time a topological
ordering ϕ = ϕ1, . . . , ϕn of G. Next, we will create an instance (T , x, k) of k-SUFFICIENTREASON
such that there is a sufﬁcient reason of size at most k for x under the decision tree T if and only if G
has a dominating set of size at most k. We create the decision tree T , of dimension n, in 2 steps.

1. Create nodes v1, . . . , vn, where node vi is labeled with ϕi The node vn will be the root of T ,
and for 2 ≤ i ≤ n, connect vi to vi−1 with an edge labeled with 1. Node v1 is connected to
a leaf labeled true through an edge labeled with 1. We will denote the path created in this
step as π.

2. For every vertex ϕi create a decision tree Ti equivalent to the boolean formula

Fi =

(cid:95)

ϕj

(ϕj ,ϕi)∈E

and create an edge from vi to the root of Ti labeled with 0. If Fi happens to be the empty
formula, Ti is deﬁned as false. Note that the nodes introduced by this step are all naturally
associated with vertices of G.

Step 2 takes polynomial time because boolean formulas in 1-DNF can easily be transformed into a
decision tree in linear time.

17

6

1

3

2

4

5

(a) Example of an input
DAG. Nodes 2 and 5, cor-
responding to the minimum
dominating set of G are em-
phasized.

3

6

1

2

4

5

(b) A topological ordering ϕ
of G.

1

6

3

1

4

6

5

5

2

2

1

4

2

5

4

5

(c) Resulting decision tree T . Edges to the left of a
node are always labeled with 0, and edges to the right
with 1. The leaves are not depicted for clarity, but: if
a node has no right child in the picture, then its right
child is true, and if it has no left child then its left child
is false. Note that in every diagonal there is an empha-
sized node, which is either 2 or 5, implying the partial
instance (⊥, 1, ⊥, ⊥, 1, ⊥) is a sufﬁcient reason for the
instance x = (1, 1, 1, 1, 1, 1).

Figure 2: Illustration of the reduction from DOM-DAG to k-SUFFICIENTREASON over decision
trees, for an example graph of 6 nodes.

We now check that T is a decision tree. Since T has a tree structure, it is enough to check that for
every path from the root to a leaf there are no two nodes on the path that have the same label (i.e., to
check that T is a valid FBDD). Note that any path from the root vn to a leaf goes ﬁrst to a certain
node vi in π, from where it either takes an edge labeled with 0, in case i (cid:54)= 1 or it simply goes to a
leaf otherwise. In case i = 1, the path from the root goes exactly through vn, vn−1, . . . , v1, which all
have different labels. In case i (cid:54)= 1, the path includes (i) nodes with labels ϕn, ϕn−1, . . . , ϕi, and (ii)
a subpath inside Ti. It is clear that all the labels in (i) are different. And as by construction Ti is a
decision tree, no two nodes inside (ii) can have the same label. It remains to check that no node in (i)
can have the same label of a node in (ii). To see this, consider that all the vertices of G associated to
the nodes in (ii) have edges to ϕi in G, and thus come before ϕi in the topological order. But (i) is
composed precisely by ϕi and the nodes who come after it in the topological ordering, so (i) and (ii)
have empty intersection.
Let x = 1n be the vector of n ones. We claim that (T , x, k) is a yes-instance of k-
SUFFICIENTREASON if and only if (G, k) is a yes-instance of DOM-DAG.

Forward direction. Consider that there is a sufﬁcient reason y for x under T of size at most k. As
x contains only 1s, y must contain only 1s and ⊥s. Consider the set S of components i where yi = 1.
Recalling that every vertex of G is canonically associated with a feature of T , we will denote DS
to the set of vertices of G that are associated with the features in S. It is clear that |DS| ≤ k. We
now prove that DS is a dominating set of G. First, in case DS = V , we are trivially done. We know
assume DS (cid:54)= V . Consider a vertex v ∈ V \ DS, corresponding to ϕi in the topological ordering,
and deﬁne z as the completion of y where the features ϕj such that j > i, are set to 1, and all other
features that are undeﬁned by y are set to 0. By hypothesis, z must be a positive instance, and so its
path on T must end in a leaf labeled with true. Note that the path of z in T necessarily takes the
path π created in Step 1 of the construction, up to the node vi, and then enters its subtree Ti. Let t be
the node of Ti whose leaf labeled with true ends the path of z in T , and ϕk its label and associated
vertex in G. As feature t is set to 1, we must have either ϕk ∈ DS (in case t is 1 because of y)
or k > i (in case t is 1 by the construction of completion z). However, the second case is not actually
possible, as if k > i, that means vk comes before vi in path π, and thus the path of z in T passes
through vk, which has label ϕk before passing through vi. But the path of z in T passes through t
before ending, which also has label ϕk. This contradicts the already proven fact that T is a decision
tree. We can therefore assume that ϕk belongs to DS. Then, as t is a node of Ti, there must be an
edge (ϕk, ϕi) in E because of the way Ti is constructed. But this means that vertex v ∈ V \ DS has
an edge coming from ϕk ∈ DS, and so v is effectively dominated by the set DS. As this holds for
every v ∈ V \ DS, we conclude that DS is indeed a dominating set of G.

18

Backward Direction. Consider that there is a dominating set D ⊆ V of size at most k. Let SD be
the set of features associated with D. We claim that the partial instance y that has 1 in the features
that belong to SD, and is undeﬁned elsewhere, is a sufﬁcient reason for x, and by construction its size
is at most k. Consider an arbitrary completion z of y, we need to show that z is a positive instance
of T . For z not to be a positive instance, its path on T would have to reach a leaf labeled with false.
This can only happen by either taking the edge labeled with 0 from v1 (the last node in path π built in
the construction), or inside a subtree Ti, corresponding to a node vi whose associated feature in z is
set to 0. We show that neither can happen. For the ﬁrst case, every dominating set must include ϕ1,
the vertex in G associated with v1, as it is the ﬁrst element in the topological ordering of G, and thus
it must has in-degree 0, which implies ϕ1 ∈ D. Therefore, it is not possible to take the edge labeled
with 0 from v1. On the other hand, suppose the path of z in Ti ends in a leaf labeled with false. Then,
by construction of Ti, there is no vertex ϕj such that (ϕj, ϕi) ∈ E whose associated feature is set
to 1 in z. But as D is a dominating set, either there is indeed a ϕj ∈ D such that (ϕj, ϕi) ∈ E
or ϕi ∈ D. The ﬁrst case is in direct contradiction with the previous statement, as ϕj ∈ D implies,
by our construction of y that the feature associated with ϕj is set to 1. The second case also creates a
contradiction, as if ϕi ∈ D, then by construction y would have a 1 in the feature vi associated to ϕi,
which contradicts the assumption of the path of z entering Ti.

Lemma 19. The MINIMUMSUFFICIENTREASON query is in PTIME for perceptrons.

Proof. Let (M = (w, b), x, k) be an instance of the problem, and let d be the dimension of the
perceptron. We will assume without loss of generality that M(x) = 1. In this proof, what we
call a minimum sufﬁcient reason for x is a sufﬁcient reason for x that has the least number of
components being deﬁned. We show a greedy algorithm that computes a minimum sufﬁcient reason
for x under M in time O(|M|). For each feature i of x we deﬁne its importance s(i) as wi if xi = 1
and −wi otherwise (just as we did in the proof of Lemma 15), and its penalty p(i) as min(0, wi).
Intuitively, s represents how good it is for a partial instance to be deﬁned in a given feature, and p
represents the penalty or cost that a partial instance incurs by not being deﬁned in a given feature. We
now assume that x and w have been sorted in decreasing order of score s. For example, if originally
w = (3, −5, −2) and x = (1, 0, 1), then after the sorting procedure we have w = (−5, 3, −2)
and x = (0, 1, 1). We now deﬁne a function ψ that takes any partial instance y as input and outputs
the worst possible value for a completion of y:

ψ(y) :=

min
z: z is a completion of y

(cid:104)w, z(cid:105) =

(cid:88)

yi(cid:54)=⊥

wiyi +

(cid:88)

yi=⊥

p(i).

The second equality is easy to see based on the deﬁnition of the function p, and the deﬁnition of ψ
implies that ψ(y) ≥ −b exactly when y is a sufﬁcient reason. For 1 ≤ l ≤ d, we deﬁne yl as the
partial instance of x such that yl
i is equal to xi if i ≤ l and to ⊥ otherwise. In simple terms, yl is the
partial instance obtained by taking the ﬁrst l features of x; continuing our example with x = (0, 1, 1),
we have for instance y2 = (0, 1, ⊥). Let j be the minimum index such that ψ(yj) ≥ −b. Such an
index always exists, because, since x is a positive instance, taking j = d is always a valid index.
Note that j can be computed in linear time.
We now prove that (†) the partial instance yj is a minimum sufﬁcient reason for x. By deﬁnition we
have that ψ(yj) ≥ −b, so yj is indeed a sufﬁcient reason for x. We now need to show that yj is
minimum. Assume, seeking for a contradiction, that there is a sufﬁcient reason y(cid:48) of x with strictly
less components deﬁned than yj; clearly we can assume without loss of generality that y(cid:48) has exactly
j − 1 components deﬁned. We will now show that ((cid:63)) yj−1 is a also a sufﬁcient reason for x, which
is a contradiction since j was assumed to be the minimal index such that yj is a sufﬁcient reason
of x, hence proving (†). If y(cid:48) = yj−1, we have that ((cid:63)) is trivially true. Otherwise, and considering
that y(cid:48) and yj−1 have the same size, and that yj−1 is deﬁned exactly on the ﬁrst j − 1 features, there
must be at least a pair of features (m, n), with m ≤ j − 1 < n, such that yj−1 is deﬁned at feature m
and y(cid:48) is not, and on the other hand y(cid:48) is deﬁned at feature n whereas yj−1 is not. In order to ﬁnish
the proof of ((cid:63)), we will prove a simpler claim that will help us conclude.

Claim 20. Assume that there is a pair of features (m, n) with m ≤ j − 1 < n such that y(cid:48)
⊥, yj−1
except that y∗

m =
n = ⊥, and let y∗ be the resulting partial instance that is equal to y(cid:48)
n := ⊥. Then we have that ψ(y∗) ≥ ψ(y(cid:48)).

n (cid:54)= ⊥, yj−1
m and y∗

m (cid:54)= ⊥ and y(cid:48)

m := yj−1

19

Proof of Claim 20. By deﬁnition, ψ(y∗) − ψ(y(cid:48)) = p(n) − p(m) + wmyj−1
n = p(n) −
p(m) + wmxm − wnxn. But because the features in yj−1 are sorted in decreasing order of score,
it must hold that s(m) ≥ s(n). Using this last inequality and reasoning by cases on the values
xm, xn and on the signs of wm, wn, one can tediously check that ψ(y∗) − ψ(y(cid:48)) ≥ 0 and thus
ψ(y∗) ≥ ψ(y(cid:48)).

m − wny(cid:48)

We now continue with the proof of ((cid:63)). As a result of Claim 20, one can iteratively modify y(cid:48) until
it becomes equal to yj−1 in such a way that the value of ψ is never decreased along the process,
implying therefore that ψ(yj−1) ≥ ψ(y(cid:48)). But ψ(y(cid:48)) ≥ −b, because y(cid:48) is assumed to be a sufﬁcient
reason, hence we have that ψ(yj−1) ≥ −b, implying that yj−1 is a sufﬁcient reason for x, and
thus concluding the proof of ((cid:63)). Therefore, (†) is proven, and since yj can clearly be computed in
polynomial time (in fact, the runtime of the whole procedure is dominated by the sorting subroutine,
which again has cost O(|M|) as it is a classical problem of sorting strings whose total length add
up to |M| and can be carried with a variant of Bucketsort [7]), this ﬁnishes the proof of the lemma;
indeed, we can output YES if j ≤ k and NO otherwise.

Lemma 21. The MINIMUMSUFFICIENTREASON query is Σp

2-complete for MLPs.

Proof. Membership in Σp
2 is clear, as one can non-deterministically guess the value of the k features
that would make for a sufﬁcient reason, and then use an oracle in co-NP to verify that no completion
of that guess has a different classiﬁcation. To show hardness, we will reduce from the problem
SHORTEST IMPLICANT CORE, deﬁned and proven to be Σp
2-hard by Umans [34, Theorem 1]. First,
we need a few deﬁnitions in order to present this problem. A formula in disjunctive normal form
(DNF) is a Boolean formula of the form ϕ = t1 ∨ t2 ∨ . . . ∨ tn, where each term ti is a conjunction of
literals (a literal being a variable of the negation thereof). An implicant for φ is a partial assignment
of the variables of φ such that any extension to a full assignment makes the formula evaluate to true;
note that we can equivalently see an implicant of φ as what we call a sufﬁcient reason of φ. For
a partial assigment C of the variables and for a set of literals t (or conjunction of literals t), we
write C ⊆ t when for every variable x, if x ∈ t then C(x) = 1 and if ¬x ∈ t then C(x) = 0
and C(x) is undeﬁned otherwise. An instance of SHORTEST IMPLICANT CORE then consists of
a DNF formula ϕ = t1 ∨ t2 ∨ . . . ∨ tn, together with an integer k. Such an instance is positive
for SHORTEST IMPLICANT CORE when there is an implicant C for ϕ such that C ⊆ tn.4 Note
that the SHORTEST IMPLICANT CORE is closer to the problem at hand than the general SHORTEST
IMPLICANT problem, as (minimum) sufﬁcient reasons of an instance x can only induce literals
according to x, in a similar fashion of implicants that can only induce literals according to the core
tn.

A reduction that does not work, and how to ﬁx it on an example.
In order to convey the main
intuition, we start by presenting a ﬁrst tentative of a reduction that does not work. Thanks to
Lemma 13 we know that it is possible to build an MLP Mϕ equivalent to ϕ. However, doing
so directly creates a problem: we would need to ﬁnd a convenient instance x such that (ϕ, k) ∈
SHORTEST IMPLICANT CORE if and only if (Mϕ, x, k) ∈ k-SUFFICIENTREASON. A natural idea
is to consider tn as a candidate for x, but the issue is that tn does not necessarily include every
variable. The next natural idea is to try with x being an arbitrary completion of tn (interpreting tn as
the partial instance that is uniquely deﬁned by its satisfying assignment). This approach fails because
there could be a sufﬁcient reason of size at most k for such an x that relies on features (variables)
that are not in tn. We illustrate this with an example for n = 4.

ϕ := x1x5 ∨ x2 x6 ∨ x3x6 ∨ x1 x2x4 ∨ x1x3x5
(cid:124) (cid:123)(cid:122) (cid:125)
t4

While it can be checked that (ϕ, 2)
(cid:54)∈ SHORTEST IMPLICANT CORE, we have that
(Mϕ, (1, 0, 1, 0, 1, 1), 2) is in fact a positive instance of k-SUFFICIENTREASON, as the partial
instance that assigns 1 to x3 and x6 and is undeﬁned for the rest of the features, is a sufﬁcient reason
of size 2 for x. The issue is that we are allowing x6 to be part of the sufﬁcient reason for x even
though x6 (cid:54)∈ t4. We can avoid this from happening by splitting each variable that is not in tn, such
as x6, into k + 1 variables, in such a way that deﬁning the value of x6 would force us to deﬁne the

4Note that, in order to keep our notation consistent, we use the symbol ⊆ where Umans uses ⊇.

20

value of all the k + 1 variables, which is of course unaffordable. Continuing with the example, we
build the formula ϕ(cid:48) as follows:

ϕ(cid:48) :=

3
(cid:94)

i=1

(cid:16)
x1x5 ∨ xi

2 xi

6 ∨ x3xi

6 ∨ x1 xi

2xi

4 ∨ x1x3x5

(cid:17)

Now we can simply take (Mϕ(cid:48), x, 1) where x is an arbitrary completion of t4 over the new set
of variables, for example, one that assigns 1 to the features 1, 3 and 5, and 0 to all other features
(variables). Note that ϕ(cid:48) is not a DNF anymore, but this is not a problem, since we only need to
compute Mϕ(cid:48). It is then easy to check that this instance is equivalent to the original input instance.

The reduction. We now present the correct reduction and prove that it works. Let (ϕ, k) be an
instance of SHORTEST IMPLICANT CORE. Let Xc be the set of variables that are not mentioned in tn.
We split every variable xj ∈ Xc into k + 1 variables x1
and for each i ∈ {1, . . . , k + 1}
we build ϕ(i) by replacing every occurrence of a variable xj, that belongs to Xc, by xi
j. Finally we
deﬁne ϕ(cid:48) as the conjunction of all the ϕ(i). That is,

j , . . . xk+1

j

ϕ(i) := ϕ[xj → xi
k+1
(cid:94)

ϕ(cid:48) :=

ϕ(i)

j, for all xj ∈ Xc]

(2)

(3)

i=1

Observe that any meaningful instance of SHORTEST IMPLICANT CORE has k < |tn|, so we can
safely assume that k is given in unary, making this construction polynomial.
We then use Lemma 13 to build an MLP Mϕ(cid:48) from ϕ(cid:48), in polynomial time. The features of this model
correspond naturally to the variables of ϕ(cid:48), and thus we refer to both features and variables without
distinction. Let y be the instance that assigns 1 to every variable that appears as a positive literal
in tn, and 0 to all other variables. We claim that (ϕ, k) ∈ SHORTEST IMPLICANT CORE if and only
if (Mϕ(cid:48), x, k) ∈ k-SUFFICIENTREASON. For the forward direction, if there is an implicant C ⊆ tn
of ϕ, of size at most k, then we claim that C is also an implicant of each ϕ(i). This follows from the
fact that every assignment σ that is consistent with C and satisﬁes ϕ, has a related assignment σi,
that for every variable xj ∈ Xc assigns σi(xi
j) = σ(xj), and that is equal to σ for every xj (cid:54)∈ Xc. It
is clear that σi(ϕ(i)) = σ(ϕ), which concludes the claim. As C is an implicant of each ϕ(i), it must
also be an implicant of ϕ(cid:48). Then, as Mϕ(cid:48) is equivalent to ϕ(cid:48) (as Boolean functions) by construction,
and x is consistent with C because it is consistent with tn, it follows that the partial instance that is
induced by C is a sufﬁcient reason for x under Mϕ(cid:48). For the backward direction, assume there is a
sufﬁcient reason y for x under Mϕ(cid:48), whose size is at most k, and let C (cid:48) be its associated implicant
for ϕ(cid:48). We cannot say yet that C (cid:48) is a proper candidate for being an implicant core of ϕ, as C (cid:48) could
c to be the set of variables of ϕ(cid:48) that are not
contain variables not mentioned by tn. Let us deﬁne X (cid:48)
present in tn. Intuitively, as there are k + 1 copies of each variable of X (cid:48)
c in ϕ(cid:48), no valuation of a
variable in X (cid:48)
c, for the formula ϕ, can be forced by a sufﬁcient reason of size at most k. We will prove
this idea in the following claim, allowing us to build an implicant C for which we can assure C ⊆ tn.
Claim 22. Assume that there is an implicant C (cid:48) of size at most k for ϕ(cid:48), and let C be the partial
valuation that sets every variable x that appear in tn and that is deﬁned by C (cid:48) to C (cid:48)(x), and that
leaves every other variable undeﬁned. Then C (cid:48) is an implicant of size at most k for ϕ.

, where X i

c , . . . , X k+1

Proof. The set X (cid:48)
c can be expressed as the union of k + 1 disjoint sets of variables,
j. Since C (cid:48) contains at
namely X 1
c
c ∩ C (cid:48) = ∅.
most k literals, and there are k + 1 disjoint sets X i
But then this implies that C is an implicant of ϕ(l). But ϕ(l) is equivalent to ϕ up to renaming of the
variables that are not present in C, therefore, the fact that C is an implicant of ϕ(l) implies that C
must be an implicant of ϕ as well.

c, there must exist an index l such that X l

c contains all variables of the form xi

By using Claim 22 we get that C is an implicant of ϕ. But we have that C ⊆ tn, which is enough to
conclude that (ϕ, k) ∈ SHORTEST IMPLICANT CORE and ﬁnishes the proof of Lemma 21.

21

Appendix D. Proof of Proposition 7

We now prove Proposition 7, whose statement we recall here:
Proposition 7. The query CHECKSUFFICIENTREASON is (1) in PTIME for FBDDs, (2) in PTIME
for perceptrons, and (3) co-NP-complete for MLPs.

We prove each claim separately.
Lemma 23. The query CHECKSUFFICIENTREASON can be solved in linear time for FBDDs.

Proof. Let (M, x, y) be an instance of the problem, with M being an FBDD. We ﬁrst check that x is
a completion of y, which can clearly be done in linear time. We the deﬁne M(cid:48) as the resulting FBDD
from the following procedure: (i) For every internal node in M with label i, delete its outgoing edge
with label 0 if yi = 1 and its outgoing edge with label 1 if yi = 0. We note here that M(cid:48) is not a
well deﬁned FBDDs, since some internal nodes may have only one outgoing edge: more precisely,
the value M(x(cid:48)) ∈ {0, 1} is well deﬁned for every instance x(cid:48) that is a completion of y, and is not
deﬁned for an instance x(cid:48) that is not a completion of y. To check whether y is a sufﬁcient reason,
we can then simply check that every leaf that is reachable from the root in M(cid:48) is labeled the same
(either true or false). This can clearly be done in linear time by standard graph algorithms.

Lemma 24. The query CHECKSUFFICIENTREASON can be solved in linear time for perceptrons.

Proof. Let (M = (w, b), x, y) be an instance of the problem. We ﬁrst check in linear time that x is
a completion of y. We then get rid of the components that are deﬁned by y, as follows. We deﬁne:

• A := (cid:80)

yi(cid:54)=⊥ yiwi;

• w(cid:48) := (wi | yi = ⊥); and

• b(cid:48) := b + A;

and let M(cid:48) be the perceptron (w(cid:48), b(cid:48)). Notice that the dimension of M(cid:48) is equal to the number of
undeﬁned components of y; we denote this number by m. It is then clear that y is a sufﬁcient reason
of x under M if and only if every instance of M(cid:48) is labeled the same. We can check this as follows.
Let J1 be the minimum possible value of (cid:104)w(cid:48), x(cid:48)(cid:105) (for x(cid:48) ∈ {0, 1}m); J1 can clearly be computed in
linear time by setting x(cid:48)
i = 1 otherwise. Similarly we can compute the maximal
possible value J2 of (cid:104)w(cid:48), x(cid:48)(cid:105). Then, every instance of M(cid:48) is labeled the same if and only if it is not
the case that J1 < −b(cid:48) and J2 ≥ −b(cid:48), thus concluding the proof.

i ≥ 0 and x(cid:48)

i = 0 if w(cid:48)

Lemma 25. The query CHECKSUFFICIENTREASON is co-NP-complete for MLPs.

Proof. We ﬁrst show membership in co-NP. Let (M, x, y) be an instance of the problem. Then y is
a sufﬁcient reason of x under M if and only if all the completions of y are labeled the same as x.
This can clearly be checked in co-NP.

In order to prove hardness we reduce from TAUT, the problem of checking whether an arbitrary
boolean formula is a satisﬁed by all possible assignments of its variables. This problem is known
to be complete for co-NP. Let F be an arbitrary boolean formula. We use Lemma 13 to build an
equivalent MLP M in polynomial time (with the features of M corresponding to the variables of F).
Then F is a tautology if and only if all completions of the partial instance y = ⊥n are positive
instances of M. First, we construct an arbitrary instance x (for instance, the one with all the features
being 0), and we reject if M(x) = 0. Then, we accept if y is a sufﬁcient reason of x under M, and
we reject otherwise. This concludes the reduction.

Appendix E. Proof of Proposition 8

We prove Proposition 8, whose statement we recall here:
Proposition 8. The query COUNTCOMPLETIONS is (1) in PTIME for FBDDs, (2) #P-complete for
perceptrons, and (3) #P-complete for MLPs.

22

As we said in the main text, the ﬁrst claim follows almost directly from the deﬁnition of FBDDs;
see [35] for instance. For the second claim, we will rely on the #P-hardness of the counting
problem #Knapsack, as deﬁned next:
Deﬁnition 26. An input of the problem #Knapsack consists of natural numbers s1, . . . , sn, k ∈ N
(given in binary). The output is the number of subsets S ⊆ {1, . . . , n} such that (cid:80)

i∈S si ≤ k.

The problem #Knapsack is well known to be #P-complete. Since we were not able to ﬁnd a proper
reference for this fact, we prove it here by using the #P-hardness of the problem #SubsetSum. An
input of the problem #SubsetSum consists of natural numbers s1, . . . , sn, k ∈ N, and the output is
the number of subsets S ⊆ {1, . . . , n} such that (cid:80)
i∈S si = k. The problem #SubsetSum is shown
to be #P-complete in [4, Theorem 4]. From this we can deduce:
Lemma 27 (Folklore). The problem #Knapsack is #P-complete.

Proof. Membership in #P is trivial. We prove hardness by polynomial-time reduction
from #SubsetSum. Let (s1, . . . , sn, k) ∈ Nn+1 be an input to #SubsetSum.
It is clear
that #SubsetSum(s1, . . . , sn, 0) = #Knapsack(s1, . . . , sn, 0), and that for k ≥ 1 we have
#SubsetSum(s1, . . . , sn, k) = #Knapsack(s1, . . . , sn, k) − #Knapsack(s1, . . . , sn, k − 1), thus es-
tablishing the reduction.

We can now show the second claim of Proposition 8.
Lemma 28. The query COUNTCOMPLETIONS is #P-complete for perceptrons.

Proof. Membership in #P is trivial. We show hardness by polynomial-time reduction from #Knap-
sack. Let (s1, . . . , sn, k) be an input of #Knapsack. Let M be the perceptron with weights s1, . . . , sn
and bias −(k + 1). Remember that we consider only perceptrons that use the step activation function,
so that an instance x ∈ {0, 1}n is positive for M if and only if (cid:80)n
i=1 xisi − (k + 1) ≥ 0. It is
then clear that #Knapsack(s1, . . . , sn, k) = 2n − COUNTPOSITIVECOMPLETIONS(M, ⊥n), thus
establishing the reduction.

Finally, the third claim of Proposition 8 simply comes from the fact that MLPs can simulate arbitrary
Boolean formulas (Lemma 13), and the fact that counting the number of satisfying assignments of a
Boolean formula (#SAT) is #P-complete.

Appendix F. Proof of Proposition 9

We now prove Proposition 9, that is:
Proposition 9. The query COUNTCOMPLETIONS can be solved in pseudo-polynomial time for
perceptrons (assuming the weights and biases to be integers given in unary).

The ﬁrst part of the proof is to show how to transform in polynomial time and arbitrary instance
of COUNTPOSITIVECOMPLETIONS for perceptrons (with the weights and bias being integers given
in unary) into an instance of #Knapsack that has the same number of solutions.
Lemma 29. Let M = (w, b) be a perceptron having at
least one positive instance,
with the weights and bias being integers given in unary, and let x be a partial instance.
time an input (s1, . . . , sm, k) ∈ Nm+1 of #Knapsack such
We can build in polynomial
that COUNTPOSITIVECOMPLETIONS(M, x) = #Knapsack(s1, . . . , sm, k), with s1, . . . , sm, k
written in unary (i.e., their value is polynomial in the input size).

Proof. The ﬁrst step is to get rid of the components that are deﬁned by x, like we did in Lemma 24.
Deﬁne

• A := (cid:80)

xi(cid:54)=⊥ xiwi;

• w(cid:48) := (wi | xi = ⊥); and

• b(cid:48) := b + A;

23

and let M(cid:48) be the perceptron (w(cid:48), b(cid:48)). Notice that the dimension of M(cid:48)
is equal to the
number of undeﬁned components of x;
It is then clear that
COUNTPOSITIVECOMPLETIONS(M, x) is equal to the number of positive instances of M(cid:48), that is,
of instances x(cid:48) ∈ {0, 1}m that satisfy

let us write m this number.

(cid:104)w(cid:48), x(cid:48)(cid:105) + b(cid:48) ≥ 0

(4)

Now, let J be the maximum possible value of (cid:104)w(cid:48), x(cid:48)(cid:105); J can clearly be computed in linear time
by setting x(cid:48)
i = 0 otherwise. We then claim that the number of solutions to
Equation 4 is equal to the number of solutions of

i ≥ 0 and x(cid:48)

i = 1 if w(cid:48)

(5)
i| for 1 ≤ i ≤ m and k := J + b(cid:48). Indeed, consider the function h : {0, 1}m →
where si := |w(cid:48)
{0, 1}m deﬁned componentwise by h(x(cid:48)
i if w(cid:48)
i otherwise. Then h is
a bijection, and we will show that for any x(cid:48) ∈ {0, 1}m, we have that x(cid:48) satisﬁes Equation 4 if and
only if h(x(cid:48)) satisﬁes Equation 5, from which our claim follows. In order to see this, consider that

i < 0 and h(x(cid:48)

i) := 1 − x(cid:48)

i) := x(cid:48)

(cid:104)s, x(cid:48)(cid:105) ≤ k,

(3) ⇐⇒

(cid:88)

i

w(cid:48)

ix(cid:48)

i ≥ −b(cid:48) ⇐⇒

⇐⇒

⇐⇒

(cid:88)

wi≥0
(cid:88)

wi≥0
(cid:88)

wi<0

w(cid:48)

ix(cid:48)

i +

(cid:88)

w(cid:48)

ix(cid:48)

i ≥ −b(cid:48)

wi<0
(cid:88)

|w(cid:48)

i|x(cid:48)

i −

|w(cid:48)

i|x(cid:48)

i ≥ −b(cid:48)

|w(cid:48)

i|x(cid:48)

i −

wi<0
(cid:88)

wi≥0

|w(cid:48)

i|x(cid:48)

i ≤ b(cid:48)

On the other hand, we have

h(x(cid:48)) satisﬁes (4) ⇐⇒

(cid:88)

|w(cid:48)

i|h(x(cid:48)

i) ≤ J + b(cid:48)

⇐⇒

i
(cid:88)

wi<0

⇐⇒ (7)

|w(cid:48)

i|x(cid:48)

i +

(cid:88)

wi≥0

|w(cid:48)

i|(1 − x(cid:48)

i) ≤

(cid:88)

wi≥0

|w(cid:48)

i| + b(cid:48)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

Last, let us observe that we have k ≥ 0, as otherwise M would not have any positive instance.
Therefore (s1, . . . , sm, k) is a valid input of #Knapsack, which concludes the proof.

We can now easily combine Lemma 29 together with a well-known dynamic programming algorithm
solving #Knaspsack in pseudo-polynomial time.

Proof of Proposition 9. Let M = (w, b) be a perceptron, with the weights and bias being inte-
gers given in unary, and let x be a partial instance. First, we check that the maximal value
of (cid:104)x, w(cid:105) is greater than −b, as otherwise M has no positive instance and we can simply re-
turn 0. We then use Lemma 29 to build in polynomial time an instance (s1, . . . , sm, k) ∈ Nm+1
of #Knapsack such that COUNTPOSITIVECOMPLETIONS(M, x) = #Knapsack(s1, . . . , sm, k), and
with s1, . . . , sm, k being written in unary (i.e., their value is polynomial in the input size). We can
then compute #Knapsack(s1, . . . , sm, k) by dynamic programming as follows. For i ∈ {1, . . . , m}
and C ∈ N, deﬁne the quantity DP[i][C] := |{S ⊆ {1, .., i}| (cid:80)
j∈S sj ≤ C}|. We wish to com-
pute DP[m][k]. We can do so by computing DP[i][C] for i ∈ {1, . . . , m} and C ∈ {0, . . . , k},
using the relation DP[i + 1][C] = DP[i][C] + DP[i][C − si+1], and starting with the convention
that DP[0][a] = 0 for all a < 0 and that DP[0][a] = 1 for all a ≥ 0. It is clear that the whole
procedure can be done in polynomial time.

Appendix G. Proof of Proposition 10

We prove in this section Proposition 10, whose statement we recall here:

24

Proposition 10. The problem COUNTCOMPLETIONS restricted to perceptrons admits an FPRAS
(and the use of randomness is not even needed in this case). This is not the case for MLPs, on the
other hand, at least under standard complexity assumptions.

The fact that the query has no FPRAS for MLPs is because MLPs can efﬁciently simulate Boolean
formulas (Lemma 13), and it is well known that the problem #SAT (of counting the number of
satisfying assignments of a Boolean formula) has no FPRAS unless NP = RP. Hence we only need
to prove our claim concerning perceptrons.

Proof of Proposition 10 for perceptrons. We can assume without loss of generality that the weights
and bias are integers, as we can simply multiply every rational by the lowest common denominator
(note that the bit lenght of the lowest common denominator is polynomial, and that it can be computed
in polynomial time5). We then transform the perceptron and partial instance to an input of #Knapsack
with the right number of solutions using Lemma 29, by observing that the construction also takes
polynomial time when the input weights are given in binary (and by considering that the s1, . . . , sm, k
are also computed in binary). We can then apply an FPTAS to this #Knapsack instance, as shown
in [18, 29].

Appendix H. Background in parameterized complexity

In this section we present the notions from parameterized complexity that we will need to prove
Proposition 11.
A parameterized problem is a language L ⊆ Σ∗ × N, where Σ is a ﬁnite alphabet. For each
element (x, k) of a parameterized problem, the second component is called the parameter of the
problem. A parameterized problem is said to be ﬁxed parameter tractable (FPT) if the question of
whether (x, k) belongs to L can be decided in time f (k) · |x|O(1), where f is a computable function.

The FPT class, as well as the other classes we will introduce in this paper, are closed under a
particular kind of reductions. A mapping φ : Σ∗ × N → Σ∗ × N between instances of a parameterized
problem A to instances of a parameterized problem B is said to be an fpt-reduction if and only if

• (x, k) is a yes-instance of A ⇐⇒ φ(x, k) is a yes-instance of B.

• φ(x, k) can be computed in time |x|O(1) · f (k);
• There exists a computable function g such that k(cid:48) ≤ g(k), where k(cid:48) is the parameter

of φ(x, k).

We deﬁne the complexity classes that are relevant for this article in terms of circuits. Recall that a
circuit is a rooted directed acyclic graph where nodes of in-degree 0 are called input gates, and that
the root of the circuit is called the output gate. Internal gates can be either OR, AND, or NOT gates.
All NOT nodes have in-degree 1. Nodes of types AND and OR can either have in-degree at most 2, in
which case they are said to be small gates, or in-degree bigger than 2, in which case they are said to
be large gates. The depth of a circuit is deﬁned as the length (number of edges) of the longest path
from any input node to the output node. The weft of a circuit is deﬁned as the maximum amount
of large gates in any path from an input node to the output node. An assignment of a circuit C is a
function from the set of input gates in C to {0, 1}. The weight of an assignment is deﬁned as the
number of input gates that are assigned 1. Assignments of a circuit naturally induce a value for each
gate of the circuit, computed according to the label of the gate. We say an assignment satisﬁes a
circuit if the value of the output gate is 1 under that assignment.

The main classes we deal with are those composing the W-hierarchy and the W(Maj)- hierarchy, a
variant proposed by Fellows et al. [14]. These complexity classes can be deﬁned upon the WEIGHTED
CIRCUIT SATISFIABILITY problem, parameterized by speciﬁc classes C of circuits, as deﬁned below.

5We need to compute the least common multiple (lcm) of a set of integers a1, . . . , an. Indeed, it is easy
to check that lcm(a1, . . . , an) = lcm(lcm(a1, . . . , an−1), an), which reduces inductively the problem to
computing the lcm of two numbers in polynomial time. It is also easy to check that lcm(a1, a2) = a1a2
gcd(a1,a2) ,
where gcd(a1, a2) is the greatest common divisor of a1 and a2. As multiplication can clearly be carried in
polynomial time, and Euclid’s algorithm allows computing the gcd function in polynomial time, we are done.

25

Problem: WEIGHTED CIRCUIT SATISFIABILITY(C), abbreviated WCS(C)

Input: A circuit C ∈ C

Parameter: An integer k

Output: YES, if there is a satisfying assignment of weight exactly k for C,

and NO otherwise.

We consider two restricted classes of circuits. First, Ct,d, the class of circuits using the connectives
AND, OR, NOT that have weft at most t and depth at most d. On the other hand, we consider Mt,d,
the class of circuits that use (only) the MAJORITY connective (that is satisﬁed exactly when more
than half of its inputs are true), have weft at most t and depth at most d. In the case of majority
gates, we allow multiple parallel edges. Observe that, even though his is not useful for circuits with
(OR, AND, NOT)-gates, it allows circuits majority gates to receive multiple times a same input. In
the case of majority gates, a gate is said to be small if its fan-in is at most 3.

We can then deﬁne each class W[t] (resp., W(Maj)[t]) as the set of parameterized problems that can
be fpt-reduced to WCS(Ct,d) (resp., WCS(Mt,d)) for some constant d. Note that the notion of can
be fpt-reduced is transitive, and thus the classes W[t] and W(Maj)[t] are closed under fpt-reductions.
As usual, a parameterized problem A is then said to be W[t]-hard (resp., W(Maj)[t]-hard) when
every parameterized problem in W[t] (resp., W(Maj)[t]) can be fpt-reduced to A.

Appendix I. Proof of Proposition 11

In this section we prove Proposition 11, that is:
Proposition 11. For every t ≥ 1 the MINIMUMCHANGEREQUIRED query over rMLPs with 3t + 3
layers is W(Maj)[t]-hard and is contained in W(Maj)[3t + 7].

We ﬁrst explain what are rMLPs, then sketch the proof, and then proceed with the proof.

Given an MLP M, with the dimension of the layers being d0, . . . , dk, we deﬁne its graph size
as N := (cid:80)k
i=0 di. We say an MLP with graph size N is restricted (abbreviated as rMLP) if each of
its weights and biases can be represented as a decimal number with at most O(log(N )) digits. More
precisely, represented as (cid:80)K
i=−K ai10i, for integers 0 ≤ ai ≤ 9 and K ∈ O(log N ). Note that all
numbers expressible in this way are also expressible by fractions, where the numerator is an arbitrary
integer bounded by a polynomial in N , and the denominator is a power of 10 whose value is bounded
as well by a polynomial in N .

We now explicit a family of parameterized problems indexed by an integer t ≥ 1.

Problem:

t-MINIMUMCHANGEREQUIRED, abbreviated t-MCR

Input: An rMLP M with at most t layers, an instance x

Parameter: An integer k

Output: YES, if there exists an instance y with d(x, y) ≤ k
and M(x) (cid:54)= M(y), and NO otherwise

We rewrite the statement of Proposition 11 with this explicit notation.
(Restatement of Proposition 11). For every t ≥ 1, the (3t + 3)-MCR problem is W(Maj)[t]-hard
and is contained in W(Maj)[3t + 7].

As the proof of Proposition 11 is quite involved, we ﬁrst present a proof sketch that summarizes the
process.

Hardness. We prove hardness in Section I.1. Showing that a parameterized problem A is W[t]-
hard (resp., W(Maj)[t]-hard) is usually complicated since, by directly using the deﬁnition, one
would have to show that for every ﬁxed d ∈ N, there exists an fpt-reduction fd from WCS(Ct,d)
(resp., from WCS(Mt,d)) to A. Instead, it is usually more convenient to prove ﬁrst some form
of normalization theorem stating that a particular class of circuits, for which one knows the value
of d, is already hard for W[t] (or W(Maj)[t]).6 Following this approach, we start by showing

6Useful normalization theorems for the W-hierarchy are proved in the work of Downey, Fellows and Regan
[11, 13], or Buss and Islam. [6]. Our normalization theorem for the W(Maj)-hierarchy is inspired from those.

26

loose normalization theorem for the W(Maj)-hierarchy in Lemma 30; namely, we prove that the
problem W CS(M3t+2,3t+3) is W(Maj)[t]-hard. The main difﬁculty here is to reduce the depth d of
the majority circuits, for any ﬁxed d ∈ N, to a depth of at most 3t + 3. We then show in Lemma 31
that rMLPs can simulate majority circuits, without increasing the depth of the circuit. In Theorem 32
we use this construction to show an fpt-reduction from W CS(M3t+2,3t+3) to (3t + 3)-MCR. This is
enough to conclude hardness for W(Maj)[t].

Membership. We prove membership in Section I.2. Presented in Theorem 34, the proof consists of 4
steps. We ﬁrst show in Lemma 35 how to transform a given rMLP M that into an MLP M(cid:48) that uses
only step activation functions and that has the same number of layers. Then, as a second step, we
build an MLP M(cid:48)(cid:48), with 3t + 4 layers and again using only the step activation function, such that M(cid:48)(cid:48)
has a satisfying assignment of weight k if and only if (M, x, k) is a positive instance of the t-MCR
problem. The third step is to use a result of circuit complexity [17] stating that circuits with weighted
thresholds gates (which are equivalent to biased step functions), can be transformed into circuits using
only majority gates, increasing the depth by no more than 1. This yields a circuit CM(cid:48)(cid:48) with 3t + 5
layers. However, the circuit CM(cid:48)(cid:48), resulting from the construction of Goldmann et al. [17], has
both positive variables and negated variables as inputs, as their model needs to be able to represent
non-monotone functions. For the fourth and last step, we build a circuit C ∗
M(cid:48)(cid:48) based on CM(cid:48)(cid:48), that ﬁts
the description of majority circuits as deﬁned by [14, 15] (i.e., the one that we use). This circuit C ∗
M(cid:48)(cid:48)
has weft 3t + 7, and we prove that (C ∗
M(cid:48)(cid:48) , k + 1) is a positive instance of the Weighted Circuit
Satisﬁability problem that characterizes the class W(Maj)[t] if and only if (M, x, k) is a positive
instance of the (3t + 3)-MCR problem. The whole construction being an fpt-reduction, this will be
enough to conclude membership in W(MAJ)[3t + 7].

Observe that (r)MLPs can be interpreted as well as rooted directed acyclic graphs, with weighted
edges and where each node is associated a layer according to its (unweighted) distance from the
root. Every node in a certain layer (cid:96) is connected to every node in layers (cid:96) − 1 and (cid:96) + 1. We will
sometimes use this equivalent interpretation, which turns out to be more handy for some of the proofs
in this section.

I.1 Hardness

As explained in the proof sketch, we start by establishing a normalization theorem for the W(Maj)-
hierarchy.
Lemma 30. The problem W CS(M3t+2,3t+3) is W(Maj)[t]-hard.

Proof. A signiﬁcant part of this proof is based on techniques due to Fellows et al. [14] and to Buss
et al. [6]. Let C be an arbitrary majority circuit of weft at most t and depth at most d ≥ t for
some constant d, and let k be the parameter of the input instance. We deﬁne a small sub-circuit as
a maximally connected sub-circuit comprising only small gates. Now, consider a path π from an
arbitrary input node of C to its output gate. We claim that π intersects at most t + 1 small sub-circuits.
Indeed, there must be at least one large gate separating every pair of small sub-circuits intersected
by π, as otherwise the maximality assumption would be broken. But in π, as in any path, there are at
most t large gates, because of the weft restriction, from where we conclude the claim. Now, for each
small sub-circuit S, consider the set IS of its inputs (that may be either large gates or input nodes
of C). As small gates have fan-in at most 3, and the depth of each small sub-circuit is at most d, we
have that |IS| ≤ 3d. We can thus enumerate in constant time all the satisfying assignments of S. We
identify each assignment with the set of variables to which it assigns the value 1. We keep a set Γ
with the satisfying assignments among IS that are minimal with respect to ⊆. Then, because of the
fact that majority circuits are monotone, S can be written in monotone DNF as

S ≡

(cid:95)

(cid:94)

x

γ∈Γ

x∈γ

Note that the size of Γ is trivially bounded by the constant 23d
on C, by following these steps:

. We then build a circuit C (cid:48), based

1. Add 3d(k + 1) extra input nodes. We distinguish the ﬁrst, that we denote as u, from

the 3d(k + 1) − 1 remaining, that we refer to by N .

27

2. Add a new output gate that is a binary majority between the old output gate and the node u.

3. Replace every small sub-circuit S by its equivalent monotone DNF formula, consisting of

one large OR-gate and many large AND-gates.

4. Relabel every large OR-gate, of fan-in (cid:96) ≤ 23d

created in the previous step to be a majority
gate with the same inputs, but to which one wires as well (cid:96) parallel edges from the input
node u.

5. Relabel every large AND-gate g, of fan-in (cid:96) ≤ 3d, to be a majority gate. If g had edges
from gates g1, . . . , g(cid:96), then replace each edge coming from a gi by k + 1 parallel edges, and
ﬁnally, wire (cid:96)(k + 1) − 1 nodes in N to g.

An illustration of the transformation ins presented in Figure 3. We now check that C (cid:48) is a (majority)
circuit in M3t+2,3t+3. To bound the depth and weft of C (cid:48) we need to account for all the sub-circuits
of depth 2 that we introduced in steps 3–5 to replace each small sub-circuit of C. Note that two
small sub-circuits that were parallel in C (meaning no input-output path could intersect both) have
corresponding sub-circuits that are parallel in C (cid:48). Consider now an arbitrary path π from a variable
to the root of C, and let π(cid:48) be the corresponding path in C (cid:48) (that goes to the new root of C (cid:48)). The
path π contains one variable gate, at most t large gates, and intersects at most t + 1 small sub-circuits.
The corresponding path π(cid:48) in C (cid:48) still contains the variable gate, the (at most t) large gates that
were in π, and for each of the at most t + 1 small-subcircuits that π intersected, π(cid:48) now contains
exactly 2 large gate (and π(cid:48) also contains the new output gate of C (cid:48)). Therefore, the length of π(cid:48) is at
most 1 + t + 2(t + 1) + 1 − 1 = 3t + 3, and it contains at most t + 2(t + 1) = 3t + 2 large gates.
Since every path π(cid:48) in C (cid:48) from a variable to the root of C (cid:48) corresponds to such a path π in C, we
obtain that the depth of C (cid:48) is at most 3t + 3 and its weft is at most 3t + 2. Hence, C (cid:48) is indeed a
majority circuit in M3t+2,3t+3.
We now prove that ((cid:63)) there is a satisfying assignment of weight k + 1 for C (cid:48) if and only if there
is a satisfying assignment of weight k for C, which would conclude our fpt-reduction. The proof
for this claim is based on how the constructions in step 4 and 5 actually simulate large OR-gates
and AND-gates, respectively.7 We prove each direction in turn.

Forward direction. Let us assume that there exists a satisfying assignment of weight k + 1 for C (cid:48).
First, because input node u is directly connected to the output gate through a binary majority, it must
be assigned to 1 in order to satisfy C (cid:48). Let C (cid:48)(cid:48) be the sub-circuit of C (cid:48) formed by all the nodes that
descend from the old output-gate in C (cid:48). Then C (cid:48)(cid:48) needs to be satisﬁed in order to satisfy C (cid:48). Since u
is not present in C (cid:48)(cid:48), an assignment of weight k + 1 that satisﬁes C (cid:48) is made by assigning 1 to u and
to exactly k other input gates. In order to prove the claim, we will show that (†) an assignment of
weight k for the inputs of C (cid:48)(cid:48) satisﬁes C (cid:48)(cid:48) if and only if its restriction to the inputs of C satisﬁes C,
assuming u is assigned to 1. As C (cid:48)(cid:48) only differs from C because of the replacement of each small
sub-circuit S by its equivalent DNF, and the additional inputs in N , we only need to prove that
steps 4 and 5 actually compute large OR and AND gates. Consider a gate g introduced in step 4,
having edges from gates g1, . . . , g(cid:96) and (cid:96) edges from node u. Therefore, g has fan-in 2(cid:96), and as u
always contributes with a value of (cid:96) to g, we have that g is satisﬁed exactly when at least one of
the gates g1, . . . , g(cid:96) is satisﬁed. Consider now a gate g introduced in step 5. By construction, g has
fan-in equal to 2(cid:96)(k + 1) − 1, from which we deduce that if all gates g1, . . . , g(cid:96) are satisﬁed, then g is
indeed satisﬁed in C (cid:48)(cid:48). On the other hand, if an assignment of weight k does not satisfy every gate gi,
then g receives at most ((cid:96) − 1)(k + 1) units from the gates gi, and as the assignment has weight k, it
receives at most k from the nodes in N . Thus, g receives at most (k + 1)(cid:96) − 1 units, which is less
than half of its fan-in, and thus, g is not satisﬁed. Thus, we have proved (†). However, notice that
the restriction of the assignment might have a weight of strictly less than k in C. But it is clear that,
since the circuit is monotone, we can increase the weight by setting some variables of C to 1, until
the weight becomes equal to k. This proves the forward direction.

Backward direction. Let us now assume an assignment of weight k for C. We then we extend
such an assignment to C (cid:48) by assigning 0 to the inputs in N and 1 to u. Thanks to (†), this is a

7Although this technique can already be found in the work of Fellows et al. [14], we include it here for

completeness.

28

(b) The majority circuit where small sub-circuits
have been replaced by depth-2 majority circuits,
corresponding to their equivalent DNF. The equiv-
alent DNF depth-2 sub-circuits are represented by
rectangles. Once again, the path determining the
weft is colored red. The longest path, determining
the depth of the circuit, is drawn with a dashed
orange line.

(a) A majority circuit where small sub-circuits are
represented with blue blobs, and black nodes cor-
respond to large majority gates. The path deter-
mining the weft is colored red. The longest path,
determining the depth of the circuit, is drawn with
a dashed orange line.

Figure 3: Illustration of the Normalization Lemma (30). In a nutshell, by paying a controlled increase
in weft, the depth of the circuit can be substantially reduced.

satisfying assignment of weight k + 1 for C (cid:48), which proves the backward direction of ((cid:63)) and thus
concludes the proof of Lemma 30.

Then, we show that rMLPs can simulate majority circuits, without increasing the depth of the circuit.
Lemma 31. Given a circuit C containing only majority gates, we can build in polynomial time an
rMLP that is equivalent to C (as a Boolean function) and whose number of layers is equal to the
depth of C.

Proof. First, note that we can assume that circuit C does not contain parallel edges by replacing
each gate g having p edges to a gate g(cid:48) by p copies g1, . . . , gp with single edges to g(cid:48). We then build
a layerized circuit (remember the deﬁnition of a layerized circuit from Appendix A) C (cid:48) from C,
by applying the same construction that we used in Lemma 13 to layerize a circuit, but using unary
majority gates as identity gates instead. Note that the depth of C (cid:48) is the same as that of C.

Next, we show how each non-output majority gate can be simulated by using two relu-gates (again,
remember the deﬁnition of a relu gate from Appendix A). First, note that (†) for any non-negative
integers x, n ∈ N, the function

fn(x) := relu

(cid:16)

x − (cid:98)

(cid:17)
(cid:99)

n
2

− relu

(cid:16)

x − (cid:98)

(cid:17)

(cid:99) − 1

n
2

is equal to

Majn(x) =

if x > n
2
otherwise

.

(cid:26)1
0

29

We will use (†) to transform the majority circuit C (cid:48) into a circuit C (cid:48)(cid:48) that has only relu gates for
the non-output gates, and that is equivalent to C (cid:48) in a sense that we will explain next. For every
non-output majority gate g of C (cid:48), we create two relu gates g(cid:48)
2 of C (cid:48)(cid:48). The idea is that ((cid:63)) for any
valuation of the input gates (we identify the input gates of C (cid:48) with those of C (cid:48)(cid:48)), the Boolean value of
1 (in C (cid:48)(cid:48))
any non-output gate g in C (cid:48) will be equal to the (not necessarily Boolean) value of gate g(cid:48)
1, g(cid:48)
1 (in C (cid:48)(cid:48)). We now explain what the biases of these new gates g(cid:48)
minus the value of the gate g(cid:48)
2 for
every majority gate g of C (cid:48) are. Letting n be the in-degree of a majority gate g in C (cid:48), the bias of g(cid:48)
1
is −(cid:98) n
2 are
and how we connect them to the other relu gates. We do this by a bottom-up induction on C (cid:48), that is,
on the level of the gates of C (cid:48) (since C (cid:48) is layerized), and we will at the same time show that ((cid:63)) is
satisﬁed. To connect the gates g(cid:48)
2 to the gates of the preceding layer, we differentiate two cases:

2 (cid:99) − 1. Next, we explain what the weights of these new gates g(cid:48)

2 (cid:99), and that of g(cid:48)

2 is −(cid:98) n

1, g(cid:48)

1, g(cid:48)

1, g(cid:48)

Base case. The inputs of the gate g are variable gates; in other words, the level of g in C (cid:48) is 1
(remember that variable gates have level 0). We then set these variable gates to be an
input of both g(cid:48)
2, and set all the weights to 1. It is clear that ((cid:63)) is satisﬁed for the
gates g, g(cid:48)

2 and g(cid:48)
2, thanks to (†).

1, g(cid:48)

Inductive case. The inputs of the gate g are other majority gates; in other words, the level of g in C (cid:48)
is > 1. Then, let 1g, . . . ,m g be the inputs8 (majority gates) of the gate g in C (cid:48), and consider
2) in C (cid:48)(cid:48). We then set all the
their associated pairs of relu gates (1g(cid:48)
gates 1g(cid:48)
2, with a weight of 1, and set all
the gates 1g2, . . . ,m g2 to be input gates of both gates g(cid:48)
2, with a weight of −1. By
induction hypothesis, and using again (†), it is clear that ((cid:63)) is satisﬁed.

1, . . . ,m g1 to be input gates of both gates g(cid:48)

2), . . . , (mg(cid:48)

1 and g(cid:48)

1 and g(cid:48)

1,m g(cid:48)

1,1 g(cid:48)

1,1 g(cid:48)

2), . . . , (mg(cid:48)

1,m g(cid:48)
1 to r(cid:48) with weight 1, and also wire each gate ig(cid:48)

Finally, based on the output gate r of C (cid:48), we create a step gate r(cid:48) in C (cid:48)(cid:48) in the following way. Let
2) their associated pairs in C (cid:48)(cid:48). Then wire
1g, . . . ,m g be the inputs of r, and (1g(cid:48)
2 to r(cid:48) with weight −1. Let −(cid:98) n
each gate ig(cid:48)
2 (cid:99) − 1 be
the bias of r(cid:48).
We have constructed a circuit C (cid:48)(cid:48) whose output gate is a step gate, and all other gates are relu gates.
Consider now a valuation x of the input gates of C (cid:48), which we identify as well as a valuation x(cid:48) of
the input gates of C (cid:48)(cid:48). We claim that C (cid:48)(x) = 1 if and only if C (cid:48)(cid:48)(x(cid:48)) = 1. But this simply comes
from the fact that for x, n ∈ N, we have x > n
2 (cid:99) + 1, and from the fact that ((cid:63)) is
satisﬁed for the input gates of r and of r(cid:48).
The last thing that we have to do is to transform the circuit C (cid:48)(cid:48), that uses only relu gates except for its
output step gate, into a valid MLP. This can be done easily as in the proof of Lemma 13 by adding
dummy connections with weights zero, because C (cid:48)(cid:48) is layerized. The resulting MLP MC is then
equivalent to C, it is clearly an rMLP, its number of layers is exactly the depth of C, and, since we
have constructed it in polynomial time, this concludes the proof.

2 ⇐⇒ x ≥ (cid:98) n

Finally, we use this construction to show an fpt-reduction from W CS(M3t+2,3t+3) to (3t + 3)-MCR.
This is enough to conclude hardness for W(Maj)[t], thanks to Lemma 30.
Theorem 32. There is an fpt-reduction from the problem W CS(M3t+2,3t+3) to the (3t + 3)-MCR
problem.

Proof. We will in fact show an fpt-reduction from W CS(Mt,t) to t-MCR, which gives the claim
when applied to 3t + 3, noting of course that W CS(M3t+3,3t+3) is trivially at least as hard as
W CS(M3t+2,3t+3). Let (C, k) be an instance of W CS(Mt,t). We ﬁrst build an MLP MC equiva-
lent to C (as Boolean functions) by using Lemma 31. The MLP MC has t layers. Then, we build an
MLP M(cid:48)

C, that is based on MC, by following the steps described below:

1. Initialize M(cid:48)

C to be an exact copy of MC.

2. Add an extra input, that we call v1, to M(cid:48)
C has dimension n + 1.

then M(cid:48)

C. This means that if MC had dimension n,

8Please excuse us for using left superscripts.

30

3. Create nodes v2, . . . , vt, all having a bias of 0, and for each 1 ≤ i < t, connect node vi to

node vi+1 with an edge of weight 1.

4. Let r be the root of M(cid:48)

C, and let m be its fan-in. We connect node vt to r with an edge of

weight m. Moreover, if the bias of r in MC was b, we set it to be b − m in M(cid:48)

C.

5. Observe that M(cid:48)

C is layerized. To make it a valid MLP (where all the neurons of a layer are
connected to all the neurons of the adjacent layers), we do as in the proof of Lemma 13 by
adding dummy null weights.

It is clear that the construction of M(cid:48)
We now prove a claim describing the behavior of M(cid:48)

C.

C takes polynomial time, and that its number of layers is again t.

Claim 33. For any instance x(cid:48) of M(cid:48)
C, expressed as the concatenation of a feature x(cid:48)
input node v1) and an instance x of MC, we have that x(cid:48) is a positive instance of M(cid:48)
if x(cid:48)

1 = 1 and x is a positive instance of MC

1 (for the extra
C if and only

Proof. Consider that, by construction, an instance x(cid:48) is positive for M(cid:48)

C if and only if

n+1
(cid:88)

i=1

i W (cid:48)(t)
h(cid:48)(t−1)

i = mh(cid:48)(t−1)

1

+

n+1
(cid:88)

i=2

i W (cid:48)(t)
h(cid:48)(t−1)

i ≥ −b + m

But by construction h(cid:48)(t−1)
= x(cid:48)
that x(cid:48) is a positive instance of M(cid:48)

1

1, and (cid:80)m+1
C if and only if

i=2 h(cid:48)(t−1)

i W (cid:48)(t)

i = (cid:80)m

i=1 h(t−1)

i W (t)

i

. This means

mx(cid:48)

1 +

m
(cid:88)

i=1

i W (t)
h(t−1)

i ≥ −b + m

Note that if x(cid:48)
positive instance. For the other direction, it is clear that it holds if x(cid:48)
is not possible. Indeed, by the construction of MC, we have that 0 ≤ (cid:80)m
also that −b ≥ 1, which makes the inequality unfeasible.

1 = 1 and x is a positive instance of MC, this inequality is achieved, making x(cid:48) a
1 = 0
i ≤ m, and

1 = 1. We show that in fact x(cid:48)
i=1 h(t−1)

i W (t)

This concludes the proof of the claim.

This claim has two important consequences:

1. As satisfying assignments of C correspond to positive instance of MC, we have that there
is a satisfying assignment of weight exactly k for C if and only if there is a positive instance
of weight exactly k + 1 for M(cid:48)

C.

2. The instance 0n+1 is negative for M(cid:48)
C

This consequences will allow us to ﬁnish the reduction. Consider the instance (M(cid:48)
C, 0n+1, k + 1)
of t-MCR. We claim that this is a positive instance for the problem if and only if (C, k) is a positive
instance of W CS(Mt).
C, 0n+1, k + 1) to be a positive instance of t-MCR. This
For the forward direction, consider (M(cid:48)
means there is an instance x∗ that has the opposite classiﬁcation as 0n+1 under M(cid:48)
C, and differs from
it in at most k + 1 features. By the second consequence of the claim, x∗ must be a positive instance.
Also, differing in at most k + 1 features from 0n+1 means that x∗ has weight at most k + 1. But as
majority gates are monotone connectives, majority circuits are monotones as well, so the existence
of a positive instance x∗ of weight at most k + 1 implies the existence of a positive instance x(cid:48)∗ of
weight exactly k + 1. Therefore, by the ﬁrst consequence of the claim, there is a satisfying assignment
of weight exactly k for C, which implies (C, k) is a positive instance of W CS(Mt,t)

For the backward direction, consider (C, k) to be a positive instance of W CS(Mt,t). This means,
by the ﬁrst consequence of the claim, that there is a positive instance x∗ of weight exactly k + 1
C. But based on the second consequence of the claim, 0n+1 is a negative instance for M(cid:48)
for M(cid:48)
C.

31

As x∗ differs from 0n+1 in no more than k + 1 features, and they have opposite classiﬁcations, we
have that (M(cid:48)

C, 0n+1, k + 1) is a positive instance of t-MCR.

As the whole construction takes polynomial time, and the reduction changes the parameter in a
computable way, from k to k + 1, it is an fpt-reduction. This concludes the proof.

I.2 Membership

In this section we prove membership in W(Maj)[3t + 7]. This will be enough to prove:
Theorem 34. There is an fpt-reduction from t-MCR to W CS(Mt+4,t+4), implying (3t + 3)-MCR
belongs to W(Maj)[3t + 7].

As explained in the proof sketch, we ﬁrst show how to transform a given rMLP M that into an
MLP M(cid:48) that uses only step activation functions and that has the same number of layers. More
formally, we prove that rMLPs using only step activation functions are powerful enough to simulate
MLPs that use relu activation functions in the internal layers (and a step function for the output
neuron). The construction is polynomial in the width (maximal number of neurons in a layer) of the
given relu-rMLP, but exponential on its depth (number of layers). We show:
Lemma 35. Given an rMLP M with relu activation functions, there is an equivalent MLP M(cid:48) that
uses only step activation functions and has the same number of layers. Moreover, if the number of
layers of M is bounded by a constant, then M(cid:48) can be computed in polynomial time.

Proof. Let (W (1), . . . , W ((cid:96))), (b(1), . . . , b((cid:96))) and (f (1), . . . , f ((cid:96))) be the sequences of weights,
biases, and activation functions of the rMLP M. Note that f (i) for 1 ≤ i ≤ (cid:96) − 1 is relu and that f ((cid:96))
is the step activation function. The ﬁrst step of the proof is to transform every weight and bias into
an integer. To this end, let L ∈ N, L > 0 be the lowest common denominator of all the weights
and biases, and let M(cid:48) be the MLP that is exactly equal to M except that all the weights have been
multiplied by L, and all the biases of layer i have been multiplied by Li. Observe that M(cid:48) has only
integer weights and biases. When w (resp., b) is a weight (resp., bias) of M, we write w(cid:48) (resp., b(cid:48))
the corresponding value in M(cid:48). We claim that M and M(cid:48) are equivalent, in the sense that for every
x ∈ {0, 1}n, it holds that M(x) = M(cid:48)(x). Indeed, for 0 ≤ i ≤ (cid:96), let h(i) and h(cid:48)(i) be the vectors
of values for the layers of M and M(cid:48), respectively, as deﬁned by Equation 1. We will show that ((cid:63))
for all 1 ≤ i ≤ (cid:96) − 1 we have h(cid:48)(i) = Li × h(i). The base case of i = 0 (i.e., the inputs) is trivially
true. For the inductive case, assume that ((cid:63)) holds up to i and let us show that it holds for i + 1. We
have:

h(cid:48)(i+1) = relu(h(cid:48)(i)W (cid:48)(i+1) + b(cid:48)(i+1))

= relu(L × h(cid:48)(i)W (i+1) + Li+1 × b(i+1)) by the deﬁnition of M(cid:48)
= relu(Li+1 × h(i)W (i+1) + Li+1 × b(i+1)) by inductive hypothesis
= Li+1 × relu(h(i)W (i+1) + b(i+1)) by the linearity of relu
= Li+1 × h(i+1),

and ((cid:63)) is proven. Since the step function (used for the output neuron) satisﬁes step(cx) = c step(x)
for c > 0, we indeed have that M(x) = M(cid:48)(x).
We now show how to build a model M(cid:48)(cid:48) that uses only step activation functions and that is equivalent
to M(cid:48). The ﬁrst step is to prove an upper bound for the values in h(cid:48). We start by bounding the
values in h. Let D be width of M, that is, the maximal dimension of a layer of M, and let C be the
maximal absolute value of a weight or bias in M; note that the value of C is asymptotically bounded
by |M|O(1) because M is an rMLP. For every instance x, we have that

0 ≤ h(i)

j = relu

(cid:32)

(cid:88)

k

k W (i)
h(i−1)

k,j + b(i)

j

(cid:33)

≤ DC max

k

h(i−1)
k

+C ≤ (D+1)C max(1, max

k

h(i−1)
k

)

Using this inequality, and the fact that maxk h(0)
((D + 1)C)i. By ((cid:63)), this implies that 0 ≤ h(cid:48)(i)

j ≤ ((D + 1)CL)i.

k ≤ 1, we obtain inductively that 0 ≤ h(i)

j ≤

32

≥ 1

≥ 2

≥ 3

Figure 4: Illustration of the conversion from a relu activation function to step activation functions,
for S = 3. The weights are unchanged, and if the bias of the original neuron was b then the bias in
the j-th copy of that neuron becomes b − j.

As all values (weights, biases and the h(cid:48) vectors) in M(cid:48) consist only of integers, and are all bounded
by the integer S := ((D + 1)CL)(cid:96), then each relu in M(cid:48) with bias b becomes equivalent to the
following function f ∗:

f ∗(x + b) := [x + b ≥ 1] + [x + b ≥ 2] + . . . + [x + b ≥ S]

(13)

Where [y ≥ j] := 1 if y ≥ j and 0 otherwise. Hence, in order to ﬁnish the proof, it is enough to show
how activation functions of the form f ∗ can be simulated with step activation functions. Namely,
we show how to build M(cid:48)(cid:48), that uses only step activation functions, from M(cid:48), in such a way that
both models are equivalent. In order to do so, we replace each f (i), W (cid:48)(i), b(cid:48)(i) for 1 ≤ i ≤ (cid:96) in
the following way. If i = (cid:96), then nothing needs to be done, as f ((cid:96)) is already assumed to be a step
activation function. When 1 ≤ i < (cid:96), we replace the weights, activations and biases in a way that
is better described in terms of the underlying graph of the MLP. We split every internal node, with
bias b into S copies, all of which will have the same incoming and outgoing edges as the original
nodes, with the same weights. The j-th copy will have a bias equal to b − j. We illustrated this step
in Figure 4. This construction is an exact simulation of the function f ∗ deﬁned in Equation 13.
The computationally expensive part of the algorithm is the replacement of each node in M(cid:48) by S
nodes, which takes time at most S = ((D + 1)CL)(cid:96) ∈ O(|M|(cid:96)(CL)(cid:96)) per node and thus at
most O(|M|(cid:96)+1(CL)(cid:96)) in total. Since (cid:96) is a constant, and C is bounded by a polynomial on M, we
only need to argue that L is bounded as well. Indeed, as M is an rMLP, each weight and bias can be
assumed to be represented as a fraction whose denominator is a power of 10 of value polynomial in
the graph size N of M. But the lowest common multiple of a set of powers of 10 is exactly the largest
power of 10 in the set. Therefore L ≤ 10p, where p ∈ O(log N ), and thus L ∈ O(N c) ⊆ O(|M|c)
for some constant c. We conclude from this that the construction takes polynomial time.

We are now ready to prove Theorem 34.

Proof of Theorem 34. Let (M, x, k) be an instance of t-MCR. During this reduction we assume
that n > 2k, as otherwise the result can be achieved trivially; if n ≤ 2k then trying all instances that
differ by at most k from x takes only O(kk), and thus we can solve the entire problem in fpt-time
and return a constant-size instance of W CS(Mt+2), completing the reduction.
We start by applying Lemma 35 to build an equivalent MLP M(cid:48) that uses only step activation
functions. As t is constant, this construction takes polynomial time, and its resulting MLP M(cid:48) has t
layers as well. If x is a negative instance of M(cid:48) (and thus of M) we do nothing. This can trivially be
checked in polynomial time, evaluating x in M(cid:48). But if x happens to be a positive instance of M(cid:48),

33

then we change the deﬁnition of M(cid:48) negating its root perceptron9, and thus making x a negative
instance. As a result, we can safely assume x to be a negative instance of M(cid:48). We can also, in the
same fashion that we assumed n > 2k, discard the case where the instance 0n is a positive instance
of M(cid:48) that differs by at most k from x, as in such scenario we could also solve the problem in
fpt-time. The same can be done for 1n.
We now build an MLP M(cid:48)(cid:48), that still uses only step activation functions, such that M(cid:48)(cid:48) has a positive
instance of weight exactly k if and only if (M, x, k) is a positive instance of t-MCR.
Let M(cid:48)(cid:48) be a copy of M(cid:48) to which we add one extra layer at the bottom. For each 1 ≤ i ≤ n, we
connect the i-th input node of M(cid:48)(cid:48) to what was the i-th input node of M(cid:48), but is now an internal
node in M(cid:48)(cid:48). If xi = 0 then the node in M(cid:48)(cid:48) corresponding to the i-th input node of M(cid:48) has a bias
of 1, and the weight of the edge coming from the i-th input node of M(cid:48)(cid:48) is also 1. On the other hand,
if xi = 1, then the node in M(cid:48)(cid:48) corresponding to the i-th input node of M(cid:48) has a bias of 0, and the
weight of the connection added to it is −1. After doing this, we add k − 1 more input nodes to M(cid:48)(cid:48),
a new node p in the t-th layer and a new root node r(cid:48)(cid:48), that is placed in the layer t + 1. We connect r(cid:48),
the previous root node, to r(cid:48)(cid:48) of M(cid:48) with weight 1, and all input nodes to node p with weights of 1. In
case p is more than one layer above the new input nodes, we connect them through paths of identity
gates, as shown in Lemma 13. We set the bias of r(cid:48)(cid:48) to −2, and the bias of p to −k. All non-input
nodes added in the construction use step activation functions.
We now prove a claim stating that M(cid:48)(cid:48) has exactly the intended behavior.

Claim 36. The MLP M(cid:48)(cid:48) has a positive instance of weight exactly k if and only if (M, x, k) is a
positive instance of t-MCR.

Proof. For the forward direction, assume M(cid:48)(cid:48) has a positive instance x(cid:48) of weight exactly k. As
the root r(cid:48)(cid:48) has a bias of −2, and two incoming edges with weight 1, and given that the output
of any node is bounded by 1, as only step activation functions are used, we conclude that both p
and r(cid:48), the children of r(cid:48)(cid:48), must have a value of 1 on x(cid:48). The fact that r(cid:48) has a value of 1 on x(cid:48)
implies that xs, the restriction of x that considers only nodes that descend from r(cid:48), must be a positive
instance for the submodel Ms induced by considering only nodes that descend from r(cid:48). But one
can easily check that by construction, we have that Ms(xs) = M(cid:48)(xs ⊕ x), where ⊕ represents
the bitwise-xor. Thus, xs ⊕ x is a positive instance for M, and consequently for M. As xs ⊕ x
differs from x by exactly the weight of xs, as 0 is the neutral element of ⊕, and the weight of xs
is by deﬁnition no more than the weight of x(cid:48), which is in turn no more than k by hypothesis, we
conclude that (M, x, k) is a positive instance of t-MCR.
For the backward direction, assume there is a positive instance x(cid:48) of M that differs from x in at
most k positions. This means that x(cid:48)(cid:48) = x ⊕ x(cid:48) has weight at most k. By the same argument used
in the forward direction, Ms(x(cid:48)(cid:48)) = M(cid:48)(x(cid:48)(cid:48) ⊕ x) = M(cid:48)(x(cid:48)), as x ⊕ x(cid:48) ⊕ x = x ⊕ x ⊕ x(cid:48) = x(cid:48),
because ⊕ is both commutative and its own inverse. But the fact that x(cid:48) is a positive instance
of M implies that it is also a positive instance for M(cid:48). As we are assuming x(cid:48)| (cid:54)= 0n, we have
that k − |x(cid:48)| ≤ k − 1. Thus, we can create an instance x(cid:48)(cid:48) for M(cid:48)(cid:48) that is equal to x(cid:48) on its
corresponding features, and that sets k − |x(cid:48)| arbitrary extra input nodes to 1, among those created in
the construction of M(cid:48)(cid:48). As the instance x(cid:48)(cid:48) has weight exactly k, it satisﬁes the submodel descending
from p, and as x(cid:48)(cid:48) its equal to x(cid:48) on the submodel descending from r(cid:48), and x(cid:48) is a positive instance
of M(cid:48), we have that this submodel must be satisﬁed as well. Both submodels being satisﬁed, the
whole model M(cid:48)(cid:48) is satisﬁed, hence we conclude the proof.

We thus have a model M(cid:48)(cid:48) with step activation functions, and t + 2 layers, such that if that model has
a satisfying assignment of weight exactly k, then (M, x, k) is a positive instance of t-MCR.

Note that step activation functions with bias are equivalent to weighted threshold gates. We then use
a result by Goldmann and Karpinski [17, Corollary 12] to build a circuit CM(cid:48)(cid:48) that is equivalent (as
Boolean functions) to M(cid:48)(cid:48) but uses only majority gates. The construction of Goldmann et al. can be
carried in polynomial time, and guarantees that CM(cid:48)(cid:48) will have at most t + 3 layers.

9Let P = (w, b) be the perceptron at the root of M(cid:48), which contains only integer values by construction.
Then, the negation of P is simply ¯P = (−w, −b + 1), as −wx ≥ −b + 1 precisely when wx ≤ b − 1, which
occurs over the integers exactly when it is not true that wx ≥ b.

34

There is however a caveat to surpass: although not explicitly stated in the work of Goldmann et
al. [17], their deﬁnition of majority circuit must assume that for representing a Boolean function
from {0, 1}n to {0, 1}, the circuit is granted access to 2n input variables x1, . . . , xn, x1, . . . , xn, as
it is usual in the ﬁeld, and described for example in the work of Allender [1]. We thus assume that
the circuit CM(cid:48)(cid:48) resulting from the construction of Goldmann et al. has this structure, which does not
match the required structure of the majority circuits deﬁning the W(Maj)-hierarchy as deﬁned by
Fellows et al [14, 15]. In order to solve this, we adapt a technique from Fellows et al. [15, p. 17].
We build a circuit C ∗
M(cid:48)(cid:48) that does ﬁt the required structure. Let n be the dimension of M(cid:48)(cid:48) (which
exceeds by k − 1 that of M). We now describe the steps one needs to apply to CM(cid:48)(cid:48) in order to
obtain C ∗

M(cid:48)(cid:48) .

1. Add a new layer with n + 1 input nodes x(cid:48)
of 2n input nodes x1, . . . , xn, x1, . . . , xn.

1, . . . , x(cid:48)

n+1, below what previously was the layer

2. For every 1 ≤ i ≤ n, connect input node x(cid:48)

i with its corresponding node xi in the second
layer, making xi a unary majority, with the same outgoing edges it had as an input node.
This enforces xi = x(cid:48)
i.

3. Create a new root r(cid:48) for the circuit, and let r(cid:48) be a binary majority between the input

node x(cid:48)

n+1 and the previous root r.

4. Replace each previous input node xi by a majority gates mi that has n + 1 − 2k incoming
j with j (cid:54)∈ {i, n + 1}. The outgoing

n+1, and one incoming edge from each x(cid:48)

edges from x(cid:48)
edges are preserved.

It is clear that the circuit C ∗
M(cid:48)(cid:48) is a valid majority circuit in the sense deﬁning the W(Maj)-hierarchy.
And it has 2 layers more than CM(cid:48)(cid:48), yielding a total of t + 5 layers, where the last one has a small
gate. However, it is not evident what this new circuit does. We now prove a tight relationship between
the circuit C ∗

M(cid:48)(cid:48) has a satisfying assignment of weight exactly k + 1 if and only if M(cid:48)(cid:48)

M(cid:48)(cid:48) and M(cid:48)(cid:48).
Claim 37. The circuit C ∗
has a positive instance of weight exactly k.

Proof. Forward Direction. Assume C ∗
the construction, in order to satisfy C ∗
As we assume that node x(cid:48)
1, . . . , x(cid:48)
among x(cid:48)
is exactly equal to

M(cid:48)(cid:48) has a satisfying assignment of weight k + 1. By step 3 of

M(cid:48)(cid:48) , the assignment must set x(cid:48)

n+1 to 1.

n+1 is set to 1, the assignment must set to 1 exactly k input nodes
n and thus the sum of inputs set to 1 of each majority gate mi constructed in step 4,

n + 1 − 2k +

(cid:88)

j(cid:54)∈{i,n+1}

j = n + 1 − 2k + (k − x(cid:48)
x(cid:48)

i) = n + 1 − k − x(cid:48)
i

i > n − k,

i = 0. This way, each gate mi corresponds to the negation of x(cid:48)
i.

and its fan-in is exactly equal to 2n − 2k. Therefore mi is activated when n + 1 − k − x(cid:48)
which happens precisely when x(cid:48)
This way, the subcircuit induced by considering only the nodes that descend from r(cid:48) computes the
same Boolean function that CM(cid:48)(cid:48) computes, under the natural mapping of their variables. Therefore,
a satisfying assignment of weight k + 1 for C ∗
M(cid:48)(cid:48) implies the existence of a satisfying assignment
for CM(cid:48)(cid:48) that chooses exactly k positive variables, and thus a positive instance of weight k for M(cid:48)(cid:48).
Backward Direction. Assume M(cid:48)(cid:48) has a positive instance of weight exactly k. That implies
that CM(cid:48)(cid:48) has a satisfying assignment σ that sets at most k positive variables to 1. Let us consider the
assignment σ(cid:48) for C ∗
M(cid:48)(cid:48) that sets to 1 the same variables that σ does, and additionally sets xn+1 to 1.
The assignment σ(cid:48) has weight exactly k+1. By the same argument used in the forward direction, under
assignment σ(cid:48) the gates mi behave like negations. Thus, the assignment σ(cid:48) induces an assignment
over the second layer of C ∗
M(cid:48)(cid:48) that corresponds precisely to a satisfying assignment of CM(cid:48)(cid:48) , and thus
makes the value of r equal to 1. As both r and xn+1 have value 1 under assignment σ(cid:48), it follows
M(cid:48)(cid:48), are 1 under σ(cid:48) as well. This means that assignment σ(cid:48),
that the value of r(cid:48), and thus of circuit C ∗
which by construction has weight k + 1, is a satisfying assignment for C ∗
M(cid:48)(cid:48) , and thus concludes the
proof.

35

By combining Claim 36 and Claim 37, and noting again that circuit C ∗
M(cid:48)(cid:48) is a valid majority circuit,
in the sense that deﬁnes the W(Maj)-hierarchy, and has weft at most t + 4, we conclude the reduction
of Theorem 34.

Appendix J. Proof of Proposition 12

Based on Proposition 11, we know that interpreting an rMLP (for the problem MCR) with 9t + 27 =
3(3t + 8) + 3 is W(Maj)[3t + 8]-hard. On the other hand, by using the same proposition, the problem
of interpreting an rMLP with 3t + 3 layers is contained in W(Maj)[3t + 7]. But by hypothesis,
W(Maj)[3t + 7] (cid:40) W(Maj)[3t + 8], which is enough to conclude the proof.

36

