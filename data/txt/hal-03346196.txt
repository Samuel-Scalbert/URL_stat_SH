Benchmarking and challenges in security and privacy for
voice biometrics
Jean-Francois Bonastre, Hector Delgado, Nicholas Evans, Tomi Kinnunen,

Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Paul-Gauthier Noe, Jose

Patino, Md Sahidullah, et al.

To cite this version:

Jean-Francois Bonastre, Hector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, et al..
Benchmarking and challenges in security and privacy for voice biometrics. SPSC 2021, 1st ISCA Sym-
posium on Security and Privacy in Speech Communication, ISCA, Nov 2021, Magdeburg, Germany.
￿10.21437/SPSC.2021-11￿. ￿hal-03346196￿

HAL Id: hal-03346196

https://hal.science/hal-03346196

Submitted on 10 Apr 2024

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Benchmarking and challenges in security and privacy for voice biometrics

Jean-Francois Bonastre, H´ector Delgado, Nicholas Evans, Tomi Kinnunen,
Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Paul-Gauthier No´e, Jose Patino,
Md Sahidullah, Brij Mohan Lal Srivastava, Massimiliano Todisco,
Natalia Tomashenko, Emmanuel Vincent, Xin Wang, Junichi Yamagishi

ASVspoof and VoicePrivacy organising committees
organisers@asvspoof.org, organisers@voiceprivacychallenge.org

Abstract

For many decades, research in speech technologies has fo-
cused upon improving reliability. With this now meeting user
expectations for a range of diverse applications, speech tech-
nology is today omni-present. As result, a focus on security
and privacy has now come to the fore. Here, the research effort
is in its relative infancy and progress calls for greater, multi-
disciplinary collaboration with security, privacy, legal and ethi-
cal experts among others. Such collaboration is now underway.
To help catalyse the efforts, this paper provides a high-level
overview of some related research. It targets the non-speech au-
dience and describes the benchmarking methodology that has
spearheaded progress in traditional research and which now
drives recent security and privacy initiatives related to voice
biometrics. We describe: the ASVspoof challenge relating to
the development of spooﬁng countermeasures; the VoicePrivacy
initiative which promotes research in anonymisation for privacy
preservation.

1. Introduction

The voice is among the most natural and convenient means to
In
human-machine interaction and biometric authentication.
some scenarios, particularly telephony or teleconferencing ap-
plications, voice can be the only available biometric. While au-
tomatic speaker veriﬁcation (ASV) systems can provide a reli-
able means to authentication, like all biometric technologies, it
is not without security and privacy concerns. Arguably, these
concerns are potentially greater for voice biometrics than they
can be for authentication systems that use alternative biometric
characteristics.

Security concerns relate to the potential for ASV systems
to be manipulated by adversaries through spooﬁng attacks [1],
now referred to as presentation attacks [2]. Fraudsters can
launch spooﬁng attacks to gain illegitimate access to protected
services or resources by presenting to the ASV system a speech
recording which has been manipulated to sound1 like another
speaker. Without adequate protection, spooﬁng attacks can sub-
stantially degrade the reliability of almost any ASV system.

While not related speciﬁcally to voice biometrics, but to
speech technology more generally, privacy concerns relate to
the potential for speech data to be exploited for purposes other
than those to which an individual might have given consent [3].
Speech signals are a rich source of personal, private informa-
In providing recordings of speech to a particular voice
tion.

1We refer to machine perception rather than human perception; hu-

mans and machines do not hear in the same way.

service, the speaker usually furnishes the service provider with
much more information than is strictly necessary in order to per-
form the expected task, hence the need for privacy preservation.
In this article we describe two speciﬁc benchmarking chal-
lenges launched by the speech processing research community
to expedite solutions to security and privacy concerns. The ﬁrst
involves solutions to protect ASV systems from being manip-
ulated by spooﬁng in the form of spooﬁng countermeasures or
presentation attack detection systems [2] whose development
is spearheaded through the ASVspoof initiative launched in
2015 [4, 5, 6]. The second relates to the VoicePrivacy initiative,
launched in 2020 [7], which aims to promote the development
of privacy preservation solutions for speech technology. The
inaugural VoicePrivacy challenge focused upon anonymisation,
namely techniques to manipulate speech data in order that it
cannot be used with ASV systems to recognise the speaker.

The article is intended as a contribution to the efforts to
build bridges between the speech community and, e.g., the le-
gal and ethical communities and, in particular, to support the re-
cently formed Security and Privacy in Speech Communication
(SPSC) special interest group of the International Speech Com-
munication Association (ISCA). It targets the non-specialist and
is hence intentionally high-level, with a focus upon the essen-
tials and clarity, rather than upon scientiﬁc and technical rigour.
We describe the importance of benchmarking campaigns, clas-
siﬁer fundamentals and the early, classical approaches to per-
formance estimation. After presenting a high-level overview
of ASV systems, the remainder of the article introduces the
ASVspoof and VoicePrivacy initiatives.

2. Evaluation-driven research

In the early days, researchers collected their own datasets to de-
velop methods (algorithms and software). To make technologi-
cal progress in complex tasks such as speech or speaker recogni-
tion, and to be able to meaningfully compare different methods,
the need for commensurable performance benchmarking was
quickly recognised. From the mid-1990s, the National Institute
of Standards and Technology (NIST) — a US-based standard-
isation body — pioneered evaluation-driven research [8, 9].
The key ingredients are: (1) commonly agreed (often public)
data, evaluation rules, and performance metrics; (2) disentan-
gled roles for researchers (evaluees) and evaluators. Perfor-
mance claims should not be reported by evaluees but instead
by independent evaluators who provide common infrastructure
(data, rules, metrics) in the form of an evaluation campaign or
challenge. Only with such a level playing ﬁeld can competing
methods can be meaningfully compared. There are many such

(a)

(b)

(c)

Figure 1:
Illustrative score distributions for (a) automatic
speaker veriﬁcation (ASV), (b) an ASV system subjected
to spooﬁng attacks and (c) an ASV system presented with
anonymised data.

challenges [10, 11] within the speech ﬁeld. Typically, they re-
quire participants [12, 13] to design special-purpose software
to solve some speciﬁc tasks. For many, the software takes the
form of a binary classiﬁer, namely a system which must choose
between two mutually exclusive hypotheses.

Even if classiﬁers are based on well-established statistical
principles and are trained objectively via numerical optimisa-
tion techniques, they can (and do) make errors. Errors are the
result of over-simpliﬁed modelling assumptions, natural vari-
ability in the input data, sources of external nuisance variation,
or as a result of limited training data, etc. The design of ever-
more reliable classiﬁers is the traditional bread and butter of
machine learning and speech research.

A binary classiﬁer makes two types of errors: misses (false
rejections) and false alarms (false acceptances). Misses imply
that the classiﬁer rejects a positive input that should be accepted.
A false alarm results from the classiﬁer accepting a negative
input that should be rejected. Classiﬁer performance can be
estimated by dividing the number of misses and false alarms by
the number of tested positive and negative cases respectively.
The resulting miss rate (Pmiss) and false alarm rate (PF A) can
be seen as proxies for user convenience and security.

Binary classiﬁer decisions are derived in two stages. The
ﬁrst involves computation of a ‘soft decision’ (the score) —
a real number that expresses the classiﬁer’s conﬁdence in the
positive case. Example raw score distributions for such a bi-
nary classiﬁer are illustrated in Fig. 1(a) were the positive case
is the target class and the negative case is the impostor class. In
practice, the score is often a logarithmic likelihood ratio (LLR)
and expresses the relative strength between the two compet-
ing hypotheses. Second, the score is compared with a pre-set
threshold δ. Scores above the threshold imply ‘accept’, whereas
scores below the threshold imply ‘reject’. By increasing the

threshold, the false alarm rate (PF A, red shaded area in Fig. 1)
can be reduced at the expense of a higher miss rate (Pmiss,
green shaded area) and vice versa. The trade-off can be readily
visualised in so-called detection error trade-off (DET) plots [14]
such as that illustrated in Fig. 2 (described below).

Since there are two error rates, which do we report? Or do
we report both? What threshold / how do we set it? The answer
to these questions is rather subtle and a detailed treatment is out-
side the scope of this paper. One particular metric, namely the
equal error rate (EER) is adopted for a broad range of tasks. It
is computed using the threshold that makes miss and false alarm
rates equal (see Fig. 2) — thereby yielding a single number to
report. The lower the EER, the more reliable the classiﬁer.

The authors acknowledge that, even if the EER is depre-
cated in ISO standards [15], it provides a compact summary of
the discrimination capabilities of a classiﬁer — how well it is
capable of observing (in speech, ‘hearing’) differences between
positive and negative inputs. In practice, however, the EER does
not provide a full picture. More comprehensive approaches to
assessment have been developed and have been broadly adopted
by the community. These provide a view of classiﬁer perfor-
mance through the lens of a formal decision policy which rep-
resents the effective trade-off between the two decision out-
comes [16].

3. Automatic speaker veriﬁcation

ASV systems provide one of the most natural and convenient
means to biometric person authentication. Test recordings
(probes) are compared with enrolment recordings (references)
to verify (or not) a claimed identity. The reference is used to
create a model2 which is stored in a reference database. At test
time, the model corresponding to the claimed identity is com-
pared to the test utterance resulting in a soft score. A hard ac-
cept/reject decision can be fully automated (e.g., online bank-
ing) or semi-automated (with some human intervention, e.g.,
when forensic practitioners present voice evidence in court).
Scores should reﬂect reliably the extent to which the strength-
of-evidence supports the same/different identity propositions:
the higher the score, the greater the similarity and vice versa.

Detection error trade-off plots for two different ASV sys-
tems assessed using the ASVspoof 2019 logical access database
and the VoicePrivacy 2020 database are illustrated in Figs. 2
and 4 and show EERs of 2.5% and 1.1% respectively (blue pro-
ﬁles). Each point on any one proﬁle corresponds to a different
decision threshold (operating point). The proﬁles show how
misses can be traded off against false alarms in order to meet
different application requirements.

4. Security vulnerabilities: ASVspoof

Without adequate protections, the reliability of ASV systems
can be compromised by the presentation of synthetic or con-
verted voice, replayed speech and impersonation [6]. Generated
automatically from a text input, today’s state-of-the-art synthe-
sis systems are capable of producing speech that the human can-
not distinguish from bona ﬁde speech [17]. Voice conversion
systems operate directly upon an input speech signal and alter
the voice to that of another speaker [18]. Unlike synthetic and
converted voice spooﬁng attacks, which both demand a certain
technical expertise and suitable training and adaptation data,

2On account of their dynamic nature and the variability in speech

signals, we refer to models, not templates.

scoreImpostorTarget𝞭PmissPFAscoreImpostorTarget𝞭PmissPFASpoofPspoofscoreImpostorTarget𝞭PmissPFAFigure 2: Detection error trade-off plot for the ASVspoof 2019
logical access task. Proﬁles shown for the baseline ASV sys-
tem (blue proﬁle) and the same system subjected to the most
effective synthetic (text-to-speech, TTS) speech spooﬁng attack
(orange proﬁle) and the most effective voice conversion (VC)
spooﬁng attack (red proﬁle).

replay attacks can be launched by the layman, requiring only
consumer-grade recording and replaying devices. All can sub-
stantially degrade ASV reliability. Impersonation, while still a
threat [19, 20], is less effective.

The impact of spooﬁng attacks upon an ASV system is il-
lustrated in Fig. 1(b). Spooﬁng attacks introduce a third class
of input so that the ASV system must now cope with target
(matching), impostor (non-matching) and spoofed utterances.
A successful spooﬁng attack circumvents the ASV system by
provoking a score above the decision threshold. The score
distribution for spooﬁng attacks is illustrated in the middle of
Fig. 1(b). Spooﬁng attacks provoke a false alarm rate Pspoof
that is greater than the original false alarm rate PF A. One can
think of spooﬁng attacks as a special case of impostors where
there is a concerted effort to deceive the ASV system.

Without the capacity to distinguish between spoofed and
bona ﬁde speech, ASV reliability will degrade as a result of
spooﬁng attacks, sometimes substantially. This degradation as-
sessed using the ASVspoof 2019 logical access database (syn-
thetic and converted voice spooﬁng attacks) is illustrated in
Fig. 2. The baseline EER (with no spooﬁng attacks) of 2.5%
increases to over 50% when impostor trials are replaced by the
most effective synthetic and converted voice spooﬁng attacks.
These results should be interpreted with caution, however. In
practice, one must consider the relative likelihood of the ASV
system being presented with target and impostor trials, versus
that of spooﬁng attacks. Without this consideration, the results
in Fig. 2 might convey an overly pessimistic view of the vulner-
abilities to spooﬁng. There is, in any case, potential to detect
attacks automatically using countermeasures.

The series of ASVspoof challenges held bi-annually since
2015 have spearheaded the development of spooﬁng counter-
measures or presentation attack detection (PAD) solutions for
ASV. Spooﬁng countermeasures can be applied prior to ASV
in order to detect attacks and prevent them from reaching the
ASV system. The most recently completed challenge was held
in 2019 and included separate logical access (synthetic and
converted voice attacks) and physical access (replay attacks)

Figure 3: As for Fig. 2 except for spooﬁng countermeasures.
Proﬁles shown for the top ﬁve performing systems.

tasks [6]. DET plots for the top-ﬁve performing spooﬁng coun-
termeasures for the ASVspoof 2019 logical access task are
shown in Fig. 3. They show EERs of as low as 0.2%.

Thus, while spooﬁng attacks can present a substantial threat
to reliability, and while human listeners may not be able to de-
tect spooﬁng attacks, countermeasures can be effective. Even
so, while ASV countermeasures have proven potential to detect
attacks, they can also degrade usability; they can erroneously
classify bona ﬁde speech as spoofed speech. Assessment and
performance estimates should hence reﬂect the impact of both
spooﬁng and countermeasures upon the ASV system; counter-
measures should be assessed in tandem with ASV. Such more
elaborate approaches to assessment, e.g. [21], are outside the
scope of the current article.

5. Privacy implications: VoicePrivacy

Speech signals contain much more than just the spoken mes-
sage or the voice identity [22, 23]. The speaker’s sex/gender,
age, socio-economic and geographical background, emotion
and health condition etc. can all be estimated automatically us-
ing recordings of speech [24, 25]. With much of this infor-
mation being personal and private there is hence an interest to
develop privacy safeguards for speech technology. This is the
goal of the VoicePrivacy initiative, founded in 2020. While so-
lutions to privacy preservation can take many different forms,
and while VoicePrivacy may explore different approaches in the
future, the inaugural challenge focused upon the development
of anonymization solutions [7].

The idea is to protect privacy by distorting speech data such
that it cannot be used by ASV systems to recognise the speaker.
Anonymisation is achieved by suppressing the information in
speech signals that is typically used by machines to infer iden-
tity. On its own, this is relatively straightforward, e.g. by adding
sufﬁcient levels of background noise, or by replacing speech
with silence. The challenge comes from the requirement to
suppress personally identiﬁable attributes contained within the
speech signal while leaving all other attributes intact. These re-
quirements imply that anonymisation should not interfere with
the application of some down stream tasks such as automatic
speech recognition, nor should it introduce processing artefacts
that might degrade subjective intelligibility or naturalness. Last,
anonymised voices should remain distinctive, meaning the task

0.20.5 1  2  5 10 20 30 40 50 60 70 80 False Alarm probability (in %)0.20.5 1  2  5 10 20 30 40 50 60 70 80 Miss probability (in %)non-target (EER=2.46 %)worst VC spoofing attack (EER=65.25 %)worst TTS spoofing attack (EER=66.42 %)EER=2.46%0.20.5 1  2  5 10 20 30 40 50 False Alarm probability (in %)0.20.5 1  2  5 10 20 30 40 50 Miss probability (in %)Team 05 (EER=0.22 %)Team 45 (EER=1.86 %)Team 60 (EER=2.64 %)Team 24 (EER=3.45 %)Team 50 (EER=3.56 %)posed deﬁnitions for a number of distinct tasks and solutions;
established evaluation driven research initiatives as a vehicle to
successfully raise the proﬁle of security and privacy research
and to build new research communities; developed protocols,
criteria and metrics for the assessment of security and privacy
safeguards. Nonetheless, we are perhaps still far from having
a comprehensive appreciation of the implications as well as the
potential of safeguards.

Will spooﬁng ever be a solved problem? Possibly not.
Speech synthesis and voice conversion are long-established re-
search ﬁelds with genuine applications, such as anonymisation,
yet the same technology poses a threat to the security of ASV.
The progress speech synthesis and voice conversion in recent
years has been impressive. Today’s technology produces syn-
thetic speech that humans cannot distinguish from bona ﬁde
speech. Whether or not similar advances may one day result
in machines that produce synthetic speech that other machines
cannot detect is an intriguing question. For the time being it
is clear that, if we seek reliable, secure approaches to person
authentication using ASV, we must intensify our efforts in anti-
spooﬁng. Future directions include a focus on more adversarial
attacks, the development of countermeasures that function reli-
ably in the wild, e.g. in the face of background noise and other
sources of nuisance variation such as bandwidth and channel
variability which typify telephony ASV scenarios, as well as
the approach used to estimate performance.

The research effort in anonymisation is relatively embry-
onic. While there is an opportunity to provide some level of pro-
tection, current solutions fall short of delivering true anonymi-
sation. Furthermore, we have observed differences in the level
of privacy that a given anonymisation solution provides to dif-
ferent individuals. Whereas the protection for some can be
strong, others are left with relatively little protection at all.
Since personally identiﬁable information encapsulates far more
than that used by most ASV systems to infer identity, since
other alternative attributes can be used instead, and since cur-
rent anonymisation solutions do not necessarily suppress them,
is full anonymisation even technically possible?

Anonymisation solutions that focus only upon the attributes
used by typical ASV systems already degrade intelligibility
and naturalness. If all personally identiﬁable information can
be successfully suppressed, then what remains? Is personally
identiﬁable information so inextricably, indelibly embedded in
speech that it cannot be (fully) removed? Even if a speech sig-
nal can be fully anonymised, will anything resembling speech
remain? Or, for a given application, what level of intelligibil-
ity/naturalness can be sacriﬁced in order to achieve anonymisa-
tion? How should we even measure anonymisation performance
and privacy? Are the methodology and metrics adequate? How
does this research ﬁt with broader solutions to privacy preserva-
tion, e.g. encryption and distributed learning?

That we have certainly raised more questions above than we
have answered serves to show that our community’s journey in
security and privacy research is only just beginning. Our efforts
must be redoubled looking to the future.

7. Acknowledgements

The work reported in this paper was supported by: the French
ANR (VoicePrivacy, VoicePersonae, DEEP-PRIVACY, Har-
pocrates); the European Commission (H2020 COMPRISE); the
Japan Science and Technology Agency (JST) with grant No.
JPMJCR18A6; JSPS KAKENHI No.21K17775; the Academy
of Finland (proj. 309629); Region Grand Est, France.

Figure 4: Detection error trade-off plot for the VoicePrivacy
2020 challenge (LibriSpeech test, male trials). Proﬁles shown
for the baseline ASV system (blue proﬁle) and the same system
presented with anonymised test utterances (orange proﬁle) and
anonymised test utterances when the ASV system is re-trained
using similarly anonymised training data (red proﬁle).

might better be referred to as pseudonymisation.

Rather than operating upon the speech signal so that it
reﬂects the voice of another, speciﬁc speaker, anonymisation
aims to prevent the speaker identity from being recognised.
Anonymisation acts to increase the confusion between utter-
ances produced by the same and different speakers. For a per-
fect anonymisation system, the score distributions correspond-
ing to impostor and target trials overlap. The desired effect of
anonymisation upon an ASV system is illustrated in Fig. 1(c).
In this case, the ASV system cannot produce simultaneously
both a low false alarm rate and a low miss rate, no matter what
the decision threshold, and the EER is 50%. Real anonymisa-
tion solutions are less effective.

The ﬁrst VoicePrivacy challenge was held in 2020 and at-
tracted submissions from 7 independent teams. A DET plot for
the primary challenge baseline [7] is illustrated in Fig. 4.
It
shows an increase in the EER from a baseline of 1% (blue pro-
ﬁle) to 52% after anonymisation (orange proﬁle). The EER of
over 50% suggests that the anonymisation goal is met. How-
ever, if an anonymisation adversary were to adapt the ASV sys-
tem in light of anonymisation, then performance is less effec-
tive; anonymised utterances still contain some personally iden-
tiﬁable information (PII) and the potential to re-identify the
speaker remains. When the ASV system is re-trained or adapted
using similarly anonymised training data, the result is a lower
EER of 10.7% (red proﬁle); true anonymisation remains elu-
sive.

The above treatment does not reﬂect impacts upon intel-
In practice,
ligibility/naturalness, nor voice distinctiveness.
the multiple objectives and complex nature of anonymisation
means several different metrics are used in practice. The devel-
opment of more suitable approaches to assessment are the focus
of current research [26, 27, 28, 29].

6. Reﬂections and further considerations

The community has made substantial progress to address the se-
curity and privacy implications of speech technology and rapid
progress has been made in the last half-decade. We have: pro-

0.5 1  2  5 10 20 30 40 50 60 70 80 False Alarm probability (in %)0.5 1  2  5 10 20 30 40 50 60 70 80 Miss probability (in %)ASV baseline (EER=1.11%ASV trained on anonymised data,anonymised test (EER=10.69%)Anonymised test (EER=52.12%8. References

[1] M. Sahidullah, H. Delgado, M. Todisco, T. Kinnunen,
N. Evans, J. Yamagishi, and K.-A. Lee, “Introduction to
voice presentation attack detection and recent advances,”
in Handbook of Biometric Anti-Spooﬁng, pp. 321–361.
Springer, 2019.

[2] “ISO/IEC 30107. Information Technology – Biometric
presentation attack detection,”
Standard, International
Organization for Standardization, Geneva, Switzerland,
2016.

[3] A. Nautsch, C. Jasserand, E. Kindt, M. Todisco, I. Tran-
coso, and N. Evans, “The GDPR & Speech Data: Reﬂec-
tions of Legal and Technology Communities, First Steps
Towards a Common Understanding,” in Proc. Interspeech
2019, 2019, pp. 3695–3699.

[4] Z. Wu, T. Kinnunen, N. Evans, J. Yamagishi, C. Hanilc¸i,
M. Sahidullah, and A. Sizov, “ASVspoof 2015: the ﬁrst
automatic speaker veriﬁcation spooﬁng and countermea-
sures challenge,” in Proc. Interspeech, 2015, pp. 2037–
2041.

[5] T. Kinnunen, M. Sahidullah, H. Delgado, M. Todisco,
N. Evans, J. Yamagishi, and K. A. Lee, “The ASVspoof
2017 Challenge: Assessing the Limits of Replay Spooﬁng
Attack Detection,” in Proc. Interspeech, 2017, pp. 2–6.

[6] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Del-
gado, A. Nautsch, J. Yamagishi, N. Evans, T. H. Kin-
nunen, and K. A. Lee, “ASVspoof 2019: future horizons
in spoofed and fake audio detection,” in Proc. Interspeech,
2019, pp. 1008–1012.

[7] N. Tomashenko, B. M. L. Srivastava, X. Wang, E. Vin-
cent, A. Nautsch, J. Yamagishi, N. Evans, J. Patino, J.-F.
Bonastre, P.-G. No´e, and M. Todisco, “Introducing the
VoicePrivacy Initiative,” in Proc. Interspeech, oct 2020,
pp. 1693–1697.

[8] K. A. Lee, O. Sadjadi, H. Li, and D. Reynolds, “Two
decades into speaker recognition evaluation - are we there
yet?,” Computer Speech & Language, vol. 61, pp. 101058,
2020.

[9] C. S. Greenberg, L. P. Mason, et al., “Two decades of
speaker recognition evaluation at the National Institute of
Standards and Technology,” Computer Speech & Lan-
guage, vol. 60, pp. 101032, 2020.

[10] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “Vox-
celeb: Large-scale speaker veriﬁcation in the wild,” Com-
puter Speech & Language, vol. 60, pp. 101027, 2020.

[11] H. Zeinali, K. A. Lee,

J. Alam, and L. Burget,
“SdSV Challenge 2020: Large-Scale Evaluation of Short-
Duration Speaker Veriﬁcation,” in Proc. Interspeech 2020,
2020, pp. 731–735.

[12] K. A. Lee, V. Hautamaki, T. Kinnunen, H. Yamamoto,
et al.,
“I4U submission to NIST SRE 2018: Leverag-
ing from a decade of shared experiences,” in Proc. In-
terspeech, 2019, pp. 1497–1501.

[13] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero,
et al., “State-of-the-art speaker recognition for telephone
the JHU-MIT submission for NIST
and video speech:
SRE18,” in Proc. Interspeech, 2019, pp. 1488–1492.

[14] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki, “The DET Curve in Assessment of Detec-
tion Task Performance,” in Proc. Eurospeech, 1997, pp.
1895–1898.

[15] “ISO/IEC IS 19795-1:2021. Information Technology –
Biometric performance testing and reporting – Part 1:
Principles and framework,” Standard, International Orga-
nization for Standardization, Geneva, Switzerland, 2021.
[16] A. Nautsch, Speaker Recognition in Unconstrained Envi-
ronments, Ph.D. thesis, Technische Universit¨at Darmstadt,
2019.

[17] J. Shen, R. Pang, R. J. Weiss, et al., “Natural TTS synthe-
sis by conditioning WaveNet on Mel spectrogram predic-
tions,” in Proc. ICASSP, 2018, pp. 4779–4783.

[18] Z. Yi, W.-C. Huang, X. Tian, J. Yamagishi, et al., “Voice
Conversion Challenge 2020 — Intra-lingual semi-parallel
in Proc. Joint
and cross-lingual voice conversion —,”
Workshop for the Blizzard Challenge and Voice Conver-
sion Challenge 2020, 2020, pp. 80–98.

[19] R. G. Hautam¨aki, T. Kinnunen, V. Hautam¨aki, T. Leino,
and A.-M. Laukkanen, “I-vectors meet imitators: on vul-
nerability of speaker veriﬁcation systems against voice
mimicry.,” in Proc. Interspeech, 2013, pp. 930–934.
[20] Z. Wu, N. Evans, T. Kinnunen, J. Yamagishi, F. Alegre,
and H. Li, “Spooﬁng and countermeasures for speaker
veriﬁcation: A survey,” Speech Communication, vol. 66,
pp. 130–153, feb 2015.

[21] T. Kinnunen, H. Delgado, N. Evans, K. A. Lee,
et al.,
“Tandem Assessment of Spooﬁng Countermea-
sures and Automatic Speaker Veriﬁcation: Fundamen-
IEEE/ACM Transactions on Audio, Speech, and
tals,”
Language Processing, vol. 28, pp. 2195–2210, 2020.
[22] A. Nautsch, A. Jim´enez, A. Treiber, J. Kolberg, et al.,
“Preserving privacy in speaker and speech characterisa-
tion,” Computer Speech & Language, vol. 58, pp. 441–
480, 2019.

[23] COMPRISE,

“Deliverable Nº5.1:

tion and GDPR requirements
https://www.compriseh2020.eu/ﬁles/2019/06/d5.1.pdf,”
2019.

[online].

Data protec-
available:

[24] I. Shafran, M. Riley, and M. Mohri, “Voice signatures,” in

Proc. ASRU, 2003, pp. 31–36.

[25] T. Schultz, “Speaker characteristics,” in Speaker classiﬁ-

cation I, pp. 47–74. Springer, 2007.

[26] B. M. L. Srivastava, A. Bellet, M. Tommasi, and E. Vin-
“Privacy-Preserving Adversarial Representation
cent,
Learning in ASR: Reality or Illusion?,” in Interspeech
2019, sep 2019, pp. 3700–3704.

[27] A. Nautsch, J. Patino, N. Tomashenko, J. Yamagishi, P.-G.
No´e, J.-F. Bonastre, M. Todisco, and N. Evans, “The Pri-
vacy ZEBRA: Zero Evidence Biometric Recognition As-
sessment,” in Proc. Interspeech, 2020, pp. 1698–1702.

[28] M. Maouche, B. M. L. Srivastava, N. Vauquier, A. Bellet,
M. Tommasi, and E. Vincent, “A Comparative Study of
Speech Anonymization Metrics,” in Proc. Interspeech,
2020, pp. 1708–1712.

[29] P.-G. No´e, J.-F. Bonastre, D. Matrouf, N. Tomashenko,
A. Nautsch, and N. Evans, “Speech Pseudonymisation
Assessment Using Voice Similarity Matrices,” in Proc.
Interspeech 2020, 2020, pp. 1718–1722.

