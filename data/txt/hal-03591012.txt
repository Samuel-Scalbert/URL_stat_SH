The Need for Empirical Evaluation of Explanation
Quality
Nicholas Halliwell, Fabien Gandon, Freddy Lecue, Serena Villata

To cite this version:

Nicholas Halliwell, Fabien Gandon, Freddy Lecue, Serena Villata. The Need for Empirical Evaluation
of Explanation Quality. AAAI 2022 - Workshop on Explainable Agency in Artificial Intelligence, Feb
2022, Vancouver, Canada. ￿hal-03591012￿

HAL Id: hal-03591012

https://hal.science/hal-03591012

Submitted on 28 Feb 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

The Need for Empirical Evaluation of Explanation Quality

Nicholas Halliwell,1 Fabien Gandon, 1 Freddy Lecue, 1, 2 Serena Villata 1
1 Inria, Universit´e Cˆote d’Azur, CNRS, I3S, France
2 CortAIx, Thales, Montreal, Canada
nicholas.halliwell@inria.fr, fabien.gandon@inria.fr, freddy.lecue@inria.fr, serena.villata@inria.fr

Abstract

Prototype networks (Li et al. 2018) provide explanations to
users using a prototype vector; that is, a vector learned by the
network representing a “typical” observation. In this work,
we propose an approach that identiﬁes relevant features in the
input space used by the Prototype network. We ﬁnd however
that empirical evaluation of explanation quality is difﬁcult
without ground truth explanations. We include a discussion
about developing methods for generating explanations, iden-
tifying when one explanation method is preferable to another,
and the complications that arise when measuring explanation
quality.

1

Introduction

Deep learning models are used to serve automated deci-
sions in settings such as banks, insurance, and health care.
These models are typically treated as a black box, where no
insight is given as to how they make decisions. This lack
of transparency has hindered adoption of these models into
production. Much research has been devoted to developing
algorithms, or explanation methods, to interpret their predic-
tions.

Indeed there are many approaches for generating post-
hoc explanations. Feature importance methods (Lundberg
and Lee 2017; Ribeiro, Singh, and Guestrin 2016; Kim
et al. 2018), where relevant dimensions are identiﬁed and
assigned a score to rank its importance relative to the
other dimensions. For image data, saliency maps (Simonyan,
Vedaldi, and Zisserman 2014; Springenberg et al. 2015;
Bach et al. 2015; Selvaraju et al. 2016; Shrikumar, Green-
side, and Kundaje 2017; Shrikumar et al. 2016; Zeiler and
Fergus 2014; Smilkov et al. 2017; Sundararajan, Taly, and
Yan 2017; Montavon et al. 2017) identify relevant pixels
in the input image. Counterfactual explanations (Wachter,
Mittelstadt, and Russell 2017) give the smallest possible
perturbation to the given input that will change the predic-
tion to a desired target outcome. Lastly, prototype explana-
tions (Chen et al. 2019; Li et al. 2018; Ming et al. 2019)
learn a continuous vector that represents a “typical” training
example, where explanations are given based on their rela-
tive distance to a prototype vector.

The Prototype network architecture from Li et al. (2018)
combines an autoencoder with a prototype layer, where each

observation in the training set is classiﬁed based on its dis-
tance to a prototype vector. The encoded input from the au-
toencoder is used as features for predictions downstream.
The prototype vectors learned by this network are deﬁned
as typical observations in the training set, and, because they
are learned in the same space as the encoded input, they can
be mapped back into the original input space for visualiza-
tion using the decoder. Explanations are given in the form
of a most similar prototype vector. The speciﬁc architecture
of this network allows us to further develop and improve the
types of explanations generated post hoc.

In this paper, we expand the type of explanations gener-
ated by the Prototype network to identify relevant features
in the input space. Due to the architecture of this network,
the latent features learned by the model can be exploited to
identify relevant input space features. We make use of the
network’s encoded input to randomly set latent features to
zero, and use the network’s decoder to determine which in-
put space values changed the most. Finally, this work allows
us to open a general discussion about generating explana-
tions, identifying when one explanation method is preferable
to another, and the complications that arise when measuring
explanation quality.

2 Prototype Network
This section provides necessary background information on
the Prototype network from Li et al. (2018), including the
architecture and loss function.

Architecture Details

The Prototype network architecture can be visualized in Fig-
ure 1. It consists of an autoencoder (the encoder deﬁned as
f : Rp → Rq and the decoder, deﬁned as g : Rq → Rp), a
prototype layer p : Rq → Rm, and a dense (fully-connected)
layer w : Rm → RK that feeds into a softmax layer. The
prototype layer takes as input encoded training points, de-
noted f (xi), and computes the L2 distance between f (xi)
and m prototype vectors, denoted p1, . . . , pm ∈ Rq. The
overall network is given by h : Rq → RK. In this proto-
type network architecture, observations are classiﬁed based
on their distance to a prototypical observation, and the loss
function ensures that each prototype vector is similar to
an encoded training point. We denote the data set D =

Figure 1: Prototype Network Architecture (Li et al. 2018).

{(xi, yi)}n
ber of classes.

i=1, where yi ∈ {1, . . . K}, and K being the num-

Loss Function
The loss function given by Li et al. (2018) is broken down
into the following four parts below:

E(h ◦ f, D) =

1
n

n
(cid:88)

K
(cid:88)

i=1

k=1

−1[yi = k]log((h ◦ f )k(xi))

R(g ◦ f, D) =

1
n

n
(cid:88)

i=1

||(g ◦ f )(xi) − xi||2

(1)

(2)

Two regularization terms are used, i.e., R1, which forces
each prototype vector to be as close as possible to one en-
coded training point, and R2, which forces every encoded
training point to be as close as possible to one prototype
vector.

R1(p1, . . . , pm, D) =

R2(p1, . . . , pm, D) =

1
m

m
(cid:88)

j=1

min
i∈[1,n]

||pj − f (xi)||2

(3)

1
n

n
(cid:88)

i=1

min
j∈[1,m]

||f (xi) − pj||2

(4)

The complete loss function is given by

L((f, g, h), D) = E(h ◦ f, D) + λ0R(g ◦ f, D)+
λ1R1(p1, . . . , pm, D) + λ2R2(p1, . . . , pm, D)

(5)

where λ0, λ1, λ2 are hyperparameters.

3 Proposed Approach
The encoder function f maps a p dimensional vector to a q
dimensional vector where p > q. This encoded input con-
tains relevant information for classiﬁcation, as it is used as
features downstream, and is using a lower dimensional rep-
resentation of the input data. Identifying relevant informa-
tion in the encoded latent space should provide further in-
sight into how the model is making decisions. For some ob-
servation x we want an explanation for, we encode the in-
put using the Prototype network’s encoder f . We then make
m copies of the encoded input f (x), and apply m different
masks element-wise. Each mask, denoted mi, is the same
dimensions as the encoded input f (x), where each element
of a mask is assigned a 1 with 90% probability and a 0 with
10% probability. The element-wise product is then averaged
across the m masks, given by:

ˆf (x) =

1
m

m
(cid:88)

i=1

f (xi) (cid:12) mi.

(6)

The result ˆf (x) is then decoded by the Prototype net-

work’s decoder g for visualization, given by:

ˆg = g( ˆf (x)).

(7)

To identify the relevant dimensions in the input space,
the input is mapped through the encoder and then decoded,
denoted g(f (x)). We then compute the absolute difference
between the decoded input and the decoded masked input
given by:

x∗ = |ˆg − g(f (x))|.

(8)

(a) 0

(b) 1

(c) 2

(d) 3

(e) 4

(f) 5

(g) 6

(h) 7

(i) 8

(j) 9

Figure 2: MNIST Images

(a) 0

(b) 1

(c) 2

(d) 3

(e) 4

(f) 5

(g) 6

(h) 7

(i) 8

(j) 9

Figure 3: Saliency maps: Proposed approach

(a) 0

(b) 1

(c) 2

(d) 3

(e) 4

(f) 5

(g) 6

(h) 7

(i) 8

(j) 9

Figure 4: Saliency maps: Proposed approach-randomly initialized untrained network

(a) 0

(b) 1

(c) 2

(d) 3

(e) 4

(f) 5

(g) 6

(h) 7

(i) 8

(j) 9

Figure 5: Saliency maps: Proposed approach-network trained on randomly permuted labels

(a)

Lime features:
Observation 1

(b)

Proposed features:
Observation 1

(c)

Lime features:
Observation 2

(d)

Proposed features:
Observation 2

Figure 6: Explanations generated by Lime and the proposed approach on California Housing dataset.

where x∗ gives the feature importance scores of x for each
dimension. Here the absolute difference gives the features in
the input space with the largest change. Code for this work
is available online.1

dataset (LeCun et al. 1998) with 3 encoding layers, 3 de-
coding layers, 1 prototype layer, and 1 fully connected layer.
This model learns 10 prototype vectors (one for each class),
achieving 99.1% accuracy on the test set.

4 Experiments

Image Data

With image data, we have the ability to visualize the ex-
planation. We train a Prototype network on the MNIST

1https://github.com/halliwelln/prototype-explanations/

Figure 3 shows saliency maps of the proposed approach
for each image in Figure 2. We can see that the proposed ap-
proach produces saliency maps that outline the digit in the
original image. We perform the model parameter random-
ization and data randomization test (Adebayo et al. 2018).
The model parameter randomization test generates saliency
maps from a model with untrained, random parameters. The
resulting saliency maps should be random noise. The data

PopulationMedIncLongitudeLatitudeHouseAgeAveRoomsAveOccupAveBedrmsPopulationMedIncLongitudeLatitudeHouseAgeAveRoomsAveOccupAveBedrmsPopulationMedIncLongitudeLatitudeHouseAgeAveRoomsAveOccupAveBedrmsPopulationMedIncLongitudeLatitudeHouseAgeAveRoomsAveOccupAveBedrmsrandomization test trains a model where the training labels
have been randomly shufﬂed. Like the model parameter ran-
domization test, the resulting saliency maps should be ran-
dom noise and the end user should not be able to determine
the object in the image. Figure 4 shows saliency maps from
an untrained Prototype network with randomly initialized
parameters (model parameter randomization test). Figure 5
shows saliency maps for a model trained on random labels
(data randomization test). From these ﬁgures we can see the
proposed approach passes the model parameter randomiza-
tion test but fails the data randomization test. In other words,
the proposed approach to generating explanations is not pro-
viding insight into what the model has learned.

Tabular Data
We demonstrate our approach on a well known tabular
dataset, the California Housing dataset (Pace and Barry
1997). Here, we are tasked with determining if houses
should be sold above or below the median price. We train
a Prototype network on the California Housing dataset with
2 encoding layers, and 2 decoding layers, 1 prototype layer,
and 1 fully connected layer. This model learns 2 prototype
vectors, achieving 84.2% accuracy on the test set.

features

Figure 6 compares

identiﬁed by
relevant
Lime (Ribeiro, Singh, and Guestrin 2016) to our proposed
approach for selected observations. For both observations,
we can see that the top 3 dimensions with the highest at-
tribution scores are the same for both explanation methods.
Although both explanations are similar, they are not exactly
equal. From these examples, which explanation method is
actually displaying what the model has learned? In other
words, which explanation method is preferable to the other?
These questions are difﬁcult to answer without ground truth
explanations to quantitatively compare against.

5 Discussion
From the experiments on tabular and image data, we found
our approach produced what looked like faithful explana-
tions on both types of data. After using the robustness tests
from Adebayo et al. (2018) on an image dataset, we were
able to determine that this was not the case. For image data,
we have the ability to visually verify any explanation gener-
ated in the input space. With tabular data, we do not have this
luxury. Depending on the type of data used for experimen-
tation, researchers can be mislead into thinking the expla-
nations their model is generating are faithful because they
are similar to a state-of-the-art method. With ground truth
explanations, researchers would not have to rely on previous
state-of-the-art explanation methods to determine if their ap-
proach is generating faithful explanations.

In general, this is a common problem in the ﬁeld of XAI.
When a new explanation method is proposed, researchers of-
ten show several “good looking” examples to display to the
reader the capability of the proposed method. Comparisons
against a state-of-the-art method typically involve a small
number of cherry-picked examples to demonstrate the abil-
ity of an explanation method. This can be misleading. Indeed
a small number of selected examples does not truly represent

how the explanation method is performing on the entire test
set. As we demonstrated on the tabular dataset, our proposed
approach can compete with Lime on “selected” examples,
however, this is not conclusive evidence that this explana-
tion method is preferable to Lime. In order to accurately
determine which explanation method is preferable, ground
truth explanations are needed.

Deﬁning ground truth explanations may be more difﬁcult
for different tasks, and different types of data. Additionally,
there may be more than one way to explain a particular ob-
servation. Datasets with ground truth explanations must in-
clude all possible ways to explain each observation. Failing
to include all possible ground truth explanations can unfairly
penalize an explanation method for identifying a correct ex-
planation not included in the ground truths.

There is existing work on qualitative evaluation of ex-
planations. Poursabzi-Sangdeh et al. (2021) perform a
user experiment to determine what makes a model inter-
pretable. Jeyakumar et al. (2020) perform a user experi-
ment to determine what style of explanation is preferred by
users. Adebayo et al. (2020) develop a series of debugging
tests, and include a user experiment to determine if users
can identify defective models. Not much existing research
focuses on quantitatively evaluating all test set explanations
for quantitative comparisons across explanation methods.
Relying on users to evaluate each explanation in the test set
does not scale to large datasets, and cannot be performed on
certain types of data (tabular data for example users shown
an explanation would not know if its an accurate explana-
tion or not). Additionally, users without a background in ma-
chine learning may not be able to determine a good explana-
tion. For quantitative evaluations of explanations that scales
to large datasets, scoring metrics must be deﬁned that give
an accurate representation of the explanation method’s per-
formance. Scoring metrics that measure explanation quality
can be formally deﬁned with ground truth explanations.

6 Conclusion
In this work, we propose a method to expand Prototype net-
works to identify relevant features in the input space. We
compare selected examples against a state-of-the-art expla-
nation method on tabular data and verify that the expla-
nations are similar. On image data however, our approach
passes the model parameter randomization test but fails the
data randomization test. It is common practice in the ﬁeld of
XAI to compare explanation methods using a few selected
examples. This is not a thorough evaluation of explanation
quality.

We discuss the development of explanation methods,
identifying when one explanation method is preferable to
another, and the complications that arise when measuring
explanation quality. Much research in the ﬁeld of XAI is de-
voted to developing new explanation methods. This paper
points out that more work should be devoted to evaluating
the quality of explanation generated. Many of these issues
can be solved with ground truth explanations. We recognize
this can be difﬁcult with tabular data. Research should be de-
voted to deﬁning ground truth explanations for all domains
in order to quantitatively evaluate explanations.

References
Adebayo, J.; Gilmer, J.; Muelly, M.; Goodfellow, I. J.; Hardt,
M.; and Kim, B. 2018. Sanity Checks for Saliency Maps. In
Bengio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.;
Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural
Information Processing Systems 31.
Adebayo, J.; Muelly, M.; Liccardi, I.; and Kim, B. 2020.
Debugging Tests for Model Explanations. In Larochelle, H.;
Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H., eds.,
Advances in Neural Information Processing Systems. Curran
Associates, Inc.
Bach, S.; Binder, A.; Montavon, G.; Klauschen, F.; M¨uller,
K.-R.; and Samek, W. 2015. On Pixel-Wise Explanations for
Non-Linear Classiﬁer Decisions by Layer-Wise Relevance
Propagation. PLOS ONE.
Chen, C.; Li, O.; Tao, D.; Barnett, A.; Rudin, C.; and Su,
J. 2019. This Looks Like That: Deep Learning for Inter-
pretable Image Recognition. In Wallach, H. M.; Larochelle,
H.; Beygelzimer, A.; d’Alch´e-Buc, F.; Fox, E. B.; and Gar-
nett, R., eds., Advances in Neural Information Processing
Systems 32.
Jeyakumar, J. V.; Noor, J.; Cheng, Y.; Garcia, L.; and Srivas-
tava, M. B. 2020. How Can I Explain This to You? An Em-
pirical Study of Deep Neural Network Explanation Meth-
In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan,
ods.
M.; and Lin, H., eds., Advances in Neural Information Pro-
cessing Systems 33.
Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C. J.; Wexler, J.;
Vi´egas, F. B.; and Sayres, R. 2018. Interpretability Beyond
Feature Attribution: Quantitative Testing with Concept Ac-
tivation Vectors (TCAV). In Dy, J. G.; and Krause, A., eds.,
Proceedings of the 35th International Conference on Ma-
chine Learning, ICML 2018, Stockholm, Sweden, Proceed-
ings of Machine Learning Research. PMLR.
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998.
Gradient-based learning applied to document recognition.
Proceedings of the Institute of Radio Engineers.
Li, O.; Liu, H.; Chen, C.; and Rudin, C. 2018. Deep
Learning for Case-Based Reasoning Through Prototypes:
A Neural Network That Explains Its Predictions. In McIl-
raith, S. A.; and Weinberger, K. Q., eds., Proceedings of the
Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
AAAI Press.
Lundberg, S. M.; and Lee, S. 2017. A Uniﬁed Approach to
Interpreting Model Predictions. In Guyon, I.; von Luxburg,
U.; Bengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.
V. N.; and Garnett, R., eds., Advances in Neural Information
Processing Systems 30.
Ming, Y.; Xu, P.; Qu, H.; and Ren, L. 2019. Interpretable and
Steerable Sequence Learning via Prototypes. In Teredesai,
A.; Kumar, V.; Li, Y.; Rosales, R.; Terzi, E.; and Karypis, G.,
eds., Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, KDD
2019. ACM.
Montavon, G.; Lapuschkin, S.; Binder, A.; Samek, W.; and
M¨uller, K. 2017. Explaining nonlinear classiﬁcation deci-
sions with deep Taylor decomposition. Pattern Recognit.

Pace, R. K.; and Barry, R. 1997. Sparse spatial autoregres-
sions. Statistics & Probability Letters, 33: 291.
Poursabzi-Sangdeh, F.; Goldstein, D. G.; Hofman, J. M.;
Vaughan, J. W.; and Wallach, H. M. 2021. Manipulating and
Measuring Model Interpretability. In Kitamura, Y.; Quigley,
A.; Isbister, K.; Igarashi, T.; Bjørn, P.; and Drucker, S. M.,
eds., CHI ’21: CHI Conference on Human Factors in Com-
puting Systems. ACM.
Precup, D.; and Teh, Y. W., eds. 2017. Proceedings of
the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, Proceedings of Ma-
chine Learning Research. PMLR.
Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ”Why
Should I Trust You?”: Explaining the Predictions of Any
Classiﬁer.
In Krishnapuram, B.; Shah, M.; Smola, A. J.;
Aggarwal, C. C.; Shen, D.; and Rastogi, R., eds., Proceed-
ings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM.
Selvaraju, R. R.; Das, A.; Vedantam, R.; Cogswell, M.;
Parikh, D.; and Batra, D. 2016. Grad-CAM: Why did
you say that? Visual Explanations from Deep Networks via
Gradient-based Localization. CoRR, abs/1610.02391.
Shrikumar, A.; Greenside, P.; and Kundaje, A. 2017. Learn-
ing Important Features Through Propagating Activation Dif-
ferences. In (Precup and Teh 2017).
Shrikumar, A.; Greenside, P.; Shcherbina, A.; and Kundaje,
A. 2016. Not Just a Black Box: Learning Important Fea-
tures Through Propagating Activation Differences. CoRR,
abs/1605.01713.
Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2014. Deep
Inside Convolutional Networks: Visualising Image Classi-
ﬁcation Models and Saliency Maps.
In Bengio, Y.; and
LeCun, Y., eds., 2nd International Conference on Learning
Representations, ICLR 2014, Workshop Track Proceedings.
Smilkov, D.; Thorat, N.; Kim, B.; Vi´egas, F. B.; and Wat-
tenberg, M. 2017. SmoothGrad: removing noise by adding
noise. CoRR, abs/1706.03825.
Springenberg, J. T.; Dosovitskiy, A.; Brox, T.; and Ried-
miller, M. A. 2015. Striving for Simplicity: The All Con-
volutional Net. In Bengio, Y.; and LeCun, Y., eds., 3rd In-
ternational Conference on Learning Representations, ICLR
2015, Workshop Track Proceedings.
Sundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic
Attribution for Deep Networks. In (Precup and Teh 2017).
Wachter, S.; Mittelstadt, B. D.; and Russell, C. 2017. Coun-
terfactual Explanations without Opening the Black Box: Au-
tomated Decisions and the GDPR. CoRR, abs/1711.00399.
Zeiler, M. D.; and Fergus, R. 2014. Visualizing and Under-
standing Convolutional Networks.
In Fleet, D. J.; Pajdla,
T.; Schiele, B.; and Tuytelaars, T., eds., Computer Vision -
ECCV 2014 - 13th European Conference, Lecture Notes in
Computer Science. Springer. ISBN 978-3-319-10589-5.

