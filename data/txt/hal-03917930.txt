Black-box language model explanation by context length
probing
Ondřej Cífka, Antoine Liutkus

To cite this version:

Ondřej Cífka, Antoine Liutkus. Black-box language model explanation by context length probing.
ACL 2023 - 61st Annual Meeting of the Association for Computational Linguistics, Jul 2023, Toronto,
Canada. pp.1067–1079, ￿10.18653/v1/2023.acl-short.92￿. ￿hal-03917930￿

HAL Id: hal-03917930

https://hal.umontpellier.fr/hal-03917930

Submitted on 13 Nov 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Black-box language model explanation
by context length probing

Ondˇrej Cífka Antoine Liutkus

Zenith Team, LIRMM, CNRS UMR 5506
Inria, Université de Montpellier, France
cifka@matfyz.cz, antoine.liutkus@inria.fr

Abstract

The increasingly widespread adoption of large
language models has highlighted the need for
improving their explainability. We present con-
text length probing, a novel explanation tech-
nique for causal language models, based on
tracking the predictions of a model as a func-
tion of the length of available context, and al-
lowing to assign differential importance scores
to different contexts. The technique is model-
agnostic and does not rely on access to model
internals beyond computing token-level prob-
abilities. We apply context length probing to
large pre-trained language models and offer
some initial analyses and insights, including
the potential for studying long-range depen-
dencies. The source code1 and an interactive
demo2 of the method are available.

1

Introduction

Large language models (LMs), typically based on
the Transformer architecture (Vaswani et al., 2017),
have recently seen increasingly widespread adop-
tion, yet understanding their behaviour remains a
difficult challenge and an active research topic.

Notably, as the length of the context that can be
accessed by LMs has grown, a question that has
attracted some attention is how this influences their
predictions. Some recent studies in this line of re-
search suggest that even “long-range” LMs focus
heavily on local context and largely fail to exploit
distant ones (O’Connor and Andreas, 2021; Sun
et al., 2021; Press et al., 2021; Sun et al., 2022). A
more nuanced understanding of how contexts of
different lengths influence LMs’ predictions may
hence be valuable for further improving their per-
formance, especially on tasks like long-form text
generation where long-range dependencies are of
critical importance.

1https://github.com/cifkao/

context-probing/

2https://cifkao.github.io/

context-probing/

Figure 1: A screenshot of a demo2 of the proposed
method. After selecting a target token (here “birds”),
the preceding tokens are highlighted according to their
(normalized) differential importance scores (green =
positive, red = negative), obtained using our method.
The user can also explore the top predictions for contexts
of different lengths (here the context “house, shouting
about lunatics. [. . .] mortally afraid of”).

In this work, we propose context length probing,
a simple explanation technique for causal (autore-
gressive) language models, based on tracking the
predictions of the model as a function of the num-
ber of tokens available as context. Our proposal
has the following advantages:

• It is conceptually simple, providing a straight-
forward answer to a natural question: How
does the length of available context impact
the prediction?

• It can be applied to a pre-trained model with-
out retraining or fine-tuning and without train-
ing any auxiliary models.

• It does not require access to model weights,

internal representations or gradients.

• It is model-agnostic, as it can be applied to
any causal LM, including attentionless archi-
tectures like RNN (Mikolov et al., 2010) and
CNN (Dauphin et al., 2017). The only require-
ment for the model is to accept arbitrary input

segments (i.e. not be limited to document pre-
fixes).

3 Method

3.1 Context length probing

Furthemore, we propose a way to use this tech-
nique to assign what we call differential importance
scores to contexts of different lengths. This can be
seen as complementary to other techniques like at-
tention or saliency map visualization. Interestingly,
contrary to those techniques, ours appears promis-
ing as a tool for studying long-range dependencies,
since it can be expected to highlight important in-
formation not already covered by shorter contexts.

2 Related work

A popular way to dissect Transformers is by vi-
sualizing their attention weights (e.g. Vig, 2019;
Hoover et al., 2020). However, it has been ar-
gued that this does not provide reliable explana-
tions and can be misleading (Jain and Wallace,
2019; Serrano and Smith, 2019). A more re-
cent line of work (Elhage et al., 2021; Olsson
et al., 2022) explores “mechanistic explanations”,
based on reverse-engineering the computations per-
formed by Transformers. These techniques are tied
to concrete architectures, which are often “toy” ver-
sions of those used in real-world applications, e.g.
attention-only Transformers in Elhage et al.

Other options include general-purpose meth-
ods like neuron/activation interpretation (e.g. Geva
et al., 2021; Goh et al., 2021; Dai et al., 2022),
saliency maps (e.g. Fong and Vedaldi, 2017; An-
cona et al., 2019) and influence functions (Koh
and Liang, 2017). These require access to internal
representations and/or the ability to backpropagate
gradients, and have some caveats of their own (Kin-
dermans et al., 2019; Kokhlikyan et al., 2021).

More closely related to our work are studies that
perform ablation (e.g. by shuffling, truncation or
masking) on different contexts to understand their
influence on predictions (O’Connor and Andreas,
2021; Sun et al., 2021; Press et al., 2021; Vafa
et al., 2021). To our knowledge, all such existing
works only test a few select contexts or greedily
search for the most informative one; in contrast,
we show that it is feasible to consider all context
lengths in the range from 1 to a maximum cmax,
which permits us to obtain fine-grained insights on
the example level, e.g. in the form of the proposed
differential importance scores. Moreover, many
existing analyses (e.g. Vafa et al., 2021; O’Connor
and Andreas, 2021) rely on specific training or fine-
tuning, which is not the case with our proposal.

A causal LM estimates the conditional probability
distribution of a token given its left-hand context
in a document:

p(xn+1 | x1, . . . , xn).

(1)

We are interested here in computing the probabil-
ities conditioned on a reduced context of length
c ∈ {1, . . . , n}:

p(xn+1 | xn−c+1, . . . , xn),

(2)

so that we may then study the behavior of this
distribution as a function of c.

An apparent obstacle in doing so is that ap-
plying the model to an arbitrary subsequence
xn−c+1, . . . , xn,
instead of the full document
x1, . . . , xN , may lead to inaccurate estimates of
the probabilities in Eq. (2). However, we note that
large LMs are not usually trained on entire docu-
ments. Instead, the training data is pre-processed
by shuffling all the documents, concatenating them
(with a special token as a separator), and splitting
the resulting sequence into chunks of a fixed length
(usually 1024 or 2048 tokens) with no particular
relation to the document length. Thus, the models
are effectively trained to accept sequences of to-
kens starting at arbitrary positions in a document
and it is therefore correct to employ them as such
to compute estimates of Eq. (2).3

It now remains to be detailed how to efficiently
evaluate the above probabilities for all positions
n and context lengths c. Specifically, for a given
document x1, . . . , xN and some maximum context
length cmax, we are interested in an (N − 1) ×
cmax × |V| tensor P , where V = (cid:8)w1, . . . , w|V|
(cid:9)
is the vocabulary, such that:

Pn,c,i = p(xn+1 = wi | xn−c+1, . . . , xn),

(3)

with Pn,c,∗ = Pn,n−1,∗ for n ≤ c.4 Observe that
by running the model on any segment xm, . . . , xn,
we obtain all the values Pm+c−1,c,∗ for c ∈
{1, . . . , n − m + 1}. Therefore, we can fill in the
tensor P by applying the model along a sliding win-
dow of size cmax, i.e. running it on N (overlapping)

3For models trained on data that is pre-processed differ-
ently, (re)training or fine-tuning with data augmentation such
as random shifts may be needed in order to apply our method,
analogously to Vafa et al. (2021), who use word dropout to
ensure compatibility with their method.

4Pn,c,∗ is a |V|-dimensional slice of P along the last axis.

segments of length at most cmax. See Appendix A
for an illustration and additional remarks.

3.2 Metrics

Having obtained the tensor P as we have just de-
scribed, we use it to study how the predictions
evolve as the context length is increased from 1 to
cmax. Specifically, our goal is to define a suitable
metric that we can compute from Pn,c,∗ and follow
it as a function of c (for a specific n or on average).
One possibility would be to use the negative log-

likelihood (NLL) loss values:

− log p(xn+1 | xn−c+1, . . . , xn).

(4)

However, this may not be a particularly suitable
metric for explainability purposes, as it depends
(only) on the probability assigned to the ground
truth xn+1, while the LM outputs a probability dis-
tribution Pn,c,∗ over the entire vocabulary, which
may in fact contain many other plausible contin-
uations. For this reason, we propose to exploit
a metric defined on whole distributions, e.g. the
Kullback-Leibler (KL) divergence. To achieve
this, we choose the maximum-context predictions
Pn,cmax,∗ as a reference and get:

Dn,c = DKL[Pn,cmax,∗ ∥ Pn,c,∗]

=

|V|
(cid:88)

i=1

Pn,cmax,i log

Pn,cmax,i
Pn,c,i

.

(5)

The rationale for (5) is to quantify the amount of
information that is lost by using a shorter context
c ≤ cmax. Interestingly, this metric is not related to
the absolute performance of the model with maxi-
mal context, but rather to how the output changes
if a shorter context is used.

3.3 Differential importance scores

We are also interested in studying how individual
increments in context length affect the predictions.
We propose to quantify this as the change in the
KL divergence metric (5) when a new token is in-
troduced into the context. Specifically, for a pair of
tokens xn+1 (the target token) and xm (the context
token), we define a differential importance score
(∆-score for short)

∆Dn,m = Dn,n−m−1 − Dn,n−m.

(6)

We may visualize these scores as a way to explain
the LM predictions, much like is often done with

name

#param #layer #head dmodel max len

117 M
gpt2
gpt2-xl
1.5 B
gpt-j-6B 6.1 B

12
48
28

12
768
25 1600
16 4096

1024
1024
2048

Table 1: Hyperparameters of the 3 models used.

attention weights, with two important differences.
First, a high ∆Dn,m should not be interpreted as
meaning that xm in isolation is important for pre-
dicting xn+1, but rather that it is salient given the
context that follows it (which might mean that it
brings information not contained in the following
context). Second, unlike attention weights, our
scores need not sum up to one, and can be negative;
in this regard, the proposed representation is more
conceptually similar to a saliency map than to an
attention map.

4 Results

We apply the proposed technique to publicly avail-
able pre-trained large Transformer language mod-
els, namely GPT-J (Wang and Komatsuzaki, 2021)
and two GPT-2 (Radford et al., 2019) variants –
see Table 1 for an overview. We use the validation
set of the English LinES treebank5 from Universal
Dependencies (UD; Nivre et al., 2020), containing
8 documents with a total length of 20 672 tokens6
and covering fiction, an online manual, and Eu-
roparl data. We set cmax = 1023. We use the
Transformers library7 (Wolf et al., 2020) to
load the pre-trained models and run inference. Fur-
ther technical details are included in Appendix B.

4.1 LM loss by context length

Fig. 2 shows the cross entropy losses (NLL means)
across the whole validation dataset as a function
of context length c. As expected, larger models
perform better than smaller ones, which is tradi-
tionally explained by their larger capacity. A less
common observation we can make thanks to this
detailed representation is that the gains in perfor-
mance come mostly from relatively short contexts
(8–256 tokens); this is consistent with prior works
(Sun et al., 2021; Press et al., 2021) which found

5https://universaldependencies.org/

treebanks/en_lines/index.html

6After concatenating all sentences and applying the GPT-2

tokenizer, which is used by both GPT-2 and GPT-J.
7https://github.com/huggingface/

transformers

our use of these differences as importance scores.

4.3 Differential importance scores

To facilitate the exploration of ∆-scores from Sec-
tion 3.3, we have created an interactive web demo,2
which allows visualizing the scores for any of the
3 models on the validation set as shown in Fig. 1.
In Fig. 5, we display the magnitudes of the ∆-
scores – normalized for each position to sum up
to 1 across all context lengths – as a function of con-
text length. The plot suggests a power-law-like in-
verse relationship where increasing context length
proportionally reduces the ∆-score magnitude on
average. We interpret this as far-away tokens being
less likely to carry information not already covered
by shorter contexts. Long contexts (see inset in
Fig. 5) bear less importance for larger models than
for smaller ones, perhaps because the additional
capacity allows relying more on shorter contexts.
In Fig. 6, we also display the mean importance
score received by each POS category, by model.
We can see that proper nouns (PROPN) are sub-
stantially more informative than other categories
(which is in line with the observations in the pre-
vious section), but less so for the smallest model.
This could mean e.g. that larger models are better at
memorizing named entities from training data and
using them to identify the topic of the document,
or simply at copying them from distant context as
observed in (Sun et al., 2021).

5 Limitations and future directions

Experiments. We acknowledge the limited scope
of our experiments,
including only 8 (closed-
domain) documents, 3 models and a single lan-
guage. This is largely due to the limited availability
of suitable large LMs and their high computational
cost. Still, we believe that our experiments are valu-
able as a case study that already clearly showcases
some interesting features of our methodology.

Figure 2: Mean LM losses by context length.

Figure 3: Mean GPT-J loss by context length and part-
of-speech (POS) tag of the target token. Only POS tags
with at least 100 occurrences in the dataset are included.
The tags are grouped (arbitrarily) for clarity.

that very long contexts bring only minimal improve-
ment (though these focused on specific long-range
architectures and on contexts beyond the range we
investigate here).

In Fig. 3, we display the same information (loss
by context length) broken down by part-of-speech
(POS) tags, for GPT-J only. For most POS tags, the
behavior is similar to what we observed in Fig. 2
and the loss appears to stabilize around context
lengths 16–64. However, we see a distinct be-
haviour for proper nouns (PROPN), which are the
hardest-to-predict category for short contexts, but
whose loss improves steadily with increasing c, sur-
passing that of regular nouns (NOUN) at c = 162
and continuing to improve beyond that point.

4.2 Per-token losses by context length

We have also examined token-level losses, as well
as the KL divergence metric (see Section 3.2); an
example plot is shown in Fig. 4 and more are found
in Appendix C.1. In general, we observe that the
values tend to change gradually with c; large differ-
ences are sparse, especially for large c, and can of-
ten be attributed to important pieces of information
appearing in the context (e.g. “owl” and “swoop”
in the context of “birds” in Fig. 4). This justifies

Computational cost. While we have demons-
trated an efficient strategy to obtain predictions
for all tokens at all possible context lengths, it still
requires running the model N times for a document
of length N .

For a k-fold reduction in computational cost, the
technique may be modified to use a sliding window
with stride k > 1 (instead of k = 1 as proposed
above). See Appendix A.1 for details.

1416642561024context length46lossgpt2gpt2-xlgpt-j-6B1416642561024context length2468lossPROPNNOUNVERB/ADJ/ADV/NUMPRON/SCONJAUX/CCONJ/ADPPART/PUNCT/DETFigure 4: NLL (left) and KL divergence (right) as a function of context length for a selected example: “[. . .] mortally
afraid of birds” (same as in Fig. 1). The x axis is reversed for visual correspondence with the left-hand context.
The 5 context tokens causing the largest drops in each metric for GPT-J are marked by red dots.

6 Conclusion and future directions

We have presented context length probing, a novel
causal LM explanation technique based on tracking
the predictions of the LM as a function of context
length, and enabling the assignment of differential
importance scores (∆-scores). While it has some
advantages over existing techniques, it answers
different questions, and should thus be thought of
as complementary rather than a substitute.

A particularly interesting feature of our ∆-scores
is their apparent potential for discovering long-
range dependencies (LRDs) (as they are expected
to highlight information not already covered by
shorter contexts, unlike e.g. attention maps).

Remarkably, our analysis suggests a power-law-
like inverse relationship between context length and
importance score, seemingly questioning the impor-
tance of LRDs in language modeling. While LRDs
clearly appear crucial for applications such as long-
form text generation, their importance may not be
strongly reflected by LM performance metrics like
cross entropy or perplexity. We thus believe that
there is an opportunity for more specialized bench-
marks of LRD modeling capabilities of different
models, such as that of Sun et al. (2022), for exam-
ple. These should further elucidate questions like to
what extent improvements in LM performance are
due to better LRD modeling, how LRDs are han-
dled by various Transformer variants (e.g. Kitaev
et al., 2020; Katharopoulos et al., 2020; Choroman-
ski et al., 2021; Press et al., 2022), or what their
importance is for different tasks.

Acknowledgments

This work was supported by the LabEx NUMEV
(ANR-10-LABX-0020) within the I-Site MUSE
(ANR-16-IDEX-0006). The authors are grateful
to the OPAL infrastructure from Université Côte
d’Azur for providing resources and support.

Figure 5: Normalized ∆-score log-magnitude (mean
and std. dev.) by context length and by model. Only
positions n ≥ 1024 are included.

Figure 6: Mean ∆-score by POS tag of the context token
and by model.

Choice of metrics. The proposed methodology
allows investigating how any given metric is im-
pacted by context, yet our study is limited to NLL
loss and the proposed KL divergence metric (the
latter for defining importance scores). These may
not be optimal for every purpose, and other choices
should be explored depending on the application.
For example, to study sequences generated (sam-
pled) from a LM, one might want to define impor-
tance scores using a metric that does depend on the
generated token, e.g. its NLL loss or its ranking
among all candidates. (Indeed, our web demo also
supports ∆-scores defined using NLL loss values.)

141664256context length246810NLLleysafraidswoopswoopowlgpt2gpt2-xlgpt-j-6B141664256context length0510KL divergencewifeswoopDafraidowlgpt2gpt2-xlgpt-j-6B416642561024context length-6-5-4-3-2-1log10(norm.||)6008001000-5-4gpt2gpt2-xlgpt-j-6B00.005SYMPARTADPCCONJPUNCTADVDETNUMADJINTJPRONAUXSCONJNOUNXVERBPROPNgpt-j-6B00.005gpt2-xl00.005gpt2References

Marco Ancona, Cengiz Oztireli, and Markus Gross.
2019. Explaining deep neural networks with a poly-
nomial time algorithm for Shapley value approxima-
tion. In Proceedings of the 36th International Con-
ference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pages 272–281.
PMLR.

Krzysztof Marcin Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane, Tamás
Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Benjamin Be-
langer, Lucy J. Colwell, and Adrian Weller. 2021.
Rethinking attention with Performers. In 9th Inter-
national Conference on Learning Representations
(ICLR 2021). OpenReview.net.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2022. Knowledge neurons in
pretrained transformers. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 8493–
8502, Dublin, Ireland. Association for Computational
Linguistics.

Yann N. Dauphin, Angela Fan, Michael Auli, and David
Grangier. 2017. Language modeling with gated con-
volutional networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning, vol-
ume 70 of Proceedings of Machine Learning Re-
search, pages 933–941. PMLR.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly,
Nova DasSarma, Dawn Drain, Deep Ganguli, Zac
Hatfield-Dodds, Danny Hernandez, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, and Chris Olah. 2021. A
mathematical framework for Transformer circuits.
Transformer Circuits Thread.

Ruth C. Fong and Andrea Vedaldi. 2017. Interpretable
explanations of black boxes by meaningful perturba-
tion. In IEEE International Conference on Computer
Vision, pages 3449–3457, Venice, Italy. IEEE Com-
puter Society.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are key-
value memories. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 5484–5495, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.

Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan
Carter, Michael Petrov, Ludwig Schubert, Alec Rad-
ford, and Chris Olah. 2021. Multimodal neurons in
artificial neural networks. Distill.

Benjamin Hoover, Hendrik Strobelt, and Sebastian
Gehrmann. 2020. exBERT: A Visual Analysis Tool

to Explore Learned Representations in Transformer
Models. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 187–196, Online. As-
sociation for Computational Linguistics.

Sarthak Jain and Byron C. Wallace. 2019. Attention is
not Explanation. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers), pages 3543–3556, Minneapolis, Minnesota.
Association for Computational Linguistics.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
RNNs: Fast autoregressive Transformers with linear
attention. In Proceedings of the 37th International
Conference on Machine Learning. PMLR.

Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo,
Maximilian Alber, Kristof T. Schütt, Sven Dähne, Du-
mitru Erhan, and Been Kim. 2019. The (un)reliability
of saliency methods. In Wojciech Samek, Grégoire
Montavon, Andrea Vedaldi, Lars Kai Hansen, and
Klaus-Robert Müller, editors, Explainable AI: Inter-
preting, Explaining and Visualizing Deep Learning,
volume 11700 of Lecture Notes in Computer Science,
pages 267–280. Springer.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient Transformer. In 8th
International Conference on Learning Representa-
tions (ICLR 2020). OpenReview.net.

Pang Wei Koh and Percy Liang. 2017. Understanding
In
black-box predictions via influence functions.
Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pages 1885–1894.
PMLR.

Narine Kokhlikyan, Vivek Miglani, Bilal Alsallakh,
Miguel Martin, and Orion Reblitz-Richardson. 2021.
Investigating sanity checks for saliency maps with
arXiv preprint
image and text classification.
arXiv:2106.07475.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
ˇCernocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH 2010, 11th Annual Conference of the Inter-
national Speech Communication Association, pages
1045–1048, Makuhari, Chiba, Japan. ISCA.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Jan Hajiˇc, Christopher D. Manning, Sampo
Pyysalo, Sebastian Schuster, Francis Tyers, and
Daniel Zeman. 2020. Universal Dependencies v2:
An evergrowing multilingual treebank collection. In
Proceedings of the 12th Language Resources and
Evaluation Conference, pages 4034–4043, Marseille,
France. European Language Resources Association.

10314–10332, Online and Punta Cana, Dominican
Republic. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems, pages 5998–6008,
Long Beach, CA, USA.

Jesse Vig. 2019. A multiscale visualization of attention
in the transformer model. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 37–42,
Florence, Italy. Association for Computational Lin-
guistics.

Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A
6 billion parameter autoregressive language model.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven
Le Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander Rush. 2020.
Transform-
ers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 38–45, Online. Association
for Computational Linguistics.

Joe O’Connor and Jacob Andreas. 2021. What context
features can transformer language models use? In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 851–864,
Online. Association for Computational Linguistics.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
Danny Hernandez, Scott Johnston, Andy Jones, Jack-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. 2022. In-context
learning and induction heads. Transformer Circuits
Thread.

Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
In The Tenth Interna-
input length extrapolation.
tional Conference on Learning Representations, Vir-
tual Event. OpenReview.net.

Ofir Press, Noah A. Smith, and Mike Lewis. 2021.
Shortformer: Better language modeling using shorter
In Proceedings of the 59th Annual Meet-
inputs.
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 5493–5505, Online. Association for Computa-
tional Linguistics.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Sofia Serrano and Noah A. Smith. 2019. Is attention in-
terpretable? In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 2931–2951, Florence, Italy. Association for
Computational Linguistics.

Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range lan-
guage models actually use long-range context? In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages 807–
822, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.

Simeng Sun, Katherine Thai, and Mohit Iyyer. 2022.
ChapterBreak: A challenge dataset for long-range
language models. In Proceedings of the 2022 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 3704–3714, Seattle,
United States. Association for Computational Lin-
guistics.

Keyon Vafa, Yuntian Deng, David Blei, and Alexander
Rush. 2021. Rationales for sequential predictions.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages

Figure 7: A step of context length probing with cmax = 10. The input tokens are shown at the top, the target tokens
at the bottom. When the LM is run on a segment of the document, the effective context length for each target token
is equal to its offset from the beginning of the segment, e.g. the context for predicting “ D” is “ the” (c = 1), the
context for “urs” is “ the D” (c = 2), etc.

A Context length probing

Fig. 7 illustrates a step of context length probing. We wish to obtain the tensor P from Eq. (3), understood
as a table where each cell contains the predictions (next-token logits) for a given position in the text and a
given context length. By running our LM on a segment of the text, we get predictions such that for the
n-th token in the segment, the effective context length is equal to n, which corresponds to a diagonal
in the table. We can thus fill in the whole table by running the LM on all segments of length cmax (plus
trailing segments of lengths cmax − 1, . . . , 1).

Notice that this process is somewhat similar to (naïvely) running the LM in generation mode, except
that at each step, the leading token is removed, preventing the use of caching to speed up the computation.
In practice, it is not necessary to explicitly construct the tensor P . Indeed, we find it more efficient
to instead store the raw logits obtained by running the model on all the segments, then do the necessary
index arithmetics when computing the metrics.

A.1 Strided context length probing

For a k-fold reduction in computational cost, we may instead use a sliding window with a stride k > 1,
i.e. run the model only on segments starting at positions k (n − 1) + 1 for all n ∈ {1, . . . , ⌈N/k⌉}, rather
than all positions. This way, for a target token xn+1, we obtain the predictions p(xn+1 | xn−c+1, . . . , xn)
only for such context lengths c that c mod k = n. In other words, predictions with context length c are
only available for tokens xc+1, xc+k+1, xc+2k+1, . . .. Consequently:

• Overall, we still cover all context lengths 1, . . . , cmax, allowing us to perform aggregate analyses like

the ones in Section 4.1.

• When analyzing the predictions for a specific target token in a document (e.g. to compute ∆-scores),
context tokens come in blocks of length k. Visualizations like the ones in Figs. 1 and 4 are still
possible for all target tokens, but become less detailed, grouping every k context tokens together.

• Computation time, as well as the space needed to store the predictions, is reduced by a factor of k.

B Technical details

Data. The LinES treebank is licensed under Creative Commons BY-NC-SA 4.0. We concatenated all
tokens from each of the documents from the treebank, then re-tokenized them using the GPT-2 tokenizer.

LMcontext length12345678910We mapped the original (UD) POS tags to the GPT-tokenized dataset in such a way that every GPT token
is assigned the POS tag of the first UD token it overlaps with.

Models. We used the models EleutherAI/gpt-j-6B (Apache 2.0 license), and gpt2-xl and
gpt2 (MIT license), all from huggingface.co.

Computation. We parallelized the inference over 500 jobs on a compute cluster,8 each running on 8
CPU cores with at least 8 GB of RAM per core, with a batch size of 16. Each job took about 10–20 min
for GPT-2 and 30–60 min for GPT-J. Additionally, computing the metrics from the logits (which take up
2 TB of disk space in float16) took between 2 and 4 h per model on a single machine with 32 CPU
cores. The total computing time was 318 core-days, including debugging and discarded runs.

C Additional plots

C.1 Token-wise metrics as a function of context length

Figs. 8 and 9 show NLL and KL divergence (5), respectively, as a function of context length, for selected
target tokens (proper nouns) from the validation set.

8Nef, the cluster computing infrastructure of Inria Sophia Antipolis Méditerranée; see https://wiki.inria.fr/

ClustersSophia

(a) . . . and attribute means (and thus how the data between them will look in a browser), XML uses the tags only to delimit pieces
of data, and leaves the interpretation of the data completely to the application that reads it. Additional information about XML
can be found on the web site. About importing XML data Access

(b) . . . he felt things were getting too quiet, and small explosions from Fred and George’s bedroom were considered perfectly
normal. What Harry found most unusual about life at Ron’s, however, wasn’t the talking mirror or the clanking ghoul: it was the
fact that everybody there seemed to like him. Mrs Weasley

(c) . . . is a great difference between Napoleon the Emperor and Napoleon the private person. There are raisons d’etat and there
are private crimes. And the talk goes on. What is still being perpetuated in all civilized discussion is the ritual of civilized
discussion itself. Tatu agrees with the Archbishop about the Russians

Figure 8: NLL losses (y axis) for 3 selected target tokens as a function of context length (x axis). Below each plot,
the target token is displayed in bold, along with a context of 60 tokens. The x axis is reversed to correspond visually
to left-hand context. The red dots show the 10 tokens that cause the largest drops in GPT-J cross entropy when
added to the context.

1416642561024468101214specifiesAccessapplicationAboutsiteAdditionalinformationAccessXMLMicrosoftgpt2gpt2-xlgpt-j-6B1416642561024024681012likeabouthimmostfrom'toulWhat.Harrygpt2gpt2-xlgpt-j-6B14166425610244681012AndAndretheagreecivilized.RussiansagreesArchbishopRussiansgpt2gpt2-xlgpt-j-6B(a) . . . and attribute means (and thus how the data between them will look in a browser), XML uses the tags only to delimit pieces
of data, and leaves the interpretation of the data completely to the application that reads it. Additional information about XML
can be found on the web site. About importing XML data Access

(b) . . . he felt things were getting too quiet, and small explosions from Fred and George’s bedroom were considered perfectly
normal. What Harry found most unusual about life at Ron’s, however, wasn’t the talking mirror or the clanking ghoul: it was the
fact that everybody there seemed to like him. Mrs Weasley

(c) . . . is a great difference between Napoleon the Emperor and Napoleon the private person. There are raisons d’etat and there
are private crimes. And the talk goes on. What is still being perpetuated in all civilized discussion is the ritual of civilized
discussion itself. Tatu agrees with the Archbishop about the Russians

Figure 9: KL divergences (y axis) from Eq. (5) for 3 selected target tokens as a function of context length (x axis).
Below each plot, the target token is displayed in bold, along with a context of 60 tokens. The x axis is reversed to
correspond visually to left-hand context. The red dots show the 10 tokens that cause the largest drops in the metric
(for GPT-J) when added to the context.

14166425610240123456youAbouttheimportingspecifiesAboutAdditionalinformationsiteXMLgpt2gpt2-xlgpt-j-6B14166425610240246810leysabouthim'tmostfromoulWhat.Harrygpt2gpt2-xlgpt-j-6B14166425610240.00.51.01.52.02.53.0ArchbishoptalksayingNapoleonuatedof.agreesaboutArchbishopgpt2gpt2-xlgpt-j-6B