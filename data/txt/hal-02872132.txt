Multispectral Fusion for Object Detection with Cyclic
Fuse-and-Refine Blocks
Heng Zhang, Elisa Fromont, Sébastien Lefèvre, Bruno Avignon

To cite this version:

Heng Zhang, Elisa Fromont, Sébastien Lefèvre, Bruno Avignon. Multispectral Fusion for Object
Detection with Cyclic Fuse-and-Refine Blocks. ICIP 2020 - IEEE International Conference on Image
Processing, Oct 2020, Abou Dabi, United Arab Emirates. pp.1-5. ￿hal-02872132￿

HAL Id: hal-02872132

https://hal.science/hal-02872132

Submitted on 17 Jun 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

MULTISPECTRAL FUSION FOR OBJECT DETECTION
WITH CYCLIC FUSE-AND-REFINE BLOCKS

Heng ZHANG1,3, Elisa FROMONT1,4, S´ebastien LEFEVRE2, Bruno AVIGNON3

1Univ Rennes, IRISA 2Univ Bretagne Sud, IRISA, 3ATERMES,4 IUF, Inria

ABSTRACT

Multispectral images (e.g. visible and infrared) may be partic-
ularly useful when detecting objects with the same model in
different environments (e.g. day/night outdoor scenes). To ef-
fectively use the different spectra, the main technical problem
resides in the information fusion process. In this paper, we
propose a new halfway feature fusion method for neural net-
works that leverages the complementary/consistency balance
existing in multispectral features by adding to the network
architecture, a particular module that cyclically fuses and re-
ﬁnes each spectral feature. We evaluate the effectiveness of
our fusion method on two challenging multispectral datasets
for object detection. Our results show that implementing our
Cyclic Fuse-and-Reﬁne module in any network improves the
performance on both datasets compared to other state-of-the-
art multispectral object detection methods.

Index Terms— Multispectral object detection, Multi-

spectral feature fusion, Deep learning

1. INTRODUCTION

Visible and thermal image channels are expected to be com-
plementary when used for object detection in the same out-
door scenes.
In particular, visible images tend to provide
color and texture details while thermal images are sensitive
to objects’ temperature, which may be very helpful at night
time. However, because they provide a very different view of
the same scene, the features extracted from different image
spectra may be inconsistent and lead to a difﬁcult, uncertain
and error-prone fusion as shown in Figure 1. In this ﬁgure, we
use a Convolutional Neural Network (CNN, detailed later) to
predict two segmentation masks based on the two (aligned)
mono-spectral extracted features from the same image and
then fuse the features to detect pedestrians in the dataset. Dur-
ing the training phase, the object detection and the semantic
segmentation losses are jointly optimised (the segmentation
ground truths are generated according to pedestrian bound-
ing box annotations). We can observe that most pedestrians
are visible either on the RGB or on the infrared segmenta-
tion masks which illustrates the complementary of the chan-
nels. However, even though the visible-thermal image pairs
are well aligned, the similarity between the two predicted seg-

Fig. 1. Examples of thermal and RGB images of the same
aligned scenes taken from KAIST multispectral pedestrian
detection dataset [1] with detected bounding boxes. The seg-
mentation masks (2nd and 4th columns) are predicted based
on the (mono-)spectral features before any fusion process.

mentation masks is small, i.e., the multispectral features may
be inconsistent.

In order to augment the consistency between features of
different spectra, we design a novel feature fusion approach
for convolutional neural networks based on Cyclic Fuse-and-
Reﬁne modules. Our main idea is to reﬁne the mono-spectral
features with the fused multispectral features multiple times
consecutively in the network. Such a fusion scheme has two
advantages: 1) since the fused features are generally more dis-
criminative than the spectral ones, the reﬁned spectral features
should also be more discriminative than the original spec-
tral features and the fuse-and-reﬁne loop gradually improves
the overall feature quality; 2) since the mono-spectral fea-
tures keep being reﬁned with the same features, their consis-
tency progressively increases, along with the decrease of their
complementary, and the consistency/complementary balance
is achieved by controlling the number of loops.

We review the related works on multispectral feature fu-
sion with CNN in Section 2. We detail our novel network
module named Cyclic Fuse-and-Reﬁne, which loops on the
fuse-and-reﬁne operations to adjust the multispectral features’
complementary/consistency balance in Section 3. In Section
4, we show experiments on the well known KAIST multispec-

visible	imagesvisible	masksthermal	masksthermal	imagesFig. 2. Illustration (folded on the left part and unfolded on the right) of the proposed Cyclic Fuse-and-Reﬁne Module with 3
loops. Better viewed in color.

tral pedestrian detection dataset [1] on which we obtain new
state-of-the-art results, and on the less known FLIR ADAS
dataset [2] on which we set a ﬁrst strong baseline.

2. RELATED WORK

Multispectral object detection approaches mainly differ on the
strategies (“when” and “how”) used to fuse the multispectral
features.
When to fuse. The ﬁrst study on CNN-based multispectral
pedestrian detection is made by [3], and they evaluate two
fusion strategies: early and late fusions. Then [4] and [5] ex-
plore this further and show that a fusion of features halfway in
the network, achieves better results than the early or the late
fusion. Since then, the halfway fusion has become the de-
fault strategy in deep learning-based multispectral (and mul-
timodal) works ([5, 6, 7, 8, 9]). We also choose to locate our
fuse-and-reﬁne fusion module halfway in the network.
How to fuse. Features extracted from each spectral chan-
nel have different physical properties and choosing how to
fuse these complementary information is another central re-
search topic. Basic fusion methods include element-wise
addition/average, element-wise maximum and concatenation
sometimes in addition to a 1 × 1 convolution to compress the
number of channels as done e.g.
in [10]. Building on this,
more advanced methods such as [5] and [6] use illumination
information to guide the multispectral feature fusion.
[11]
apply Gated Fusion Units (GFU) [12] to combine two SSD
networks [13] on color and thermal inputs.
[8] propose a
cross-modality interactive attention network to dynamically
weight the fusion of thermal/visible features. Our strategy is
different: we suggest a cyclic fusion scheme to progressively
improve the quality of the spectral features and automatically
adjust the complementary/consistence balance.

3. PROPOSED APPROACH

Overview. The fusion and reﬁnement operations are the main
ones of our proposed approach. They are repeated (through a

t

, f i−1
v

t + f i

v + f i

v = H(f i−1

cycle) multiple times to increase the consistency of the mul-
tispectral features and to decrease the complementarity of the
features. An illustration of our Cyclic Fuse-and-Reﬁne mod-
ule with 3 loops in the cycle is presented in Fig. 2.
Fuse-and-Reﬁne. In each loop i, for the fused (f ), visible
(v) and thermal (t) features, the multispectral feature fusion
f = F(σ(f i−1
can be formalized as f i
)), where σ is a
feature concatenation operation, and F is a 3 × 3 convolution
followed by a batch normalization operation. For simplicity
and to avoid over-ﬁtting, the Conv-BN operations F in all
loops share the same weights. The fused features are then
assigned as residuals of the spectral features for reﬁnement:
t = H(f i−1
f ), f i
f i
f ). H is the activation
function (e.g. ReLU).
Semantic supervision. In order to prevent the vanishing gra-
dient problem when learning the parameters of the network
and to better guide the multispectral feature fusion, an auxil-
iary semantic segmentation task is used to bring separate su-
pervision information for each reﬁned spectral features. Con-
cretely, after being reﬁned with the fused features, the thermal
and visible features go through a 1 × 1 convolution to pre-
dict two pedestrian segmentation masks, one for each chan-
nel. These predicted masks are also used to tune (or at least
visualize) the number of loops in the cyclic module according
to the complementary/consistency variations in the features.
Final fusion. We aggregate all the reﬁned spectral features
to generate the ﬁnal fused features that will be used for the
object detection part of the network. The aggregation is a
simple element-wise average function. Let I be the number
of loops, the ﬁnal computation is: 1

t + (cid:80)I

2I ((cid:80)I

1 f i

1 f i

v).

4. EXPERIMENTS

We evaluate the proposed Cyclic Fuse-and-Reﬁne Module
on KAIST Multispectral Pedestrian Detection [1] and FLIR
ADAS dataset [2], and compare our results with the state-
of-the-art multispectral methods. Examples of image pairs
from these datasets with their ground truth bounding boxes
are shown in Figure 3.

fusedfeaturesthermalfeaturesvisiblefeaturesconcat+convresidualresidualcycling	for	3	timesunfoldfusedfeaturesff1thermalfeaturesft0visiblefeaturesfv0concat+convidentityresidualresidualidentityfusedfeaturesff2thermalfeaturesft1visiblefeaturesfv1concat+convidentityresidualresidualidentityfusedfeaturesff3thermalfeaturesft2visiblefeaturesfv2concat+convidentitythermalfeaturesft3residualresidualvisiblefeaturesfv3identitysemantic		supervisionsemantic		supervisionobject detector [13]. Note that our proposed module is in-
dependent from the chosen network architecture. Following
[4] and [5], the mono-spectral features are extracted indepen-
dently through a VGG16 [15] network, and fused after the
conv4 3 layer (halfway through the network). Our baseline
architecture uses the element-wise average for the multispec-
tral feature fusion and we integrate and evaluate the proposed
module with different number of loops.
Data augmentation. As implemented in SSD [13] and FSSD
[14], a few data augmentation methods are applied, such as
image random cropping, padding, ﬂipping and distorting for
both visible and thermal images.
Anchor designing. Following [16], the anchor designing
strategy is adapted for the pedestrian detection for KAIST
dataset: we ﬁx the aspect ratio of each anchor box to 0.41
and we only keep three detection layers with scales 32 and
2 from ﬁne to coarse
32
respectively. For FLIR, we use the same scale settings but we
augment the aspect ratio setting to {1, 2, 1
Loss functions. To improve object detection, SDS RCNN
[17] and MSDS RCNN [7] use an additional task, semantic
segmentation, and jointly optimize the loss for the segmenta-
tion and detection tasks while training the network. To fairly
compare our work to these competitors, we also use this aux-
iliary loss to supervise the training of the proposed module.

2, 128 and 128

2, 64 and 64

2 }.

√

√

√

4.3. Comparison with state-of-the-art methods

On KAIST. We compare the experimental results of our ap-
proach with state-of-the-art methods in Table 1. For these ex-
periments, we make 3 loops in the Fuse-and-Reﬁne cycle. De-
pending on what was done in the literature and to allow a fair
comparison, we report our detection accuracy with sanitized
and original training annotations respectively. All the deep
learning-based methods [4, 18, 5, 6, 7] use the same input im-
age resolution (640 × 512) and the same backbone network
(VGG16). The results show that our proposed method allows
us to obtain better detection results than all its competitors for
both the sanitized and original training annotations.
On FLIR. Because of the misalignment problems in the
dataset, there is, to our knowledge, no paper which uses the
FLIR dataset [2] for multispectral object detection. We use
our sanitized version of the dataset and compare the mAP
percentage of two different models: a baseline model which
uses the traditional halfway fusion architecture (with the
VGG backbone) and the same model with our proposed mod-
ule. Again, we can see in Table 2 that our method provides
important mAP gains for all the considered object categories.

4.4. Ablation study

We study in details (on the KAIST dataset with the sani-
tized training annotations and the reasonable test set) the ef-
fectiveness of the proposed fusion module and the relation-

Fig. 3. Examples of visible/thermal image pairs with their
ground truth from the KAIST dataset (in the ﬁrst line) and
from the FLIR dataset in the second and third lines (the
ground truth annotations are given according to the thermal
images). The third line gives an example of misaligned pairs
in the FLIR dataset. Better viewed in color and zoomed in.

4.1. Datasets

KAIST. We use the processed version of this multispec-
tral pedestrian detection dataset which contains 7,601 color-
thermal image pairs for training and 2,252 pairs for testing.
We kept the bounding boxes annotated as “person”, “person?”
or “people” as positive pedestrian examples. [7] proposed a
“sanitized” version of the training annotations which elimi-
nated some of the annotation errors from the original training
annotations. According to [4], inaccurate annotations in the
test set leads to unfair comparisons, so we only use their “san-
itized” testing annotations for our evaluation, with the usual
“Miss Rate” performance metric under reasonable setting,
i.e., a test subset containing not/partially occluded pedestri-
ans which are larger than 55 pixels.
FLIR. This recently released multispectral (multi-)object de-
tection dataset contains around 10k manually-annotated ther-
mal images with their corresponding reference visible images,
collected during daytime and nighttime. We only kept the 3
more frequent classes which are “bicycle”, “car” and “per-
son”. We manually removed the misaligned visible-thermal
image pairs and ended with 4,129 well-aligned image pairs
for training and 1,013 image pairs for test 1. Some exam-
ples of the well-aligned and misaligned visible-thermal image
pairs are shown in Figure 3.

4.2. Training details

Network architecture. We implemented our Cyclic Fuse-
and-Reﬁne module on the single stage object detector FSSD
[14], which is an improved version of the well known SSD

1This new aligned dataset can be downloaded here:

http://

shorturl.at/ahAY4

Methods

Miss Rate (lower, better)
R-Day

R-Night

R-All

7.49%
5.92%
8.09%
6.13% 7.68% 3.19%

Training with sanitized annotations:
MSDS-RCNN [7]
CFRM 3
Training with original annotations:
ACF+T+THOG [1]
Halfway Fusion [4]
Fusion RPN+BF [18]
IAF R-CNN [5]
IATDNN+IASS [6]
MSDS-RCNN [7]
CFRM 3

47.24% 42.44% 56.17%
26.15% 24.85% 27.59%
16.53% 16.39% 18.16%
16.22% 13.94% 18.28%
15.78% 15.08% 17.22%
11.63% 10.60% 13.73%
10.05% 9.72% 10.80%

Fig. 4. Examples of pedestrian segmentation masks predicted
on 2 visible/thermal image pairs (one taken at day time, one
taken at night) of the KAIST dataset after a different number
of loops (1-3) in the fuse-and-reﬁne cycle. Zoom in to see
details.

ship between the number of loops in the fuse-and-reﬁne cy-
cle and the multispectral feature complementary/consistency
balance. The experimental results are summarised in Table
3. We provide the Miss Rate and DICE scores [19] between
the pedestrian masks predicted by each version of the reﬁned
thermal/visible features. These DICE scores are used as an in-
dicator of similarity between the spectral features. From the
table we observe successive accuracy gains from the baseline
(no loop) to 3 loops, and a decrease after 4 loops; meanwhile
the value of DICE scores continue to increase along with the
number of loops. We then visualize, on two sample image
pairs, the pedestrian masks predicted by visible/thermal fea-
tures after each reﬁnement in Figure 4. The ﬁrst column cor-
responds to input images marked with the detected pedestri-
ans; The second, third and fourth columns correspond to seg-
mentation masks predicted after 1 to 3 loops. The ﬁrst and
third lines (resp. second and fourth) are for visible (res. ther-
mal) images and their corresponding segmentation masks. It
can be observed that the quality and similarity of the masks
gradually increase with the number of loops. With the in-
crease of similarity between the spectral features, their con-
sistency increases and their complementarity decreases. As
mentioned in Section 1, the lack of consistency between the
multispectral features is harmful; on the contrary, too much
consistency leads to sharp emerge/plunge in the feature val-
ues, and makes the fusion meaningless. That explains why
the Miss Rate starts to decrease after 4 loops. In practice the
number of loops should be tuned for any dataset but we be-
lieve that very few values should be tried (between 2 and 5).

Table 1. Detection accuracy comparisons in terms of Miss
Rate percentage on KAIST Dataset [1]. Our competitors’ re-
sults are taken from [5] and [7].

Methods
Baseline
CFRM 3

mAP

Bicycle Car

Person
71.17% 56.39% 83.90% 73.28%
72.39% 57.77% 84.91% 74.49%

Table 2. mAP results for two CNN object detection archi-
tectures which use (or not) our cyclic fuse-and-reﬁne module
(CFRM) on FLIR dataset [2].

Methods Miss Rate DICE Scores
Baseline
CFRM 1
CFRM 2
CFRM 3
CFRM 4

-
{64.53%}
{78.89%, 89.70%}
{74.60%, 90.60%, 94.17%}
{58.25%, 85.91%, 92.9%, 96.11%}

7.68%
6.90%
6.40%
6.13%
7.09%

Table 3. Miss rates versus DICE scores w.r.t. different num-
bers of Fuse-and-Reﬁne loops. Each experiment is repeated
ﬁve times and we report the average performance.

5. CONCLUSION

This paper proposes a novel cycle fuse-and-reﬁne module to
improve the multispectral feature fusion while taking into ac-
count the complementary/consistency balance of the features.
Experiments on KAIST [1] and FLIR [2] datasets show that
integrating the proposed fusion module to a “vanilla” multi-
spectral pedestrian detector leads to substantial accuracy im-
provements. Several visible/thermal image pairs have a mis-
alignment problem in FLIR dataset. This problem could be
more serious in real world applications due to calibration er-
rors or temporal shifts. A Region Feature Alignment (RFA)
module was proposed by [20] to tackle such cross-modality
disparity problem in a supervised manner and in a two-stage
object detection setting. In the future, we would like to ex-
plore a more general solution to this problem with a similar
cyclic-align scheme.

input	imagesfirst	refinethird	refinesecond	refine[11] Yang Zheng, Izzat H. Izzat, and Shahrzad Ziaee, “GFD-
SSD: gated fusion double SSD for multispectral pedes-
trian detection,” CoRR, vol. abs/1903.06999, 2019.

[12] John Edison Arevalo Ovalle, Thamar Solorio,
Manuel Montes y G´omez, and Fabio A. Gonz´alez,
“Gated multimodal units
information fusion,”
CoRR, vol. abs/1702.01992, 2017.

for

[13] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.
Berg, “Ssd: Single shot multibox detector,” in Proceed-
ings of the European Conference on Computer Vision
(ECCV), 2016.

[14] Zuoxin Li and Fuqiang Zhou,

sion single shot multibox detector,”
abs/1712.00960, 2017.

“FSSD: feature fu-
CoRR, vol.

[15] K. Simonyan and A. Zisserman, “Very deep convolu-
tional networks for large-scale image recognition,” in
International Conference on Learning Representations,
2015.

[16] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaim-
ing He, “Is faster r-cnn doing well for pedestrian detec-
tion?,” arXiv:1607.07032, 2016.

[17] Garrick Brazil, Xi Yin, and Xiaoming Liu, “Illuminat-
ing pedestrians via simultaneous detection & segmenta-
tion,” in Proceedings of the IEEE International Confer-
ence on Computer Vision, Venice, Italy, 2017.

[18] Daniel K¨onig, Michael Adam, Christian Jarvers, Georg
Layher, Heiko Neumann, and Michael Teutsch, “Fully
convolutional region proposal networks for multispec-
in 2017 IEEE Conference on
tral person detection,”
Computer Vision and Pattern Recognition Workshops,
CVPR Workshops 2017, Honolulu, HI, USA, July 21-26,
2017, 2017, pp. 243–250.

[19] Lee R. Dice, “Measures of the amount of ecologic as-
sociation between species,” Ecology, vol. 26, no. 3, pp.
297–302, 1945.

[20] Lu Zhang, Zhiyong Liu, Xiangyu Chen, and Xu Yang,
“The cross-modality disparity problem in multispec-
tral pedestrian detection,” CoRR, vol. abs/1901.02645,
2019.

6. REFERENCES

[1] Soonmin Hwang, Jaesik Park, Namil Kim, Yukyung
Choi, and In So Kweon, “Multispectral pedestrian de-
tection: Benchmark dataset and baselines,” in Proceed-
ings of IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2015.

[2] “Free ﬂir
ing,”
adas-dataset-form/.

thermal dataset

algorithm train-
for
https://www.flir.com/oem/adas/

[3] J¨org Wagner, Volker Fischer, Michael Herman, and
Sven Behnke, “Multispectral pedestrian detection us-
ing deep fusion convolutional neural networks,”
in
24th European Symposium on Artiﬁcial Neural Net-
works, ESANN 2016, Bruges, Belgium, April 27-29,
2016, 2016.

[4] Jingjing Liu, Shaoting Zhang, Shu Wang, and Dim-
itris N. Metaxas, “Multispectral deep neural networks
for pedestrian detection,” in Proceedings of the British
Machine Vision Conference 2016, BMVC 2016, York,
UK, September 19-22, 2016, 2016.

[5] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang,
“Illumination-aware faster R-CNN for robust multispec-
tral pedestrian detection,” Pattern Recognition, vol. 85,
pp. 161–171, 2019.

[6] Dayan Guan, Yanpeng Cao, Jiangxin Yang, Yanlong
Cao, and Michael Ying Yang, “Fusion of multispectral
data through illumination-aware deep neural networks
for pedestrian detection,” Information Fusion, vol. 50,
pp. 148–157, 2019.

[7] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang,
“Multispectral pedestrian detection via simultaneous de-
tection and segmentation,” in British Machine Vision
Conference 2018, BMVC 2018, Northumbria Univer-
sity, Newcastle, UK, September 3-6, 2018, 2018, p. 225.

[8] Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang,
Hong Qiao, Kaizhu Huang, and Amir Hussain, “Cross-
modality interactive attention network for multispectral
pedestrian detection,” Information Fusion, vol. 50, pp.
20–29, 2019.

[9] Lu Zhang, Xiangyu Zhu, Xiangyu Chen, Xu Yang,
Zhen Lei, and Zhiyong Liu,
“Weakly aligned cross-
modal learning for multispectral pedestrian detection,”
in The IEEE International Conference on Computer Vi-
sion (ICCV), October 2019.

[10] Min Lin, Qiang Chen, and Shuicheng Yan, “Network in

network,” arXiv:1312.4400, 2013.

