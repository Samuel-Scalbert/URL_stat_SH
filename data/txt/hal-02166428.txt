Trade-offs in Large-Scale Distributed Tuplewise
Estimation and Learning
Robin Vogel, Aurélien Bellet, Stéphan Clémençon, Ons Jelassi, Guillaume

Papa

To cite this version:

Robin Vogel, Aurélien Bellet, Stéphan Clémençon, Ons Jelassi, Guillaume Papa. Trade-offs in Large-
Scale Distributed Tuplewise Estimation and Learning. ECML PKDD 2019 - European Conference
on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, Sep 2019,
Würzburg, Germany. ￿hal-02166428￿

HAL Id: hal-02166428

https://inria.hal.science/hal-02166428

Submitted on 26 Jun 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Trade-oﬀs in Large-Scale Distributed Tuplewise
Estimation and Learning

Robin Vogel1,2 ((cid:0)), Aur´elien Bellet3, Stephan Cl´emen¸con1, Ons Jelassi1, and
Guillaume Papa1

1 Telecom Paris, LTCI, Institut Polytechnique de Paris
first.last@telecom-paris.fr
2 IDEMIA, France
first.last@idemia.com
3 INRIA, France
first.last@inria.fr

Abstract. The development of cluster computing frameworks has al-
lowed practitioners to scale out various statistical estimation and ma-
chine learning algorithms with minimal programming eﬀort. This is es-
pecially true for machine learning problems whose objective function
is nicely separable across individual data points, such as classiﬁcation
and regression. In contrast, statistical learning tasks involving pairs (or
more generally tuples) of data points — such as metric learning, clus-
tering or ranking — do not lend themselves as easily to data-parallelism
and in-memory computing. In this paper, we investigate how to bal-
ance between statistical performance and computational eﬃciency in
such distributed tuplewise statistical problems. We ﬁrst propose a sim-
ple strategy based on occasionally repartitioning data across workers
between parallel computation stages, where the number of repartition-
ing steps rules the trade-oﬀ between accuracy and runtime. We then
present some theoretical results highlighting the beneﬁts brought by the
proposed method in terms of variance reduction, and extend our results
to design distributed stochastic gradient descent algorithms for tuplewise
empirical risk minimization. Our results are supported by numerical ex-
periments in pairwise statistical estimation and learning on synthetic and
real-world datasets.

Keywords: Distributed Machine Learning · Distributed Data Process-
ing · U -Statistics · Stochastic Gradient Descent · AUC Optimization

1

Introduction

Statistical machine learning has seen dramatic development over the last decades.
The availability of massive datasets combined with the increasing need to per-
form predictive/inference/optimization tasks in a wide variety of domains has
given a considerable boost to the ﬁeld and led to successful applications. In
parallel, there has been an ongoing technological progress in the architecture of
data repositories and distributed systems, allowing to process ever larger (and

2

R. Vogel et al.

possibly complex, high-dimensional) data sets gathered on distributed storage
platforms. This trend is illustrated by the development of many easy-to-use
cluster computing frameworks for large-scale distributed data processing. These
frameworks implement the data-parallel setting, in which data points are par-
titioned across diﬀerent machines which operate on their partition in parallel.
Some striking examples are Apache Spark [26] and Petuum [25], the latter being
fully targeted to machine learning. The goal of such frameworks is to abstract
away the network and communication aspects in order to ease the deployment
of distributed algorithms on large computing clusters and on the cloud, at the
cost of some restrictions in the types of operations and parallelism that can
be eﬃciently achieved. However, these limitations as well as those arising from
network latencies or the nature of certain memory-intensive operations are often
ignored or incorporated in a stylized manner in the mathematical description and
analysis of statistical learning algorithms (see e.g., [2, 15, 4, 1]). The implementa-
tion of statistical methods proved to be theoretically sound may thus be hardly
feasible in a practical distributed system, and seemingly minor adjustments to
scale-up these procedures can turn out to be disastrous in terms of statistical
performance, see e.g. the discussion in [18]. This greatly restricts their practi-
cal interest in some applications and urges the statistics and machine learning
communities to get involved with distributed computation more deeply [3].

In this paper, we propose to study these issues in the context of tuplewise
estimation and learning problems, where the statistical quantities of interest
are not basic sample means but come in the form of averages over all pairs (or
more generally, d-tuples) of data points. Such data functionals are known as
U -statistics [19, 21], and many empirical quantities describing global properties
of a probability distribution fall in this category (e.g., the sample variance, the
Gini mean diﬀerence, Kendall’s tau coeﬃcient). U -statistics are also natural
empirical risk measures in several learning problems such as ranking [13], metric
learning [24], cluster analysis [11] and risk assessment [5]. The behavior of these
statistics is well-understood and a sound theory for empirical risk minimization
based on U -statistics is now documented in the machine learning literature [13],
but the computation of a U -statistic poses a serious scalability challenge as
it involves a summation over an exploding number of pairs (or d-tuples) as
the dataset grows in size. In the centralized (single machine) setting, this can
be addressed by appropriate subsampling methods, which have been shown to
achieve a nearly optimal balance between computational cost and statistical
accuracy [12]. Unfortunately, naive implementations in the case of a massive
distributed dataset either greatly damage the accuracy or are ineﬃcient due to
a lot of network communication (or disk I/O). This is due to the fact that, unlike
basic sample means, a U -statistic is not separable across the data partitions.

Our main contribution is to design and analyze distributed methods for sta-
tistical estimation and learning with U -statistics that guarantee a good trade-oﬀ
between accuracy and scalability. Our approach incorporates an occasional data
repartitioning step between parallel computing stages in order to circumvent the
limitations induced by data partitioning over the cluster nodes. The number of

Trade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

3

repartitioning steps allows to trade-oﬀ between statistical accuracy and compu-
tational eﬃciency. To shed light on this phenomenon, we ﬁrst study the setting
of statistical estimation, precisely quantifying the variance of estimates corre-
sponding to several strategies. Thanks to the use of Hoeﬀding’s decomposition
[17], our analysis reveals the role played by each component of the variance in
the eﬀect of repartitioning. We then discuss the extension of these results to
statistical learning and design eﬃcient and scalable stochastic gradient descent
algorithms for distributed empirical risk minimization. Finally, we carry out some
numerical experiments on pairwise estimation and learning tasks on synthetic
and real-world datasets to support our results from an empirical perspective.

The paper is structured as follows. Section 2 reviews background on U -
statistics and their use in statistical estimation and learning, and discuss the
common practices in distributed data processing. Section 3 deals with statisti-
cal tuplewise estimation: we introduce our general approach for the distributed
setting and derive (non-)asymptotic results describing its accuracy. Section 4
extends our approach to statistical tuplewise learning. We provide experiments
supporting our results in Section 5, and we conclude in Section 6. Proofs, tech-
nical details and additional results can be found in the supplementary material.

2 Background

In this section, we ﬁrst review the deﬁnition and properties of U -statistics, and
discuss some popular applications in statistical estimation and learning. We then
discuss the recent randomized methods designed to scale up tuplewise statistical
inference to large datasets stored on a single machine. Finally, we describe the
main features of cluster computing frameworks.

2.1 U -Statistics: Deﬁnition and Applications

U -statistics are the natural generalization of i.i.d. sample means to tuples of
points. We state the deﬁnition of U -statistics in their generalized form, where
points can come from K ≥ 1 independent samples. Note that we recover classic
sample mean statistics in the case where K = d1 = 1.

Deﬁnition 1. (Generalized U -statistic) Let K ≥ 1 and (d1,
. . . , dK) ∈
N∗K. For each k ∈ {1, . . . , K}, let X{1, ..., nk} = (X (k)
1 , . . . , X (k)
nk ) be an inde-
pendent sample of size nk ≥ dk composed of i.i.d. random variables with values in
some measurable space Xk with distribution Fk(dx). Let h : X d1
K → R
be a measurable function, square integrable with respect to the probability dis-
tribution µ = F ⊗d1
. . . , x(K)) is
symmetric within each block of arguments x(k) (valued in X dk
k ). The generalized
(or K-sample) U -statistic of degrees (d1, . . . , dK) with kernel H is deﬁned as

K . Assume w.l.o.g. that h(x(1),

1 ⊗ · · · ⊗ F ⊗dK

1 ×· · ·×X dK

Un(h) =

1

(cid:81)K

k=1

(cid:1)

(cid:0)nk
dk

(cid:88)

. . .

(cid:88)

I1

IK

h(X(1)
I1

, X(2)
I2

, . . . , X(K)
IK

),

(1)

4

R. Vogel et al.

denotes the sum over all (cid:0)nk
where (cid:80)
. . . , X (k)
)
idk
dk
related to a set Ik of dk indexes 1 ≤ i1 < . . . < idk ≤ nk and n = (n1, . . . , nK).

(cid:1) subsets X(k)
Ik

= (X (k)
i1

Ik

,

1

, . . . , X (K)

, . . . , X (K)
dK

1 , . . . , X (1)
d1

The U -statistic Un(h) is known to have minimum variance among all unbiased es-
)(cid:3).
timators of the parameter µ(h) = E(cid:2)h(X (1)
The price to pay for this low variance is a complex dependence structure exhib-
ited by the terms involved in the average (1), as each data point appears in
multiple tuples. The (non)asymptotic behavior of U -statistics and U -processes
(i.e., collections of U -statistics indexed by classes of kernels) can be investigated
by means of linearization techniques [17] combined with decoupling methods
[21], reducing somehow their analysis to that of basic i.i.d. averages or empiri-
cal processes. One may refer to [19] for an account of the asymptotic theory of
U -statistics, and to [23] (Chapter 12 therein) and [21] for nonasymptotic results.
U -statistics are commonly used as point estimators for inferring certain global
properties of a probability distribution as well as in statistical hypothesis testing.
Popular examples include the (debiased) sample variance, obtained by setting
K = 1, d1 = 2 and h(x1, x2) = (x1 − x2)2, the Gini mean diﬀerence, where
K = 1, d1 = 2 and h(x1, x2) = |x1 − x2|, and Kendall’s tau rank correlation,
where K = 2, d1 = d2 = 1 and h((x1, y1), (x2, y1)) = I{(x1 − x2) · (y1 − y2) > 0}.
U -statistics also correspond to empirical risk measures in statistical learning
problems such as clustering [11], metric learning [24] and multipartite ranking
[14]. The generalization ability of minimizers of such criteria over a class H of
kernels can be derived from probabilistic upper bounds for the maximal deviation
of collections of centered U -statistics under appropriate complexity conditions
on H (e.g., ﬁnite VC dimension) [13, 12]. Below, we describe the example of
multipartite ranking used in our numerical experiments (Section 5). We refer to
[12] for details on more learning problems involving U -statistics.

Example 2 (Multipartite Ranking). Consider items described by a random vec-
tor of features X ∈ X with associated ordinal labels Y ∈ {1, . . . , K}, where
K ≥ 2. The goal of multipartite ranking is to learn to rank items in the same
preorder as that deﬁned by the labels, based on a training set of labeled ex-
amples. Rankings are generally deﬁned through a scoring function s : X → R
transporting the natural order on the real line onto X . Given K independent
samples, the empirical ranking performance of s(x) is evaluated by means of the
empirical VUS (Volume Under the ROC Surface) criterion [14]:
n1(cid:88)

nK(cid:88)

. . .

I{s(X (1)
i1

) < . . . < s(X (K)
iK

)},

(cid:91)V U S(s) =

(2)

1
(cid:81)K
k=1 nk

i1=1

iK =1

which is a K-sample U -statistic of degree (1, . . . , 1) with kernel hs(x1, . . . , xK) =
I{s(x1) < . . . < s(xK)}.

2.2 Large-Scale Tuplewise Inference with Incomplete U -statistics

The cost related to the computation of the U -statistic (1) rapidly explodes as
the sizes of the samples increase. Precisely, the number of terms involved in the

Trade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

5

(cid:1)×· · ·×(cid:0)nK
dK

summation is (cid:0)n1
(cid:1), which is of order O(nd1+...+dK ) when the nk’s are
d1
all asymptotically equivalent. Whereas computing U -statistics based on subsam-
ples of smaller size would severely increase the variance of the estimation, the
notion of incomplete generalized U -statistic [6] enables to signiﬁcantly mitigate
this computational problem while maintaining a good level of accuracy.

Deﬁnition 3. (Incomplete generalized U -statistic) Let B ≥ 1. The
incomplete version of the U -statistic (1) based on B terms is deﬁned by:

(cid:101)UB(H) =

1
B

(cid:88)

h(X(1)
I1

, . . . , X(K)
IK

)

(3)

I=(I1, ..., IK )∈DB

where DB is a set of cardinality B built by sampling uniformly with replacement
in the set Λ of vectors of tuples ((i(1)
1 , . . . , i(1)
)), where
d1
1 ≤ i(k)

, . . . , i(K)
dK

≤ nk and 1 ≤ k ≤ K.

), . . . , (i(K)

1 < . . . < i(k)
dk

1

Note incidentally that the subsets of indices can be selected by means of other
sampling schemes [12], but sampling with replacement is often preferred due
to its simplicity. In practice, the parameter B should be picked much smaller
than the total number of tuples to reduce the computational cost. Like (1), the
quantity (3) is an unbiased estimator of µ(H) but its variance is naturally larger:

Var( (cid:101)UB(h)) =

(cid:16)

1 −

(cid:17)

1
B

Var(Un(h)) +

1
B

Var(h(X (1)

1 , . . . , X (K)
dK

)).

(4)

√

The recent work in [12] has shown that the maximal deviations between (1)
and (3) over a class of kernels H of controlled complexity decrease at a rate
of order O(1/
B) as B increases. An important consequence of this result is
that sampling B = O(n) terms is suﬃcient to preserve the learning rate of
order OP((cid:112)log n/n) of the minimizer of the complete risk (1), whose computa-
tion requires to average O(nd1+...+dK ) terms. In contrast, the distribution of a
complete U -statistic built from subsamples of reduced sizes n(cid:48)
k drawn uniformly
at random is quite diﬀerent from that of an incomplete U -statistic based on
(cid:1) terms sampled with replacement in Λ, although they involve
B = (cid:81)K
the summation of the same number of terms. Empirical minimizers of such a
complete U -statistic based on subsamples achieve a much slower learning rate of
OP((cid:112)log(n)/n1/(d1+...+dK )). We refer to [12] for details and additional results.
We have seen that approximating complete U -statistics by incomplete ones
is a theoretically and practically sound approach to tackle large-scale tuplewise
estimation and learning problems. However, as we shall see later, the implemen-
tation is far from straightforward when data is stored and processed in standard
distributed computing frameworks, whose key features are recalled below.

(cid:0)n(cid:48)
k
dk

k=1

2.3 Practices in Distributed Data Processing

Data-parallelism, i.e. partitioning the data across diﬀerent machines which op-
erate in parallel, is a natural approach to store and eﬃciently process massive

6

R. Vogel et al.

datasets. This strategy is especially appealing when the key stages of the com-
putation to be executed can be run in parallel on each partition of the data. As a
matter of fact, many estimation and learning problems can be reduced to (a se-
quence of) local computations on each machine followed by a simple aggregation
step. This is the case of gradient descent-based algorithms applied to standard
empirical risk minimization problems, as the objective function is nicely sepa-
rable across individual data points. Optimization algorithms operating in the
data-parallel setting have indeed been largely investigated in the machine learn-
ing community, see [3, 8, 1, 22] and references therein for some recent work.

Because of the prevalence of data-parallel applications in large-scale machine
learning, data analytics and other ﬁelds, the past few years have seen a sustained
development of distributed data processing frameworks designed to facilitate the
implementation and the deployment on computing clusters. Besides the semi-
nal MapReduce framework [16], which is not suitable for iterative computations
on the same data, one can mention Apache Spark [26], Apache Flink [10] and
the machine learning-oriented Petuum [25]. In these frameworks, the data is
typically ﬁrst read from a distributed ﬁle system (such as HDFS, Hadoop Dis-
tributed File System) and partitioned across the memory of each machine in
the form of an appropriate distributed data structure. The user can then easily
specify a sequence of distributed computations to be performed on this data
structure (map, ﬁlter, reduce, etc.) through a simple API which hides low-level
distributed primitives (such as message passing between machines). Importantly,
these frameworks natively implement fault-tolerance (allowing eﬃcient recovery
from node failures) in a way that is also completely transparent to the user.

While such distributed data processing frameworks come with a lot of beneﬁts
for the user, they also restrict the type of computations that can be performed
eﬃciently on the data. In the rest of this paper, we investigate these limitations in
the context of tuplewise estimation and learning problems, and propose solutions
to achieve a good trade-oﬀ between accuracy and scalability.

3 Distributed Tuplewise Statistical Estimation

In this section, we focus on the problem of tuplewise statistical estimation in the
distributed setting (an extension to statistical learning is presented in Section 4).
We consider a set of N ≥ 1 workers in a complete network graph (i.e., any pair
of workers can exchange messages). For convenience, we assume the presence of
a master node, which can be one of the workers and whose role is to aggregate
estimates computed by all workers.

For ease of presentation, we restrict our attention to the case of two sample
U -statistics of degree (1, 1) (K = 2 and d1 = d2 = 1), see Remark 7 in Section 3.3
for extensions to the general case. We denote by Dn = {X1, . . . , Xn} the ﬁrst
sample and by Qm = {Z1, . . . , Zm} the second sample (of sizes n and m respec-
tively). These samples are distributed across the N workers. For i ∈ {1, . . . , N },
we denote by Ri the subset of data points held by worker i and, unless otherwise
N ∈ N.
noted, we assume for simplicity that all subsets are of equal size |Ri| = n+m

Trade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

7

i and RZ

The notations RX
i respectively denote the subset of data points held
by worker i from Dn and Qm, with RX
i = Ri. We denote their (possibly
random) cardinality by ni = |RX
i | and mi = |RZ
i |. Given a kernel h, the goal is
to compute a good estimate of the parameter U (h) = E[h(X1, Z1)] while meeting
some computational and communication constraints.

i ∪ RZ

3.1 Naive Strategies

Before presenting our approach, we start by introducing two simple (but ineﬀec-
tive) strategies to compute an estimate of U (h). The ﬁrst one is to compute the
complete two-sample U -statistic associated with the full samples Dn and Qm:

Un(h) =

1
nm

n
(cid:88)

m
(cid:88)

k=1

l=1

h(Xk, Zl),

(5)

with n = (n, m). While Un(h) has the lowest variance among all unbiased esti-
mates that can be computed from (Dn, Qm), computing it is a highly undesirable
solution in the distributed setting where each worker only has access to a subset
of the dataset. Indeed, ensuring that each possible pair is seen by at least one
worker would require massive data communication over the network. Note that a
similar limitation holds for incomplete versions of (5) as deﬁned in Deﬁnition 3.
A feasible strategy to go around this problem is for each worker to compute
the complete U -statistic associated with its local subsample Ri, and to send it
to the master node who averages all contributions. This leads to the estimate

Un,N (h) =

1
N

N
(cid:88)

i=1

URi(h) where URi(h) =

1
nimi

(cid:88)

(cid:88)

k∈RX
i

l∈RZ
i

h(Xk, Zl).

(6)

Note that if min(ni, mi) = 0, we simply set URi(h) = 0.

Alternatively, as the Ri’s may be large, each worker can compute an incom-
plete U -statistic (cid:101)UB,Ri(h) with B terms instead of URi, leading to the estimate

(cid:101)Un,N,B(h) =

1
N

N
(cid:88)

i=1

(cid:101)UB,Ri(h) where (cid:101)UB,Ri(h) =

1
B

(cid:88)

(k,l)∈Ri,B

h(Xk, Zl),

(7)

with Ri,B a set of B pairs built by sampling uniformly with replacement from
the local subsample RX

i × RZ
i .

As shown in Section 3.3, strategies (6) and (7) have the undesirable prop-
erty that their accuracy decreases as the number of workers N increases. This
motivates our proposed approach, introduced in the following section.

3.2 Proposed Approach

The naive strategies presented above are either accurate but very expensive (re-
quiring a lot of communication across the network), or scalable but potentially

8

R. Vogel et al.

Fig. 1. Graphical summary of the statistics that we compare: with/without repartition
and with/without subsampling. Note that {(σt, πt)}T
t=1 denotes a set of T independent
couples of random permutations in Sn × Sm.

inaccurate. The approach we promote here is of disarming simplicity and aims
at ﬁnding a sweet spot between these two extremes. The idea is based on reparti-
tioning the dataset a few times across workers (we keep the repartitioning scheme
abstract for now and postpone the discussion of concrete choices to subsequent
sections). By alternating between parallel computation and repartitioning steps,
one considers several estimates based on the same data points. This allows to
observe a greater diversity of pairs and thereby reﬁne the quality of our ﬁnal
estimate, at the cost of some additional communication.

Formally, let T be the number of repartitioning steps. We denote by Rt
i
(h) the
i. At each step t ∈ {1, . . . , T }, each worker
(h) and sends it to the master node. After T steps, the master

the subsample of worker i after the t-th repartitioning step, and by URt
complete U -statistic associated with Rt
i computes URt
node has access to the following estimate:

i

i

(cid:98)Un,N,T (h) =

1
T

T
(cid:88)

t=1

U t

n,N (h),

(8)

n,N (h) = 1
where U t
N
compute incomplete U -statistics (cid:101)UB,Rt

i=1 URt

i

(cid:80)N

i

(h). Similarly as before, workers may alternatively

(h) with B terms. The estimate is then:

(cid:101)Un,N,B,T (h) =

1
T

T
(cid:88)

t=1

(cid:101)U t

n,N,B(h),

(9)

where (cid:101)U t
Section 3.1 which do not rely on repartition, are summarized in Figure 1.

(h). These statistics, and those introduced in

n,N,B(h) = 1
N

i=1 (cid:101)UB,Rt

i

(cid:80)N

Of course, the repartitioning operation is rather costly in terms of runtime
so T should be kept to a reasonably small value. We illustrate this trade-oﬀ by
the analysis presented in the next section.

3.3 Analysis

In this section, we analyze the statistical properties of the various estimators in-
troduced above. We focus here on repartitioning by proportional sampling with-

Trade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

9

out replacement (prop-SWOR). Prop-SWOR creates partitions that contain the
same proportion of elements of each sample: speciﬁcally, it ensures that at any
| = m
step t and for any worker i, |Rt
N .
We discuss the practical implementation of this repartitioning scheme as well as
some alternative choices in Section 3.4.

N with |Rt,X

N and |Rt,Z

i| = n+m

| = n

i

i

All estimators are unbiased when repartitioning is done with prop-SWOR.
We will thus compare their variance. Our main technical tool is a linearization
technique for U -statistics known as Hoeﬀding’s Decomposition (see [17, 13, 12]).

Deﬁnition 4. (Hoeffding’s decomposition) Let h1(x) = E[h(x, Z1)], h2(z) =
E[h(X1, z)] and h0(x, z) = h(x, z) − h1(x) − h2(z) + U (h). Un(h) − U (h) can be
written as a sum of three orthogonal terms:

Un(h) − U (h) = Tn(h) + Tm(h) + Wn(h),

(cid:80)n

where Tn(h) = 1
n
sums of independent r.v, while Wn(h) = 1
nm
erate U -statistic (i.e., E[h(X1, Z1)|X1] = U (h) and E[h(X1, Z1)|Z1] = U (h)).

k=1 h1(Xk) − U (h) and Tm(h) = 1
l=1 h2(Zl) − U (h) are
m
(cid:80)m
l=1 h0(Xk, Zl) is a degen-

(cid:80)n

k=1

(cid:80)n

This decomposition is very convenient as the two terms Tn(h) and Tm(h) are
decorrelated and the analysis of Wn(h) (a degenerate U -statistic) is well docu-
mented [17, 13, 12]. It will allow us to decompose the variance of the estimators of
interest into single-sample components σ2
2 = Var(h2(Z))
on the one hand, and a pairwise component σ2
0 = Var(h0(X1, Z1)) on the other
1 + σ2
hand. Denoting σ2 = Var(h(X1, Z1)), we have σ2 = σ2
2.

1 = Var(h1(X)) and σ2

0 + σ2

It is well-known that the variance of the complete U -statistic Un(h) can be
written as Var(Un(h)) = σ2
nm (see supplementary material for details).
Our ﬁrst result gives the variance of the estimators which do not rely on a
repartitioning of the data with respect to the variance of Un(h).

m + σ2

n + σ2

1

2

0

Theorem 5. If the data is distributed over workers using prop-SWOR, we have:

Var(Un,N (h)) = Var(Un(h)) + (N − 1)

Var( (cid:101)Un,N,B(h)) =

1 −

(cid:18)

(cid:19)

1
B

Var(Un,N (h)) +

σ2
N B

.

σ2
0
nm

,

Theorem 5 precisely quantiﬁes the excess variance due to the distributed set-
ting if one does not use repartitioning. Two important observations are in order.
First, the variance increase is proportional to the number of workers N , which
clearly defeats the purpose of distributed processing. Second, this increase only
depends on the pairwise component σ2
0 of the variance. In other words, the av-
erage of U -statistics computed independently over the local partitions contains
all the information useful to estimate the single-sample contributions, but fails
to accurately estimate the pairwise contributions. The resulting estimates thus
lead to signiﬁcantly larger variance when the choice of kernel and the data dis-
tributions imply that σ2
1. The extreme case

0 is large compared to σ1

2 and/or σ2

10

R. Vogel et al.

Fig. 2. Theoretical variance as a function of the number of evaluated pairs for diﬀerent
estimators under prop-SWOR, with n = 100, 000, m = 200 and N = 100.

happens when Un(h) is a degenerate U -statistic, i.e. σ2
0 > 0,
which is veriﬁed for example when h(x, z) = x · z and X, Z are both centered
random variables.

2 = 0 and σ2

1 = σ2

We now characterize the variance of the estimators that leverage data repar-

titioning steps.

Theorem 6. If the data is distributed and repartitioned between workers using
prop-SWOR, we have:

Var( (cid:98)Un,N,T (h)) = Var(Un(h)) + (N − 1)

σ2
0
nmT

,

Var( (cid:101)Un,N,B,T (h)) = Var( (cid:98)Un,N,T (h)) −

1
T B

Var(Un,N (h)) +

σ2
N T B

.

Theorem 6 shows that the value of repartitioning arises from the fact that
the term accounting for the pairwise variance in (cid:98)Un,N,T (h) is T times lower than
that of Un,N (h). This validates the fact that repartitioning is beneﬁcial when the
pairwise variance term is signiﬁcant in front of the other terms. Interestingly,
Theorem 6 also implies that for a ﬁxed budget of evaluated pairs, using all pairs
on each worker is always a dominant strategy over using incomplete approxi-
mations. Speciﬁcally, we can show that under the constraint N BT = nmT0/N ,
Var( (cid:98)Un,N,T0(h)) is always smaller than Var( (cid:101)Un,N,B,T (h)), see supplementary ma-
terial for details. Note that computing complete U -statistics also require fewer
repartitioning steps to evaluate the same number of pairs (i.e., T0 ≤ T ).

We conclude the analysis with a visual illustration of the variance of various
estimators with respect to the number of pairs they evaluate. We consider the im-
balanced setting where n (cid:29) m, which is commonly encountered in applications
such as imbalanced classiﬁcation, bipartite ranking and anomaly detection. In
this case, it suﬃces that σ2
2 be small for the inﬂuence of the pairwise component
of the variance to be signiﬁcant, see Fig. 2 (left). The ﬁgure also conﬁrms that
complete estimators dominate their incomplete counterparts. On the other hand,
when σ2
2 is not small, the variance of Un mostly originates from the rarity of the
minority sample, hence repartitioning does not provide estimates that are signif-
icantly more accurate (see Fig. 2, right). We refer to Section 5 for experiments
on concrete tasks with synthetic and real data.

Trade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

11

Remark 7 (Extension to high-order U -statistics). The extension of our analysis
to general U -statistics is straightforward and left to the reader (see [12] for
a review of the relevant technical tools). We stress the fact that the beneﬁts
of repartitioning are even stronger for higher-order U -statistics (K > 2 and/or
larger degrees) because higher-order components of the variance are also aﬀected.

3.4 Practical Considerations and Other Repartitioning Schemes

The analysis above assumes that repartitioning is done using prop-SWOR, which
has the advantage of exactly preserving the proportion of points from the two
samples Dn and Qm even in the event of signiﬁcant imbalance in their size.
However, a naive implementation of prop-SWOR requires some coordination be-
tween workers at each repartitioning step. To avoid exchanging many messages,
we propose that the workers agree at the beginning of the protocol on a number-
ing of the workers, a numbering of the points in each sample, and a random seed
to use in a pseudorandom number generator. This allows the workers to imple-
ment prop-SWOR without any further coordination: at each repartitioning step,
they independently draw the same two random permutations over {1, . . . , n} and
{1, . . . , m} using the common random seed and use these permutations to assign
each point to a single worker.

Of course, other repartitioning schemes can be used instead of prop-SWOR.
A natural choice is sampling without replacement (SWOR), which does not re-
quire any coordination between workers. However, the partition sizes generated
by SWOR are random. This is a concern in the case of imbalanced samples,
where the probability that a worker i does not get any point from the minority
sample (and thus no pair to compute a local estimate) is non-negligible. For these
reasons, it is diﬃcult to obtain exact and concise theoretical variances for the
SWOR case, but we show in the supplementary material that the results with
SWOR should not deviate too much from those obtained with prop-SWOR. For
completeness, in the supplementary material we also analyze the case of propor-
tional sampling with replacement (prop-SWR): results are quantitatively similar,
aside from the fact that redistribution also corrects for the loss of information
that occurs because of sampling with replacement.

Finally, we note that deterministic repartitioning schemes may be used in
practice for simplicity. For instance, the repartition method in Apache Spark
relies on a deterministic shuﬄe which preserves the size of the partitions.

4 Extensions to Stochastic Gradient Descent for ERM

The results of Section 3 can be extended to statistical learning in the empir-
ical risk minimization framework. In such problems, given a class of kernels
H, one seeks the minimizer of (6) or (8) depending on whether repartition is
used.4 Under appropriate complexity assumptions on H (e.g., of ﬁnite VC di-
mension), excess risk bounds for such minimizers can be obtained by combining

4 Alternatively, for scalability purposes, one may instead work with their incomplete

counterparts, namely (7) and (9) respectively.

12

R. Vogel et al.

our variance analysis of Section 3 with the control of maximal deviations based
on Bernstein-type concentration inequalities as done in [13, 12]. Due to the lack
of space, we leave the details of such analysis to the readers and focus on the
more practical scenario where the ERM problem is solved by gradient-based
optimization algorithms.

4.1 Gradient-based Empirical Minimization of U -statistics

In the setting of interest, the class of kernels to optimize over is indexed by a
real-valued parameter θ ∈ Rq representing the model. Adapting the notations of
Section 3, the kernel h : X1 × X2 × Rq → R then measures the performance of
a model θ ∈ Rq on a given pair, and is assumed to be convex and smooth in θ.
Empirical Risk Minimization (ERM) aims at ﬁnding θ ∈ Rq minimizing

Un(θ) =

1
nm

n
(cid:88)

m
(cid:88)

k=1

l=1

h(Xk, Zl; θ).

(10)

The minimizer can be found by means of Gradient Descent (GD) techniques.5
Starting at iteration s = 1 from an initial model θ1 ∈ Rq and given a learning
rate γ > 0, GD consists in iterating over the following update:

θs+1 = θs − γ∇θUn(θs).

(11)

Note that the gradient ∇θUn(θ) is itself a U -statistic with kernel given by ∇θH,
and its computation is very expensive in the large-scale setting. In this regime,
Stochastic Gradient Descent (SGD) is a natural alternative to GD which is
known to provide a better trade-oﬀ between the amount of computation and the
performance of the resulting model [7]. Following the discussion of Section 2.2,
a natural idea to implement SGD is to replace the gradient ∇θUn(θ) in (11)
by an unbiased estimate given by an incomplete U -statistic. The work of [20]
shows that SGD converges much faster than if the gradient is estimated using a
complete U -statistic based on subsamples with the same number of terms.

However, as in the case of estimation, the use of standard complete or incom-
plete U -statistics turns out to be impractical in the distributed setting. Building
upon the arguments of Section 3, we propose a more suitable strategy.

4.2 Repartitioning for Stochastic Gradient Descent

The approach we propose is to alternate between SGD steps using within-
partition pairs and repartitioning the data across workers. We introduce a pa-
rameter nr ∈ Z+ corresponding to the number of iterations of SGD between each
redistribution of the data. For notational convenience, we let r(s) := (cid:100)s/nr(cid:101) so
that for any worker i, Rr(s)
denotes its data partition at iteration s ≥ 1 of SGD.

i

5 When H is nonsmooth in θ, a subgradient may be used instead of the gradient.

Trade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

13

Given a local batch size B, at each iteration s of SGD, we propose to adapt
the strategy (9) by having each worker i compute a local gradient estimate using
i,B of B randomly sampled pairs in its current local partition Rr(s)
a set Rs

:

i

∇θ (cid:101)UB,Rr(s)

i

(θs) =

1
B

(cid:88)

(k,l)∈Rs

i,B

∇θh(Xk, Zl; θs).

This local estimate is then sent to the master node who averages all contribu-
tions, leading to the following global gradient estimate:

∇θ (cid:101)Un,N,B(θs) =

1
N

N
(cid:88)

i=1

∇θ (cid:101)UB,Rr(s)

i

(θs).

(12)

The master node then takes a gradient descent step as in (11) and broadcasts
the updated model θs+1 to the workers.

Following our analysis in Section 3, repartitioning the data allows to reduce
the variance of the gradient estimates, which is known to greatly impact the con-
vergence rate of SGD (see e.g. [9], Theorem 6.3 therein). When nr = +∞, data is
never repartitioned and the algorithm minimizes an average of local U -statistics,
leading to suboptimal performance. On the other hand, nr = 1 corresponds to
repartitioning at each iteration of SGD, which minimizes the variance but is very
costly and makes SGD pointless. We expect the sweet spot to lie between these
two extremes: the dominance of (cid:98)Un,N,T over (cid:101)Un,N,B,T established in Section 3.3,
combined with the common use of small batch size B in SGD, suggests that
occasional redistributions are suﬃcient to correct for the loss of information in-
curred by the partitioning of data. We illustrate these trade-oﬀs experimentally
in the next section.

5 Numerical Results

In this section, we illustrate the importance of repartitioning for estimating and
optimizing the Area Under the ROC Curve (AUC) through a series of numeri-
cal experiments. The corresponding U -statistic is the two-sample version of the
multipartite ranking VUS introduced in Example 2 (Section 2.1). The ﬁrst ex-
periment focuses on the estimation setting considered in Section 3. The second
experiment shows that redistributing the data across workers, as proposed in
Section 4, allows for more eﬃcient mini-batch SGD. All experiments use prop-
SWOR and are conducted in a simulated environment.

Estimation experiment. We seek to illustrate the importance of redistribution for
estimating two-sample U -statistics with the concrete example of the AUC. The
AUC is obtained by choosing the kernel h(x, z) = I{z < x}, and is widely used as
a performance measure in bipartite ranking and binary classiﬁcation with class
imbalance. Recall that our results of Section 3.3 highlighted the key role of the
pairwise component of the variance σ2
0 being large compared to the single-sample

14

R. Vogel et al.

Fig. 3. Relative variance estimated over 5000 runs, n = 5000, m = 50, N = 10 and
T = 4. Results are divided by the true variance of Un deduced from (13) and Theorem 5.

Fig. 4. Learning dynamics for diﬀerent repartition frequencies computed over 100 runs.

components. In the case of the AUC, this happens when the data distributions
are such that the expected outcome using single-sample information is far from
the truth, e.g. in the presence of hard pairs. We illustrate this on simple discrete
distributions for which we can compute σ2
2 in closed form. Consider
positive points X ∈ {0, 2}, negative points Z ∈ {−1, +1} and P (X = 2) = q,
P (Z = +1) = p. It follows that:

1 and σ2

0, σ2

1 = p2q(1 − q),
σ2

2 = (1 − q)2p(1 − p), and σ2 = p(1 − p + pq)(1 − q). (13)
σ2

σ2
0
1 +σ2
σ2
2

Assume that the scoring function has a small probability (cid:15) to assign a low score
to a positive instance or a large score to a negative instance. In our formal
setting, this translates into letting p = 1 − q = (cid:15) for a small (cid:15) > 0, which implies
∞. We thus expect that as the true AUC U (h) = 1 − (cid:15)2
that
gets closer to 1, repartitioning the dataset becomes more critical to achieve good
relative precision. This is conﬁrmed numerically, as shown in Fig. 3. Note that
in practice, settings where the AUC is very close to 1 are very common as they
correspond to well-functioning systems, such as face recognition systems.

2(cid:15) →
(cid:15)→0

= 1−(cid:15)

Learning experiment. We now turn to AUC optimization, which is the task of
learning a scoring function s : X → R that optimizes the VUS criterion (2) with
K = 2 in order to discriminate between a negative and a positive class. We learn a

10−310−210−1(cid:15)123Rel.var.UnUn,NUn,N,TTrade-oﬀs in Large-Scale Distributed Tuplewise Estimation and Learning

15

linear scoring function sw,b(x) = w(cid:62)x + b, and optimize a continuous and convex
surrogate of (2) based on the hinge loss. The resulting loss function to minimize
is a two-sample U-statistic with kernel gw,b(x, z) = max(0, 1 + sw,b(x) − sw,b(z))
indexed by the parameters (w, b) of the scoring function, to which we add a small
L2 regularization term of 0.05(cid:107)w(cid:107)2
2.

We use the shuttle dataset, a classic dataset for anomaly detection.6 It con-
tains roughly 49,000 points in dimension 9, among which only 7% (approx. 3,500)
are anomalies. A high accuracy is expected for this dataset. To monitor the gen-
eralization performance, we keep 20% of the data as our test set, corresponding
to 700 points of the minority class and approx. 9,000 points of the majority class.
The test performance is measured with complete statistics over the 6.3 million
pairs. The training set consists of the remaining data points, which we distribute
over N = 100 workers. This leads to approx. 10, 200 pairs per worker. The gra-
dient estimates are calculated following (12) with batch size B = 100. We use
an initial learning rate of 0.01 with a momentum of 0.9. As there are more than
100 million possible pairs in the training dataset, we monitor the training loss
and accuracy on a ﬁxed subset of 4.5 × 105 randomly sampled pairs.

Fig. 4 shows the evolution of the continuous loss and the true AUC on the
training and test sets along the iteration for diﬀerent values of nr, from nr =
1 (repartition at each iteration) to nr = +∞ (no repartition). The lines are
the median at each iteration over 100 runs, and the shaded area correspond
to conﬁdence intervals for the AUC and loss value of the testing dataset. We
can clearly see the beneﬁts of repartition: without it, the median performance
is signiﬁcantly lower and the variance across runs is very large. The results
also show that occasional repartitions (e.g., every 25 iterations) are suﬃcient to
mitigate these issues signiﬁcantly.

6 Future Work

We envision several further research questions on the topic of distributed tuple-
wise learning. We would like to provide a rigorous convergence rate analysis of
the general distributed SGD algorithm introduced in Section 4. This is a chal-
lenging task because each series of iterations executed between two repartition
steps can be seen as optimizing a slightly diﬀerent objective function. It would
also be interesting to investigate settings where the workers hold sensitive data
that they do not want to share in the clear due to privacy concerns.

References

1. Arjevani, Y., Shamir, O.: Communication complexity of distributed convex learn-

ing and optimization. In: NIPS (2015)

2. Balcan, M.F., Blum, A., Fine, S., Mansour, Y.: Distributed Learning, Communi-

cation Complexity and Privacy. In: COLT (2012)

6 http://odds.cs.stonybrook.edu/shuttle-dataset/

16

R. Vogel et al.

3. Bekkerman, R., Bilenko, M., Langford, J.: Scaling Up Machine Learning: Parallel

and Distributed Approaches. Cambridge University Press (2011)

4. Bellet, A., Liang, Y., Garakani, A.B., Balcan, M.F., Sha, F.: A Distributed Frank-
Wolfe Algorithm for Communication-Eﬃcient Sparse Learning. In: SDM (2015)
5. Bertail, P., Tressou, J.: Incomplete generalized U -statistics for food risk assess-

ment. Biometrics 62(1), 66–74 (2006)

6. Blom, G.: Some properties of incomplete U -statistics. Biometrika 63(3), 573–580

(1976)

7. Bottou, L., Bousquet, O.: The Tradeoﬀs of Large Scale Learning. In: NIPS (2007)
8. Boyd, S.P., Parikh, N., Chu, E., Peleato, B., Eckstein, J.: Distributed Optimiza-
tion and Statistical Learning via the Alternating Direction Method of Multipliers.
Foundations and Trends in Machine Learning 3(1), 1–122 (2011)

9. Bubeck, S.: Convex Optimization: Algorithms and Complexity. Foundations and

Trends in Machine Learning 8(3–4), 231–357 (2015)

10. Carbone, P., Katsifodimos, A., Ewen, S., Markl, V., Haridi, S., Tzoumas, K.:
Apache FlinkTM: Stream and Batch Processing in a Single Engine. IEEE Data
Engineering Bulletin 38(4), 28–38 (2015)

11. Cl´emen¸con, S.: A statistical view of clustering performance through the theory of

U-processes. Journal of Multivariate Analysis 124, 42–56 (2014)

12. Cl´emen¸con, S., Bellet, A., Colin, I.: Scaling-up Empirical Risk Minimization: Op-
timization of Incomplete U-statistics. Journal of Machine Learning Research 13,
165–202 (2016)

13. Cl´emen¸con, S., Lugosi, G., Vayatis, N.: Ranking and empirical risk minimization

of U -statistics. The Annals of Statistics 36(2), 844–874 (2008)

14. Cl´emen¸con, S., Robbiano, S.: Building conﬁdence regions for the ROC surface.

Pattern Recognition Letters 46, 67–74 (2014)

15. Daum´e III, H., Phillips, J.M., Saha, A., Venkatasubramanian, S.: Protocols for

Learning Classiﬁers on Distributed Data. In: AISTATS (2012)

16. Dean, J., Ghemawat, S.: Mapreduce: simpliﬁed data processing on large clusters.

Communications of the ACM 51(1), 107–113 (2008)

17. Hoeﬀding, W.: A class of statistics with asymptotically normal distribution. Annals

of Mathematics and Statistics 19, 293–325 (1948)

18. Jordan, M.: On statistics, computation and scalability. Bernoulli 19(4), 1378–1390

(2013)

19. Lee, A.: U -statistics: Theory and practice. Marcel Dekker, Inc., New York (1990)
20. Papa, G., Bellet, A., Cl´emen¸con, S.: SGD Algorithms based on Incomplete U-

statistics: Large-Scale Minimization of Empirical Risk. In: NIPS (2015)

21. de la Pena, V., Gin´e, E.: Decoupling: from Dependence to Independence. Springer

(1999)

22. Smith, V., Forte, S., Ma, C., Tak´ac, M., Jordan, M.I., Jaggi, M.: CoCoA: A Gen-
eral Framework for Communication-Eﬃcient Distributed Optimization. Journal of
Machine Learning Research 18(230), 1–49 (2018)

23. Van Der Vaart, A.: Asymptotic Statistics. Cambridge University Press (2000)
24. Vogel, R., Bellet, A., Cl´emen¸con, S.: A Probabilistic Theory of Supervised Simi-

larity Learning for Pointwise ROC Curve Optimization. In: ICML (2018)

25. Xing, E.P., Ho, Q., Dai, W., Kim, J.K., Wei, J., Lee, S., Zheng, X., Xie, P., Kumar,
A., Yu, Y.: Petuum: A New Platform for Distributed Machine Learning on Big
Data. IEEE Transactions on Big Data 1(2), 49–67 (2015)

26. Zaharia, M., Chowdhury, M., Franklin, M.J., Shenker, S., Stoica, I.: Spark : Cluster

Computing with Working Sets. In: HotCloud (2012)

