A Comparative Study of Gamma Markov Chains for
Temporal Non-Negative Factorization
Louis Filstroff, Olivier Gouvert, Cédric Févotte, Olivier Cappé

To cite this version:

Louis Filstroff, Olivier Gouvert, Cédric Févotte, Olivier Cappé. A Comparative Study of Gamma
Markov Chains for Temporal Non-Negative Factorization. IEEE Transactions on Signal Processing,
2021, ￿10.1109/TSP.2021.3060000￿. ￿hal-02883800v3￿

HAL Id: hal-02883800

https://hal.science/hal-02883800v3

Submitted on 1 Mar 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

A Comparative Study of Gamma Markov
Chains for Temporal Non-Negative
Factorization

Louis Filstroﬀ 1 Olivier Gouvert 2 C´edric F´evotte 3 Olivier Capp´e 4
1 Department of Computer Science, School of Science, Aalto University, Finland

2 Mila - Quebec Artiﬁcial Intelligence Institute

3 IRIT, Universit´e de Toulouse, CNRS, France

4 DI ENS, CNRS, INRIA, Universit´e PSL

February 25, 2021

Non-negative matrix factorization (NMF) has become a well-established
class of methods for the analysis of non-negative data. In particular, a lot of
eﬀort has been devoted to probabilistic NMF, namely estimation or inference
tasks in probabilistic models describing the data, based for example on Pois-
son or exponential likelihoods. When dealing with time series data, several
works have proposed to model the evolution of the activation coeﬃcients as
a non-negative Markov chain, most of the time in relation with the Gamma
distribution, giving rise to so-called temporal NMF models. In this paper,
we review four Gamma Markov chains of the NMF literature, and show that
they all share the same drawback: the absence of a well-deﬁned station-
ary distribution. We then introduce a ﬁfth process, an overlooked model of
the time series literature named BGAR(1), which overcomes this limitation.
These temporal NMF models are then compared in a MAP framework on a
prediction task, in the context of the Poisson likelihood.

Keywords: Non-negative matrix factorization, Time series data, Gamma
Markov chains, MAP estimation

1. Introduction

1.1. Non-negative matrix factorization

Non-negative matrix factorization (NMF) (Paatero and Tapper, 1994; Lee and Seung,
1999) has become a widely used class of methods for analyzing non-negative data. Let us

1

consider N samples in RF
+. We can store these samples column-wise in a matrix, which
we denote by V (therefore of size F × N ). Broadly speaking, NMF aims at ﬁnding an
approximation of V as the product of two non-negative matrices:

V (cid:39) WH,

(1)

where W is of size F × K, and H is of size K × N . W and H are referred to as the
dictionary and the activation matrix, respectively. The factorization rank K is usually
chosen such that K (cid:28) min(F, N ), hence producing a low-rank approximation of V. This
factorization is often retrieved as the solution of an optimization problem, which we can
write as:

min
W≥0, H≥0

D(V|WH),

(2)

where D is a measure of ﬁt between V and its approximation WH, and the notation
A ≥ 0 denotes the non-negativity of the entries of the matrix A. One of the key aspects
to the success of NMF is that the non-negativity of the factors W and H yields an
interpretable, part-based representation of each sample: vn (cid:39) Whn (Lee and Seung,
1999).

Various measures of ﬁt have been considered in the literature, for instance the fam-
ily of β-divergences (F´evotte and Idier, 2011), which includes some of the most popular
cost functions in NMF, such as the squared Euclidian distance, the generalized Kullback-
Leibler divergence, or the Itakura-Saito divergence. As it turns out, for many of these
cost functions, the optimization problem described in Eq. (2) can be shown to be equiv-
alent to the joint maximum likelihood estimation of the factors W and H in a statistical
model, that is:

max
W,H

p(V|W, H).

(3)

This leads the way to so-called probabilistic NMF, i.e., estimation or inference tasks in
probabilistic models whose observation distribution may be written as:

vn ∼ p( . ; Whn, Θ), W ≥ 0, H ≥ 0,

(4)

that is to say that the distribution of vn is parametrized by the dot product of the factors
W and hn. Other potential parameters of the distribution are generically denoted by
Θ. Most of the time these distributions are such that E(vn) = Whn.

This large family encompasses many well-known models of the literature, for example
models based on the Gaussian likelihood (Schmidt et al., 2009) or the exponential likeli-
hood (F´evotte et al., 2009; Hoﬀman et al., 2010). It also includes factorization models for
count data, which are most of the time based on the Poisson distribution1 (Canny, 2004;
Cemgil, 2009; Zhou et al., 2012; Gopalan et al., 2015), but can also make use of distri-
butions with a larger tail, e.g., the negative binomial distribution (Zhou, 2018). Finally,
more complex models using the compound Poisson distribution have been considered

1These models are sometimes generically referred to as “Poisson factorization” or “Poisson factor anal-

ysis”.

2

(S¸im¸sekli et al., 2013; Basbug and Engelhardt, 2016; Gouvert et al., 2019), allowing to
extend the use of the Poisson distribution to various supports (N, R+, R, . . . ).

In the vast majority of the aforementioned works, prior distributions are assumed on
the factors W and H. This is sometimes referred to as Bayesian NMF. In this case, the
columns of H are most of the time assumed to be independent:

p(H) =

N
(cid:89)

n=1

p(hn).

(5)

The factors being non-negative, a standard choice is the Gamma distribution2, which
can be sparsity-inducing if the shape parameter is chosen to be lower than one. The
inverse Gamma distribution has also been considered.

1.2. Temporal structure of the activation coeﬃcients

In this work, we are interested in the analysis of speciﬁc matrices V whose columns
cannot be treated as exchangeable, because the samples vn are correlated. Such a
scenario arises in particular when the columns of V describe the evolution of a process
over time.

From a modeling perspective, this means that correlation should be introduced in
the statistical model between successive columns of V. This can be achieved by lifting
the prior independence assumption of Eq. (5), thus introducing correlation between
successive columns of H. In this paper, we consider a Markov structure on the columns
of H:

p(H) = p(h1)

p(hn|hn−1).

(6)

(cid:89)

n≥2

We will refer to such a model as a dynamical NMF model. Note that recent works
go beyond the Markovian assumption, i.e., assume dependency with multiple past time
steps, and are labeled as “deep” (Gong and Huang, 2017; Guo et al., 2018).

Several works (F´evotte et al., 2013; Schein et al., 2016, 2019) assume that the transition
distribution p(hn|hn−1) makes use of a transition matrix Π of size K × K to capture
relationships between the diﬀerent components.
In this case, the distribution of hkn
depends on a linear combination of all the components at the previous time step:

p(hn|hn−1) =

p(hkn|

(cid:89)

k

(cid:88)

l

πklhl(n−1)).

(7)

In this work, we will restrict ourselves to Π = IK. Equivalently, this amounts to

assuming that the K rows of H are a priori independent, and we have

p(H) =

(cid:89)

k

p(hk1)

(cid:89)

n≥2

p(hkn|hk(n−1)).

(8)

2Throughout the article, we consider the “shape and rate” parametrization of the Gamma distribution,

i.e. Gamma(x|α, β) ∝ xα−1 exp(−βx).

3

We will refer to such a model as a temporal NMF model.

A ﬁrst way of dealing with the temporal evolution of a non-negative variable is to map
it to R+. It is then commonly assumed that this variable evolves in Gaussian noise. This
is for example exploited in the seminal work of Blei and Laﬀerty (2006) on the extension
of latent Dirichlet allocation to allow for topic evolution3. A similar assumption is made
in Charlin et al. (2015), which introduces dynamics in the context of a Poisson likelihood
(factorizing the user-item-time tensor). Gaussian assumptions allow to use well-known
computational techniques, such as Kalman ﬁltering, but result in loss of interpretability.
We will focus in this paper on naturally non-negative Markov chains. Various non-
negative Markov chains have been proposed in the NMF literature (see Section 2 and
references therein). They are all built in relation with the Gamma (or inverse Gamma)
distribution. As a matter of fact, these models exhibit the same drawback: the chains
all have a degenerate stationary distribution. This can lead to undesirable behaviors,
such as the instability or the degeneracy of realizations of the chains. We emphasize that
this is problematic from the probabilistic perspective only, since these prior distributions
may still represent an appropriate regularization in a MAP setting.

1.3. Contributions and organization of the paper

The contributions of this paper are 4-fold:

• We review the existing non-negative Markov chains of the NMF literature and
discuss some of their limitations. In particular we show that these chains all have
a degenerate stationary distribution;

• We present an overlooked non-negative Markov chain from the time series liter-
ature, the ﬁrst-order autoregressive Beta-Gamma process, denoted as BGAR(1)
(Lewis et al., 1989), whose stationary distribution is Gamma. To the best of our
knowledge, this particular chain has never been considered to model temporal
dependencies in matrix factorization problems;

• We derive majorization-minimization-based algorithms for maximum a posteriori
(MAP) estimation in the NMF models (with a Poisson likelihood) with four of the
presented prior structures on H, including BGAR(1);

• We compare the performance of all these models on a prediction task on three

real-world datasets.

The paper is organized as follows. Section 2 introduces and compares non-negative
Markov chains from the literature. Section 3 presents MAP estimation in temporal NMF
models. Experimental work is conducted in Section 4, before concluding in Section 5.

3Note that this particular mapping is actually slightly more complex, as the K-dimensional real vector

must be mapped to the (K − 1) simplex due to further constraints in the model.

4

2. Comparative study of Gamma Markov chains

This section reviews existing models of Gamma Markov chains, i.e., Markov chains
which evolve in R+ in relation with the Gamma distribution. We have identiﬁed four
diﬀerent models in the NMF literature:

1. Chaining on the rate parameter of a Gamma distribution (Section 2.1);

2. Chaining on the rate parameter of a Gamma distribution with an auxiliary variable

(Section 2.2);

3. Chaining on the shape parameter of a Gamma distribution (Section 2.3);

4. Chaining on the shape parameter of a Gamma distribution with an auxiliary vari-

able (Section 2.4).

As will be discussed in these subsections, these four models all lack a well-deﬁned sta-
tionary distribution, which leads to the degeneracy of the realizations of the chains. A
ﬁfth model from the time series literature, called BGAR(1), is presented in Section 2.5.
It is built to have a well-deﬁned stationary distribution (it is marginally Gamma dis-
tributed). The realizations of the chain are not degenerate and exhibit some interesting
properties. To the best of our knowledge, this kind of process has never been used in a
probabilistic NMF problem to model temporal evolution.

Throughout the section, (hn)n≥1 denotes the (scalar) Markov chain of interest, where
the index k as in Eq. (8) has been dropped for enhanced readability. It is further assumed
that h1 is set to a ﬁxed, deterministic value.

2.1. Chaining on the rate parameter

2.1.1. Model

Let us consider a general Gamma Markov chain model with a chaining on the rate

parameter:

hn|hn−1 ∼ Gamma

α,

(cid:18)

(cid:19)

.

β
hn−1

As it turns out, Eq. (9) can be rewritten as a multiplicative noise model:

hn = hn−1 × φn,

where φn are i.i.d. Gamma random variables with parameters (α, β). We have

E(hn|hn−1) =

α
β

hn−1,

var(hn|hn−1) =

α
β2 h2

n−1.

(9)

(10)

(11)

This model was introduced in F´evotte et al. (2009) to add smoothness to the activation
coeﬃcients in the context of audio signal processing. The parameters were set to α > 1
and β = α − 1, such that the mode would be located at hn = hn−1. It is also a particular
case of the dynamical model of F´evotte et al. (2013) and is considered in Virtanen and
Girolami (2020). A similar inverse Gamma Markov chain was also considered in F´evotte
et al. (2009) and in F´evotte (2011).

5

2.1.2. Analysis

From Eq. (10) we can write:

hn = h1

n
(cid:89)

i=2

φi.

The independence of the φi yields:
(cid:18) α
β
(cid:34)(cid:18) α2

E(hn) = h1

(cid:19)n−1

var(hn) = h2
1

β2 +

,

α
β2

(cid:19)n−1

−

(cid:18) α2
β2

(cid:19)n−1(cid:35)

.

(12)

(13)

(14)

We enumerate all the possible regimes (n → +∞), which all give rise to degenerate

stationary distributions for diﬀerent reasons:

• β > (cid:112)α(α + 1): both mean and variance go to zero;
• β = (cid:112)α(α + 1): variance converges to 1, however the mean goes to zero;

• β ∈

(cid:105)
α; (cid:112)α(α + 1)

(cid:104)

: variance goes to inﬁnity, mean goes to zero;

• β = α: mean is equal to 1, but the variance goes to inﬁnity;

• β < α: both mean and variance go to inﬁnity.

Each subplot of Figure 1 displays ten independent realizations of the chain, for a
diﬀerent set of parameters (α, β). As we can see, the realizations of the chain either
collapse to 0, or diverge.

2.2. Hierarchical chaining on the rate parameter

2.2.1. Model

Let us consider the following Gamma Markov chain model introduced in Cemgil and

Dikmen (2007):

zn|hn−1 ∼ Gamma(αz, βzhn−1),
hn|zn ∼ Gamma(αh, βhzn).

As it turns out, this model can also be rewritten as a multiplicative noise model:

hn = hn−1 × ˜φn,

(15)

(16)

(17)

where ˜φn are i.i.d. random variables deﬁned as the ratio of two independent Gamma ran-
dom variables with parameters (αh, βh) and (αz, βz). The distribution of ˜φn is actually
known in closed form, namely

˜φn ∼ BetaPrime

(cid:16)

(cid:17)
αh, αz, 1, ˜β

,

(18)

6

with ˜β = βz
βh

(see Appendix A for a deﬁnition). We have

αh
αz − 1

E(hn|hn−1) = ˜β
var(hn|hn−1) = ˜β2 αh(αh + αz − 1)
(αz − 1)2(αz − 2)

hn−1

for αz > 1,

for αz > 2.

(19)

(20)

h2
n−1

This model is less straightforward in its construction than the previous one, as it
makes use of an auxiliary variable zn (note that a similar inverse Gamma construction
was proposed as well in Cemgil and Dikmen (2007)). There are two motivations behind
the introduction of this auxiliary variable:

1. Firstly, it ensures what is referred to as “positive correlation” in Cemgil and Dik-
men (2007), i.e., E(hn|hn−1) ∝ hn−1 (something the model described by Eq. (9)
does as well).

2. Secondly, it ensures the so-called conjugacy of the model, i.e., the conditional dis-
tributions p(zn|hn−1, hn) and p(hn|zn, zn+1) remain Gamma distributions. Indeed,
these are the distributions of interest when considering Gibbs sampling or varia-
tional inference. This property is not satistﬁed by the model described by Eq. (9)
(i.e., p(hn|hn−1, hn+1) is neither Gamma, nor a known distribution).

This particular chain has been used in the context of audio signal processing in Virtanen
et al. (2008) (under the assumption of a Poisson likelihood, which does not ﬁt the nature
of the data), and also to model the evolution of user and item preferences in the context
of recommender systems (Jerfel et al., 2017; Do and Cao, 2018).

2.2.2. Analysis

From Eq. (17), we can write:

hn = h1

n
(cid:89)

i=2

˜φi.

(21)

We have by independence of the ˜φi:

E(hn) = h1

(cid:18)

˜β

(cid:19)n−1

αh
αz − 1
(cid:34)(cid:18)

var(hn) = h2
1

˜β2(n−1)

for αz > 1,

(22)

αh(αh + αz − 1)
(αz − 1)2(αz − 2)

(cid:19)n−1

for αz > 2.

(23)

α2
h
(αz − 1)2 +
α2
h
(αz − 1)2

(cid:18)

−

(cid:19)n−1(cid:35)

As in the previous model, we can show that either the expectation or the variance
diverges or collapses as n → ∞ for every possible choice of parameters, which means

7

that they all give rise to a degenerate stationary distribution of the chain. Each subplot
of Figure 2 displays ten independent realizations of the chain, for a diﬀerent set of
parameters (αz, βz, αh, βh). As we can see, the realizations of the chain either collapse
to 0 or diverge.

2.3. Chaining on the shape parameter

2.3.1. Model

Let us consider a general Gamma Markov chain model with a chaining on the shape

parameter:

We have

hn|hn−1 ∼ Gamma(αhn−1, β).

E(hn|hn−1) =

α
β

hn−1,

var(hn|hn−1) =

α
β2 hn−1.

(24)

(25)

In contrast with the two models presented previously, this model cannot be rewritten
as a multiplicative noise model. This model is therefore more intricate to interpret. It
was introduced in Acharya et al. (2015) in the context of Poisson factorization. It is
mainly motivated by a data augmentation trick that can be used when working with
a Poisson likelihood, which enables a Gibbs sampling procedure. The authors set the
value of α to 1 (although the same trick can be applied for any value of α). This model
is also a particular case of the dynamical model of Schein et al. (2016). It has since been
used in the context of topic modeling (Acharya et al., 2018).

2.3.2. Analysis

Using the law of total expectation and total variance, it can be shown that

E(hn) = h1

(cid:19)n−1

(cid:18) α
β

, var(hn) = h1

(cid:18) α
β

1
β

(cid:19)n−1 n−2
(cid:88)

i=0

(cid:19)i

.

(cid:18) α
β

(26)

The discussion is hence driven by the value of r = α/β.

• If r < 1, mean and variance go to zero;

• If r = 1, mean is ﬁxed but variance goes to inﬁnity (linearly);

• If r > 1, mean and variance go to inﬁnity.

This chain only exhibits degenerate stationary distributions. Each subplot of Figure 3
displays ten independent realizations of the chain, for a diﬀerent set of parameters (α, β).
As we can see, the realizations of the chain either collapse to 0, or diverge.

8

2.4. Hierarchical chaining on the shape parameter

2.4.1. Model

Let us consider the following Gamma Markov chain model

zn|hn−1 ∼ Poisson(βhn−1),

hn|zn ∼ Gamma(α + zn, β).

(27)

(28)

This model is a particular case of the dynamical model ﬁrstly introduced in Schein et al.
(2019). It cannot be rewritten as a multiplicative noise model. Using the law of total
expectation and total variance, we obtain

E(hn|hn−1) = hn−1 +

α
β

,

var(hn|hn−1) =

2
β

hn−1 +

α
β2 .

(29)

(30)

The motivation behind the introduction of this model is once again computational: it
leads to closed-form conditional distributions when considering a Poisson likelihood. As
stated in Schein et al. (2019), the auxiliary variable zn can actually be marginalized out,
leading to the so-called randomized Gamma distribution of the ﬁrst type (RG1), whose
analytical expression makes use of modiﬁed Bessel functions.

The authors also consider the particular limit case α = 0, which leads here to a chain

which will only take value 0 after obtaining zn = 0.

2.4.2. Analysis

Using the law of total expectation and total variance, it can be shown that

E(hn) = h1 + (n − 1)

α
β

,

var(hn) = (n − 1)

2
β

h1 + (n − 1)2 α
β2 .

(31)

(32)

As such, for α, β > 0, when n → +∞, both the expectation and variance of hn diverge,
leading to a degenerate stationary distribution of the chain. Each subplot of Figure 4
displays ten independent realizations of the chain, for a diﬀerent set of parameters (α, β).

2.5. BGAR(1)

We now discuss the ﬁrst order autoregressive Beta-Gamma process of Lewis et al.
(1989), a stochastic process which is marginally Gamma distributed. The authors re-
ferred to the process as “BGAR(1)”. However, to the best of our knowledge, no extension
to higher-order autoregressive processes exists in the time series literature. As such, from
now on, we will simply refer to it as “BGAR”.

9

2.5.1. Model

Consider α > 0, β > 0, ρ ∈ [0, 1[. The BGAR process is deﬁned as:

h1 ∼ Gamma(α, β),
hn = bnhn−1 + (cid:15)n

for n ≥ 2,

where bn and (cid:15)n are i.i.d. random variables distributed as:

bn ∼ Beta(αρ, α(1 − ρ)),
(cid:15)n ∼ Gamma(α(1 − ρ), β).

(33)

(34)

(35)

(36)

The sequence (hn)n≥1 is called the BGAR process.
It is parametrized by α, β and
ρ. We emphasize that the distribution p(hn|hn−1) is not known in closed form. Only
p(hn|hn−1, bn) is known; it is a shifted Gamma distribution. The generative model may
therefore be rewritten as

h1 ∼ Gamma(α, β),
bn ∼ Beta(αρ, α(1 − ρ))

for n ≥ 2,

hn|bn, hn−1 ∼ Gamma(α(1 − ρ), β, loc = bnhn−1)

for n ≥ 2,

(37)

(38)

(39)

where the distribution in Eq. (39) is a shifted Gamma distribution with a location
parameter “loc”.

We have

E(hn|hn−1) = ρhn−1 +

α(1 − ρ)
β

,

var(hn|hn−1) =

ρ(1 − ρ)
α + 1

h2
n−1 +

α(1 − ρ)
β2

.

(40)

(41)

2.5.2. Analysis

To study the marginal distribution of the process, we recall the following lemma.

Lemma 1. If X ∼ Beta(a, b) and Y ∼ Gamma(a + b, c) are independent random vari-
ables, then Z = XY is Gamma(a, c) distributed.

Proposition 1. Let (hn)n≥1 be a BGAR process. Then hn is marginally Gamma(α, β)
distributed.

Proof. Follows by induction. Consider n such that hn is Gamma(α, β) distributed.
Then, (cid:15)n+1hn is Gamma(αρ, β) distributed (Lemma 1). Finally, hn+1 = (cid:15)n+1hn + bn+1
is Gamma(α, β) distributed (sum of independent Gamma random variables), which con-
cludes the proof.

10

Therefore the parameters α and β control the marginal distribution. The parame-
ter ρ controls the correlation between successive values, as discussed in the following
proposition.

Proposition 2. Let (hn)n≥1 be a BGAR process. Let n and r be two integers such that
r > 1. We have corr(hn, hn+r) = ρr.

Proof. See Appendix B for r = 1.

Proposition 2 implies that the BGAR(1) process admits a (second order) AR(1) rep-

resentation. Two limit cases of BGAR can be exhibited:

• When ρ = 0, the hn are i.i.d. random variables;

• When ρ → 1, the process is not random anymore, and hn = h1 for all n (note that

ρ = 1 is not an admissible value).

Finally, from Eq. (40), we have

(cid:18)

E(hn|hn−1) > hn−1

(cid:19)

(cid:18)

⇔

hn−1 <

(cid:19)
.

α
β

(42)

If hn−1 is below the mean of the marginal distribution E(hn) = α
expectation above hn−1, and vice-versa.

β , then hn will be in

Note that BGAR is not the only Markovian process with a marginal Gamma distri-
bution considered in the literature. We mention the GAR(1) process (ﬁrst-order autore-
gressive Gamma process) of Gaver and Lewis (1980), which is also marginally Gamma
distributed. However, this particular process is piecewise deterministic, and its param-
eters are “coupled”: the parameters of the marginal distribution also have an inﬂuence
on other properties of the model. As such, it is less suited to our problem, and will not
be considered here.

Figure 5 displays three realizations of the BGAR process, with parameters ﬁxed to
α = 2 and β = 1, and a diﬀerent parameter ρ in each subplot. The mean of the marginal
distribution is displayed in red. When ρ = 0.5, the correlation is weak, and no particular
structure is observed. However, as ρ goes to 1, the correlation becomes stronger, and we
typically observe piecewise constant trajectories.

11

Figure 1: Realizations of the Markov chain deﬁned in Eq. (9). The initial value h1 is
set to 1, and chains were simulated until n = 50. Each subplot contains ten
independent realizations, with the value of the parameters (α, β) given at the
top of the subplot. log10(hn) is displayed.

Figure 2: Realizations of the Markov chain deﬁned in Eq. (15)-(16). The initial value
h1 is set to 1, and chains were simulated until n = 50. Each subplot contains
ten independent realizations, with the value of the parameters (αz, βz, αh, βh)
given at the top of the subplot. log10(hn) is displayed.

Figure 3: Realizations of the Markov chain deﬁned in Eq. (24). The initial value h1 is
set to 1, and chains were simulated until n = 50. Each subplot contains ten
independent realizations, with the value of the parameters (α, β) given at the
top of the subplot. log10(hn) is displayed.

12

0102030405010−4510−3610−2710−1810−9100α=0.5, β=10102030405010−1410−1110−810−510−2101α=1.0, β=1010203040501011041071010α=2.0, β=10102030405010−2610−2110−1610−1110−610−1αz=5, βz=1, αh=2, βh=10102030405010−710−510−310−1101αz=5, βz=1, αh=4, βh=101020304050100102104106αz=5, βz=1, αh=6, βh=10102030405010−2010−1610−1210−810−4100α=0.5, β=10102030405010−2010−1610−1210−810−4100α=1.0, β=10102030405010−2010−1410−810−210410101016α=2.0, β=1Figure 4: Realizations of the Markov chain deﬁned in Eq. (27)-(28). The initial value h1
is set to 1, and chains were simulated until n = 50. Each subplot contains ten
independent realizations, with the value of the parameters (α, β) given at the
top of the subplot. log10(hn) is displayed.

Figure 5: Three realizations of the BGAR(1) process, with parameters ﬁxed to α = 2
and β = 1, and a diﬀerent parameter ρ in each subplot. The mean of the
process is displayed by a dashed red line.

13

0102030405001020304050α=0.1, β=101020304050050100150200250α=1.0, β=10102030405002004006008001000α=10.0, β=105010015020025002468ρ=0.5, α=2, β=10501001502002500123456ρ=0.9, α=2, β=10501001502002500.00.51.01.52.02.53.03.5ρ=0.99, α=2, β=13. MAP estimation in temporal NMF models

We now turn to the problem of maximum a posteriori (MAP) estimation in temporal

NMF models. More precisely, we assume a Poisson likelihood, that is

vf n ∼ Poisson([WH]f n),

(43)

and we also assume that W is a deterministic variable. The variables V and H then
deﬁne a hidden Markov model, as displayed on Figure 6.

We consider four diﬀerent models corresponding to the temporal structures on H
presented in subsections 2.1, 2.2, 2.3, and 2.5. Only the temporal structure presented
in 2.4 is left out. Indeed, deriving a MAP algorithm in this model using the auxiliary
variables Z (similar to the one of Section 3.3) would involve integer programming. This
leads to technical developments which are out-of-scope of our current study.

hn−1

hn

hn+1

vn−1

vn+1

vn

•
W

Figure 6: Hidden Markov model arising in temporal NMF models. vn is of dimension

F , while hn is of dimension K. Observed variables are in blue.

Generally speaking, joint MAP estimation in such models amounts to minimizing the

following criterion

C(W, H) = − log p(V, H; W)

= − log p(V|H; W) −



log p(hk1) +

(cid:88)

k

(cid:88)

n≥2

(44)



log p(hkn|hk(n−1))

 ,

(45)

that is to say that the factors W and H are going to be estimated. Both shape hyperpa-
rameters (αk or ρk) and scale hyperparameters (βk) will be treated as ﬁxed and selected
using a validation set. However, note that deriving the maximum likelihood estimate
of βk is feasible in closed form for all the models presented below. Unfortunately, esti-
mating βk this way led to overly ﬂat priors in our experience (likely due to the MAP
estimation setting).

The optimization of the function C is carried out with a block coordinate descent
scheme over the variables W and H. We resort to a majorization-minimization (MM)
scheme, which consists in iteratively majorizing the function C (by a so-called auxil-
iary function, tight for some ˜W or ˜H), and minimizing this auxiliary function instead.

14

We refer the reader to Hunter and Lange (2004) for a detailed tutorial. Under this
scheme, the function C is non-increasing. As it turns out, only the Poisson likelihood
term − log p(V|H; W) needs to be majorized. This is a well-studied issue in the NMF
literature. As stated in Lee and Seung (2000); F´evotte and Idier (2011), the function

with the notations

G1(H; ˜H) = −

(cid:88)

k,n

pkn log(hkn) +

(cid:88)

k,n

qkhkn,

pkn = ˜hkn

(cid:88)

wf k

f

vf n
[W ˜H]f n

,

qk =

(cid:88)

f

wf k,

is a tight auxiliary function of − log p(V|H; W) at H = ˜H. Similarly the function

with the notations

G2(W; ˜W) = −

p(cid:48)
f k log(wf k) +

(cid:88)

f,k

q(cid:48)
kwf k,

(cid:88)

f,k

p(cid:48)
f k = ˜wf k

(cid:88)

hkn

n

vf n
[ ˜WH]f n

,

q(cid:48)
k =

(cid:88)

n

hkn,

is a tight auxiliary function of − log p(V|H; W) at W = ˜W.

3.1. Minimization w.r.t. W

(46)

(47)

(48)

(49)

The optimization w.r.t. W is common to all algorithms, and amounts to minimizing
G2(W; ˜W) only. The scale of W must be however be ﬁxed in order to prevent potential
degenerate solutions such that W → +∞ and H → 0. Indeed, consider W(cid:63) and H(cid:63)
minimizers of Eq. (44), and let Λ be a diagonal matrix with non-negative entries. Then

C(W(cid:63)Λ−1, ΛH(cid:63)) = − log p(V|ΛH(cid:63); W(cid:63)Λ−1) − log p(ΛH(cid:63))

= − log p(V|H(cid:63); W(cid:63)) − log p(ΛH(cid:63)),

(50)

(51)

and depending on the choice of the prior distribution p(H), we may obtain C(W(cid:63)Λ−1, ΛH(cid:63)) <
C(W(cid:63), H(cid:63)), i.e., a contradiction. Therefore, in the following we impose that ||wk||1 = 1.

The constrained optimization is performed with the following update rule

wf k =

p(cid:48)
f k
(cid:80)
f p(cid:48)
f k

,

(52)

see Appendix C for the proof.

The following subsections detail the optimization w.r.t. H (and other variables when
necessary) in the four considered models, which amounts to the minimization of G1(H; ˜H)−
log p(H).

15

Table 1: Coeﬃcients of the polynomial equation Eq. (53)

n

1

a2,kn

qk

a1,kn

αk − p1k

a0,kn

−βkhk2

2, . . . , N − 1

qk + βk

hk(n−1)

1 − pkn

−βkhk(n+1)

N

0

qk +

β
hk(N −1)

1 − αk − pN

3.2. Chaining on the rate parameter

The transition distribution p(hkn|hk(n−1)) is given by Eq. (9). The optimization w.r.t.

hkn amounts to solving an order-2 polynomial equation on R+

a2,knh2

kn + a1,knhkn + a0,kn = 0.

(53)

As it turns out, there is always exactly one non-negative root. The coeﬃcients of the
polynomial equation are given in Table 1. This bears resemblance with the methodology
described in F´evotte et al. (2009), where the authors aimed at retrieving MAP estimates
with a EM-like algorithm (with an exponential likelihood).

3.3. Hierarchical chaining on the rate parameter

In this case, we resort to using the auxiliary variables Z, which results in the the

slightly more involved following criterion

C(W, H, Z) = − log p(V|H; W)



log p(hk1) +

(cid:88)

n≥2

(cid:88)

−

k

(cid:0)log p(zkn|hk(n−1)) + log p(hkn|zkn)(cid:1)

(54)



 .

We recall that p(zkn|hk(n−1)) and p(hkn|zkn) are given by Eq. (15) and Eq. (16), respec-
tively. Note that Cemgil and Dikmen (2007) proposed a Gibbs sampler and variational
inference, and as such the development of the MAP algorithm is novel.

Imposing αh,k ≥ 1, we obtain the following update for zkn

zkn =

αz,k + αh,k − 1
βz,khk(n−1) + βh,khkn

,

(55)

16

and the following updates for hkn

hk1 =

hkn =

hkN =

pk1 + αz,k
qk + βz,kzk2

,

pkn + αh,k + αz,k − 1
qk + βh,kzkn + βz,kzk(n+1)
pkN + αh,k − 1
qk + βh,kzkN

.

, n ∈ {2, . . . , N − 1},

(56)

(57)

(58)

3.4. Chaining on the shape parameter

The transition distribution p(hkn|hk(n−1)) is given by Eq. (24). The optimization w.r.t.

hkn amounts to solving the following equations on R+

−pk1 + (qk − αk log(βkhk2) + αkΨ(αhk1))hk1 = 0,

(59)

(1 − αkhk(n−1) − pkn) + (qk + βk − αk log(βkhk(n+1)))hkn + αkΨ(αkhkn)hkn = 0, (60)
for n ∈ {2, . . . , N − 1},

where Ψ denotes the digamma function. Solving such equations can be done numerically
with Newton’s method. Finally the update for hkN is given by

hkN =

pkn + αkhk(N −1) − 1
qk + βk

.

(61)

Note that a Gibbs sampling procedure is proposed in Acharya et al. (2015); Schein

et al. (2016), and as such the development of the MAP algorithm is novel.

3.5. BGAR(1)

In this case, since the transition distribution p(hkn|hk(n−1)) is not known in closed

form, we resort to optimizing the slightly more involved following criterion

C(W, H, B) = − log p(V|H; W)



log p(hk1) +

(cid:88)

n≥2

(cid:88)

−

k

(cid:0)log p(hkn|hk(n−1), bkn) + log p(bkn)(cid:1)

(62)



 .

In the following, we will use the notations γk = αk(1 − ρk) and ηk = αkρk.

3.5.1. Constraints

By construction, the variables hkn and bkn must lie in a speciﬁc interval given the
values of all the other variables. Indeed, as hkn = bknhk(n−1) + (cid:15)kn (see Eq. (34)), where
(cid:15)kn is a non-negative random variable, we obtain hkn ≥ bknhk(n−1), bkn ≤ hkn
, and

hk(n−1)

hkn ≤

hk(n+1)
bk(n+1)

.

17

Table 2: Coeﬃcients of the polynomial equation Eq. (68). Def. int. = Deﬁnition interval.

n

1

Def. interval

a3,kn

a2,kn

a1,kn

a0,kn

[0, dk1]

0

−(qk + βk(1 − bk2)) −(1 − αk − pk1) +
(qk + βk(1 −
bk2))dk1 − (1 − γk)

(1 − αk − pk1)dk1

2, . . . , N − 1

[ckn, dkn]

−(qk + βk(1 −
bk(n+1)))

pkn − 2(1 −
γk) + (qk + βk(1 −
bk(n+1)) (ckn + dkn)

N

[ckN , +∞[

0

qk + βk

This leads to the following constraints

pknckndkn

−pkn (ckn + dkn) +
−
(1
γk) (ckn + dkn) −
(qk + βk(1 −
bk(n+1)))ckndkn

−pkN − ckN (qk +
βk) + (1 − γk)

ckN pkN

0 ≤ hk1 ≤

bknhk(n−1) ≤ hkn ≤

bkN hk(N −1) ≤ hkN ,

,

hk2
bk2
hk(n+1)
bk(n+1)

n ∈ {2, . . . , N − 1},

and

0 ≤ bkn ≤ min

1,

(cid:18)

(cid:19)

.

hkn
hk(n−1)

We therefore introduce the notations

ckn = bknhk(n−1),

dkn =

hk(n+1)
bk(n+1)

,

xkn =

hkn
hk(n−1)

,

as these quantities arise naturally in our derivations.

3.5.2. Minimization w.r.t. hkn

(63)

(64)

(65)

(66)

(67)

The optimization of Eq. (62) w.r.t. hkn may give rise to intractable problems, due
to the logarithmic terms in the objective function. To alleviate this issue, we propose
to control the limit values of the auxiliary function, by restricting ourselves to certain
values of the hyperparameters. In particular, choosing (1 − γk) < 0 ensures the existence
of at least one minimizer.

For all n, the optimization w.r.t. hkn amounts to solving an order-3 polynomial

equation

a3,knh3

kn + a2,knh2

kn + a1,knhkn + a0,kn = 0.

(68)

The coeﬃcients of the equation and deﬁnition intervals are given in Table 2. If several
roots belong to the deﬁnition interval, we simply choose the root which gives the lowest
objective value.

18

Figure 7: Admissible values of the hyperparameters in the MAP algorithm presented in

Section 3.5. Admissible values are in white.

3.5.3. Minimization w.r.t. bkn

Similarly, logarithmic terms of the objective function may give rise to degenerate
solutions. Using the same reasoning, we choose to impose (1 − γk) < 0 and (1 − ηk) < 0
to ensure the existence of at least one minimizer.

The minimization of the auxiliary function w.r.t. bkn amounts to solving the following

order 3 polynomial over the interval [0, min(1, xkn)]

a3,knb3

kn + a2,knb2

kn + a1,knbkn + a0,kndkn = 0,

where

a3,kn = −βkhk(n−1),
a2,kn = 2(1 − γk) + (1 − ηk) + βkhk(n−1)(xkn + 1),
a1,kn = −(1 − γk)(xkn + 1) − (1 − ηk)(xkn + 1)

− βkhk(n−1)xkn,

a0,kn = (1 − ηk)xkn.

(69)

(70)

(71)

(72)

(73)

3.5.4. Admissible values of hyperparameters

To recap the discussion on admissible values of hyperparameters, to ensure the exis-

tence of minimizers of the auxiliary function, we have restricted ourselves to

(cid:26) αk(1 − ρk) > 1,
αkρk > 1.

(74)

This set is graphically displayed on Figure 7. As we can see, choosing the value of ρk to
be close to one (to ensure correlation) leads to high values of αk.

19

0.00.20.40.60.81.0rho0246810alpha4. Experimental work

We now compare the performance of all considered temporal NMF models on a pre-
diction task on three real datasets. This task will consist in hiding random columns of
the considered datasets and estimating those missing values. We will also include the
performance of a naive baseline, which we detail in the following subsection. Adapting
the MAP algorithms presented in Section 2 in a setting with a mask of missing values
only consist in a slight modiﬁcation, presented in Appendix D. Python code is available
online4.

4.1. Experimental protocol

For each considered dataset, the experimental protocol is as follows.
First of all, a value of the factorization rank K (which will be used for all considered
methods) must be selected. To do so, we apply the standard KL-NMF algorithm (Lee
and Seung, 2000; F´evotte and Idier, 2011) on 10 random training sets, which consist
of 80% of the original data, with a pre-deﬁned grid of values for K. We then select
the value of K which yields the lowest generalized Kullback-Leibler error (KLE) (see
deﬁnition below) on the remaining 20% of the data, on average.

For the prediction experiment itself, we create 5 random splits of the data matrix,
where 80% corresponds to the training set, 10% to the validation set, and the remaining
10% to the test set. To do so, we randomly select non-adjacent columns of the data
matrix (excluding the ﬁrst one and always including the last one), half of which will
make up the validation set and the other half the test set (the last column is always
included in the test set). We also consider 5 diﬀerent random initializations.

Then, for each split-initialization pair, all the algorithms are run from this initializa-
tion point on the training set until convergence (the algorithms are stopped when the
relative decrease of the objection function falls under 10−5). For each method, a grid
of hyperparameters is considered, and their selection is based on the lowest KLE on the
validation set. Details of the grids used for each method can be found in Appendix E.
The predictive performance of each method is then computed on the test set by compar-
ing the original value vf n and its associated estimate ˆvf n = [WH]f n with the following
metric. Denoting by T the test set, we compute the generalized Kullback-Leibler error
(KLE), which is deﬁned as

KLE =

(cid:20)

(cid:88)

vf n log

(f,n)∈T

(cid:19)

(cid:18) vf n
ˆvf n

(cid:21)

− vf n + ˆvf n

.

(75)

Finally, we construct a baseline based on the Gamma-Poisson (GaP) model of Canny
(2004). The GaP model is based on independent Gamma priors on H, i.e., a non-
temporal prior. However, it is unable to estimate columns hn associated with missing
columns vn. We propose to set hn = 1
2 (hn−1 + hn+1) and hN = hN −1 for these columns.
MAP estimation in the GaP model is described in Dikmen and F´evotte (2012) and is
recalled in Appendix F.

4https://github.com/lfilstro/TemporalNMF

20

Model

KLE-S

KLE-F

GaP (App. F)
Rate (3.2)
Hier (3.3)
Shape (3.4)
BGAR (3.5)

6.19 × 104 ± 9.69 × 103
6.07 × 104 ± 8.97 × 103
6.06 × 104 ± 9.18 × 103
9.37 × 104 ± 2.99 × 104
6.17 × 104 ± 8.24 × 103

1.08 × 105 ± 2.67 × 103
1.03 × 105 ± 3.53 × 103
3.24 × 105 ± 2.09 × 105
1.30 × 105 ± 2.35 × 104
1.36 × 105 ± 2.00 × 103

Table 3: Prediction results on the NIPS dataset. Lower values are better. The mean and

standard deviation of each metric are reported over 25 runs.

4.2. Datasets

The following datasets are considered

• The NIPS dataset5, which contains word counts (with stop words removed) of all
the articles published at the NIPS6 conference between 1987 and 2015. We grouped
the articles per publication year, yielding an observation matrix of size 11463 × 29.
We obtained K = 3.

• The last.fm dataset, based on the so-called “last.fm 1K” users7, which contains
the listening history with timestamps information of users of the music website
last.fm. We preprocessed this dataset to obtain the monthly evolution of the
listening counts of artists with at least 20 diﬀerent listeners. This yields a dataset
of size 7017 × 53 (i.e., we have the listening history of 7017 artists over 53 months).
We obtained K = 5.

• The ICEWS dataset8, an international relations dataset, which contains the number
of interactions between two countries for each day of the year 2003. The matrix is
of size 6197 × 365. We obtained K = 5.

4.3. Experimental results

As previously mentioned, the test set consists of 10 % of the columns of the data
matrix V, always including the last one. The KLE will be computed separately on all
the columns minus the last one (denoted by ”S” for smoothing), and on the last one
(denoted by ”F” for forecasting). Their averaged values over the 25 split-initialization
pairs are reported on Table 3 for the NIPS dataset, on Table 4 for the last.fm dataset,
and on Table 5 for the ICEWS dataset.

All considered temporal models achieve comparable predictive performance, both on
smoothing and forecasting tasks, and the slight advantage of one method over the others

5https://archive.ics.uci.edu/ml/datasets/NIPS+Conference+Papers+1987-2015
6Now called NeurIPS.
7http://ocelma.net/MusicRecommendationDataset/
8https://github.com/aschein/pgds

21

Model

KLE-S

KLE-F

GaP (App. F)
Rate (3.2)
Hier (3.3)
Shape (3.4)
BGAR (3.5)

1.30 × 104 ± 3.35 × 102
1.23 × 104 ± 2.35 × 102
1.23 × 104 ± 2.95 × 102
1.58 × 104 ± 2.45 × 103
1.24 × 104 ± 3.00 × 102

6.89 × 103 ± 5.84 × 101
7.76 × 103 ± 4.13 × 101
6.35 × 103 ± 1.57 × 103
2.04 × 104 ± 9.92 × 103
9.65 × 103 ± 2.91 × 103

Table 4: Prediction results on the last.fm dataset. Lower values are better. The mean

and standard deviation of each metric are reported over 25 runs.

Model

KLE-S

KLE-F

GaP (App. F)
Rate (3.2)
Hier (3.3)
Shape (3.4)
BGAR (3.5)

8.91 × 104 ± 2.82 × 103
9.17 × 104 ± 2.99 × 103
9.11 × 104 ± 2.81 × 103
9.95 × 104 ± 3.82 × 103
8.99 × 104 ± 2.80 × 103

1.59 × 103 ± 6.34 × 101
1.62 × 103 ± 7.57 × 101
1.62 × 103 ± 9.02 × 101
1.84 × 103 ± 1.37 × 102
1.78 × 103 ± 8.62 × 101

Table 5: Prediction results on the ICEWS dataset. Lower values are better. The mean

and standard deviation of each metric are reported over 25 runs.

seems to be data-dependent. The methods “Rate” and “Hier” rank ﬁrst or second most
of time but not always signiﬁcantly so. The method “Shape” tends to achieve worse
results than others, but not consistently. This suggests that in the MAP estimation
framework, prior distributions do not act as strong regularization terms, and are out-
weighed by the likelihood term. Moreover, the baseline based on the GaP model also
achieves good performance. This might be attributed to the high correlation between
successive columns on the datasets, and as such, using adjacent columns for estimation
is reasonable.

We conclude this section by saying a few words about computational complexity, which
can act as a diﬀerentiating criterion, as all models achieve similar predictive performance.
The algorithms for chains involving the rate parameters of the Gamma distribution,
described in Sections 3.2 and 3.3 have closed-form update rules for all their variables.
This leads to eﬃcient block-descent algorithms. This is in contrast with the algorithms
for the model based on the chaining on the shape parameter (Section 3.4), which involves
solving K(N − 1) equations numerically at each iteration, and the algorithm for BGAR
(Section 3.5), which involves serially solving 2K(N − 1) order-3 polynomials at each
iteration.

22

5. Conclusion

In this paper, we have reviewed existing temporal NMF models in a uniﬁed MAP
framework and introduced a new one. These models diﬀer by the choice of the Markov
chain structure used on the activation coeﬃcients to induce temporal correlation. We
began by studying the previously proposed Gamma Markov chains of the NMF literature,
only to ﬁnd that they all share the same drawback, namely the absence of a well-
deﬁned stationary distribution. This leads to problematic behaviors from the generative
perspective, because the realizations of the chains are degenerate (although this is not
necessarily a problem in MAP estimation). We then introduced a Markovian process
from the time series literature, called BGAR(1), which overcomes this limitation, and
which, to the best of our knowledge, had never been exploited for learning tasks.

We then derived MAP estimation algorithms in the context of a Poisson likelihood,
which allowed for a comprehensive comparison on a prediction task on real datasets.
As it turns out, we cannot claim that there is a single model which outperforms all the
others. It seems that in our framework, MAP estimation will tend to homogenize the
performance of all the models.

Future work will focus on ﬁnding a way to perform inference with the BGAR prior for
a less restrictive set of hyperparameters, which might increase the performance of this
particular model. Moreover, it should be noted that this work can easily be extended
to other likelihoods than Poisson thanks to the MM framework. To illustrate this, we
present in Appendix G the derivation of an algorithm for MAP estimation in a model
consisting in an exponential likelihood and BGAR(1) temporal prior. Finally, it would
be interesting to carry out similar experimental work within a fully Bayesian estimation
paradigm, which might make the diﬀerences between the models more striking.

Acknowledgments

This work has received funding from the European Research Council (ERC) under the
European Union’s Horizon 2020 research and innovation program under grant agreement
No 681839 (project FACTORY). Louis Filstroﬀ and Olivier Gouvert were with IRIT,
Univ. Toulouse, CNRS, France at the time this research was conducted.

23

Appendix A The Beta-Prime distribution

Distribution for a continuous random variable in [0, +∞[, with parameters α > 0,

β > 0, p > 0 and q > 0. Its probability density function writes, for x ≥ 0:

(cid:16) x
q

p

f (x; α, β, p, q) =

(cid:17)αp−1 (cid:16)

1 +

(cid:16) x
q
qB(α, β)

(cid:17)p(cid:17)−α−β

.

(76)

Appendix B BGAR(1) linear correlation

We have between two successive values hn and hn+1:

corr(hn, hn+1)

=

=

=

=

E(hnhn+1) − E(hn)E(hn+1)
σ(hn)σ(hn+1)

E(hn(bn+1hn + (cid:15)n+1)) − E(hn)E(hn+1)
σ(hn)σ(hn+1)
n) + E(hn)E((cid:15)n+1) − E(hn)E(hn+1)

E(bn+1)E(h2

σ(hn)σ(hn+1)
α(1−ρ)
β − α

β

β

α(α+1)

β2 + α

α
β

αρ
αρ+α(1−ρ)

α
β2

= ρ.

(77)

(78)

(79)

(80)

(81)

Appendix C Constrained optimization

We want to optimize G2(W; ˜W) w.r.t. W s.t. (cid:80)

f wf k = 1. Rewriting this with

Lagrange multipliers λ = [λ1, . . . , λK]T, this is tantamount to

G2(W; ˜W) +

min
W,λ

(cid:88)

k

λk(||wk||1 − 1).

Deriving w.r.t wf k yields

wf k =

p(cid:48)
f k
q(cid:48)
k + λk

.

(82)

(83)

We retrieve the constraint by summing this expression over f . This gives the expression
of the Lagrange multiplier: λk = (cid:80)
k. Substituting this expression into Eq. (83),
we obtain the following update rule

f k − q(cid:48)

f p(cid:48)

wf k =

p(cid:48)
f k
(cid:80)
f p(cid:48)
f k

.

24

(84)

Appendix D Algorithms with missing values

In the context of missing values, let us consider a mask matrix M of size F × N such
that mf n = 1 if the entry vf n is observed and 0 otherwise. The likelihood term can then
be written as

− log p(V|H; W) = −

mf n log p(vf n|[WH]f n).

(85)

(cid:88)

f,n

The auxiliary function G1 of Eq. (46) and G2 of Eq. (48) can then be written is the same
way, with

pkn = ˜hkn

(cid:88)

wf k

f

mf nvf n
[W ˜H]f n

,

qkn =

for G1, and

for G2.

p(cid:48)
f k = ˜wf k

(cid:88)

hkn

n

mf nvf n
[ ˜WH]f n

,

q(cid:48)
kn =

Appendix E Hyperparameter grids

(cid:88)

f

(cid:88)

n

mf nwf k,

mf nhkn,

(86)

(87)

For all methods, we have considered constant hyperparameters w.r.t. k (for example
αk = α for all k). Additional details regarding each method can be found in the list
below.

• For GaP, we have considered a two-dimensional grid for the parameters α and β.

Values were α = {0.1, 1, 10} and β = {0.1, 1, 10}.

• For “Rate”, we have set α = β, which implies that E(hkn|hk(n−1)) = hk(n−1). We

considered a one-dimensional grid with values {1.5, 10, 100}.

• For “Hier”, we have set αh = βh, and αz = βz, which implies E(zkn|hk(n−1)) =
hk(n−1) and E(hkn|zkn) = zkn. We considered a two-dimensional grid with values
αh = {1.5, 10, 100} and αz = {1.5, 10, 100}.

• For “Shape”, we have set α = β, which implies that E(hkn|hk(n−1)) = hk(n−1). We

considered a one-dimensional grid with values {0.1, 1, 10}.

• For “BGAR”, we have set ρ = 0.9, and considered a two-dimensional grid for
the parameters α and β (note that setting ρ = 0.9 implies α > 10 in our MAP
framework). Values were α = {11, 110, 1100} and β = {0.1, 1, 10}.

Appendix F MAP estimation in the GaP model

The prior distribution on H is such that

hkn ∼ Gamma(αk, βk).

(88)

25

MAP estimation amounts to minimizing

C(W, H) = − log p(V|H; W)

(cid:88)

+

k,n

((1 − αk)hkn + βkhkn) ,

which leads to the following MM update rule (Dikmen and F´evotte, 2012)

hkn =

(cid:26) 0

pkn+αk−1
qkn+βk

if pkn + αk − 1 ≤ 0,
else.

(89)

(90)

Appendix G BGAR with an exponential likelihood

Another popular likelihood used in probabilistic NMF models is the Exponential like-

lihood (F´evotte et al., 2009; Hoﬀman et al., 2010) which writes

vf n ∼ Exp

(cid:18)

1
[WH]f n

(cid:19)

,

(91)

where Exp(β) = Gamma(1, β) refers to the exponential distribution with mean 1/β.
This model underlies so-called Itakura-Saito NMF and has most notably been used in
audio signal processing applications. We consider MAP estimation in this model with
BGAR(1) prior on H. To do so, we resort to the same MM scheme than what was
presented in the beginning of Section 3. In this case, the majorization of the likelihood
term is known from (Cao et al., 1999; F´evotte and Idier, 2011). In particular, the function

with the notations

G(H; ˜H) =

(cid:18) pkn
hkn

(cid:88)

k,n

(cid:19)

+ qknhkn

,

pkn = ˜h2
kn

(cid:88)

f

wf kvf n
[W ˜H]2
f n

,

qkn =

(cid:88)

f

wf k
[W ˜H]f n

,

(92)

(93)

is a tight auxiliary function of − log p(V; W, H) at H = ˜H (up to irrelevant constants).
The exact same constraints on hkn and admissible values of hyperparameters detailed
in Section 3.5 apply. The minimization w.r.t. hkn then amounts to solving an order-4
polynomial equation

a4,knh4

kn + a3,knh3

kn + a2,knh2

kn + a1,knh1

kn + a0,kn = 0,

(94)

whose coeﬃcients are detailed below.

26

For hk1:

a4,kn = 0,
a3,kn = −(qk1 + βk(1 − bk2)),
a2,kn = (qk1 + βk(1 − bk2))dk1 − (1 − αk) − (1 − γk),
a1,kn = (1 − αk)dk1 + pk1,
a0,kn = −pk1dk1.

For hkn, n ∈ {2, . . . , N − 1}:

a4,kn = −(qkn + βk(1 − bk(n+1))),
a3,kn = (qkn + βk(1 − bk(n+1)))(ckn + dkn) − 2(1 − γk),
a2,kn = −ckndkn(qkn + βk(1 − bk(n+1)))

+ pkn + (1 − γk)(ckn + dkn),

a1,kn = −pkn(ckn + dkn),
a0,kn = pknckndkn.

For hkN :

a4,kn = 0,
a3,kn = qkN + βk,
a2,kn = −(qkN + βk)ckN + (1 − γk),
a1,kn = −pkN ,
a0,kn = ckN pkN .

(95)

(96)

(97)

(98)

(99)

(100)

(101)

(102)

(103)

(104)

(105)

(106)

(107)

(108)

(109)

27

References

Acharya, A., Ghosh, J., and Zhou, M. (2015). Nonparametric Bayesian Factor Analy-
sis for Dynamic Count Matrices. In Proceedings of the International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), pages 1–9.

Acharya, A., Ghosh, J., and Zhou, M. (2018). A dual markov chain topic model for
dynamic environments. In Proceedings of the ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining (KDD), pages 1099–1108.

Basbug, M. E. and Engelhardt, B. E. (2016). Hierarchical Compound Poisson Factor-
ization. In Proceedings of the International Conference on Machine Learning (ICML),
pages 1795–1803.

Blei, D. M. and Laﬀerty, J. D. (2006). Dynamic Topic Models. In Proceedings of the

International Conference on Machine Learning (ICML), pages 113–120.

Canny, J. (2004). GaP: A Factor Model for Discrete Data. In Proceedings of the In-
ternational ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 122–129.

Cao, Y., Eggermont, P. P., and Terebey, S. (1999). Cross burg entropy maximization
and its application to ringing suppression in image reconstruction. IEEE Transactions
on Image Processing, 8(2):286–292.

Cemgil, A. T. (2009). Bayesian Inference for Nonnegative Matrix Factorisation Models.

Computational Intelligence and Neuroscience, (Article ID 785152).

Cemgil, A. T. and Dikmen, O. (2007). Conjugate Gamma Markov random ﬁelds for
modelling nonstationary sources. In Proceedings of the International Conference on
Independent Component Analysis and Signal Separation (ICA), pages 697–705.

Charlin, L., Ranganath, R., McInerney, J., and Blei, D. M. (2015). Dynamic Pois-
son Factorization. In Proceedings of the ACM Conference on Recommender Systems
(RecSys), pages 155–162.

Dikmen, O. and F´evotte, C. (2012). Maximum marginal likelihood estimation for non-
IEEE Transactions on

negative dictionary learning in the Gamma-Poisson model.
Signal Processing, 60(10):5163–5175.

Do, T. D. T. and Cao, L. (2018). Gamma-Poisson Dynamic Matrix Factorization Embed-
ded with Metadata Inﬂuence. In Advances in Neural Information Processing Systems
(NeurIPS), pages 5829–5840.

F´evotte, C. (2011). Majorization-Minimization Algorithm for Smooth Itakura-Saito
Nonnegative Matrix Factorization. In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), pages 1980–1983.

28

F´evotte, C., Bertin, N., and Durrieu, J.-L. (2009). Nonnegative matrix factorization with
the itakura-saito divergence: With application to music analysis. Neural Computation,
21(3):793–830.

F´evotte, C. and Idier, J. (2011). Algorithms for nonnegative matrix factorization with

the β-divergence. Neural Computation, 23(9):2421–2456.

F´evotte, C., Le Roux, J., and Hershey, J. R. (2013). Non-negative Dynamical System
with Application to Speech and Audio.
In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3158–3162.

Gaver, D. and Lewis, P. (1980). First-order autoregressive gamma sequences and point

processes. Advances in Applied Probability, 12(3):727–745.

Gong, C. and Huang, W.-B. (2017). Deep Dynamic Poisson Factorization Model. In

Advances in Neural Information Processing Systems (NIPS), pages 1666–1674.

Gopalan, P., Hofman, J. M., and Blei, D. M. (2015). Scalable Recommendation with
Hierarchical Poisson Factorization. In Proceedings of the Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pages 326–335.

Gouvert, O., Oberlin, T., and F´evotte, C. (2019). Recommendation from Raw Data with
Adaptive Compound Poisson Factorization. In Proceedings of Uncertainty in Artiﬁcial
Intelligence (UAI).

Guo, D., Chen, B., Zhang, H., and Zhou, M. (2018). Deep Poisson Gamma Dynamical
Systems. In Advances in Neural Information Processing Systems (NeurIPS), pages
8451–8461.

Hoﬀman, M. D., Blei, D. M., and Cook, P. R. (2010). Bayesian Nonparametric Matrix
Factorization for Recorded Music. In Proceedings of the International Conference on
Machine Learning (ICML), pages 439–446.

Hunter, D. R. and Lange, K. (2004). A Tutorial on MM Algorithms. The American

Statistician, 58(1):30–37.

Jerfel, G., Basbug, M. E., and Engelhardt, B. E. (2017). Dynamic Collaborative Filtering
With Compound Poisson Factorization. In Proceedings of the International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pages 738–747.

Lee, D. D. and Seung, H. S. (1999). Learning the parts of objects by non-negative matrix

factorization. Nature, 401(6755):788–791.

Lee, D. D. and Seung, H. S. (2000). Algorithms for Non-negative Matrix Factorization.

In Advances in Neural Information Processing Systems (NIPS), pages 556–562.

Lewis, P., McKenzie, E., and Hugus, D. K. (1989). Gamma processes. Communications

in Statistics. Stochastic Models, 5(1):1–30.

29

Paatero, P. and Tapper, U. (1994). Positive matrix factorization: A non-negative factor
model with optimal utilization of error estimates of data values. Environmetrics,
5(2):111–126.

Schein, A., Linderman, S., Zhou, M., Blei, D., and Wallach, H. (2019). Poisson-
Randomized Gamma Dynamical Systems. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), pages 782–793.

Schein, A., Wallach, H. M., and Zhou, M. (2016). Poisson-Gamma Dynamical Systems.
In Advances in Neural Information Processing Systems (NIPS), pages 5005–5013.

Schmidt, M. N., Winther, O., and Hansen, L. K. (2009). Bayesian non-negative matrix
factorization. In Proceedings of the International Conference on Independent Compo-
nent Analysis and Signal Separation (ICA), pages 540–547.

Virtanen, S. and Girolami, M. (2020). Dynamic content based ranking. In Proceedings
of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pages 2315–2324.

Virtanen, T., Cemgil, A. T., and Godsill, S. (2008). Bayesian Extensions to Non-negative
Matrix factorisation for Audio Signal Modelling. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1825–
1828.

Zhou, M. (2018). Nonparametric Bayesian Negative Binomial Factor Analysis. Bayesian

Analysis, 13(4):1065–1093.

Zhou, M., Hannah, L., Dunson, D., and Carin, L. (2012). Beta-Negative Binomial
Process and Poisson Factor Analysis. In Proceedings of the International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1462–1471.

S¸im¸sekli, U., Cemgil, A. T., and Yılmaz, Y. K. (2013). Learning the beta-divergence
In Proceedings of the

in Tweedie compound Poisson matrix factorization models.
International Conference on Machine Learning (ICML), pages 1409–1417.

30

