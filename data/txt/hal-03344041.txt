Toward Generic Abstractions for Data of Any Model
Nelly Barret, Ioana Manolescu, Prajna Upadhyay

To cite this version:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay. Toward Generic Abstractions for Data of Any
Model. BDA 2021 - Informal publication only, Oct 2021, Paris, France. ￿hal-03344041v2￿

HAL Id: hal-03344041

https://inria.hal.science/hal-03344041v2

Submitted on 14 Sep 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Toward Generic Abstractions for Data of Any Model

Nelly Barret, Ioana Manolescu, Prajna Upadhyay
Inria & Institut Polytechnique de Paris
nelly.barret@inria.fr,ioana.manolescu@inria.fr,prajna-devi.upadhyay@inria.fr

ABSTRACT
Digital data sharing leads to unprecedented opportunities to de-
velop data-driven systems for supporting economic activities, the
social and political life, and science. Many open-access datasets are
RDF graphs, but others are CSV files, Neo4J property graphs, JSON
or XML documents, etc.

Potential users need to understand a dataset in order to decide if
it is useful for their goal. While some datasets come with a schema
and/or documentation, this is not always the case. Data summariza-
tion or schema inference tools have been proposed, specializing in
XML, or JSON, or the RDF data models. In this work, we present a
dataset abstraction approach, which (𝑖) applies on relational, CSV,
XML, JSON, RDF or Property Graph data; (𝑖𝑖) computes an abstrac-
tion meant for humans (as opposed to a schema meant for a parser);
(𝑖𝑖𝑖) integrates Information Extraction data profiling, to also classify
dataset content among a set of categories of interest to the user.
Our abstractions are conceptually close to an Entity-Relationship
diagram, if one allows nested and possibly heterogeneous structure
within entities.

1 INTRODUCTION
Open-access data is multiplying over the Internet. This leads on
one hand, to the development of new businesses, economic op-
portunities and applications, and on the other hand, to circulating
knowledge on a variety of topics, from health to education, envi-
ronment, the arts, or leisure activities.

Many of the openly available datasets follow the RDF standard,
the W3C’s recommendation for sharing data. The Linked Open Data
Cloud portal lists thousands of such datasets containing, e.g., na-
tional or worldwide statistics, music, scientific bibliographies, and
many other interesting, open RDF graphs are not listed there. How-
ever, Open Data sets are not (or not only) RDF, as demonstrated by
the following examples of very popular open datasets. (𝑖) CSV files
(each of which can be seen as a table) are shared on machine learn-
ing portals such as Kaggle or the French public portal data.gouv.fr;
The French national transparency database HATVP (Haute Autorité
pour la Transparence de la Vie Publique) also publishes CSV files;
(𝑖𝑖) relational databases, comprising several interrelated tables, are
used e.g. to disseminate the DBLP bibliographic data; (𝑖𝑖𝑖) XML is
the format used in hundreds of million of bibliographic notices, e.g.,
on PubMed; DBLP and HATVP data has also been shared as XML;
(𝑖𝑣) JSON has become more recently the format of choice, used e.g.
to describe the complete activity of the French parliament on the
websites NosDeputes.fr and NosSenateurs.fr; (𝑣) property graphs,
such as pioneered by Neo4J, are oriented graphs whose nodes and
edges may have labels and properties; this is the format used by
the International Consortium of Investigative Journalists to share
their investigative data.

Decades of data management research have shown that no new
data model completely replaces the previous ones. Some models

thought obsolete re-surface under new incarnations (think of nested
relations vs. document stores, or object databases vs. property
graphs). The model in which a dataset is produced or exported is
decided by the producers, depending on what they understand/are
familiar with, the system at their disposal for storing the data, and
the needs of foreseeable data users. The Open Data exchange sce-
narios, where data users often lack any institutional connection to
the producers, also forces users to cope with the data as it is, since
the producers have no incentive (and, often, lack the resources) to
restructure the data in a different format. Thus, we believe the data
model variety in Open Data is here to stay.

Users who must decide whether to use a dataset in an appli-
cation need to have a basic understanding of its content and
the suitability to their need. Towards this goal, schemas may be
available to describe the data structure, and/or documentation (text)
may describe its content in natural language. As help to the users,
schemas and documentations have some limitations: (𝑎) schemas are
often unavailable, especially for semistructured data formats such
as JSON, RDF or XML and documentation is also often unavailable
or too terse to inform the users; (𝑏) even when they are published,
or built by automated tools, e.g., [3–5, 7–9, 15], schemas are not
helpful (or too complex) for casual users, who ignore what an “XML
element”, “JSON array” or “RDF property node” is; (𝑐) schemas as
well as documentation describe the data according to the data pro-
ducer’s terminology, not according to the consumer’s. For instance,
in the XML dataset at the left in Figure 1, a public library labels
its data entries item, with the implicit knowledge that these are
documents (books, magazines, etc.), whereas from the users’ per-
spective, this dataset describes books; (𝑑) by design, schemas do not
quantitatively reflect the data, whereas such information could be
very useful as a first insight on the data.
Towards a data model-independent dataset abstraction To fa-
cilitate the understanding of a dataset by a user, we compute a
compact description of the data, focusing on its most frequent content,
free of data model-specific syntactic details, and formulated in terms
that interest the user. To that end, we develop a single, integrated
method, applicable to any of the data models mentioned above. For
example, given any of the four bibliographic datasets in Figure 1
and a user interested in “books”, our approach would correctly
identify the dataset as pertinent for the user’s question. As we will
explain, users can specify their categories of interest through a few
hints; with the help of popular knowledge bases, our system makes
suggestions to enlarge the set of hints and improve the chances of
users to have an accurate description of their dataset. We proceed
in several steps, which also organize the remainder of the paper.

(1.) We view each dataset as holding records, that is: objects
with some internal structure, simple or complex, representing a
concept or an object. For instance, the XML data in Figure 1 contains
two item records. Further, we identify collections grouping similar
records (some records may belong to no collection). For instance, the

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 1: Motivating example: four bibliographic datasets, which we view as Collections of CreativeWork records.

bibliography XML element in Figure 1 can be seen as a collection
which holds the two records. We identify a set of requirements
for our record and collection detection method, and formalize the
problem of deriving them automatically from a dataset (Section 2).
(2.) We propose an algorithm for automatically identifying records
and collections in a dataset of any of the supported data models
(Section 3). For instance, given the XML dataset in Figure 1 (or the
relational or the RDF dataset from the same figure), our algorithm
understands it as a collection of records, each record corresponding
to a book, and having: a title, a collection of authors, and possibly
a collection of reviews.

(3.) We separate collections of Entities from collections of Rela-
tionships, in order to report to the user a structured, meaningful
dataset abstraction (Section 4).

(4.) To help users understand the data in their terms, we attempt
to assign each collection to a category from a predefined category
set, derived from a general-purpose ontology and/or based on the
concepts of interest to users, e.g., books in the above example. We
provide a method for our tool to enrich its knowledge of user-
specified categories by selectively gathering information from large
online knowledge bases. For instance, our algorithm classifies the
bibliography in Figure 1 as a Creative Work (a Schema.org standard
type including books, paintings, songs, etc.)
Deployment scenarios On one hand, data producers can use our
tool to automatically derive data abstractions, to be shared next to
the data; on the other hand, users can generate the abstractions
after downloading a candidate dataset, in order to assess its interest.

2 PROBLEM STATEMENT
We start by analyzing a set of requirements of our problem, then
formally state it.

2.1 Requirements
R1: data model-independent abstractions Our method needs to
go beyond the syntactic details to extract semantically meaningful
records, and understand if they are organized in collections.
R2: structurally rich abstractions The above problem can be
seen as reverse-engineering a dataset to identify its conceptual
model, which generalizes the classic Entity-Relationship model
behind relational databases [13] by allowing records to have multi-
valued attributes, and to contain other records and/or collections. For
instance, the second item record in Figure 1 contains a collection of
review records. A collection uniformly represents a list, set or bag
of records: data order and possible duplicates are not reflected in
our abstractions.
R3: see beyond the data structures In some cases, data syntax
features are insufficient to distinguish records from collections. On
one hand, in some data models, such as XML or RDF, the syntax does
not distinguish them, e.g., in XMark [14] benchmark documents,
an ⟨open_auctions⟩ element is clearly a collection, while a ⟨user⟩
element describes a record. On the other hand, even when the data
model distinguishes nodes that should naturally be records (e.g.,
JSON maps), from others that should be collections (e.g., JSON
arrays), we should not make this decision purely based on syntax.
Indeed, as also noted in [15], a short array could in fact designate an
object (e.g., three coordinates describe a geographical point), while
a map may be used to encode a list, e.g., with attributes named “1”,
“2”, “3” etc., as in Le Monde’s Decodex dataset. It is important that
such variations in data design do not confuse our abstraction.
R4: implicit or explicit collections In our motivating example,
each of the four datasets holds a collection of books. In XML, the
collection is explicit (materialized by the ⟨bibliography⟩ node), and
the table node labeled Book plays the same role in the relational
dataset. In contrast, in the RDF dataset, there is no common parent

Toward Generic Abstractions for Data of Any Model

of the books; we say that the collection here is implicit. Note that
whether collections are explicit or implicit does not depend on the
original data model: in the relational dataset, the book collection is
explicit but the review collection is not; similarly, the RDF dataset
could have included a common parent to all the book nodes, which
would have made that collection explicit.
Availability and role of schemas and types What schema in-
formation can we expect to have, and how should we treat it?

Relational databases always have a schema, describing elemen-
tary data types, the attributes of each table, and possible integrity
constraints. In a CSV file, the number of attributes can be identi-
fied easily; their names may or may not be present, and data types
are not explicitly declared; data profiling [1] is needed to infer
their domains. XML documents may or may not have a schema,
expressed as a Document Type Description (DTD) or XML Schema
Description (XSD); these specify the allowed children for each type
of element. JSON documents usually come without a schema, but
recent methods [3, 4, 15] derive schemas from the documents. RDF
graphs are often schemaless, but they may be endowed with: (𝑖) an
ontology, expressed in RDF Schema or OWL, describing relation-
ships between the types and properties present in the graph, e.g.,
Any Student is a Person, or Anyone taking a Course is a Student;
(𝑖𝑖) a SHACL (Shapes Constraint Language) schema specification,
against which a graph may be validated or not. Schemas for property
graphs are being investigated actively [10], although no standard
has emerged yet.

Data types are basic components of schemas. When present,
types encapsulate valuable insights into the data organization and
semantics. Thus, we formulate the following requirement:
R5: explicit types When available, types should guide our identifi-
cation of records and collections, even though our approach should
not depend on them.

2.2 Abstraction approach
To satisfy requirement R1, we leverage the ConnectionLens sys-
tem [2, 6] which models the information from any relational data-
base, CSV, XML, JSON, RDF document, or property graph, as a
graph 𝐺 = (𝑁 , 𝐸) where 𝐸 ⊆ 𝑁 × 𝑁 is a set of directed edges, and
𝜆𝑁 , 𝜆𝐸 are two functions labeling each node (respectively, edge)
with a label (a string), that could in particular be 𝜖 (the empty label).
Figure 2 illustrates this; for now, just focus on the nodes and edges
(their colors and the shaded areas will be explained later).

Relational data A relational table or a CSV dataset can be seen
as a set of tuples, each with the same attributes. This can be turned
into a graph by modeling the dataset as a table node, having one
child tuple node for each tuple (or line in the CSV file); this child
has one child attribute node for each attribute. Figure 2 illustrates
this for the sample relational dataset in Figure 1. Following R5, we
leverage foreign key constraints expressed in a relational database
schema as follows. Whenever relation 𝑆 includes a foreign key to
relation 𝑅, an edge is created leading from each tuple in 𝑆, to the
respective 𝑅 tuple node.

XML and JSON data are naturally converted into trees.
RDF data Each triple (𝑠, 𝑝, 𝑜) from an RDF graph is converted
into an edge between the (single) node labeled 𝑠 to the (single) node
labeled 𝑜; the edge is labeled 𝑝. We denote by 𝜏 the special RDF type

property used to explicitly connect an URI to its type (which is also
a node in the graph).

Property graphs (PG) In this rich, directed graph data model:
(𝑖) each node may have a set of attributes with a name and a value;
(𝑖𝑖) each edge can similarly have attributes; (𝑖𝑖𝑖) zero or more labels
may be attached to each node and/or edge, playing roughly the
role of a type. In our graphs, each node/edge has at most one label.
Therefore, we transform a property graph as follows. Each PG node
becomes a node 𝑛 ∈ 𝑁 , labeled 𝜖. Each label 𝑙 of a PG node 𝑛
𝜏
becomes an edge 𝑛
−→ 𝑛𝑙 ∈ 𝐸, where 𝑛𝑙 is a leaf node labeled 𝑙 and
𝜏 is the RDF type property mentioned above. Each attribute of 𝑛,
𝑎
named 𝑎 and whose value is 𝑏, becomes an edge 𝑛
−→ 𝑛𝑏 ∈ 𝐸 where
𝑛𝑏 is a leaf node labeled 𝑏. Each PG edge 𝑒 is turned into a node
𝑛𝑒 , labeled 𝜖, plus two edges, connecting it to its source and target
nodes in the original PG; 𝑛𝑒 also has outgoing edges modeling the
attributes of 𝑒, similarly to PG nodes.

Extracted entities The graph built by ConnectionLens out of
any dataset is enriched through entity extraction, applied on each
value (string, leaf) node present in the dataset [2]. In our example,
Alice, Bob, Carole and David are recognized as Person entities.
The set 𝑇𝐸 of entity types also includes: Location, Organization,
Date, URIs, emails, hashtags, etc. Entities will be used to categorize
collections (Section 5).
Problem statement Given the graph 𝐺 = (𝑁 , 𝐸) obtained as
above, our goal is to:

(1) Identify records and collections, that is: find (𝑖) a set R ⊆ 𝑁 of
nodes which we call records; (𝑖𝑖) a set of sets C = {𝐶1, 𝐶2, . . .},
where each 𝐶𝑖 is a set of elements from R, the 𝐶𝑖 ’s are pair-
wise disjoint, and for each 𝐶𝑖 , there may exist a node 𝑛𝐶𝑖
such that (𝑛𝐶𝑖 , 𝑟 𝑗
𝑖 ∈ 𝐶𝑖 , in other words:
𝑛𝐶𝑖 is a parent (in 𝐺) of all the nodes from 𝐶𝑖 ; (𝑖𝑖𝑖) for each
𝑟 ∈ R, a set of nodes and edges of 𝐺 which we view as part
of the record 𝑟 .

𝑖 ) ∈ 𝐸 for every 𝑟 𝑗

(2) Separate the collections in C into C𝐸 and C𝑅, respectively
collections of entities and collections of relationships.
(3) Classify the collections of entities: given a set of categories,
and a set of hints (see Section 5), assign to each collection
the category it is closest to (or none if does not fit the given
categories).

The nodes 𝑁 \ R \ {𝑛𝐶𝑖

| 𝐶𝑖 ∈ C} which are neither records nor
collection nodes are called sub-records and their set is denoted SR.
Together with edges connecting them to each other and to a record
𝑟 , sub-records form the record content sought in (3b) below.

As shown above, for now, only collections of entities are classified
(not those of relationships). The reason not to classify relationships
is that entities (“things”) seem even easier to understand for non-IT
users, and it is easier for them to provide hints (see later) about
entities, than about relationships.

3 IDENTIFYING RECORDS AND

COLLECTIONS

We start by a convenient transformation on the graph 𝐺. Some of
its edges have labels (e.g., RDF triples, edges in JSON maps) while
others carry the empty label 𝜖. For uniformity, we transform 𝐺 into
𝑙
an unlabeled graph 𝐺 ′, replacing each labeled edge 𝑛1
−→ 𝑛2 with

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 2: Graph representations of the four bibliographic datasets shown in Figure 1.

an intermediary node labeled 𝑙, and connected to 𝑛1, 𝑛2 as follows:
𝜖
−→ 𝑛2. From now on, we will work on 𝐺 ′, whose nodes
𝑛1
and edges will be simply denoted as (𝑁 ′, 𝐸 ′). Then:

𝜖
−→ 𝑛𝑙

(1) First, we compute a quotient summary of 𝐺 ′, that is: we
identify a partition P = {𝑁𝑖 }𝑖 of its nodes 𝑁 ′, such that
(cid:208)𝑖 𝑁𝑖 = 𝑁 ′ and the 𝑁𝑖 are pairwise disjont. We say the
nodes from a given set 𝑁𝑖 are equivalent, and call 𝑁𝑖 an
equivalence class. Then, the quotient summary of 𝐺 ′ is a
graph whose nodes are the equivalence classes, and such
that whenever 𝐺 ′ contains an edge 𝑛1 → 𝑛2, its summary
contains the edge 𝐸𝐶 (𝑛1) → 𝐸𝐶 (𝑛2), where 𝐸𝐶 (𝑛𝑖 ) denotes
the equivalence class of 𝑛𝑖 for 𝑖 ∈ {1, 2}. Many quotient sum-
marization techniques have been proposed [5]; we discuss
some of them below. Note that while the quotient summary
guides the abstraction, it may still differ from it quite substan-
tially: a set 𝑁𝑖 may contain records from multiple collections;
it may contain an explicit collection node; finally, it may
contain nodes which turn out to be in SR.

(2) Next, we compute the signature of each equivalence class, i.e.
an object reflecting the entities extracted out of the nodes
of that equivalence class. This signature holds such statis-
tics for every entity type in 𝑇𝐸 (Section 2.2). For instance,
given the XML document presented in Figure 2, the signa-
ture of the equivalence class representing the nodes ⟨author⟩
is: {"total_length":14, "PERSON": {"occurrences":3,
"extracted_length":14}} because the three authors have
been recognised as Person entities.

(3) We consider that each 𝑁𝑖 is a union of one or more collections,
plus possibly (more rarely) a few records. To separate these,
we proceed as follows:
(a) We cluster the nodes in each 𝑁𝑖 according to their structure.
(i) We transform 𝑁𝑖 into a set of transactions D, by turning
each node into a transaction, whose item set is the set
of labels of the node’s non-leaf children. Thus, the first
book from the XML bibliography in Figure 2 has the
items title, id, reviews and authors while the second
has title, id and authors.

(ii) Given an item set 𝑋 ∈ D, we denote by 𝑆 (𝑋 ) the sup-
port of 𝑋 , defined by 𝑆 (𝑋 ) = |{𝑌 ∈ D | 𝑋 ⊆ 𝑌 }|. We
also define the transferred support of an item set 𝑋 , de-
noted 𝑇 (𝑋 ), based on the set of itemsets Y obtained as
follows. Y is initialized with all the proper subsets of 𝑋
(not 𝑋 and not the empty set) that appear in D. Then,
we traverse Y in the decreasing order of the itemset
size, and remove all subsets of 𝑌 from Y. For a given
𝑋 and Y, the transferred support 𝑇 (𝑋 ) is defined as:
𝑇 (𝑋 ) = 𝑆 (𝑋 ) + (cid:205)𝑌 |𝑌 ∈Y 𝑇 (𝑌 ).
Our clustering algorithm starts by computing 𝑆 (𝑋 ) and
𝑇 (𝑋 ) for each 𝑋 in D. Then, it proceeds in a greedy
manner, choosing the 𝑋 ∈ D with the highest value
of 𝑇 (𝑋 ), and creating a collection 𝑐 containing 𝑋 and
all the D transactions whose items are all included in
those of 𝑋 . We then remove 𝑐 and the previously se-
lected transactions from D and repeat the procedure.
Each 𝑐 of more than 𝑡 elements (where 𝑡 is a threshold,

Toward Generic Abstractions for Data of Any Model

e.g., 𝑡 = 2) is considered a collection; if the 𝑐 elements
have a common parent, that becomes the collection
node, otherwise, the collection is implicit. For each 𝑐,
every child 𝑟 ∈ 𝑐 is considered a record, part of 𝑐. The
elements of 𝑐 that are not considered collections nor
records are considered sub-records. For instance, in
Figure 2, the XML sample describes 4 collections (blue
nodes): a ⟨bibliography⟩ containing ⟨item⟩ records (or-
ange nodes), two sets of ⟨authors⟩ containing ⟨author⟩
records and a set of ⟨reviews⟩ containing ⟨review⟩ records.
The sub-records are the green nodes.

(b) For each record 𝑟 ∈ R, we build its content, i.e. a directed
acyclic graph (DAG) 𝑑𝑟 by following edges outgoing from
𝑟 , until we reach leaf nodes, or another record node 𝑟 ′,
or a collection node. The content of each record is repre-
sented by a light yellow box in Figure 2. For instance, in
the XML bibliography, the second ⟨book⟩ record includes
the ⟨title⟩, the two ⟨author⟩, and the two ⟨review⟩. Similarly,
for the RDF example, the second ⟨book⟩ record consists
of its ⟨title⟩, plus few ⟨authors⟩ and ⟨reviews⟩. For the re-
lational database, the ⟨book⟩ record consists of the ⟨title⟩
and the ⟨id⟩ of the book. Note that the content of a record
is extensive using transitivity (e.g. the ⟨author⟩ collection
is part of the ⟨book⟩).

In (1), knowledge about possible node types should be injected
in P, thus satisfying R5. R2 is met by including in a record many
nodes reachable from it, in step (3b). Finally, step (3a) satisfies R3
by operating on the graph content (not on the original syntax), and
R4 by detecting both implicit and explicit collections.

We now discuss possible choices for the quotient summary tech-
nique. The most general method, applicable to arbitrary graphs,
consists of building a quotient graph summary [5], such as those
described in [8] which can be built in linear time in the input size. In
particular, a type-first quotient summary [8] uses type information
when available to group nodes by their set of most general types
(an RDF node may have several types, and a PG node may have
several labels), while partitioning untyped nodes according to their
incoming and outgoing nodes. Alternatively, in the particular case
of tree data models, such as XML or JSON, P may be a Dataguide [9],
which can also be constructed in linear time in the size of the input.

4 DISTINGUISHING ENTITIES FROM

RELATIONSHIPS

We now analyse the collections to separate C into C𝐸 , the set of
entity collections, from C𝑅, the set of relationships collection. For
instance, the ⟨authors⟩ node in the XML data describes entities while
the ⟨wrote⟩ nodes in the property graph describe relationships.

We say a collection 𝑐 is in C𝑅 iff there exist two collections 𝑎
and 𝑏 such that: ∀𝑟𝑐 ∈ 𝑐, ∃!𝑟𝑎 ∈ 𝑎, ∃!𝑟𝑏 ∈ 𝑏 such that 𝑟𝑎, 𝑟𝑐 are
connected by an edge in 𝐸 ′ and similarly 𝑟𝑏, 𝑟𝑐 are connected by an
𝐸 ′ edge. If this is not the case, we consider that 𝑐 is a collection of
entities. For instance, in the property graph in Figure 2, each record
in the implicit collection ⟨wrote⟩ links one ⟨author⟩ and one ⟨book⟩,
therefore the collection ⟨wrote⟩ contains relationships; the ⟨author⟩
and ⟨book⟩ collections are collections of entities. This check can be
sped up by exploiting connection statistics that can be gathered
while computing the quotient summary of 𝐺 ′.

5 CLASSIFYING COLLECTIONS
Our next step is to classify each collection in C𝐸 into a given set
K of categories of interest to the user, or Other if no category
is pertinent. In our example, we consider the categories Person,
Organization, Location, Event and Creative Work. As input to the
classification process, we are also given a set of hints H . A hint ℎ ∈
H is a tuple (𝐴, 𝑙, 𝐵) where 𝐴 ⊆ K, 𝑙 is a label and 𝐵 is a signature
pattern, which is matched (satisfied) by an individual signature, or
not. Such a hint states that a node which has a child labeled 𝑙 and
its signature matches 𝐵, should be classified as one of the types in 𝐴.
For instance, the hint ({𝑂𝑟𝑔𝑎𝑛𝑖𝑧𝑎𝑡𝑖𝑜𝑛}, ℎ𝑎𝑠𝐶𝐸𝑂, {𝑃𝑒𝑟𝑠𝑜𝑛}) states
that a record having a property hasCEO, whose signature matches
Person, should be assigned to the category Organization.

5.1 Classification algorithm
Algorithm 1 details our classification.

(1) For each record 𝑟 ∈ 𝑐, we initialize K𝑟 , a multiset of candidate
categories for 𝑟 , and a vector of scores of 𝑟 for each hint. K𝑟 is
a multiset to accomodate the possibility that a given category
may be suggested by several hints.

(2) If the record 𝑟 has a label semantically close to one of the cat-
egories in K, this category is stored as a candidate category
in K𝑟 , and the similarity score recorded in scores.

(3) For each child 𝑛𝑐 of the record 𝑟 , we create a pair 𝜋 containing

the label of 𝑛𝑐 and the signature of 𝑛𝑐.

(4) Next, we compute the similarity of 𝜋 with each hint ℎ in
H according to Equation 1. This equation gives the similar-
ity between a node and a hint, based on the label and the
signature of both elements:

𝑠𝑖𝑚(𝜋, ℎ) = 𝑠𝑖𝑔_𝑠𝑖𝑚(𝜋 .𝑆, ℎ.𝐵)

(cid:213)

+

𝑘𝑖 ∈𝐾1,𝑘 𝑗 ∈𝐾2

𝑐𝑜𝑠𝑖𝑛𝑒_𝑠𝑖𝑚(𝑉 (𝑘𝑖 ), 𝑉 (𝑘 𝑗 ))

(1)

where 𝐾1 is the set of keywords present in 𝜋 .𝑙𝑎𝑏𝑒𝑙, 𝐾2 is
the set of keywords present in ℎ.𝑙, and 𝑠𝑖𝑔_𝑠𝑖𝑚(𝜋 .𝑆, ℎ.𝐵)
is the similarity between the individual signature 𝜋 .𝑆 and
the signature pattern ℎ.𝐵, 𝑉 (𝑘𝑖 ) is a vector representation
(embedding) of keyword 𝑘𝑖 in a multidimensional space. Such
embeddings enable detecting that a node labeled writer is
close to a hint labeled author, even if they are different words.
We currently use the Word2Vec model [11] for this task.
(5) For each 𝜋, we choose the hint ℎ leading to the highest
similarity score for 𝜋. Each category indicated by the domain
of ℎ is added to K𝑟 .

(6) We classify the record 𝑟 in the category that is the most
frequent in K𝑟 , if one category is more frequent than 50%;
otherwise, we classify it as Other.

(7) Finally, we classify the collection 𝑐 in the most popular cate-

gory among its records.

5.2 Constructing hints
The classification of the collections depends on the quality of hints
provided as input. Users looking for a concept, e.g., creative work,
may have in mind a few properties that creative works have, such
as title or author, and may provide them as hints. Our approach
works better if there are many hints, in order to obtain a decisive
category vote. To overcome burdening human experts, we use

Algorithm 1: Classifying a collection 𝑐
Input: a collection 𝑐, hints H , categories K

1 foreach 𝑟 ∈ C do
K𝑟 ← ∅
2
scores ← ∅
foreach 𝑘 ∈ K do

4

3

5

6

7

8

9

10

11

12

if the similarity between 𝑘 and the label of 𝑟 is higher
than a threshold then
K𝑟 ← K𝑟 ∪ {𝑘 }

foreach 𝑛𝑐 ∈ 𝑟 .children do

𝜋 ← (𝑛𝑐.𝑙𝑎𝑏𝑒𝑙, 𝑛𝑐.𝑠𝑖𝑔𝑛𝑎𝑡𝑢𝑟𝑒)
foreach ℎ ∈ H do

scores ← scores (cid:208)(ℎ, 𝑠𝑖𝑚(𝜋, ℎ))

bestHint ← argmax(scores)
K𝑟 ← K𝑟 ∪ bestHint.domain

Classify 𝑟 in the most frequent 𝑘 ∈ K𝑟 , or Other
13
14 Classify 𝑐 with the most frequent category of its records

knowledge bases like Wikidata [16] and Yago [12] to enhance an
existing set of hints, as follows.

A knowledge base 𝐾𝐵 consists of triples of the form ⟨a,r,b⟩,
where r is the relationship between the entities a and b. For example,
the triple ⟨Albert Wessels, spouse, Elisabeth Eybers⟩ states that
Albert Wessels’ spouse is Elisabeth Eybers. It also makes statements
about the entity type, such as ⟨Albert Wessels, type, Person⟩,
from which we can obtain the category an entity belongs to.

Let 𝑘 ∈ K be the category we want to enhance hints for and 𝐾𝐵
be the knowledge base. The set of properties 𝑃𝑘 that are likely to
be associated with 𝑘 can be acquired using the following equation:

𝑃𝑘 = {𝑟 | ⟨a,r,b⟩ ∈ 𝐾𝐵 ∧ ⟨a,type,k⟩ ∈ 𝐾𝐵}
(2)
For example, the triple ⟨Albert Wessels, type, Person⟩ exists in
YAGO, and Albert Wessels participates in triples such as ⟨Albert
Wessels, nationality, South Africa⟩ and ⟨Albert Wessels, spouse,
Elisabeth Eybers⟩ among many others. So, nationality and spouse
would be added to the set 𝑃𝑃𝑒𝑟𝑠𝑜𝑛.

The acquired set of properties may contain some inaccurate
information, e.g., for the category Organization, some properties
such as date of birth were retrieved. This happens because knowl-
edge bases such as Wikidata are collaboratively created, which can
lead to errors, as evident from the triples ⟨Steven Shankman, type,
Organization⟩ and ⟨Steven Shankman, date of birth, 1947⟩. We
avoid such errors by scoring each property in the set 𝑃𝑘 , i.e. errors
such as the one reported above will lead to a low score. Formally:
Category score. This score is set proportional to the number
of instances of the category the property was participating with.
𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝, 𝑘) for each 𝑝 ∈ 𝑃𝑘 is computed as follows:

𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝, 𝑘) = |{𝑎 | ⟨a,p,b⟩ ∈ 𝐾𝐵 ∧ ⟨a,type,k⟩ ∈ 𝐾𝐵}|

(3)

While this prunes away errors, we might still get some properties
which are common for all the categories in K, thus are not very
useful in distinguishing between these categories. For example, the
property Google Knowledge Graph ID appears for all the different
categories. We introduce another score to penalize such properties:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Inverse category score. This score quantifies how unique a prop-
erty is for distinguishing between categories. Let 𝑃𝑎𝑙𝑙 = {𝑃𝑘1
, . . . ,
𝑃𝑘|K | } be the set of sets of properties retrieved for each category
𝑘𝑖 ∈ K according to Equation 2. The inverse category score of a
property 𝑝 is computed as follows:
𝑖𝑣𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) = 𝑙𝑜𝑔

1 + |K |
1 + |{𝑃 | 𝑝 ∈ 𝑃, 𝑃 ∈ 𝑃𝑎𝑙𝑙 }|

, 𝑃𝑘2

(4)

(cid:18)

(cid:19)

, 𝑃𝑘2

A property 𝑝 that appears in all sets 𝑃𝑘𝑖
, ∀𝑖 ∈ 1, 2, ..., |K | will get
𝑖𝑣𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) = 0 since 𝑙𝑜𝑔(1) = 0. A property 𝑝 which appears in
only one of the sets 𝑃𝑘1

, ..., 𝑃𝑘|K | will get the highest score.

Total score. The total score of a property is given by a product of
category score and inverse category score, as shown by Equation 5.
𝑡𝑜𝑡_𝑠𝑐𝑜𝑟𝑒 (𝑝) = 𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) ∗ 𝑖𝑣𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) + 1
(5)
We score the properties in decreasing order of 𝑡𝑜𝑡_𝑠𝑐𝑜𝑟𝑒 and
retain the top-𝑧 𝑃𝑘𝑧 properties for each category 𝑘. For each 𝑘 ∈ K
and 𝑝 ∈ 𝑃𝑘𝑧 , we create a hint ({𝐴}, 𝑝, 𝑟𝑎𝑛𝑔𝑒 (𝑝)), where 𝐴 = {𝑘 :
𝑝 ∈ 𝑃𝑘𝑧 } and 𝑟𝑎𝑛𝑔𝑒 (𝑝) is the range of the property 𝑝 if available
from 𝐾𝐵, and ∅ if not present. We are currently working to make
the signature patterns 𝑟𝑎𝑛𝑔𝑒 (𝑝) more specific, by exploiting the
knowledge the 𝐾𝐵 may have about the domain of the property 𝑝.

6 CONCLUSION AND PERSPECTIVES
The approach described above aims at producing expressive abstrac-
tion of datasets organized in a variety of data models. This goes
through their transformation in graphs, identifying records and col-
lections, analyzing and classifying collections. The implementation
and fine-tuning of our system is ongoing.
Acknowledgments. This work is funded by DIM RFSI PHD 2020-
01 and AI Chair SourcesSay project (ANR-20-CHIA-0015-01) grants.

REFERENCES
[1] Z. Abedjan, L. Golab, F. Naumann, and T. Papenbrock. Data Profiling. Synthesis

Lectures on Data Management. Morgan & Claypool Publishers, 2018.

[2] A. C. Anadiotis, O. Balalau, C. Conceicao, H. Galhardas, M. Y. Haddad,
I. Manolescu, T. Merabti, and J. You. Graph integration of structured, semistruc-
tured and unstructured data for data journalism. Information Systems, July 2021.
[3] M. A. Baazizi, C. Berti, D. Colazzo, G. Ghelli, and C. Sartiani. Human-in-the-loop

schema inference for massive JSON datasets. In EDBT, 2020.

[4] M. A. Baazizi, D. Colazzo, G. Ghelli, and C. Sartiani. Parametric schema inference

for massive JSON datasets. VLDB J., 28(4), 2019.

[5] S. Cebiric, F. Goasdoué, H. Kondylakis, D. Kotzinos, I. Manolescu, G. Troullinou,
and M. Zneika. Summarizing Semantic Graphs: A Survey. The VLDB Journal,
28(3), June 2019.

[6] C. Chanial, R. Dziri, H. Galhardas, J. Leblay, M. L. Nguyen, and I. Manolescu.
ConnectionLens: Finding connections across heterogeneous data sources (demon-
stration). PVLDB, 11(12), 2018.

[7] D. Colazzo, G. Ghelli, and C. Sartiani. Schemas for safe and efficient XML

processing. In ICDE. IEEE Computer Society, 2011.

[8] F. Goasdoué, P. Guzewicz, and I. Manolescu. RDF graph summarization for

first-sight structure discovery. The VLDB Journal, 29(5), Apr. 2020.

[9] R. Goldman and J. Widom. Dataguides: Enabling query formulation and opti-

mization in semistructured databases. In VLDB, 1997.

[10] H. Lbath, A. Bonifati, and R. Harmer. Schema inference for property graphs. In

EDBT. OpenProceedings.org, 2021.

[11] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed represen-

tations of words and phrases and their compositionality. In NIPS, 2013.

[12] T. Pellissier Tanon, G. Weikum, and F. Suchanek. Yago 4: A reason-able knowledge

base. In ESWC, 2020.

[13] R. Ramakhrishnan and J. Gehrke. Database Management Systems (3rd edition).

McGraw-Hill, 2003.

[14] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse. Xmark:

A benchmark for XML data management. In PVLDB, 2002.

[15] W. Spoth, O. A. Kennedy, Y. Lu, B. Hammerschmidt, and Z. H. Liu. Reducing

ambiguity in JSON schema discovery. In SIGMOD, 2021.

[16] D. Vrandečić and M. Krötzsch. Wikidata: A free collaborative knowledgebase.

Commun. ACM, 57(10), Sept. 2014.

