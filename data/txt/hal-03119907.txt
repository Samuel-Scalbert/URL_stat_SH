Guided Attentive Feature Fusion for Multispectral
Pedestrian Detection
Heng Zhang, Elisa Fromont, Sébastien Lefèvre, Bruno Avignon

To cite this version:

Heng Zhang, Elisa Fromont, Sébastien Lefèvre, Bruno Avignon. Guided Attentive Feature Fusion
for Multispectral Pedestrian Detection. WACV 2021 - IEEE Winter Conference on Applications of
Computer Vision, Jan 2021, Waikoloa /Virtual, United States. pp.1-9. ￿hal-03119907￿

HAL Id: hal-03119907

https://hal.science/hal-03119907

Submitted on 25 Jan 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Guided Attentive Feature Fusion for Multispectral Pedestrian Detection

Heng ZHANG1,3, Elisa FROMONT1,4, S´ebastien LEFEVRE2 and Bruno AVIGNON3

1Univ Rennes 1, IRISA, France

3ATERMES company, France

2Univ Bretagne Sud, IRISA, France
4IUF, Inria.

Abstract

Multispectral image pairs can provide complementary
visual information, making pedestrian detection systems
more robust and reliable. To beneﬁt from both RGB and
thermal IR modalities, we introduce a novel attentive mul-
tispectral feature fusion approach. Under the guidance of
the inter- and intra-modality attention modules, our deep
learning architecture learns to dynamically weigh and fuse
the multispectral features. Experiments on two public multi-
spectral object detection datasets demonstrate that the pro-
posed approach signiﬁcantly improves the detection accu-
racy at a low computation cost.

1. Introduction

Real world pedestrian detection applications require
accurate detection performance under various conditions,
such as darkness, rain, fog, etc. In these conditions, it is dif-
ﬁcult to perform precise detection using only standard RGB
cameras. Instead, multispectral systems try to combine the
information coming from e.g. thermal and visible cameras
to improve the reliability of the detections.

Deep learning-based methods, more speciﬁcally, two-
stream convolutional neural networks, nowadays largely
dominate the ﬁeld of multispectral pedestrian detection
[6, 9, 10, 11, 14, 18, 19, 20]. As illustrated in Fig. 1, a
typical two-stream pedestrian detection network consists of
two separate spectra-speciﬁc feature extraction branches, a
multispectral feature fusion module and a pedestrian detec-
tion network operating on the fused features. The system
uses some aligned thermal-visible image pairs as input and
outputs the joint detection results on each image pair.

Thermal and visible cameras have different imaging
characteristics under different conditions. As shown in
Fig. 2, visible cameras provide precise visual details (such
as color and texture) in a well-lit environment, while ther-
mal cameras are sensitive to temperature changes, which
is extremely useful for nighttime or shadow detection. An
adaptive fusion of thermal and visible features should take
such differences into account, and should identify and lever-

Figure 1: Multispectral pedestrian detection via a two-
stream convolutional neural network.

Figure 2: Typical examples of thermal-visible image pairs
captured during the day (ﬁrst two rows) and night (bottom
row). For each pair, the thermal image is on the left and the
RGB image is on the right.

age the information from the most relevant modality.

An intuitive solution to adapt the feature fusion to the
different weather and lighting conditions is to manually
identify multiple usage scenarios and design a speciﬁc so-
lution for each scenario. For example, [6] proposes an
illumination-aware network consisting of a day illumina-
tion sub-network and a night illumination sub-network. The

Joint detection resultsMultispectral image pairsPedestriandetection networkMultispectralfeature fusionThermal featureextractionVisible featureextractiondetection results from the two sub-networks are then fused
according to the prediction of the illumination context.
Such a kind of hand-crafted fusion mechanism improves
the resilience of the model to a certain extent, nonetheless,
there are still two limitations: ﬁrstly, cherry-picked scenar-
ios may not cover all conditions, e.g., different illumina-
tion/season/weather conditions; Secondly, the situation may
be completely different even in the same usage scenario,
e.g., at nighttime, lighting conditions in urban areas are dif-
ferent from those in rural areas.

In this paper, we propose a novel and fully adaptive mul-
tispectral feature fusion approach, named Guided Attentive
Feature Fusion (GAFF). By combining the intra- and inter-
modality attention modules, the proposed approach allows
the network to learn the adaptive weighing and fusion of
multispectral features. These two attention mechanisms are
guided by the prediction and comparison of the pedestrian
masks in the multispectral feature fusion stage. Speciﬁcally,
at each spatial position, thermal or visible features are en-
hanced when they are located in the area of a pedestrian
(intra-modality attention) or when they possess a higher
quality than in the other modality (inter-modality attention).
To the best of our knowledge, GAFF is the ﬁrst work that
regards the multispectral feature fusion as a sub-task in the
network optimization and that introduces a speciﬁc guid-
ance in this task to improve the multispectral pedestrian
detection. Extensive experiments on KAIST multispectral
pedestrian detection dataset [8] and FLIR ADAS dataset
[1] demonstrate that, compared with common feature fusion
methods (such as addition or concatenation), GAFF brings
important accuracy gains at a low computational cost.

This paper is organized as follows: Section 2 reviews
some representative work applying static/adaptive feature
fusion for multispectral pedestrian detection; Section 3 in-
troduces implementation details on how to integrate GAFF
into a typical two-stream convolutional neural network; In
Section 4, we evaluate our methods on two public multi-
spectral object detection datasets [8, 1], then we provide an
extensive ablation study and visualization results to discuss
the reasons of the accuracy improvements; Section 5 con-
cludes the paper.

2. Related Work

2.1. Static multispectral feature fusion

KAIST released the ﬁrst large-scale multispectral pedes-
trian detection dataset [8], which contains approximately
95k well-aligned and manually annotated thermal-visible
image pairs captured during daytime and nighttime. Some
example image pairs are shown in Fig. 2. Then [18] demon-
strated the ﬁrst application of deep learning-based solutions
in multispectral pedestrian detection. They compared the
early and late fusion architectures and found that the late

fusion architecture is superior to the early one and the tra-
ditional ACF method [4]. This late-stage fusion architec-
ture can be regarded as a prototype of a two-stream neural
network, in which multispectral features are fused through
concatenation operations. Both [14] and [9] adapted Faster
R-CNN [16] to a two-stream network architecture for multi-
spectral pedestrian detection. They compare different mul-
tispectral fusion stages and came to the conclusion that
the fusion in the middle stage outperforms the fusion in
the early or late stage. Based on this, MSDS-RCNN [10]
adopted a two-stream middle-level fusion architecture and
combined the pedestrian detection task and the semantic
segmentation task to further improve the detection accuracy.

2.2. Adaptive multispectral feature fusion

As mentioned in Section 1, thermal and visible cameras
have different imaging characteristics and the adaptive mul-
tispectral fusion can improve the resilience and the detec-
tion accuracy of the system. This has become the main
focus of the multispectral pedestrian detection research in
recent years. Both [11] and [6] use the illumination infor-
mation as a clue for the adaptive fusion: they train a separate
network to estimate the illumination value from a given im-
age pair, then [11] uses the predicted illumination value to
weigh the detection results from both the thermal and vis-
ible images. [6] uses the illumination value to weigh the
detection results from a day illumination sub-network and
a night illumination sub-network. As mentioned in the pre-
vious section, such a handcrafted weighing scheme is lim-
ited and produces sub-optimal performance. CIAN [20] ap-
plies the channel-level attention in the multispectral feature
fusion stage to model the cross-modality interaction and
weigh each feature map extracted from the different spec-
trum. This network realizes a fully adaptive fusion of ther-
mal and visible features, however, in this approach, the fu-
sion module is optimized directly while solving the pedes-
trian detection task which means that the network uses in-
formation about what (pedestrian or background) and where
(bounding box) relevant elements are in the images but it
does not use the fact that some features may contain more
relevant information than others. We believe and we show
that with these additional information (that we include in
our method through the guidance mechanism), we can im-
prove the detection precision.

3. Proposed approach

The proposed Guided Attentive Feature Fusion (GAFF),
shown in Fig. 3, takes place in the multispectral feature fu-
sion stage of a two-stream convolutional neural network.
It consists of two components: an intra-modality attention
module and an inter-modality one.

Figure 3: The overall architecture of Guided Attentive Feature Fusion (GAFF). Green, blue and purple blocks represent ther-
mal, visible and fused features. Yellow and red paths represent the intra- and inter-modality attention modules, respectively.

3.1. Intra-modality attention module

3.2. Inter-modality attention module

The intra-modality attention module aims at enhanc-
ing the thermal or visible features in a monospectral view.
Speciﬁcally, as illustrated by the yellow paths on Fig. 3,
features of an area with a pedestrian are highlighted by
multiplying the learnt features with the predicted pedes-
trian mask. Moreover, in order to avoid directly affecting
the thermal or visible features, the highlighted features are
added as a residual to enhance the mono-spectral features.
This procedure can be formalized as:

f t
intra = f t ⊗ (1 + mt
intra = f v ⊗ (1 + mv
f v

intra)
intra)

where

mt
mv

intra = σ(F t
intra = σ(F v

intra(f t))
intra(f v))

(1)

(2)

Superscripts (t or v) denote the thermal (t) or visible (v)
modality; ⊗ denotes the element-wise multiplication; σ rep-
resents the sigmoid function; Fintra represents a convolu-
tion operation to predict the intra-modality attention masks
(pedestrian masks) mintra; f and fintra represent the orig-
inal and enhanced features, respectively.

The prediction of the pedestrian masks is supervised
by the semantic segmentation loss, where the ground truth
mask (mgt
intra) is converted from the object detection anno-
tations. As illustrated in Fig. 3 the bounding box annota-
tions are transformed into some ﬁlled ellipses to approxi-
mate the shape of the true pedestrians.

Thermal and visible cameras have their own imaging
characteristics, and under certain conditions, one sensor has
superior imaging quality (i.e. is more relevant for the con-
sidered task) than the other. To leverage both modalities, we
propose the inter-modality attention module, which adap-
tively selects thermal or visible features according to the
dynamic comparison of their feature quality. Concretely,
an inter-modality attention mask is predicted based on the
combination of thermal and visible features. This predicted
mask has two values for each pixel, corresponding to the
weights for thermal and visible features (summing to 1).
This attention module is illustrated as the red paths in Fig. 3.
It can be formulated as:

f t
inter = f t ⊗ (1 + mt
inter = f v ⊗ (1 + mv
f v

inter)
inter)

where

mt

inter, mv

inter = δ(Finter([f t, f v]))

(3)

(4)

Here, δ denotes the softmax function; [·] denotes the fea-
ture concatenation operation; Finter represents a convolu-
tion operation to predict the inter-modality attention mask
minter. At each spatial position of the mask, the sum of
mt
inter equals to 1. Note that this formalization
could theoretically allow for more than two modalities to be
fuse following the same principles.

inter and mv

The inter-modality attention module allows the network
to adaptively select the most reliable modality. However, in

Thermal featuresSoftmaxfunctionConvolutionoperationFeatureconcatenationConvolutionoperationSigmoidfunctionConvolutionoperationSigmoidfunctionVisible featuresFused featuresThermal input imageVisible input imageGround truth maskIntra-modalityattention pathFeature additionoperationPedestriandetection networkMultispectralfeature fusionThermal featureextractionVisible featureextractionInter-modalityattention pathFeature multiplicationoperationorder to train this module, we should need a costly ground
truth information about the best pixel-level modality qual-
ity. Our solution to relieve the annotation cost is to assign
labels according to the prediction of the pedestrian masks
from the intra-modality attention module, i.e., we force the
network to select one modality if its intra-modality mask
prediction is better (i.e. closer to the ground truth pedes-
trian mask) than the other. Speciﬁcally, we ﬁrst calculate an
error mask for each spectrum with the following formula:

et
intra = | mt
intra = | mv
ev

intra − mgt
intra − mgt

intra |
intra |

(5)

then the label for the modality selection is deﬁned as:

mgt

inter =






1, 0
0, 1
ignored

intra − et
intra − ev

if (ev
if (et
otherwise

intra) > margin
intra) > margin

(6)
Here, | · | denotes the absolute function; eintra repre-
sents the error mask, deﬁned by the L1 distance between the
predicted intra-modality mask mintra and the ground truth
intra-modality mask mgt
intra; mgt
inter is the ground truth
mask for inter-modality attention (2 values at each mask po-
sition); margin is a hyper-parameter to be tuned.

An example of the label assignment for the inter-
modality attention mask is shown in Fig. 3.
If the intra-
modality pedestrian masks are predicted as shown in the
yellow paths, the inter-modality (weak) ground truth masks
are then deﬁned as the ones shown on the red paths, where
white, black and gray areas denote the classiﬁcation la-
bels 1,0 and ignored, respectively. Here, the thermal fea-
tures produce a better intra-modality mask prediction for
the pedestrians on the left side of the input images in
Fig. 3. Therefore, according to Eq. 6, the label for the inter-
modality mask on this area is assigned as 1,0 (1 for the ther-
mal mask and 0 for the visible mask). For regions where
the two intra-modality masks have comparable prediction
qualities (i.e., the difference between prediction errors is
smaller than the predeﬁned margin), the optimization of the
inter-modality attention mask prediction on these areas are
ignored (i.e., do not participate in the loss calculation).

3.3. Combining intra- and inter-modality attention

The intra-modality attention module enhances features
on areas with pedestrians and the inter-modality attention
module adaptively selects features from the most reliable
modality. When these two modules are combined, the fused
features are obtained by:

where

f t
hybrid = f t ⊗ (1 + mt
hybrid = f v ⊗ (1 + mv
f v

intra) ⊗ (1 + mt
intra) ⊗ (1 + mv

inter)
inter)

(8)

Here, mintra and minter are predicted intra- and inter-
modality attention masks from Eq. 2 and Eq. 4; fhybrid
represents features enhanced by both attention modules;
f f used represents the ﬁnal fused features.

As mentioned in Section 2, the optimization of the mul-
tispectral feature fusion task may not beneﬁt enough from
the sole optimization of the object detection task (as done
e.g. in [20]). In GAFF, we propose two speciﬁc feature fu-
sion losses, including the pedestrian segmentation loss for
the intra-modality attention and the modality selection loss
for the inter-modality attention, to guide the multispectral
feature fusion task. These losses are jointly optimized with
the object detection loss. The ﬁnal training loss Ltotal is:

Ltotal = Ldet + Lintra + Linter

(9)

where, Ldet, Lintra and Linter are the pedestrian de-
tection, the intra- and inter-modality attention loss, respec-
tively.

4. Experiments

In this section, we conduct experiments on KAIST Mul-
tispectral Pedestrian Detection Dataset [8] and FLIR ADAS
Dataset [1] to evaluate the effectiveness of the proposed
method. Moreover, we attempt to interpret the reasons for
improvements by visualizing the predicted attention masks.
Finally, we provide inference speed analysis on two differ-
ent target platforms.

4.1. Datasets

KAIST dataset contains 7,601 training image pairs and
2,252 pairs testing ones. Some example image pairs from
this dataset are shown in Fig. 2. [10] proposes a ”sanitized”
version of the annotations, where numerous annotation er-
rors are removed. Our experiments are conducted with the
original as well as the “sanitized” version of annotations
for fair comparisons with our competitors. We found out
that the “sanitized” annotations substantially improve the
detection accuracy for different network architectures. All
models are evaluated with the improved testing annotations
from [14] and the usual pedestrian detection metric: log-
average Miss Rate over the range of [10−2, 100] false posi-
tives per image (FPPI) under a “reasonable” setting [5], i.e.,
only pedestrians taller than 50 pixels under no or partial oc-
clusions are considered 1.

f f used =

hybrid + f v
f t

hybrid

2

(7)

1We use the evaluation code provided by [10]: https://github.com/Li-
Chengyang/MSDS-RCNN/tree/master/lib/datasets/KAISTdevkit-matlab-
wrapper

We also conduct experiments on FLIR ADAS Dataset
[1]. [19] proposed an ”aligned” version of the dataset for
multispectral object detection. This new version contains
5,142 well-aligned multispectral image pairs (4,129 pairs
for training and 1,013 pairs for testing). FLIR covers three
object categories: “person”, “car” and “bicycle”. Models
are evaluated with the usual object detection metric intro-
duced with MS-COCO[13]:
the mean Average Precision
(mAP) averaged over ten different IoU thresholds.

4.2. Implementation details

The proposed GAFF module can be included in any type
of two-stream convolutional neural networks. In these ex-
periments, we choose RetinaNet [12] as our base detector. It
is transformed into a two-stream convolutional neural net-
work by adding an additional branch for the extraction of
thermal features. A ResNet18 [7] or a VGG16 [17] net-
work is pre-trained on ImageNet [2], then adopted as our
backbone network. The input image resolution is ﬁxed to
640×512 for training and evaluation. Our baseline detector
applies the basic addition as the multispectral feature fusion
method. GAFF is implemented by adding the intra- and
inter-modality attention modules, corresponding to the yel-
low and the red branches in Fig. 3. Focal loss [12] and Bal-
anced L1 loss [15] are adopted as the classiﬁcation loss and
the bounding box regression loss to optimize the object de-
tection task. In order to introduce our speciﬁc guidance, we
adopt the DICE [3] loss as the pedestrian segmentation loss
(Lintra in Eq. 9) and the cross-entropy loss as the modality
selection loss (Linter in Eq. 9).

4.3. Ablation study

M argin

0.05
0.1
0.2

All

Miss Rate
Night
Day
6.92% 8.47% 3.68%
6.48% 8.35% 3.46%
7.47% 9.31% 4.22%

Table 1: Detection results of GAFF with different margin
values in the inter-modality attention module.

Hyper-parameter tuning. As reported in Table 1, we
conduct experiments with different margin values in the
inter-modality attention module on KAIST dataset [8] with
“sanitized” annotations. The Miss Rate scores on the
Reasonable-all, Reasonable-day and Reasonable-night sub-
sets are listed. We observe that the optimal Miss Rate is
achieved when margin = 0.1. Thus, we use margin =
0.1 for all the following experiments.

Residual attention. As mentioned in Section 3, attention
enhanced features are added as residual to avoid directly af-
fecting the thermal or visible features. We verify this choice

Residual

(cid:88)

All

Miss Rate
Day
Night
7.46% 8.88% 4.85%
6.48% 8.35% 3.46%

Table 2: Detection results of GAFF where the attention
masks are directly applied or added as residual.

by comparing in Table 2 the Miss Rate of GAFF where the
attention masks are directly applied to mono-spectral fea-
tures (fintra = f ⊗ mintra and finter = f ⊗ minter) or
added as residual (as in Eq. 1 and Eq. 3).

Necessity of attention. We compare in Tab. 3 the detec-
tion accuracy on KAIST dataset with different attention set-
tings, different backbone networks, and different annotation
settings (original and “sanitized”). When conducting exper-
iments with inter-modality but without intra-modality atten-
tion, the pedestrian masks are predicted but are not multi-
plied with the corresponding mono-spectral features. For
each backbone network or annotation setting, both intra-
and inter-modality attention modules consistently improve
the baseline detection accuracy, and their combination leads
to the lowest overall Miss Rate under all experimental set-
tings. The present ﬁndings conﬁrm the effectiveness of the
proposed guided attentive feature fusion modules.

Necessity of guidance. To explore the effect of the pro-
posed multispectral feature fusion guidance, we compare
our guided approach to one with a similar architecture as
ours but where the optimization of the speciﬁc fusion losses
(Lintra and Linter in Eq. 9) are removed from the train-
ing process, i.e., the fusion is only supervised by the ob-
ject detection loss (as done with [20]). We report in Tab. 4
the detection performance with and without guidance, un-
der different backbone networks and annotations settings.
The results conﬁrm our assumption that the object detec-
tion loss is not relevant enough for the multispectral feature
fusion task: even though the non-guided attentive fusion
module improves the baseline Miss Rate to some degree
(e.g., with the “sanitized” annotations and VGG16 back-
bone, non-guided model improves the base detector’s Miss
Rate from 9.28% to 8.38%), it could be further improved
when the speciﬁc fusion guidance is added (from 8.38% to
6.48%).

Attention mask interpretation. Fig. 4 provides the vi-
sualization results of the intra-modality, the inter-modality
and the hybrid attention masks during daytime and night-
time. For each ﬁgure, the top and bottom two rows of
images are visualization results of guided and non-guided
attentive feature fusions, respectively. We can see on the

(a) Daytime

(b) Nighttime

Figure 4: Visualization examples of attention masks on KAIST dataset. Zoom in to see details.

Backbone

GAFF

Intra.

Inter.

ResNet18

VGG16

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

All

Miss Rate
Night
Day
13.04% 13.83% 11.60%
12.13% 11.97% 11.99%
11.15% 10.68% 11.67%
10.74% 10.46% 11.10%
12.72% 11.37% 15.57%
11.78% 11.45% 12.50%
11.03% 10.99% 11.44%
10.62% 10.82% 10.14%

(a) Original annotations

Backbone

GAFF

Intra.

Inter.

ResNet18

VGG16

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

All

Miss Rate
Night
Day
9.98% 12.46% 5.29%
9.26% 11.51% 5.32%
9.29% 11.97% 5.14%
7.93%
4.33%
9.79%
9.28% 11.73% 5.17%
8.70% 11.42% 3.55%
7.73% 10.35% 2.81%
3.46%
8.35%
6.48%

(b) “Sanitized” annotations

Table 3: Ablation study of two attentive fusion modules on
KAIST dataset [8] with original (top) or “sanitized” (bot-
tom) annotations.

intra-modality attention masks that the guided attention
mechanism focuses on pedestrian areas, even though, some-
times, it is not accurate from a single mono-spectral view.
For example, the trafﬁc cone is misclassiﬁed as a pedestrian

Backbone

Guidance

ResNet18

VGG16

(cid:88)

(cid:88)

All
13.15%
10.74%
13.67%
10.62%

Miss Rate
Day
13.71%
10.46%
13.19%
10.82%

Night
11.54%
11.10%
14.51%
10.14%

(a) Original annotations

Backbone

Guidance

ResNet18

VGG16

(cid:88)

(cid:88)

All
9.05%
7.93%
8.38%
6.48%

Miss Rate
Day
10.63%
9.79%
10.39%
8.35%

Night
6.01%
4.33%
4.44%
3.46%

(b) “Sanitized” annotations

Table 4: Comparison between guided and non-guided mod-
els on KAIST dataset [8] with both annotation settings.

due to its human-like shape on the thermal image of Fig. 4a,
and the pedestrian in the middle right position is missed due
to insufﬁcient lighting on the RGB image of Fig. 4b. For
inter-modality attention masks, it appears that the guided
attentive fusion tends to select visible features on well-lit
areas (such as upside of images in Fig. 4b) and brightly
coloured areas (e.g., trafﬁc cone, road sign, speed bump, car
tail light, etc), and to select thermal features on dark areas
and uniform areas (such as sky and road). Note that these
attention preferences are automatically learnt via our inter-
modality attention guidance. On the contrary, despite the
fact that the non-guided attention mechanism brings some
accuracy improvements, the predicted attention masks are

Detection resultsIntra-modality attention maskInter-modality attention maskHybrid attention maskGuided Attentive Feature FusionNon-guided Attentive Feature FusionDetection resultsIntra-modality attention maskInter-modality attention maskHybrid attention maskGuided Attentive Feature FusionNon-guided Attentive Feature Fusionquite difﬁcult to interpret. More visualization results are
shown in Fig. 5. Besides, an interesting error case is shown
in Fig. 5c, where the pedestrian on the steps is not detected
with the guided model but detected with the non-guided
model. As mentioned earlier, GAFF selects thermal fea-
tures on uniform areas, which is intuitive since thermal cam-
eras are sensitive to temperature change and there exist few
objects on uniform areas of the thermal image. However,
in this particular case, the pedestrian is not captured on the
thermal image, which leads to the ﬁnal detection error.

Figure 6: Intra- and inter-modality attention accuracy evo-
lution during training.

Attention accuracy evolution We plot in Fig. 6 the evo-
lution of intra- and inter-modality attention accuracy during
training. Speciﬁcally, red solid and dashed lines represent
the pedestrian segmentation accuracy (via DICE score [3]
Dice = 2|A∩B|
|A|+|B| ) from thermal and visible features in intra-
modality attention module; blue line indicates the modal-
ity selection accuracy in inter-modality attention module.
From the plot, we can conclude that thermal images are gen-
erally better for recognition than visible images. This ob-
servation is consistent with our mono-spectral experiments,
where thermal-only model reaches 18.8% of Miss Rate
while visible-only model achieves 20.74% (both trained
with “sanitized” annotations). Interestingly, as the segmen-
tation accuracy increases for both images, the modality se-
lection task becomes more and more challenging. Note
that this accuracy is irrelevant at the beginning of the train-
ing, where predicted pedestrian masks are almost zero for
both thermal and visible features, thus the difference be-
tween their error masks is minor and the set of margin
makes most areas ignored for modality selection optimiza-
tion. Such mechanism avoids the “cold start” problem.

Runtime analysis
In Tab. 5 we report the total number of
learnable parameters and the average inference runtime on
two different computation platforms. Speciﬁcally, the mod-
els are implemented with Pytorch (TensorRT) framework

(a) Daytime

(b) Nighttime

(c) Error case

Figure 5: More visualization examples of attention masks
on KAIST dataset. Zoom in to see details.

Guided Attentive Feature FusionNon-guided Attentive Feature FusionDetection resultsIntra-modality attention maskInter-modality attention maskHybrid attention maskDetection resultsIntra-modality attention maskInter-modality attention maskHybrid attention maskGuided Attentive Feature FusionNon-guided Attentive Feature FusionDetection resultsIntra-modality attention maskInter-modality attention maskHybrid attention maskGuided Attentive Feature FusionNon-guided Attentive Feature Fusion01000200030004000Iterations0.00.10.20.30.40.50.60.7Dice socreThermal mask dice scoreVisible mask dice score0.40.50.60.70.80.91.0AccuracyIntra- and inter-modality attention accuracy evolutionModality selection accuracyBackbone GAFF

Param.

ResNet18

VGG16

(cid:88)

(cid:88)

23,751,725
23,765,553
31,403,053
31,430,705

Runtime

1080Ti
10.31ms
10.85ms
8.87ms
9.34ms

TX2
10.5ms
12.1ms
10.3ms
11.6ms

Table 5: Runtime on different computing platforms.

Methods

ACF+T+THOG [8]
Halfway Fusion [14]
Fusion RPN+BF [14]
IAF R-CNN [11]
IATDNN+IASS [6]
CIAN [20]
MSDS-RCNN [10]
CFR [19]
GAFF (ours)

All

Miss Rate
Night
Day
47.24% 42.44% 56.17%
26.15% 24.85% 27.59%
16.53% 16.39% 18.16%
16.22% 13.94% 18.28%
15.78% 15.08% 17.22%
14.12% 14.77% 11.13%
11.63% 10.60% 13.73%
10.05% 9.72% 10.80%
10.62% 10.82% 10.14%

Methods

Platform
ACF+T+THOG [8] MATLAB
Halfway Fusion [14]
Fusion RPN+BF [14] MATLAB

Titan X

IAF R-CNN [11]
IATDNN+IASS [6]
CIAN [20]
MSDS-RCNN [10]
CFR [19]
GAFF (ours)

Titan X
Titan X
1080Ti
Titan X
1080Ti
1080Ti

Runtime
2730ms
430ms
800ms
210ms
250ms
70ms
220ms
50ms
9.34ms

Table 7: Runtime comparisons with different methods on
KAIST dataset [8].

Backbone GAFF

ResNet18

VGG16

(cid:88)

(cid:88)

mAP
AP50
AP75
36.6% 31.9% 72.8%
37.5% 32.9% 72.9%
36.3% 30.2% 71.9%
37.3% 30.9% 72.7%

(a) Original annotations

Table 8: Detection results on FLIR dataset [1].

Methods

MSDS-RCNN [10]
CFR [19]
GAFF(ours)

All
7.49%
6.13%
6.48%

Miss Rate
Day
8.09%
7.68%
8.35%

Night
5.92%
3.19%
3.46%

(b) “Sanitized” annotations

Table 6: Detection results on KAIST dataset [8] with origi-
nal (top) or “sanitized” (bottom) annotations.

for an inference time testing on the Nvidia GTX 1080Ti
(Nvidia TX2) platform. Since GAFF only involves 3 con-
volution layers, the additional parameters and computation
cost is low, i.e., it represents less than 0.1% of additional
parameters and around 0.5ms (1.5ms) of inference time on
1080Ti (TX2). Note that the time for post-processing treat-
ments (such as Non-Maximum Suppression) is not taken
into account for the benchmarking. Our model meets the
requirement of real-time treatment on embedded devices,
which is essential for many applications.

4.4. Comparison with State-of-the-art Multispec-

tral Pedestrian Detection Methods

KAIST Dataset Tab. 6 shows the detection results of ex-
isting methods and our GAFF with the original and “sani-
tized” annotations on KAIST. It can be observed that GAFF
achieves state-of-the-art performance on this dataset (it is
slightly less accurate than CFR [19], which applies cas-
caded Fuse-and-Reﬁne blocks for sequential feature en-
hancement and needs more computation than GAFF (see

Table 7). According to Tab. 7, thanks to the lightweight de-
sign of GAFF, our model has substantial advantage in terms
of inference speed compared to e.g. [19].

FLIR Dataset Tab. 8 reports the detection results with
and without GAFF on FLIR dataset. We can observe that
the average precision is improved for all IoU thresholds
with GAFF (around 1% of mAP improvement for both
backbone networks), which shows that our method can gen-
eralize well to different types of images. For comparison,
the more costly CFR [19] reaches 72.39% of AP50 on this
dataset, whereas our best result is 72.9%.

5. Conclusion

We argue that the lack guidance is a limitation for efﬁ-
cient and effective multispectral feature fusion, and we pro-
pose Guided Attentive Feature Fusion (GAFF) to guide this
fusion process. Without hand-crafted assumptions or addi-
tional annotations, GAFF realizes a fully adaptive fusion of
thermal and visible features. Experiments on KAIST and
FLIR datasets demonstrate the effectiveness of GAFF and
the necessity of attention and guidance in the feature fu-
sion stage. We noticed that certain thermal-visible image
pairs are slightly misaligned in the above datasets, such a
problem could be more critical in real life applications. Our
future research is devoted to the development of a real-time
feature calibration module based on the predicted attention
masks from GAFF.

Conference 2016, BMVC 2016, York, UK, September 19-22,
2016, 2016.

[15] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng,
Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards bal-
anced learning for object detection. In IEEE Conference on
Computer Vision and Pattern Recognition, 2019.

[16] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
Faster R-CNN: towards real-time object detection with re-
gion proposal networks. In Advances in Neural Information
Processing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-12, 2015,
Montreal, Quebec, Canada, pages 91–99, 2015.

[17] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations, 2015.

[18] J¨org Wagner, Volker Fischer, Michael Herman, and Sven
Behnke. Multispectral pedestrian detection using deep fu-
sion convolutional neural networks. In 24th European Sym-
posium on Artiﬁcial Neural Networks, ESANN 2016, Bruges,
Belgium, April 27-29, 2016, 2016.

[19] Heng Zhang, Elisa Fromont, S´ebastien Lef`evre, and Bruno
Avignon. Multispectral Fusion for Object Detection with
Cyclic Fuse-and-Reﬁne Blocks. In ICIP 2020 - IEEE Inter-
national Conference on Image Processing, pages 1–5, Abou
Dabi, United Arab Emirates, Oct. 2020.

[20] Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang, Hong
Qiao, Kaizhu Huang, and Amir Hussain. Cross-modality
interactive attention network for multispectral pedestrian de-
tection. Information Fusion, 50:20–29, 2019.

References

ﬂir

[1] Free
ing.
adas-dataset-form/.

thermal
algorithm train-
for
https://www.flir.com/oem/adas/

dataset

[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09, 2009.

[3] Lee R. Dice. Measures of the amount of ecologic association

between species. Ecology, 26(3):297–302, 1945.

[4] Piotr Dollar, Ron Appel, Serge Belongie, and Pietro Perona.
Fast feature pyramids for object detection. IEEE Trans. Pat-
tern Anal. Mach. Intell., 36(8):1532–1545, Aug. 2014.
[5] P. Dollar, C. Wojek, B. Schiele, and P. Perona. Pedes-
trian detection: An evaluation of the state of the art. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
34(4):743–761, 2012.

[6] Dayan Guan, Yanpeng Cao, Jiangxin Yang, Yanlong Cao,
and Michael Ying Yang. Fusion of multispectral data through
illumination-aware deep neural networks for pedestrian de-
tection. Information Fusion, 50:148–157, 2019.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages
770–778, 2016.

[8] Soonmin Hwang, Jaesik Park, Namil Kim, Yukyung Choi,
and In So Kweon. Multispectral pedestrian detection:
Benchmark dataset and baselines. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2015.

[9] D. K¨onig, M. Adam, C. Jarvers, G. Layher, H. Neumann,
and M. Teutsch. Fully convolutional region proposal net-
works for multispectral person detection. In 2017 IEEE Con-
ference on Computer Vision and Pattern Recognition Work-
shops (CVPRW), pages 243–250, 2017.

[10] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang.
Multispectral pedestrian detection via simultaneous detec-
tion and segmentation. In British Machine Vision Conference
2018, BMVC 2018, Northumbria University, Newcastle, UK,
September 3-6, 2018, page 225, 2018.

[11] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang.
Illumination-aware faster R-CNN for robust multispectral
Pattern Recognition, 85:161–171,
pedestrian detection.
2019.

[12] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll´ar. Focal loss for dense object detection. In
IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017, pages 2999–3007,
2017.

[13] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zit-
nick. Microsoft COCO: Common objects in context.
In
ECCV. European Conference on Computer Vision, Septem-
ber 2014.

[14] Jingjing Liu, Shaoting Zhang, Shu Wang, and Dimitris N.
Metaxas. Multispectral deep neural networks for pedestrian
In Proceedings of the British Machine Vision
detection.

