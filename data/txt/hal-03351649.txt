Breaking Down the Invisible Wall of Informal Fallacies
in Online Discussions
Saumya Yashmohini Sahai, Oana Balalau, Roxana Horincar

To cite this version:

Saumya Yashmohini Sahai, Oana Balalau, Roxana Horincar. Breaking Down the Invisible Wall of
Informal Fallacies in Online Discussions. ACL-IJCNLP 2021 - Joint Conference of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing, Aug 2021, Online, France. ￿hal-03351649￿

HAL Id: hal-03351649

https://inria.hal.science/hal-03351649

Submitted on 22 Sep 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Breaking Down the Invisible Wall of Informal Fallacies in
Online Discussions

Saumya Yashmohini Sahai
The Ohio State University, USA
sahai.17@osu.edu

Oana Balalau
Inria, Institut Polytechnique de Paris, France
oana.balalau@inria.fr

Roxana Horincar
Thales Research & Technology, France
roxana.horincar@thalesgroup.com

Abstract

People debate on a variety of topics on online
platforms such as Reddit, or Facebook. De-
bates can be lengthy, with users exchanging
a wealth of information and opinions. How-
ever, conversations do not always go smoothly,
and users sometimes engage in unsound argu-
mentation techniques to prove a claim. These
techniques are called fallacies. Fallacies are
persuasive arguments that provide insufﬁcient
or incorrect evidence to support the claim. In
this paper, we study the most frequent falla-
cies on Reddit, and we present them using
the pragma-dialectical theory of argumenta-
tion. We construct a new annotated dataset of
fallacies, using user comments containing fal-
lacy mentions as noisy labels, and cleaning the
data via crowdsourcing. Finally, we study the
task of classifying fallacies using neural mod-
els. We ﬁnd that generally the models perform
better in the presence of conversational con-
text.We have released the data and the code
at github.com/sahaisaumya/informal_
fallacies.

1

Introduction

Argumentation plays a critical part in our lives as
it helps us make decisions and reason about the
world around us. Studies (Sanders et al., 1994)
have shown that learning how to argue increases the
ability to identify weak arguments and decreases
the tendency to use verbal aggressiveness. Fallacies
are weak arguments that seem convincing, however,
their evidence does not prove or disprove the argu-
ment’s conclusion. Fallacies are usually divided
into formal and informal, where the former can
be easily described using logical representations,
while for the latter, an analysis of the content is

Part of this work was done while the ﬁrst author was an

intern at Inria, France.

more appropriate. Fallacies are prevalent in public
discourse. For example, The New York Times la-
beled the tweets of Donald Trump between 2015
and 2020 and found thousands of insults addressed
to his adversaries. If made in an argument, an in-
sult is an ad hominem fallacy: an attack on the
opponent rather than on their argument.
In pri-
vate conversations, other types of fallacies might
be more prevalent, for example, appeal to tradition
or appeal to nature. Appeal to tradition dismisses
calls to improve gender equality by stating that
“women have always occupied this place in soci-
ety”. Appeal to nature is often used to ignore calls
to be inclusive of the LGBTQ+ community by stat-
ing “gender is binary”. The underlying premises
of such arguments are “traditions are correct” and
“what occurs in nature is good”.

Creating a dataset of fallacious arguments is
difﬁcult, given that there are over 100 types of
fallacious arguments (Scalambrino, 2018). There
have been several attempts to create comprehensive
datasets: Habernal et al. (2017) proposed a game
in which players add fallacies in the hope of foul-
ing other participants, in Habernal et al. (2018a)
ad hominem fallacies are found using a subred-
dit’s rule violations, while in Da San Martino et al.
(2019) fallacies are annotated together with other
propaganda techniques in news articles. However,
our work is the ﬁrst to propose a viable solution
for ﬁnding fallacious arguments belonging to many
different fallacy types.

In this work, we study fallacies in public discus-
sions on online forums. Our salient contributions
are: i) we align informal fallacies mentioned on
Reddit within the pragma-dialectic theory of argu-
mentation (van Eemeren and Grootendorst, 1995);
ii) we design a methodology for mining and label-
ing easily fallacies in online discussions; iii) we
construct a large and balanced dataset of fallacious
arguments; iv) ﬁnally, we evaluate several neural

models on the task of predicting fallacious argu-
ments, and we ﬁnd that taking into consideration
additional conversational context is important for
this task.

2 Background

2.1 Fallacies in Argumentation Theory

Humans use argumentation when they evaluate the
validity of new ideas, or they want to solve a dif-
ference of opinion. An argument contains: i) a
proposition called claim, conclusion or standpoint,
to be validated; ii) the premises called also evi-
dence, which are the backing propositions; iii) an
inference relation between the evidence and con-
clusion that validates or disproves the conclusion.
A fallacy is a ﬂawed argument, where the inference
relation or the premises are incorrect. Fallacies are
generally divided into formal and informal falla-
cies. Formal fallacies are arguments that can be
easily represented as invalid logical formulas, such
as denying the antecedent, which is a wrong appli-
cation of modus tollens. Although many informal
fallacies can be also represented as invalid argu-
ments, informal fallacies are easier to describe and
understand without resorting to logical representa-
tions (Hansen, 2020).

In this work, we follow the pragma dialectic
theory of argumentation. The theory developed
by van Eemeren and Grootendorst (1995) views
argumentation as a complex speech act. The dialec-
tical aspect is represented by two parties who try
to resolve a difference of opinion by engaging in a
discussion, each party making a move towards res-
olution. The pragmatic aspect describes the moves
in the discussion as speech acts, more precisely as
the illocutionary acts introduced by Searle (1979).
van Eemeren and Grootendorst (1995) also devel-
oped ten rules which should guide argumentative
discussions. The goal of the rules is to further the
understanding of the difference of opinions and to
create a fruitful discussion. For example, a rule
states that parties must not prevent each other from
advancing standpoints or from casting doubt on
standpoints, while a second rule asks that a party
may defend a standpoint only by advancing argu-
mentation relating to that standpoint. An argument
that prevents the resolution and thus violates one
of the rules is a fallacy. In our work, we align fre-
quent fallacies on Reddit with these rules, with the
goal of formalizing their deﬁnitions.

Another well-known model that considers fal-

lacies is the argumentation scheme introduced by
Douglas Walton (Walton, 2005). A scheme con-
sists of a conclusion, a set of premises, and a set
of critical questions. The critical questions should
be answered in order to prove that the premises
support the conclusion, hence the argument is not
a fallacy. For example, the scheme for an argu-
ment from expert opinion (Walton, 2005) has the
premises E is an expert in domain D, E asserts
that A is known to be true, A is within D and the
conclusion therefore, A may plausibly be taken to
be true. Some critical questions for this scheme
are: i) Trustworthiness: Is E personally reliable as
a source? ii) Backup Evidence: Is E’s assertion
based on evidence? Argumentation schemes have
two main drawbacks: ﬁrst, for each new fallacy, a
new scheme should exist or be deﬁned; and sec-
ond, in the context of labeling an existing argument,
many of the critical questions might be unanswer-
able as none of the parties discussed them.

2.2 Related Work

An initial effort for creating an extensive dataset of
fallacies was made in Habernal et al. (2017). The
authors created a platform for educative games,
where players learn how to become better debaters.
New fallacies are added to the platform by play-
ers that try to earn points by fouling other partici-
pants with invalid arguments. A follow-up on this
work (Habernal et al., 2018a) mentioned a dataset
of only around 300 arguments created via the plat-
form, thus showing the need of ﬁnding other meth-
ods for creating larger datasets of fallacies.

Ad hominem fallacies in conversations have
been addressed in (Habernal et al., 2018b). The
authors used the subreddit ChangeMyView, which
is a forum for civilized discussions, “a place to post
an opinion you accept may be ﬂawed, in an effort
to understand other perspectives on the issue”. The
dataset of fallacies consists of comments that were
removed by the moderators as they violated the
rule of not being rude or hostile, hence committing
an ad hominem fallacy.

Fallacious arguments are often made in the dis-
semination of propaganda. In Da San Martino et al.
(2019), the authors annotate journal articles with 18
propaganda techniques, out of which 12 techniques
are fallacies. Although an important resource in
the study of fallacies, their labelling method and
dataset have a few drawbacks. First, the dataset
is highly unbalanced with 6 fallacies having a fair

number of mentions: name-calling (1294), appeal
to fear and prejudice (367), ﬂag-waving (330),
causal oversimpliﬁcation (233), appeal to authority
(169), black and white fallacy (134), and 6 fallacies
having less than 100 mentions: whataboutism (76),
reductio ad hitlerum (66), red herring (48), band-
wagon (17), labeling, obfuscation or intentional
vagueness (17), straw men (15). Second, the task
of ﬁnding the correct label for a span of text from a
large set of labels (18 in their case) is intellectually
complex and time-consuming. Our work focuses
on collecting and annotating a balanced dataset of
fallacy mentions while providing a methodology
that can easily scale to a larger number of fallacies.
In our approach, an annotator has to just verify that
a comment contains one type of fallacy. In addition,
we target fallacies in online conversations, where
the style of argumentation is less structured than in
a journal article.

3 Fallacies on Reddit

Finding a large sample of fallacious arguments is a
challenging task as it assumes going through long
conversations, ﬁnding arguments, and then veri-
fying if the arguments are sound. Another major
issue, even if we recognize the argument is ﬂawed,
is to ﬁnd the exact fallacy that is committed, given
that more than 100 types of fallacies have been
proposed in the literature (Scalambrino, 2018).

Our goal is to construct an annotated dataset of
fallacies using a mixed strategy: i) ﬁrst, as noisy
labels, we leverage user comments that mention
the name of a fallacy, and second, ii) we clean
this dataset by removing false-positive samples via
crowdsourcing. Our intuition is that a person will
mention a fallacy as a reply to another comment to
highlight that the previous comment’s argument is
fallacious, as shown in Figure 1. This might not
always be the case, as users could discuss falla-
cies in general, hence the need to further label the
discussion using crowdsourcing.

We use the Pushshift Reddit API (Baumgartner
et al., 2020) to retrieve data from Reddit. The API
allows searching comments and submissions by
their IDs or by a set of keywords. We start by
making an exhaustive list of fallacies informed by
Wikipedia. We chose Wikipedia as a resource for
creating the list of fallacies as it is one of the most
well-known sources of information, hence a Red-
dit user could peruse it easily to understand what
fallacy was committed in the discussion. For each

Figure 1: The redditor user3 is pointing out a fallacy.

fallacy we ﬁnd all its different designations, for
example, appeal to tradition is also known under
its Latin name, argumentum ad antiquitatem. We
then do a keyword search for these fallacy types
on Reddit comments, restricting the results to one
year, May 2019 to May 2020. We retrieve in total
105K comments that match at least one fallacy. For
comparison, in 2019, 1.7 billion comments were
posted on Reddit. While it is very likely that many
more posts contain fallacies, the small number of
matches highlights the importance of choosing with
care the comments to annotate. To understand in
which subreddits people were more likely to men-
tion names of fallacies, we compute the top 10 sub-
reddits with the highest ratio of matched comments
per number of subscribers, as shown in Table 1.
The subreddits are broadly divided into subreddits
on religion, morality, and science, with one sub-
reddit dedicated to discussions on fallacies. The
subreddits’ focus is on debating, which involves
creating, defending, and attacking arguments, there-
fore accusing the opponent of committing a fallacy
might win you the debate.

From the list of most frequently mentioned fal-
lacies we retained the top fallacies with more than
400 mentions, resulting in 32 fallacy types. This
shortlist of frequent fallacies is presented in our Ap-
pendix A, with a deﬁnition, example, and argumen-
tation rule violation (according to the pragma di-
alectic theory) for each fallacy. From this shortlist
we do not consider the fallacies that were already
studied in Habernal et al. (2018b), as their labeled
dataset is also based on Reddit comments. We do
not exclude fallacy types annotated in Da San Mar-
tino et al. (2019), as these are fallacious arguments

Submission title:What is something massively outdated that humanity has yet to upgrade?Link: https://www.reddit.com/r/AskReddit/comments/b3nwm6/The 5-day work weekI know a lot of people don't like it, but a 9 to 5 office job is a pretty bigstep up from slavery, feudalism and indentured servitude. Though I do agree with studies saying that working for less than 8 hours a day is more productive.fallacy of relative privationAre you majoring in psychology? HahahaComment by user1Comment by user2Comment by user3Comment by user2Subreddit

Description

Abortiondebate
AskAChristian
fallacy
DebateVaccines
DebateEvolution Reddit’s premiere debate venue for the evolution versus creationism controversy.

A subreddit for debating abortion: ethics, religion, politics all welcome.
A casual discussion forum - ask questions to Christians of various backgrounds.
A subreddit on fallacies.
Debate and discuss issues surrounding vaccinations.

Quraniyoon
DebateReligion
DebateAChristian A curated community designed speciﬁcally for rational debates about Christian subjects.
AskConservatives A subreddit for asking questions to conservatives.

Discuss the Qur’an Alone.
A place to discuss and debate religion.

DebateAVegan

A place for open discussion about veganism and vegan issues.

Table 1: Top 10 subreddits with highest ratio of comments mentioning a fallacy per number of subscribers.

in journal articles. We take random samples of
20 comments that mention one of our frequent fal-
lacies and the comment to which they reply (the
potential fallacious comment), and we check if the
users have a good understanding of the respective
fallacies. We keep the fallacies for which users
generally had a correct sense of their deﬁnition. In
addition, we ﬁlter fallacy types if more than 60%
of potential fallacious comments were not true fal-
lacious arguments. These conditions assure that
the comments we will label have good quality and
that we will ﬁnd sufﬁcient actual fallacy examples.
The remaining fallacies are selected for the cre-
ation of an annotated dataset of fallacies. These 8
fallacies are:

Appeal to authority / argument from authority
fallacy / argumentum ad verecundiam. Deﬁni-
tion. The claim is supported by the opinion of a
person with authority, hence the claim is true. Ex-
ample. Being vegan makes no sense because my
father said so.

Appeal to majority / bandwagon argument / ap-
peal to widespread belief / appeal to the people
fallacy / argumentum ad populum. Deﬁnition.
A claim is true because many people believe it to
be true. Example. Being vegan makes no sense
because so many of us are meat eaters.

Appeal to nature / naturalistic fallacy. Deﬁni-
tion. An action A is justiﬁed/unjustiﬁed because it
occurs/does not occur in nature. Example. Being
vegan makes no sense as our body is designed for
eating meat.

Appeal to tradition fallacy / argumentum ad
antiquitatem. Deﬁnition. An action A is justi-
ﬁed/unjustiﬁed because it has always been consid-

ered as such in the past. Example. Being vegan
makes no sense as our ancestors have been meat
eaters.

Appeal to worse problems / relative privation /
not as bad as fallacy. Deﬁnition. There exists
problem A that is worse than problem B, therefore
B is justiﬁed.

false dilemma /

false di-
Black-or-white /
chotomy / bifurcation fallacy. Deﬁnition.
In
this argument, the claim is that only an event/action
A should be considered. The ﬁrst premise is that
only two events, A and B are possible when there
is at least a third event C possible. The second
premise is that one of the events is bad, for ex-
ample B, thus only event A should be considered.
Example. You must wear a mask each time you go
out, otherwise, you will die of COVID-19.

Hasty generalization fallacy. Deﬁnition. The
claim is supported by insufﬁcient evidence through
inductive generalization. More precisely, we know
that predicate P is true for a population sample,
and we suppose it is true for the entire population.
However, the sample is too small or it is not rep-
resentative of the population. Example. The ﬁrst
week of September has been sunny, which means
the rest of the month will be the same.

Slippery slope / thin edge of the wedge / camel’s
nose fallacy. Deﬁnition. A small event A will
have a big unwanted consequence C. There is at
least one more event B in the chain of causality (A
will cause B, B will cause C), hence the slippery
slope name of the fallacy. Example. If you break
your diet and have one cookie tonight, you will just
want to eat 10 cookies tomorrow and 20 the day
after, and before you know it, you will have gained

back the 15 pounds you lost.

Rule violation. According to the pragma dialec-
tic theory, an argument is a fallacy if it violates
a critical discussion rule. The arguments above
violate one of two rules, hence they are fallacies.
The ﬁrst rule violated states that defending a claim
must occur through an appropriate argumentation
scheme that is correctly applied. Argumentation
schemes in van Eemeren and Grootendorst (1995)
are different than schemes in Walton (2005). They
are a formalization of the relation between the evi-
dence presented and the standpoint to be defended.
This rule is violated by all fallacies, except black-
or-white. For example, in slippery slope, the argu-
mentation is not valid as there is no clear causality
chain between A and C. Black-or-white fallacy vio-
lates the rule that a party should not falsely present
a premise as an accepted starting point, by stating
that only events A and B are possible.

4 Dataset

Noisy labels. We used Amazon Mechanical Turk
to create our annotated dataset. We selected 4 Mas-
ter annotators, which had the highest agreement
with the authors on identifying a set of fallacies (70
samples). An annotation task, deﬁned as a HIT1
consists of 10 items. Each item presents a sam-
ple extracted from a Reddit discussion. A Reddit
discussion is started by a submission, e.g., a news
article or a piece of text, to which users engage by
writing comments. The comments and submission
are organized in a tree-like structure: the submis-
sion is the root, and comments are nodes in the
tree; we will use the terms grandparent, parent, and
child to denote relations between comments. A
sample given for annotation includes the title and
the link of the original Reddit submission and four
comments:

• the comment containing the mention of the

fallacy (this is the label comment);

• the parent of the label comment, which should
contain the fallacious argument (the comment
of interest or COI);

• the parent of the COI, to give more context

for the discussion;

• a direct reply to the label comment; preference
was given to replies that had the same author
as the COI; if no such comment existed, then
we choose the top-rated comment.

1Human Intelligence Task on Amazon Turk

An example of a sample is shown in Figure 1.

For each fallacy described in Section 3, we re-
trieve all the label comments mentioning it and the
context needed for creating a sample discussion
(item). We keep the items for which: i) the com-
ments are relatively short: the label comment has
less than 500 characters (a shorter text will more
likely be an accusation of committing a fallacy),
and the other comments have less than 1000 char-
acters; ii) we have enough context to understand
the discussion:
the COI is a direct reply to the
submission or the child comment of a direct reply;
iii) the COI or its parent do not contain the sub-
string ‘fallac’, a sign that this could be a discussion
on fallacies and therefore the COI does not contain
a fallacious argument, but it merely discusses or
points out one. iv) we have access to the original
discussion: the user or a moderator did not delete
the comments, and the submission is not from a
banned subreddit (the annotators can visit the link
provided with the submission title); v) all the com-
ments are in English.

Crowdsourcing task. Workers were presented
with concise descriptions of the main concepts in-
volved: argument, claim, evidence and fallacy. All
the items in a HIT have to be annotated only for
one fallacy. For example, we retrieved all the items
where the label comment mentioned “hasty gen-
eralization fallacy” and we split them into HITs.
We note that the fallacy committed in the comment
might not be the same as the one signaled by the
user. However, the authors have reviewed a large
sample of comments (for the third vote explained
further in this section) and did not encounter this
situation. Hence, even if this might still occur, it
should be rare. For each selected fallacy, we of-
fered the deﬁnition together with an example of the
fallacy, where we identiﬁed the claim and evidence.
Furthermore, we instruct the workers not to label
as a fallacy a comment that is sarcastic (sometimes
accompanied by the explicit tag “/s”) or a comment
that is disproving the fallacy, e.g., Who would think
that we shouldn’t become vegans just because our
body is able to digest meat?.

The workers are asked if the fallacy occurs in the
comment of interest and if yes, they are prompted
to highlight the corresponding text span. They are
also asked to write the claim that is addressed by
the comment of interest. Finally, they have to an-
swer a question speciﬁc to each fallacy to prove
their good understanding of the task. The ques-

tions are: i) appeal to authority: “What authority
is being appealed to in the comment of interest,
and hence is used as the basis for the argument?”;
ii) appeal to majority: no question; iii) appeal to
nature: “What natural phenomenon/event/activity
is considered natural here?” iv) appeal to tradition:
“What tradition is being appealed to in the comment
of interest, and hence is used as the basis for the ar-
gument?”; v) appeal to worse problems: “Describe
why the current problem (problem 1) is not a trivial
issue.” vi) black-or-white: “Name any additional al-
ternative, which is possible but is not mentioned in
the comment of interest.” vii) hasty generalization:
“Describe a case where the (hasty) generalization
will fail.” viii) slippery slope: “Please list any one
event in the chain of slippery slope argument.” By
answering these questions, the workers would take
the time to understand why the argument was a
fallacy.

Annotated dataset. A HIT is annotated by two
workers. We compute the Cohen’s κ agreement for
the task of deciding if a comment contains a fallacy
(comment-level annotation), and γ inter-annotator
agreement (Mathet et al., 2015) for the task of high-
lighting the tokens of the fallacy within the COI
(token-level annotation), as shown in Table 2. For
both measures, 1. implies perfect agreement. The
comment-level annotation agreement varies from
fair (black-or-white and hasty generalization) to
substantial (appeal to authority), with the majority
of fallacies in the moderate interval. The token-
level agreement is moderate for appeal to worse
problems and substantial for the rest.

Fallacy

Comment
(Cohen’s κ)

Token
(γ)

Appeal to authority
Appeal to majority
Appeal to nature
Appeal to tradition
Appeal to worse problems
Black-or-white
Hasty generalization
Slippery slope

0.64
0.47
0.60
0.55
0.59
0.40
0.38
0.49

0.68
0.79
0.74
0.80
0.60
0.68
0.71
0.61

Table 2: Agreement between annotators.

In addition to the workers’ votes, an expert an-
notator casts a third vote on comments, whenever
there is a disagreement on the label. A comment is
marked as fallacious if it has received two fallacy
votes. The corresponding fallacious tokens of the

comment are the union of the tokens highlighted by
the annotators. We annotated comments until we
reached roughly 200 fallacious comments per fal-
lacy type. The details of the dataset are presented
in Table 3.

Fallacy

Number of
comments

Mean tokens
in spans

Appeal to authority
Appeal to majority
Appeal to nature
Appeal to tradition
Appeal to worse problems
Black-or-white
Hasty generalization
Slippery slope

212
196
208
210
239
211
204
228

21.49 ± 15.00
15.52 ± 11.55
15.16 ± 9.61
16.35 ± 9.07
25.71 ± 17.44
21.80 ± 14.77
19.76 ± 12.72
27.98 ± 19.23

Overall

1708

20.69 ± 14.93

Table 3: Fallacious comments and tokens.

The total size of our annotated dataset, includ-
ing comments and tokens that are non fallacious,
consists of 3358 comments and 160K tokens. We
observe that to ﬁnd 1708 fallacious comments, we
annotated only about two times more comments.
This shows that our technique of ﬁnding fallacious
comments is efﬁcient.

We investigate if the label comment (i.e., the
comment containing mention of the fallacy) is truly
indicative of a fallacy in the COI. This can be use-
ful for ﬂagging the label comments that are likely
to point to fallacious COI, therefore eliminating
or reducing the need for crowdsourcing. Our in-
tuition is that a classiﬁcation method might dif-
ferentiate when comments are accusations or just
mention of fallacies. To investigate this, we used
the fallacy/no-fallacy annotation as classes for label
comment and trained a binary BERT classiﬁer (De-
vlin et al., 2019). We obtained an F1 score of 67.41,
indicating that the label comment’s content is not
sufﬁciently reliable. In conclusion, human annota-
tors are still needed for annotating the true class of
the COI.

Non fallacious comments. The comments for
which two annotators conﬁrmed they were not fal-
lacious represent our annotated negatives (1650
comments). In order to have a more diverse set
of negative examples, i.e. on similar and different
topics, we construct a second set of negative exam-
ples (6400 comments) as follows. We retrieve all
the users that wrote a label comment to a COI and
the COI was identiﬁed as fallacious in the annota-

tion, our gold users. We take all their comments
after the timestamp of the label comment that do
not mention a fallacy name, and retrieve their par-
ent comment. For each comment in the annotated
dataset, we select one sample from our pool of
parent comments from the same subreddit (if this
exists) and one from a subreddit not seen in the
annotated dataset. We retrieve a total of 6400 sam-
ples. These comments are used together with the
annotated dataset, to create our full dataset, used
to train classiﬁcation models. The intuition of the
sampling strategy is that, the gold users were able
to recognize true fallacies at least one time, so they
should spot other fallacies. Hence, if they reply
to a comment without ﬂagging it, the parent com-
ment is likely to be non fallacious. There could be
fallacious comments in this sample; however, we
consider it less likely than a random sample.

5 Models and Discussion

Tasks. We address four tasks leveraging our an-
notated dataset, listed in the order of increasing
granularity: i) comment-level (CL) fallacy iden-
tiﬁcation (binary task of predicting if a comment is
fallacious or not); ii) comment-level fallacy type
identiﬁcation (multi class prediction of the type
of fallacy, with non-fallacious as one class in the
9 classes); iii) token-level (TL) fallacy identiﬁca-
tion (binary task of predicting if tokens in the COI
belong to a fallacy or not); iv) token-level fallacy
type identiﬁcation (multi class prediction of to-
kens in the COI into one of the eight fallacy classes
or the non-fallacy class).

5.1 Models

Random. We generate predictions by respecting
the class distributions in the training set.

BERT. We ﬁne-tune BERT by adding a linear
layer on top of generated contextual representa-
tions. We use the token level embedding in token
detection tasks and [CLS] embedding in the case
of classiﬁcation tasks.

MGN. We adopt the best architecture reported in
Da San Martino et al. (2019), which is a multi-
granularity network that uses lower granularity
sentence-level (which is comment-level in this set-
ting) representation together with higher granular-
ity token-level representations to jointly train the
network. We set the dimension of lower granularity
embedding representation equal to the number of

classes in the task. We jointly train tasks where
number of classes are the same, that is, CL & TL
fallacy identiﬁcation tasks are trained together and
so are CL & TL fallacy type identiﬁcation tasks.
We use sigmoid activation as it is the best model
for their fragment (token) level classiﬁcation and
is comparable for the sentence level classiﬁer. This
model has been shown to give good results for
predicting propaganda techniques, which include
fallacies.

Conversation context. Our dataset is rich in
textual information related to the COI, which
could improve prediction. We deﬁne context
as the parent comment of COI (if it exists, the
string “None” otherwise) or the submission title.
This is provided to the classiﬁer in the format:
[CLS] COI Tokens [SEP] Context
tokens [SEP]. The Context tokens get a
‘non-fallacy’ token-level label at the training time,
but during the validation or test set evaluation,
only the COI token labels are used. The [CLS]
token is used for CL tasks. This results in four
extensions of the previous models: BERT-T,
BERT-P, MGN-T, MGN-P, where T stands for
title and P for parent comment.

Setup. We use PyTorch (Paszke et al., 2019) and
the pre-trained BERT model (Devlin et al., 2019;
Wolf et al., 2020). We ﬁne-tune BERT using batch
size 8, maximum sequence length 256 for COI &
64 for context, and monitored the macro-averaged
F12 score on the validation set, as identiﬁcation
of all classes is equally important. We use the
AdamW optimizer, with a learning rate of 5e−5.
We weigh the cross-entropy loss function accord-
ing to the class distribution in training data. We
split the dataset into training (70%), validation
(20%) and test (10%) sets, hence the full dataset
has 6823, 1950 & 977 and annotated dataset has
2351, 671 & 336 comments respectively. We re-
peat the experiments with 5 different random seeds
for the network intialization and we average the
results.

5.2 Discussion

In Table 4, we show the results of comment level
fallacy and fallacy type identiﬁcation. All the re-
sults are macro scores (precision, recall and F1).
The MGN models obtain the best results, most
often when context is added. The full dataset pro-

2All reported F1 scores are macro F1.

Model

P

Binary
R

F1

Multi class
R

P

F1

Full dataset

Random
BERT
BERT-T
BERT-P
MGN
MGN-T
MGN-P

Annotated data

Random
BERT
BERT-T
BERT-P
MGN
MGN-T
MGN-P

47.67
66.31
67.54
68.73
69.50
70.73
71.15

46.35
66.918
66.76
66.72
67.72
69.57
69.53

47.67
66.28
69.01
68.75
70.01
68.76
68.72

46.35
64.00
66.57
66.54
67.54
69.27
68.99

47.67
66.15
67.99
68.52
69.69
69.61
69.62

46.35
61.16
66.44
66.45
67.45
69.20
68.86

9.98
50.03
46.93
38.08
47.87
51.18
50.06

13.01
62.25
62.03
61.08
59.81
62.72
62.96

10.02
48.80
49.49
49.85
48.59
48.22
50.38

13.16
55.63
55.88
56.61
54.72
55.91
55.85

10.00
48.30
46.57
41.83
47.14
49.06
48.53

13.04
57.83
57.93
57.90
56.19
58.41
58.17

Table 4: Comment level (CL) prediction for COI.

Model

P

Binary
R

F1

Multi class
R

P

F1

Full dataset

Random
BERT
BERT-T
BERT-P
MGN
MGN-T
MGN-P

Annotated data

Random
BERT
BERT-T
BERT-P
MGN
MGN-T
MGN-P

49.74
78.01
76.24
77.16
77.36
76.71
76.75

50.38
69.09
68.25
68.43
69.30
70.95
70.08

49.74
73.59
73.71
74.15
74.61
74.09
74.55

50.38
66.16
68.23
67.95
68.52
70.06
69.73

49.74
75.52
74.87
75.51
75.86
75.26
75.57

50.38
63.33
68.15
68.09
68.83
70.26
69.88

10.94
44.83
43.67
43.94
41.20
40.56
41.26

11.04
52.20
51.26
52.04
50.59
51.79
50.01

10.93
50.08
52.14
52.15
48.31
50.70
51.69

11.02
55.16
56.78
55.90
53.96
55.45
56.08

10.94
46.64
46.76
47.07
43.74
44.37
45.12

11.03
52.80
53.21
53.44
51.65
53.02
52.28

Table 5: Token level (TL) prediction for COI.

Full data

Annotated data

Fallacy

CL

TL

CL

TL

Appeal to authority
Appeal to majority
Appeal to nature
Appeal to tradition
Appeal to worse problems
Black-or-white
Hasty generalization
Slippery slope

44.47
45.11
69.16
56.92
35.31
42.03
18.76
39.54

85.65
26.69
51.22
55.08
20.81
31.69
21.60
37.68

54.37
66.41
72.16
66.08
43.89
51.29
44.24
57.37

75.11
36.11
57.55
60.43
30.73
38.05
43.41
55.57

Table 6: F1 score per fallacy from best classiﬁers.

vides a wider mix of topics via noisy negative sam-
ples and pronounces the class imbalance, closer to
a real sample of Reddit conversations. Despite this,
the classiﬁer is able to learn across all four tasks.

Table 5 presents the results for token level fal-

lacy and fallacy type identiﬁcation. BERT mod-
els obtain better results for the multi class setting,
while MGN for the binary setting. This is compa-
rable with the results reported in Da San Martino
et al. (2019), where the authors observe a smaller
improvement in classiﬁcation for the token level
prediction using MGN.

Adding more context in the form of title or parent
of the COI generally led to improved performance.
While the results are slightly better when adding
the title, the differences are small. We speculate
that parent and COI provided a complete argument,
making fallacy detection a bit easier.

In Table 6, we show the F1 score per fallacy class.
Appeal to authority, nature, and tradition perform
well (F 1 > 40%) across all four tasks. Hasty
generalization has a rather poor performance; this
can be attributed to this fallacy’s general difﬁculty,
given that the workers also had low agreement on
this fallacy (Table 2). We observe that generally
the comment level prediction task is easier than the
token level prediction, which is expected due to the
granularity difference.

Topical confounds. While fallacies might ap-
pear more frequently in discussions on certain top-
ics, a fallacy detection approach should identify
the underlying argument structure, and not just the
presence of a topic. For example, we do not want
to label all discussions about nature as appeal to
nature fallacies. To identify if the classiﬁers are
sensitive to topical biases, we use the approach
presented in (Kumar et al., 2019). We compute
statistically overrepresented tokens in each propa-
ganda technique in the training set using log-odds
ratio with Dirichlet prior (Monroe et al., 2008). We
present the top 10 tokens per fallacy in Table 7.
We observe that for appeal to authority, nature and
tradition, the tokens are topically cohesive, as they
revolve around notions of authority, nature and tra-
dition. For the other fallacies, while it is intuitive
why some words may be overrepresented, there is
no clear topical cohesiveness. To verify that our
classiﬁers learn linguistic patterns and not topics,
we replace the top 30 tokens strongly associated
with each fallacy (computed from the training set)
with a special token in the test set. We evaluate
only the comment level prediction, as results on
the token level might be hard to interpret given
that we replace tokens. We show the results in
Table 8. We observe a large decrease in F1 score
(more than 10% on the full data) for 2 fallacies:

Fallacy

Overrepresented tokens

medical, experts, expert, ﬁeld, university, listen, degree, dr., professional, academic
Appeal to authority
majority, billion, reality, cult, christianity, followers, believe, nations, news, believed
Appeal to majority
animals, nature, eat, natural, meat, humans, food, species, killing, animal
Appeal to nature
meat, years, marriage, history, eating, culture, vegan, thousands, tradition, ancestors
Appeal to tradition
Appeal to worse problems worse, world, problems, country, people, living, compared, dying, poverty, priorities
Black-or-white
Hasty generalization
Slippery slope

pick, review, tax, gun, god, instead, absurd, proﬁts, industry, paycheck
http, grew, friends, muslim, went, business, seen, grade, jesus, drivers
government, slippery, slope, ban, stop, speech, remove, guns, line, start

Table 7: Top 10 tokens statistically overrepresented in each fallacy in the training set.

Fallacy

Full data Annotated data

Appeal to authority
Appeal to majority
Appeal to nature
Appeal to tradition
Appeal to worse problems
Black-or-white
Hasty generalization
Slippery slope

42.47
41.59
21.62
41.80
27.25
39.10
13.40
34.76

54.03
64.02
33.28
49.20
32.66
48.72
41.36
54.44

Table 8: F1 score on comment level (CL) per fallacy
after removing top 30 overrepresented words.

appeal to nature and appeal to tradition. A big
drop in the F1 score on the full data is more signif-
icant than on the annotated data, as the classiﬁer
would have seen more negative examples contain-
ing the confounds. Given the observed decrease in
F1 score for these fallacies, an important future di-
rection is to annotate more discussions containing
the overrepresented words to ﬁnd a better quality
negative set, i.e., non-fallacious comments on the
same topics. We note that for the other fallacies,
the models appear to learn more complex language
structures as they are less sensitive to the removal
of the overrepresented words.

6 Conclusion and Future Work

In this work, we present a methodology for mining
and labeling fallacious comments in online discus-
sions. We ﬁnd frequent fallacy mentions on Reddit
and the subreddits in which they are the most preva-
lent. We create a large corpus of annotated com-
ments and experiment with several neural methods
for classiﬁcation. We explore methods that con-
sider the context of the discussion, and we show
that they give better results.

There are several exciting directions for continu-
ing this work. First, using our methodology, we can
annotate more comments for the eight fallacies we

studied in this paper, we can improve the negative
example set or explore other types of fallacies. Sec-
ond, we can study another aspect of the discussion,
the speech acts. According to the pragma dialectic
theory, an argument is composed of several speech
acts. Investigating if certain speech acts are more
prevalent in fallacious discussions might lead to im-
proved detection of fallacies. Lastly, in the pragma
dialectic theory of argumentation, fallacies are vio-
lations of rules of critical discussion, for example,
the fallacies we annotated violate two rules, as de-
scribed in Section 3. Given the signiﬁcant number
of fallacy types, we believe that a hierarchical ap-
proach to their detection could prove more efﬁcient:
identifying if a conversation violates one of the ten
rules of critical conversation, and then for that par-
ticular rule identifying the type of fallacy.

Acknowledgements

We would like to thank the ACL reviewers for their
helpful feedback. We would also like to thank
Meghana M. Bhat and Dravyansh Sharma for their
helpful comments on the initial draft. This work
was performed using HPC resources from GENCI-
IDRIS (Grant 2020-AD011011614).

References

Jason Baumgartner, Savvas Zannettou, Brian Keegan,
Megan Squire, and Jeremy Blackburn. 2020. The
pushshift reddit dataset. Proceedings of the Interna-
tional AAAI Conference on Web and Social Media,
14(1):830–839.

Giovanni Da San Martino, Seunghak Yu, Alberto
Barr´on-Cede˜no, Rostislav Petrov,
and Preslav
Nakov. 2019. Fine-grained analysis of propaganda
In Proceedings of the 2019 Con-
in news article.
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-

IJCNLP), pages 5636–5646, Hong Kong, China. As-
sociation for Computational Linguistics.

Kong, China. Association for Computational Lin-
guistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Frans H. van Eemeren and Rob Grootendorst. 1995.
Argumentation, communication, and fallacies: A
Philosophy and
pragma-dialectical perspective.
Rhetoric, 28(4):426–430.

Ivan Habernal, Raffael Hannemann, Christian Pol-
lak, Christopher Klamm, Patrick Pauli, and Iryna
Gurevych. 2017. Argotario: Computational argu-
mentation meets serious games. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations,
pages 7–12, Copenhagen, Denmark. Association for
Computational Linguistics.

Ivan Habernal, Patrick Pauli, and Iryna Gurevych.
2018a. Adapting serious game for fallacious argu-
mentation to German: Pitfalls, insights, and best
In Proceedings of the Eleventh Interna-
practices.
tional Conference on Language Resources and Eval-
uation (LREC 2018), Miyazaki, Japan. European
Language Resources Association (ELRA).

Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,
and Benno Stein. 2018b. Before name-calling: Dy-
namics and triggers of ad hominem fallacies in web
argumentation. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
386–396, New Orleans, Louisiana. Association for
Computational Linguistics.

Hans Hansen. 2020. Fallacies. In Edward N. Zalta, ed-
itor, The Stanford Encyclopedia of Philosophy, sum-
mer 2020 edition. Metaphysics Research Lab, Stan-
ford University.

Sachin Kumar, Shuly Wintner, Noah A. Smith, and
Yulia Tsvetkov. 2019. Topics to avoid: Demoting
In Proceed-
latent confounds in text classiﬁcation.
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 4153–4163, Hong

Yann Mathet, Antoine Widl¨ocher, and Jean-Philippe
M´etivier. 2015. The uniﬁed and holistic method
gamma (ˆI³)
inter-annotator agreement mea-
sure and alignment. Computational Linguistics,
41(3):437–479.

for

Burt L Monroe, Michael P Colaresi, and Kevin M
Quinn. 2008. Fightin’words: Lexical feature selec-
tion and evaluation for identifying the content of po-
litical conﬂict. Political Analysis, 16(4):372–403.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Py-
Junjie Bai, and Soumith Chintala. 2019.
torch: An imperative style, high-performance deep
learning library.
In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 32, pages 8024–8035. Curran Asso-
ciates, Inc.

Judith A. Sanders, Richard L. Wiseman, and Robert H.
Gass. 1994. Does Teaching Argumentation Facil-
itate Critical Thinking? Communication Reports,
7(1):27–35.

Frank Scalambrino. 2018. Psychologist’s Fallacy: 100
of the Most Important Fallacies in Western Philoso-
phy, pages 204–207.

John R Searle. 1979. Expression and meaning: Studies

in the theory of speech acts.

Douglas Walton. 2005. Justiﬁcation of argumentation
schemes. The Australasian Journal of Logic, 3.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online. Asso-
ciation for Computational Linguistics.

A Frequent Fallacies on Reddit

In this appendix, we review the most frequent falla-
cies on Reddit. Our goal is to understand how easy
would be to annotate such fallacies, by looking at
their deﬁnition and examples of how well Reddit
users understand those deﬁnitions.

Let A,B,C,D be examples of persons, events, or
actions. An argument consists of a standpoint S
(known also as a claim or conclusion) and the sup-
porting evidence (known also as the premises) for
the standpoint. Let the person making/supporting
the standpoint be referred to as the protagonist and
the person disputing the standpoint as the antago-
nist. When referring to either protagonist or antag-
onist, we use the term party.

An argumentation scheme is a formalization of
the relation between the evidence presented and the
standpoint to be defended. Types of schemes:

• symptomatic argumentation: what is stated in
the argument premise is an expression or a
sign of what is stated in the conclusion.

Tu quoque

Association / (guilt by/ honor by) association
/ reductio ad Hitlerum fallacy. Deﬁnition. The
antagonist is disproving the claim of the protagonist
by stating that this claim was supported by a bad
group, hence the protagonist is also a bad person.
Example. You say public healthcare is a good thing,
but the communists say the same thing.
/

/
whataboutism fallacy.
In this
argument, the protagonist makes a claim S. The
antagonist states that the claim S is in contradiction
with the previous actions/attitudes of the protago-
nist (showing hypocrisy), thus the claim must be
false. Example. To the statement “Putin is a killer”,
Trump responded, “There are a lot of killers. You
think our country’s so innocent?” - interview with
Fox News’ Bill O’Reilly.

to hypocrisy

Deﬁnition.

appeal

Poisoning the well fallacy. Deﬁnition. This ar-
gument is a preemptive ad hominem, where the
protagonist is attacked before advancing a stand-
point. Example. I am sure Anna will say she gave
the money back, but you know she always lies.

• argumentation based on similarities: anal-
ogy between what is stated in the argument
premise and what is stated in the conclusion.

Rule 2. A party that advances a standpoint is
obliged to defend it if the other party asks him
to do so.

• instrumental argumentation: argument and the
conclusion are linked by a very broad relation
of causality.

Fallacies are classiﬁed based on the argumen-
tation rules they break out of the ten introduced
in (van Eemeren and Grootendorst, 1995). Each
fallacy is presented by giving all its possible name
variations that link back to the same deﬁnition, its
deﬁnition, and an example.

Rule 1. Parties must not prevent each other
from advancing standpoints or casting doubt
on standpoints.

Genetic fallacy. Deﬁnition. The antagonist re-
jects a claim stating that the source of the claim
should not be trusted. The unexpressed premise
is that every claim coming from the same source
is likely to be false. Example. Fox News always
writes junk news, I am sure that Hunter Biden did
not break the law.

Ad hominem fallacy. Deﬁnition. The antago-
nist rejects a standpoint based not on the strength
of the argument, but on perceived ﬂaws of the pro-
tagonist, who is defending it. Example. You are
such a bad student, I don’t believe you got an A at
maths.

ignorantiam /

Argumentum ad

/ burden of proof

onus
/ argument
probandi
from ignorance / appeal to ignorance fallacy.
Deﬁnition. The protagonist claims that a standpoint
must be true because there is no or not sufﬁcient
evidence against it. As pointed out in (van
Eemeren and Grootendorst, 1995), there can be
two situations: i) the protagonist is challenging the
antagonist to prove that their standpoint is wrong
(rule 2) or ii) the protagonist is stating that because
the negation of their standpoint cannot be proven
true, then their standpoint is true (rule 9). Example.
I have heard that vaccines are bad, prove me that
they are good for your health!

Rule 3. A party’s attack on a standpoint must
relate to the standpoint that has indeed been ad-
vanced by the other party.

Straw man fallacy. The antagonist is: i) dis-
torting the standpoint advanced by the protagonist
(rule 3) or ii) attributing a false standpoint (rule
5). Example. Protagonist: I believe that women
should have the right to abortion in the ﬁrst term.
Antagonist: So you’re okay with killing babies.

Nirvana / perfect solution fallacy. Deﬁnition.
In this argument, the protagonist is advancing the

claim that an action A is desirable as it will achieve
a positive result. The antagonist rebuts this claim
by stating that A will not achieve the perfect out-
come, even if the perfect outcome is not speciﬁed
in the claim. The antagonist modiﬁes the claim,
by stating “Action A will achieve the perfect out-
come”. Example. Protagonist: Using less plastic is
good for the planet. Antagonist: We need to stop
using plastic altogether to make any progress.

Moving the goalposts / raising the bar fallacy.
Deﬁnition. This fallacy is similar to the nirvana
fallacy, however, the antagonist is not aiming for
the perfect outcome, but for better outcome than the
one initially described by the protagonist. Example.
We should stop killing animals for food, they feel
pain. What about plants, how do you know if they
don’t feel?

Rule 4. A party may defend his standpoint
only by advancing argumentation relating to
that standpoint.

/

Ignoratio elenchi

irrelevant conclusion /
missing the point fallacy. Deﬁnition. In this argu-
ment, the protagonist uses premises that are irrel-
evant to the claim. Example. His policies are not
good enough, but my cousin says he talks well.

Rule 5. A party may not falsely present some-
thing as a premise that has been left unex-
pressed by the other party or deny a premise
that they himself have left implicit.

Straw man fallacy. Already deﬁned for rule 3.

Rule 6. A party may not falsely present a
premise as an accepted starting point nor deny
a premise representing an accepted starting
point.

Circulus in demonstrando / petitio principii
/ begging the question / circular reasoning fal-
lacy. Deﬁnition. In this argument, the evidence
assumes that the claim is true. Example. Everyone
likes me because I am the most liked politician.

Plurium interrogationum / fallacy of many
questions / fallacy of presuppositions / complex
question / loaded question fallacy. Deﬁnition.
The standpoint brought forward by the protagonist
is implying that at least another standpoint should
be true. Example. Annie is a better person than that
horrible guy John.

False dilemma / false dichotomy / bifurcation
/ black-or-white fallacy. Deﬁnition. The protago-
nist pushes the standpoint S that only the event or

action A should be considered. The ﬁrst premise
is that only two events A and B are possible, when
there is at least a third event C possible. The sec-
ond premise is that one of the events is bad, for
example B, thus only event A should be considered.
Example. You must wear a mask each time you go
out, otherwise you will die of COVID-19.

Rule 7. A party may not regard a standpoint
as conclusively defended if the defense does not
take place by means of an appropriate argu-
mentation scheme that is correctly applied.

Relative privation / appeal to worse problems
/ not as bad as fallacy. Deﬁnition. The protago-
nist states that there exists A that is worse than B,
therefore B is justiﬁed. The applied argumentation
scheme is argumentation based on similarity. A
and B are both bad actions, events or people, but
instead of stressing the similarity, the protagonist
tries to stress how A is bad, thus making B look
better. Example. You shouldn’t complain if the
food is stale as there are millions of people starving
who would be grateful for any meal they get.

Gambler’s fallacy. Deﬁnition. The protagonist
defends a probabilistic claim such as “an event A
is very likely to occur”. The mistake in argumen-
tation appears if the evidence is based on falsely
supposing that event A and event B are dependent,
so if event A occurs, the probability of B occur-
ring changes. This argument violates rule 7 as is
it based on a faulty application of instrumental ar-
gumentation. Example. My coin landed twice in a
row on heads, hence it should land next on tails.

Slippery slope / thin edge of the wedge /
camel’s nose fallacy. Deﬁnition. This fallacy con-
sists in claiming that a small event A has a big un-
wanted consequence C. There is at least one more
event B in the chain of causality (A will cause B,
B will cause C), hence the slippery slope name of
the fallacy. This argument violates rule 7, as the
instrumental argumentation does not hold given
that there is no clear causality chain between A and
C. Example. If you break your diet and have one
cookie tonight, you will just want to eat 10 cook-
ies tomorrow and 20 the day after, and before you
know it, you will have gained back the 15 pounds
you lost.

No true Scotsman fallacy. Deﬁnition. The pro-
tagonist tries to make a generalization, which is
a valid instrumental argumentation scheme: when
a predicate P is true for an arbitrary member of a
group, then it is true for any member of the group.

However, it changes the deﬁnition of the predi-
cate P, so therefore the argument violates rule 7,
as the instrumental argumentation does not hold
anymore. Example. “No Scotsman puts sugar on
his porridge”.

Post hoc ergo propter hoc / temporal se-
quence implies causation fallacy. Deﬁnition. The
protagonist states that because event A occurred
ﬁrst and event B occurred second, A caused B. This
argument violates rule 7, as it tries to present an
instrumental argumentation, without providing ev-
idence that shows how event A and B are linked.
Example. My boyfriend left me after he saw you,
it must have been something you said.

Argumentum ad verecundiam / appeal to au-
thority / argument from authority fallacy. Deﬁ-
nition. In this argument, because the claim is sup-
ported by the opinion of a person with authority,
then the claim is true. Rule 7 is violated because
the symptomatic argumentation is incorrectly used:
while authorities can make true claims, we can not
consider them true as such if they are not backed
up by evidence. Example. Being vegan makes no
sense because my father said so.

Argumentum ad populum / appeal
to
widespread belief / bandwagon argument / ap-
peal to the majority / appeal to the people fal-
lacy. Deﬁnition. A claim is presented as true be-
cause many people believe it to be true. Rule 7 is
violated because the symptomatic argumentation is
not used correctly: while people do believe many
things that are true, belief is not sufﬁcient. Exam-
ple. Being vegan makes no sense because so many
of us are meat eaters.

Appeal to nature / naturalistic fallacy. Deﬁ-
nition. The protagonist states that an action A is
justiﬁed or good. The premise is that action A is
good because it is natural. The argument violates
rule 7 as it uses the symptomatic argumentation
in a wrong way: some actions that are natural are
good, however we cannot conclude they are good
because they are natural. Example. Being vegan
makes no sense as our body is designed for eating
meat.

Argumentum ad antiquitatem / appeal to tra-
dition fallacy. Deﬁnition. The protagonist states
that an action A is justiﬁed or good. The premise
is that it has always been considered as such in the
past, but no further justiﬁcation is given. The unex-
pressed premise of the argument is that everything
that is done since a long time is good or justiﬁed as
it has withstood criticism. However, this premise

is also an opinion and not a fact. Example. Being
vegan makes no sense as our ancestors have been
meat eaters.

Divine / argument from incredulity / appeal
to common sense fallacy. Deﬁnition. The stand-
point appears incredible and not common sense
from the perspective of the antagonist, and such it
can be dismissed as false. In addition, everything
that appears as common sense should be true. The
argument uses the symptomatic argumentation in
a wrong way: some actions that are incredible are
false, however we cannot conclude that all incredi-
ble actions are false. Example. As disinfectant is
efﬁcient against Covid-19, it should be effective
also if we drink it.

Hasty generalization fallacy. Deﬁnition.
In
this argument, the claim is supported by insufﬁcient
evidence through inductive generalization. More
precisely, we know that predicate P is true for a
sample of a population and we suppose it is true
for the entire population. However, in this case the
sample is either too small or it is not representative
of the population. Example. The ﬁrst two weeks
of September were sunny, it means the rest of the
month will be the same.

Volvo / anecdotal / proof by selected instances
/ person who fallacy. Deﬁnition. This fallacy is
very similar to the hasty generalization, as a claim
is not supported by sufﬁcient evidence, but only
a small set of examples. The difference between
the two fallacies is that the examples in anecdotal
fallacy are usually personal examples. Example.
Two years ago when I visited Paris in September
it was so nice and sunny, I am sure this year it will
be the same.

Cherry picking / suppressed evidence / in-
complete evidence fallacy. Deﬁnition. In this ar-
gument, a claim is backed by incomplete evidence,
that is only a subset of facts that support the claim,
while a large body of facts is overlooked. Example.
My son is very smart, look he got an A at English!
But what about all his bad grades before that?

Accident fallacy. Deﬁnition. The premises
brought forward are generalizations that do not
apply to the speciﬁc instances mentioned in the
claim. Example. People bleed when they are ill, it
means that your period is a sign of an illness.

Fallacy of composition. Deﬁnition. The claim
is that a property P is true of a ﬁnite set S, also
called in literature a whole. The evidence is that
the property P is true for an element E that is part

of S. The unexpressed premise is that all the ele-
ments of the set are similar, which might be false
and needs evidence. While this is similar to hasty
generalization, in the latter there is no notion of
a whole. Rule 7 is violated, as the instrumental
argumentation does not hold. Example. Because
the leaves of a tree are green, the tree is also green.
Fallacy of division. Deﬁnition. The fallacies of
composition and division are the converse of one
another. The claim is that something is true of an
element E (let this be a property P), which belongs
to a set S, called a whole. The evidence is that P is
true for the set S. The unexpressed premise is that
all the elements of the set are similar, which might
be false and needs evidence. Example. If this tree
is 100 years old, then each branch is 100 years old.
Argumentum ad temperantiam / argument
to moderation / false compromise / middle
ground fallacy / fallacy of the mean. Deﬁnition.
Let S1 and S2 be two standpoints that represent
very different opinions on the same topic. The
claim is that a third statement, S3, which is the
middle point between the two, is true. Example.
S1 : We are having ﬁnancial issues, we should ﬁre
all new hires. S2 : No, we shouldn’t ﬁre any of the
new people. S3 : We should ﬁre half of them.

Continuum / sorites / line-drawing / bald
man fallacy / fallacy of the beard / fallacy of the
heap. Deﬁnition. Let S1 and S2 be two extreme
standpoints. Because there isn’t a clear point where
we pass from S1 to S2, it is supposed that there is
no difference between them. Example. Once you
drink a sip of alcohol you will become irresponsi-
ble and put your life in danger.

Rule 8.
In his argumentation a party may
only use arguments that are logically valid or
capable of being validated by making explicit
one or more unexpressed premises.

Special pleading fallacy. Deﬁnition. The pro-
tagonist applies rules or principles to other people
or situations, but says this does not apply to the
current situation without providing a justiﬁcation.
This is an application of a double standard. Exam-
ple. While it is true he is only a teenager, I am
sure he wasn’t raped, he wanted to have intercourse
with that woman.

Rule 9. A failed defense of a standpoint must
result in the party that put forward the stand-
point retracting it and a conclusive defense in
the other party retracting his doubt about the
standpoint.

ignorantiam /

Argumentum ad

onus
/ argument
probandi
from ignorance / appeal to ignorance fallacy.
Already deﬁned for rule 2.

/ burden of proof

Rule 10. A party must not use formulations
that are insufﬁciently clear or confusingly am-
biguous and he must interpret the other party’s
formulations as carefully and accurately as pos-
sible.

Equivocation fallacy. Deﬁnition. In this argu-
ment a word or expression is used with multiple
meanings, thus trying to capitalize on the confusion
to approve or disprove a claim. Example. If Amer-
icans are free, why do they have prisons? - here
freedom has two meanings: the right to speak and
act as one wants and the state of being imprisoned.

B Ethical Considerations

Worker compensation. Before assigning the
tasks to crowd workers, the authors did several
rounds of annotations themselves to determine the
average time it takes to ﬁnish one HIT (10 falla-
cies). On an average it took about 20 minutes to
annotate 10 fallacies. So we paid workers $5 per
HIT, averaging to $15/hour. We still provided them
1 hour, in order to not put them under undue stress.
Also, we did not request any personal information
or opinions from the workers.

Banned and deleted content. Subreddits are
closely monitored by the moderators. Users have to
comply with Reddit’s content policy, a lists a set of
rules enforced by the admins on every community.
Any rule violation (like bullying, use of hate speech,
attacking marginalized or vulnerable groups, etc.)
leads to the removal of posts/comments and, in
some cases, banning a subreddit if the moderators
fail to comply. The removal of such comments and
posts ensures that we do not have any banned or
deleted content in our dataset either.

Privacy of authors. None of our proposed meth-
ods does any proﬁling of Reddit users who made
comments that appeared in our dataset. No identiﬁ-
cation of post/comment or their authors appears in
our ﬁnal dataset or input to the models.

Data quality. We describe our data collection
process extensively in section 4. All the data sam-
ples appearing are annotated by two workers and
resolved by authors if there is a disagreement be-
tween the workers.

