Statistically Significant Discriminative Patterns
Searching
Hoang Son Pham, Gwendal Virlet, Dominique Lavenier, Alexandre Termier

To cite this version:

Hoang Son Pham, Gwendal Virlet, Dominique Lavenier, Alexandre Termier. Statistically Significant
Discriminative Patterns Searching. DaWaK 2019 - 21st International Conference on Big Data Analyt-
ics and Knowledge Discovery, Aug 2019, Linz, Austria. pp.105-115, ￿10.1007/978-3-030-27520-4_8￿.
￿hal-02190793￿

HAL Id: hal-02190793

https://hal.science/hal-02190793

Submitted on 22 Jul 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Statistically Signiﬁcant Discriminative Patterns
Searching

Hoang Son Pham1, Gwendal Virlet2, Dominique Lavenier2, and Alexandre
Termier2

1 ICTEAM, UCLouvain, Belgium
2 Univ Rennes, Inria, CNRS, IRISA

Abstract. In this paper, we propose a novel algorithm, named SSDPS,
to discover patterns in two-class datasets. The SSDPS algorithm owes
its eﬃciency to an original enumeration strategy of the patterns, which
allows to exploit some degrees of anti-monotonicity on the measures of
discriminance and statistical signiﬁcance. Experimental results demon-
strate that the performance of the SSDPS algorithm is better than oth-
ers. In addition, the number of generated patterns is much less than the
number of the other algorithms. Experiment on real data also shows that
SSDPS eﬃciently detects multiple SNPs combinations in genetic data.

Keywords: Discriminative patterns, Discriminative Measures, Statistical Sig-
niﬁcance, Anti-Monotonicity.

1

Introduction

Recently, the use of discriminative pattern mining (also known under other
terms such as emerging pattern mining [1], contrast set mining [2]) has been
investigated to tackle various applications such as bioinformatics [3], data clas-
siﬁcation [4]. In this paper, we propose a novel algorithm, named SSDPS, that
discovers discriminative patterns in two-class datasets. This algorithm aims at
searching patterns satisfying both discriminative scores and conﬁdence intervals
thresholds. These patterns are deﬁned as statistically signiﬁcant discrimi-
native patterns. The SSDPS algorithm is based on an enumeration strategy
in which discriminative measures and conﬁdence intervals can be used as anti-
monotonicity properties. These properties allow the search space to be pruned
eﬃciently. All patterns are directly tested for discriminative scores and conﬁ-
dence intervals thresholds in the mining process. Only patterns satisfying both
of thresholds are considered as the target output. According to our knowledge,
there doesn’t exist any algorithms that combine discriminative measures and
statistical signiﬁcance as anti-monotonicity to evaluate and prune the discrimi-
native patterns.

The SSDPS algorithm has been used to conduct various experiments on both
synthetic and real genomic data. As a result, the SSDPS algorithm eﬀectively
deploys the anti-monotonic properties to prune the search space. In comparison

2

H.S. Pham et al.

with other well-known algorithms such as SFP-GROWTH [5] or CIMCP [6], the
SSDPS obtains a better performance. In addition the proportion of generated
patterns is much smaller than the amount of patterns output by these algorithms.
The rest of this paper is organized as follows: Section 2 precisely deﬁnes the
concept of statistically signiﬁcant discriminative pattern, and Section 3 presents
the enumeration strategy used by the SSDPS algorithm. In Section 4, the design
of the SSDPS algorithm is described. Section 5 is dedicated to experiments and
results. Section 6 concludes the paper.

2 Problem Deﬁnition

The purpose of discriminative pattern mining algorithms is to ﬁnd groups of
items satisfying some thresholds. The formal presentation of this problem is
given in the following:

Let I be a set of m items I = {i1, ..., im} and S1, S2 be two labels. A
transaction over I is a pair ti = {(xi, yi)}, where xi ⊆ I, yi ∈ {S1, S2}. Each
transaction ti is identiﬁed by an integer i, denoted tid (transaction identiﬁer). A
set of transactions T = {t1, ..., tn} over I can be termed as a transaction dataset
D over I. T can be partitioned along labels S1 and S2 into D1 = {ti | ti =
(xi, S1) ∈ T } and D2 = {ti | ti = (xi, S2) ∈ T }. The associated tids are denoted
D1.tids and D2.tids.

For example, Table 1 presents a dataset including 9 transactions (identiﬁed by
1..9) which are described by 10 items (denoted by a..j). The dataset is partitioned
into two classes (class label 1 or 0).

Table 1: Two-class data example

a b c
a b c
a b c
b

b c
a b c

Tids
1
2
3
4
5
6
7
8
9

Items
f

e

g

i
i

f

f

f

h

g
i
g h i
g h
g h

h i

g h

d e
d

e

Class
1
1
1
1
1
0
0
0
0

j

j
j
j
j

j

b c d e
d e

a

A set p ⊆ I is called an itemset (or pattern) and a set q ⊆ {1..n} is called a
tidset. For convenience we write a tidset {1, 2, 3} as 123, and an itemset {a, b, c}
as abc. The number of transactions in Di containing p is denoted by |Di(p)|. The
relational support of p in class Di (denoted sup(p, Di)), and negative support of
p in Di (denoted sup(p, Di)), are deﬁned as follows:

sup(p, Di) =

|Di(p)|
|Di|

;

sup(p, Di) = 1 − sup(p, Di)

Statistically Signiﬁcant Discriminative Patterns Searching

3

To evaluate the discriminative score of pattern p in a two-class dataset D,
diﬀerent measures are deﬁned over the relational supports of p. The most popular
discriminative measures are support diﬀerence, grown rate support and odds ratio
support which are calculated by formulas 1, 2, 3 respectively.

SD(p, D) = sup(p, D1) − sup(p, D2)

GR(p, D) =

sup(p, D1)
sup(p, D2)
sup(p, D1)/sup(p, D1)
sup(p, D2)/sup(p, D2)

ORS(p, D) =

(1)

(2)

(3)

A pattern p is discriminative if its score is not less than a given threshold α.
For example, let α = 2 be the threshold of growth rate support. Pattern abc is
discriminative since GR(abc, D) = 2.4.

Deﬁnition 1. (Discriminative pattern). Let α be a discriminative thresh-
old, scr(p, D) be the discriminative score of pattern p in D. The pattern p is
discriminative if scr(p, D) ≥ α.

In addition to the discriminative score, to evaluate the statistical signiﬁcance
of a discriminative pattern we need to consider the conﬁdence intervals (CI).
Conﬁdence intervals are the result of a statistical measure. They provide in-
formation about a range of values (lower conﬁdence interval (LCI) to upper
conﬁdence interval (U CI)) in which the true value lies with a certain degree
of probability. CI is able to assess the statistical signiﬁcance of a result [7]. A
conﬁdence level of 95% is usually selected. It means that the CI covers the true
value in 95 out of 100 studies.

Let a = |D1(p)|, b = |D1| − |D1(p)|, c = |D2(p)|, d = |D2| − |D2(p)|, the 95%

LCI and U CI of GR are estimated as formulas 4 and 5 repectively.

LCIGR = e(ln(GR)−1.96

√

U CIGR = e(ln(GR)+1.96

√

1

a − 1

a+b + 1

c − 1

c+d )

1

a − 1

a+b + 1

c − 1

c+d )

(4)

(5)

Similarly, the 95% LCI and U CI of OR are estimated as formulas 6 and 7
repectively.

(cid:16)
LCIORS = e

(cid:16)

U CIORS = e

ln(ORS)−1.96

√

1

a + 1

b + 1

c + 1

d

(cid:17)

ln(ORS)+1.96

√

1

a + 1

b + 1

c + 1

d

(cid:17)

(6)

(7)

For example, consider the pattern abc in the previous example, the 95%CI of
GR are LCIGR = 0.37, U CIGR = 16.60. Thus the GR score of abc is statistically
signiﬁcant because this score lies between LCI and U CI values.

Deﬁnition 2. (Statistically signiﬁcant discriminative pattern). Given a
discriminance score scr ∈ {GR, ORS}, a discriminative threshold α and a lower
conﬁdence interval threshold β, the pattern p is statistically signiﬁcant discrim-
inative in D if scr(p, D) ≥ α and lciscr(p, D) > β.

4

H.S. Pham et al.

Problem statement: Given a two-class dataset D, a discriminance score
scr and two thresholds α and β, the problem is to discover the complete set of
patterns P that are statically signiﬁcant discriminative for dataset D, discrim-
inative measure scr, discriminative threshold α and lower conﬁdence interval
threshold β.

3 Enumeration Strategy

The main practical contribution of this paper is SSDPS, an eﬃcient algorithm
for mining statistically signiﬁcant discriminative patterns. This algorithm will
be presented in the next section (Section 4).

SSDPS owes its eﬃciency to an original enumeration strategy of the pat-
terns, which allows to exploit some degree of anti-monotonicity on the mea-
sures of discriminance and statistical signiﬁcance. In pattern mining enumera-
tion strategies, anti-monotonicity properties is an important component. When
enumerating frequent itemsets, one can notice that if an itemset p is unfrequent
(sup(p, D) < min sup), then no super-itemsets p(cid:48) ⊃ p can be frequent (nec-
essarily sup(p(cid:48), D) < sup(p, D) < min sup). This allows to stop any further
enumeration when an unfrequent itemset p is found, allowing a massive reduc-
tion in the search space [8]. As far as we know, no such anti-monotonicity could
be deﬁned on measures of discriminance or statistical signiﬁcance.

The enumeration strategy proposed in SSDPS also builds an enumeration
tree. However, it is based on the tidsets and not the itemsets. Each node of the
enumeration tree is a tidset (with the empty tidset at the root). For example,
consider the node represented by 12 : 8 in Figure 1: this node corresponds to
the tidset 128 in which 12 ⊂ D1.tids, and 8 ⊂ D2.tids.

Before presenting details of the enumeration strategy we ﬁrst explain how to
recover the itemsets from the tidsets. This is a well known problem: itemsets and
tidsets are in facts dual notions, and they can be linked by two functions that
form a Galois connection [9]. The main diﬀerence in our deﬁnition is that the
main dataset can be divided into two parts (D = D1 ∪ D2), and we want to be
able to apply functions of the Galois connection either in the complete dataset
D or in any of its parts D1 or D2.

Deﬁnition 3 (Galois connection). For a dataset D = D1 ∪ D2:

– For any tidset q ⊆ {1..n} and any itemset p ⊆ I, we deﬁne:

f (q, D) = {i ∈ I | ∀k ∈ q i ∈ tk};

g(p, D) = {k ∈ {1..n} | p ⊆ tk}

– For any tidset q1 ⊆ D1.tids and any itemset p ⊆ I, we deﬁne:

f1(q1, D1) = {i ∈ I | ∀k ∈ q1 i ∈ tk};

g1(p, D1) = {k ∈ D1 | p ⊆ tk}

– For any tidset q2 ⊆ D2.tids and any itemset p ⊆ I, we deﬁne:

f2(q2, D2) = {i ∈ I | ∀k ∈ q2 i ∈ tk};

g2(p, D2) = {k ∈ D2 | p ⊆ tk}

Statistically Signiﬁcant Discriminative Patterns Searching

5

Note that this deﬁnition marginally diﬀers from the standard deﬁnition pre-
sented in [9]: here for convenience we operate on the set of tids {1..n}, whereas
the standard deﬁnition operates on the set of transaction {t1, ..., tn}.

In Figure 1, under each tidset q, its associated itemset f (q, D) is displayed.
For example for node 12:8 , the itemset f (128, D) = bci is displayed. One can
verify in Table 1 that bci is the only itemset common to the transactions t1, t2
and t8. A closure operator can be deﬁned over the use of the Galois connection.

Deﬁnition 4 (Closure operator). For a dataset D and any tidset q ⊆ {1..n},
the closure operator is deﬁned as: c(q, D) = g ◦ f (q, D). The output of c(q, D) is
the tidset of the closed itemset having the smallest tidset containing q.

We can similarly deﬁne c1(q1, D1) = g1 ◦ f1(q1, D1) for q1 ⊆ D1.tids and

c2(q2, D2) = g2 ◦ f2(q2, D2) for q2 ⊆ D2.tids.

The basics of the enumeration have been given: the enumeration proceeds by
augmenting tidsets (starting from the empty tidset), and for each tidset function
f of the Galois connection gives the associated itemset. The speciﬁcity of our
enumeration strategy is to be designed around statistically signiﬁcant discrimi-
native patterns. This appears ﬁrst in our computation of closure: we divide the
computation of closure in the two sub-datasets D1 and D2. This intermediary
step allows some early pruning. Second, most measures of discriminance require
the pattern to have a non-zero support in D2 (GR and ORS). The same condi-
tion apply for measures of statistical signiﬁcance: in both cases we need to defer
measures of interest of patterns until it has some tids in D2. Our enumeration
strategy thus operates in two steps:

1. From the empty set, it enumerates closed tidsets containing only elements

of D1 (case group).

2. For each of those tidset containing only tids of D1, augmentations using
only tids of D2 are generated and their closure is computed. Any subsequent
augmentation of such nodes will only be allowed to be augmented by tids of
D2.

More formally, let q ⊆ {1..n} be a tidset, with q = q+ ∪ q−, where q+ ⊆

D1.tids and q− ⊆ D2.tids. Then the possible augmentations of q are:

– (Rule 1) if q− = ∅: q can either:

• (Rule 1a) be augmented with k ∈ D1.tids such that k < min(q+)
• (Rule 1b) be augmented with k ∈ D2.tids

– (Rule 2) if q− (cid:54)= ∅: q can only be augmented with tid k ∈ D2.tids such that

k < min(q−)

This enumeration strategy allows to beneﬁt from an anti-monotonicity prop-

erty on the measures of statistical signiﬁcance and discriminance.

Theorem 1 (Anti-monotonicity). Let q1 and q2 be two tidsets such as: q+
q+
1 ⊂ q−
2 and q−
f (q2, D). Then:

1 =
2 (cid:54)= ∅). Let p1 = f (q1, D) and p2 =

2 (we have q+

1 (cid:54)= ∅ and q−

6

H.S. Pham et al.

1. scr(p1, D) > scr(p2, D) with scr a discriminance measure in {SD, GR, ORS}.
2. lci(p1, D) > lci(p2, D) with lci a lower conﬁdence interval in {LCIORS, LCIGR}.

Please refer to the full paper at https://arxiv.org/abs/1906.01581 for the

detailed demonstration of this part.

This theorem provides pruning by anti-monotonicity in our enumeration
strategy: for a node having a tidset with tids both from D1.tids and D2.tids, if
the discriminance or statistical signiﬁcance measures are below a given thresh-
old, then necessarily its augmentations will also be under the threshold. Hence
this part of the enumeration tree can be pruned. For example, node 2:8 has
associated itemset bcei, and ORS(bcei, D) = 3/4. Suppose the ORS threshold
α = 2 this node can be pruned and its augmentations need not be computed.
This allows to signiﬁcantly reduce the search space.

{}

1 : -
abcf ij

2 : -
abcegi

3 : -
abcf hj

12 : -
abci

2 : 6
bceg

2 : 7
abcg

2 : 8
bcei

2 : 9
aeg

13 : -
abcf j

23 : -
abc

12 : 6
bc

12 : 7
abc

12 : 8
bci

12 : 9
a

2 : 67
bcg

2 : 68
dc

2 : 78
bc

2 : 69
e

2 : 79
a

2 : 89
e

123 : -
abc

12 : 68
bc

12 : 78
bc

12 : 678
bc

2 : 678
b

4 SSDPS: Algorithm Design

Fig. 1: Tidset-itemset search tree

123 : 7
abc

123 : 67
bc

This section presents the SSDPS algorithm which exploits the enumeration strat-
egy presented in the Section 3.

As mentioned in the previous section, our algorithm is based on an enu-
meration of the tidsets. It discovers statistically signiﬁcant discriminative closed
patterns. The main procedure for enumerating tidsets is given in Algorithm 1.
This procedure calls the recursive procedure positiveExpand (Algorithm 2) to
ﬁnd closed frequent itemsets in the positive class. Computing discriminative
patterns relies on the recursive procedure negativeExpand (Algorithm 3).

Delving more into details, positiveExpand (Algorithm 2) is based on the
principles of the LCM algorithm [10], the state of the art for mining closed fre-
quent itemsets. positiveExpand takes as input the tidset t of a pattern that is
closed in D1 and a tid e ∈ D1.tids that can be used to augment t. This augmen-
tation is performed on line 1, and the pattern p associated to the augmented
tidset t+ = t ∪ {e} is computed in line 2. If p = ∅, there are no items common

Statistically Signiﬁcant Discriminative Patterns Searching

7

Algorithm 1: SSDPS algorithm

input : D, α, β
output: a set of statistically signiﬁcant discriminative patterns

1 t = ∅
2 for each transaction id e in positive class do
3

positiveExpand(t, e, D, α, β)

to all transactions of t+ so the enumeration can stop (test of line 3). Else, we
can continue the enumeration by applying Rule 1 of enumeration presented in
Section 3. Lines 4 to 9 apply the LCM principles of enumerating closed itemsets
without redundancies (the interested reader in referred to [11] Section 3.2 for a
recent description of these principles). At this step of the enumeration, the clo-
sure is computed in D1 (line 4). The test of line 5 veriﬁes if the closure actually
extends the tidset, requiring a further veriﬁcation in line 6, and the construction
of the new extended tidset (line 7).

Lines 8 to 10 implement Rule 1a of enumeration, allowing to grow the positive
part of the tidset. Lines 11 to 12 implement Rule 1b of enumeration, stopping
the growth of the positive part and starting to grow the negative part of the
tidset. The same logic is followed in lines 14 to 18, in the case where the tidset
is not extended by the closure (test of line 5 is false).

Algorithm 2: positiveExpand Function
1 t+ ← t ∪ {e}
2 p ← f (t+, D)
3 if p is not empty then
t ext+ ← c1(t+, D1)
4
if t ext+ (cid:54)= t+ then

5

6

7

8

9

10

11

12

13

14

15

16

17

18

if max(t ext+) < e then
q ← t+ ∪ t ext+
for each e+ in D1.tids \ q do

if e+ < e then

positiveExpand(q, e+, D, α, β)

for each e− in D2.tids do

negativeExpand(q, e−, D, α, β)

else

for each e+ in D1.tids do
if e+ < min(t+) then

positiveExpand(t+, e+, D, α, β)

for each e− in D2.tids do

negativeExpand(t+, e−, D, α, β)

The ﬁnal expansion of the tidset is handled by negativeExpand (Algorithm
3), that can only perform augmentations with negative tidsets. It is very similar

8

H.S. Pham et al.

to positiveExpand, with several key diﬀerences. The ﬁrst obvious one is that
the closure is this time computed in D2 (line 4). The second one is that only
Rule 2 of enumeration can be applied (lines 13 and 20). The third and most
important diﬀerence is that because we have tidsets with positive and negative
tids, we can compute discriminance as well as statistical signiﬁcance measures.
Hence, Theorem 1 can be applied to beneﬁt from pruning by anti-monotonicity.
This is done in line 3.

Algorithm 3: negativeExpand Function
1 t− ← t ∪ {e} ;
2 if p is not empty then
3

p ← f (t−, D)

if check signif icance(p, D, α, β) is true then

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

t ext− ← c2(t−, D2)
if t ext− (cid:54)= t− then

if max(t ext−) < e then
q ← t− ∪ t ext−;
if q ext is empty then

q ext ← c(q, D);

p(cid:48) ← f (q, D)

if check signif icance(p(cid:48), D, α, β) is true then

output: p(cid:48)

for each e− ∈ D2.tids \ q do

if e− < e then

negativeExpand(q, e−, D, α, β)

else

t ext ← c(t−, D)
if t ext is empty then

output: p

for each e− ∈ D2.tids \ t− do

if e− < e then

negativeExpand(t−, e−, D, α, β)

5 Experimental Results

This section presents various experiments to evaluate the performance of the
SSDPS algorithm. In addition, we apply the SSDPS to discover multiple SNPs
combinations in a real genomic dataset. The details of the result was presented in
the full paper which was published at https://arxiv.org/abs/1906.01581. All ex-
periments have been conducted on a laptop with Core i7-4600U CPU @ 2.10GHz,
16GB memory and Linux operating system.

A synthetic two-class data was created to evaluate the pruning strategy as
well as compare SSDPS with other algorithms. This dataset includes 100 trans-
actions (50 transactions for each class). Each transaction contains 262 items
which are randomly set by value 0 or 1. The density of data is set up to 33%.

Statistically Signiﬁcant Discriminative Patterns Searching

9

5.1 Pruning Eﬃciency Evaluation

To evaluate the pruning eﬃciency of the SSDPS algorithm, we executed 2 setups
on the synthetic dataset.

– Setup 1: use OR as discriminative measure; the discriminative threshold

α = 2.

– Setup 2: use OR as discriminative measure and LCI of OR as statistically
signiﬁcant testing; the discriminative threshold α = 2, and LCI threshold
β = 2.

As the result, the running time and the number of output patterns signiﬁ-
cantly reduce when applying LCIORS. In particular, with the setup 1, the SS-
DPS algorithm generates 179,334 patterns in 38.69 seconds while the setup 2
returns 18,273 patterns in 9.10 seconds. This result shows that a large amount
of patterns is removed by using statistically signiﬁcant testing.

5.2 Comparison with Existing Algorithms

We compare the performance of the SSDPS algorithm with two well-known
algorithms: CIMCP [12] and SFP-Growth [5]. Note that these algorithms deploy
discriminative measures which are diﬀerent from the measures of SSDPS . In
particular, CIMCP uses one of measures such as chi-square, information-gain
and gini-index as a constraint to evaluate discriminative patterns while SFP-
GROWTH applies −log(p value). For this reason, the number of output patterns
and the running times of these algorithms should be diﬀerent. It is hence not fair
to directly compare the performance of SSDPS with these algorithms. However,
to have an initial comparison of the performance as well as the quantity of
discovered patterns, we select these algorithms.

We ran three algorithms on the same synthetic data. The used parameters

and results are given in Table 2.

Table 2: Used parameters and results of 3 algorithms

Algorithms Measure
SSDPS
CIMCP
SFP-GROWTH -log(p value)

OR, LCI ORS α = 2, β = 2
Chi-square

Threshold #Patterns
49,807
5,403,688

Time(seconds)
73.69
143
* > 172 (out of memory)

2
3

As the result, the SSDPS algorithm ﬁnds 49,807 patterns in 73.69 seconds;
CIMCP discovers 5,403,688 patterns in 143 seconds. The SFP-GROWTH runs
out of storage memory after 172 seconds. Hence the number of patterns isn’t
reported in this case.

In comparison with these algorithms the SSDPS gives a comparable perfor-
mance, while the number of output patterns is much smaller. The reason is that
the output patterns of SSDPS are tested for statistical signiﬁcance by CI while
other algorithms use only the discriminative measure. However, this amount of
patterns is also larger for real biological analysis. Thus, searching for a further
reduced number of signiﬁcant patterns should be taken into account.

10

H.S. Pham et al.

6 Conclusion and Perspectives

In this paper we propose a novel algorithm, called SSDPS, that eﬃciently dis-
cover statistically signiﬁcant discriminative patterns from a two-class dataset.
The algorithm directly uses discriminative measures and conﬁdence intervals as
anti-monotonic properties to eﬃciently prune the search space. Experimental
results show that the performance of the SSDPS algorithm is better than other
discriminative pattern mining algorithms. However, the number of patterns gen-
erated by SSDPS is still large for manual analysis. To reduce the amount of
patterns our ﬁrst perspective is to investigate a heuristic approach. Another per-
spective is to apply statistical techniques such as minimum description length or
multiple hypothesis testing in order to further remove uninteresting patterns.

References

1. G. Dong and J. Li, “Eﬃcient mining of emerging patterns: Discovering trends and
diﬀerences,” in Fifth ACM SIGKDD, ser. KDD ’99. New York, NY, USA: ACM,
1999, pp. 43–52.

2. S. Bay and M. Pazzani, “Detecting group diﬀerences: Mining contrast sets,” Kluwer

Academic Publishers, vol. 5, no. 3, pp. 213–246–, 2001.

3. H. Cheng, X. Yan, J. Han, and P. S. Yu, “Direct discriminative pattern mining for
eﬀective classiﬁcation,” ser. ICDE ’08. Washington, DC, USA: IEEE Computer
Society, 2008, pp. 169–178.

4. M. Garca-Borroto, J. Martnez-Trinidad, and J. Carrasco-Ochoa, “A survey of
emerging patterns for supervised classiﬁcation,” Springer Netherlands, vol. 42,
no. 4, pp. 705–721, 2014.

5. L. Ma, T. L. Assimes, N. B. Asadi, C. Iribarren, T. Quertermous, and W. H. Wong,
“An almost exhaustive search-based sequential permutation method for detecting
epistasis in disease association studies,” Genetic Epidemiology, vol. 34, no. 5, pp.
434–443, 2010.

6. T. Guns, S. Nijssen, and L. D. Raedt, “Itemset mining: A constraint programming

perspective,” Elsevier, 2011.

7. J. A. Morris and M. J. Gardner, “Statistics in medicine: Calculating conﬁdence
intervals for relative risks (odds ratios) and standardised ratios and rates,” British
Medical Journal, vol. 296, no. 6632, pp. 1313–1316, May 1988.

8. R. Agrawal, T. Imieli´nski, and A. Swami, “Mining association rules between sets
of items in large databases,” SIGMOD Rec., vol. 22, no. 2, pp. 207–216, Jun. 1993.
9. N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, “Discovering frequent closed
itemsets for association rules,” ser. ICDT ’99. London, UK, UK: Springer-Verlag,
1999, pp. 398–416.

10. T. Uno, M. Kiyomi, and H. Arimura, “Lcm ver. 2: Eﬃcient mining algorithms
for frequent/closed/maximal itemsets,” in Workshop Frequent Item Set Mining
Implementations, 2004.

11. V. Leroy, M. Kirchgessner, A. Termier, and S. Amer-Yahia, “Toppi: An eﬃcient

algorithm for item-centric mining,” Inf. Syst., vol. 64, pp. 104–118, 2017.

12. T. Guns, S. Nijssen, and L. De Raedt, “Itemset mining: A constraint programming

perspective,” Artiﬁcial Intelligence, vol. 175, no. 12, pp. 1951–1983, 2011.

