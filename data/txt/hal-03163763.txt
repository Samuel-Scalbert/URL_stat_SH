Bandit Algorithm for Both Unknown Best Position and
Best Item Display on Web Pages
Camille-Sovanneary Gauthier, Romaric Gaudel, Elisa Fromont

To cite this version:

Camille-Sovanneary Gauthier, Romaric Gaudel, Elisa Fromont. Bandit Algorithm for Both Unknown
Best Position and Best Item Display on Web Pages. IDA 2021 - 19th International Symposium on
Intelligent Data Analysis, Apr 2021, Porto (virtual), Portugal. pp.12. ￿hal-03163763￿

HAL Id: hal-03163763

https://hal.science/hal-03163763

Submitted on 9 Mar 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Bandit Algorithm for Both Unknown Best
Position and Best Item Display on Web Pages

Camille-Sovanneary Gauthier1,3, Romaric Gaudel2, and Elisa Fromont3

1 Louis Vuitton, Paris (FR) camille-sovanneary.gauthier@louisvuitton.com
2 Univ. Rennes, Ensai, CNRS, CREST, Rennes (FR) romaric.gaudel@ensai.fr
3 Univ. Rennes, IUF, Inria, IRISA, Rennes (FR) elisa.fromont@irisa.fr

Abstract. Multiple-play bandits aim at displaying relevant items at
relevant positions on a web page. We introduce a new bandit-based
algorithm, PB-MHB, for online recommender systems which uses the
Thompson sampling framework with Metropolis-Hastings approxima-
tion. This algorithm handles a display setting governed by the position-
based model. Our sampling method does not require as input the prob-
ability of a user to look at a given position in the web page which is
diﬃcult to obtain in some applications. Experiments on simulated and
real datasets show that our method, with fewer prior information, deliv-
ers better recommendations than state-of-the-art algorithms.

Keywords: Multi-armed Bandit, Position-Based Model, Thomson Sam-
pling, Metropolis-Hasting.

1

Introduction

An online recommender systems (ORS) chooses an item to recommend to a user
among a list of N potential items. The relevance of the item is measured by
the users’ feedback: clicks, time spent looking at the item, rating, etc. Since a
feedback is only available when an item is presented to a user, the ORS needs
to present both attractive items (a.k.a. exploit) to please the current user, and
some items with an uncertain relevance (a.k.a. explore) to reduce this uncertainty
and perform better recommendations to future users. It faces the exploration-
exploitation dilemma expressed by the multi-armed bandit setting [3].

On websites, online recommender systems select L items per time-stamp,
corresponding to L speciﬁc positions in which to display an item. Typical exam-
ples of such systems are (i) a list of news, visible one by one by scrolling; (ii) a
list of products, arranged by rows; or (iii) advertisements spread in a web page.
To be selected (clicked) by a user in such context, an item neds to be relevant by
itself, but also to be displayed at the right position. Several models express the
way a user behaves while facing such a list of items [23,9] and they have been
transposed to the bandit framework [15,14].

Retailers often spread their commercials over a web page or display their
items on several rows all at once. Thus, in this paper, we will focus on the
Position-Based Model (PBM) [23]. This model assumes that the probability to

2

Gauthier et al.

click on an item i in position (cid:96) results only from the combined impact of this item
and its position: items displayed at other positions do not impact the probability
to consider the item at position (cid:96). PBM also gives a user the opportunity to give
more than one feedback: she may click on all the items relevant for her. It means
we are facing the so-called multiple-play (semi-)bandit setting [5]. PBM setting
is particularly interesting when the display is dynamic, as often on modern web
pages, and may depend on the reading direction of the user (which varies from
one country to another) and on the ever-changing layout of the page.

Contribution. We introduce PB-MHB (Position Based Metropolis-Hastings
Bandit), a bandit algorithm designed to handle PBM with a Thompson sampling
framework. This algorithm does not require the knowledge of the probability of
a user to look at a given position: it learns this probability from past recom-
mendations/feedback. This is a strong improvement w.r.t. previous attempts
in this research line [13,17] as it allows the use of PB-MHB in contexts where
this information is not obvious. This improvement results from the use of the
Metropolis-Hastings framework [22] to sample parameters given their true a pos-
teriori distribution, even thought it is not a usual distribution. While Markov
Chain Monte Carlo methods are well-known and extensively used in Bayesian
statistics, they were rarely used for Thomson Sampling [12], [10], [24], [21] and it
is the ﬁrst time that the Metropolis-Hastings framework is used in the PBM set-
ting. Besides this speciﬁcity, we also experimentally show that PB-MHB suﬀers
a smaller regret than its competitors.

The paper is organized as follows: Section 2 presents the related work and
Section 3 precisely deﬁnes our setting. PB-MHB is introduced in Section 4 and
is experimentally compared to state-of-the-art algorithms in Section 5. We con-
clude in Section 6.

2 Related Work

PBM [9,23] relies on two vectors of parameters: θθθ ∈ [0, 1]N and κκκ ∈ [0, 1]L, where
θθθi is the probability for the user to click on item i when she observes that item,
and κκκ(cid:96) is the probability for the user to observe the position (cid:96). These parameters
are unknown, but they may be inferred from user behavior data: we need to ﬁrst
record the user feedback (click vs. no-click per position) for each set of displayed
items, then we may apply an expectation-maximization framework to compute
the maximum a posteriori values for (θθθ, κκκ) given these data [7].

PBM is transposed to the bandit framework in [13,14,17]. [13] and [17] pro-
pose two approaches based on a Thompson sampling (TS) framework, with two
diﬀerent sampling strategies. [17] also introduce several approaches based on the
optimism in face of uncertainty principle [3]. However, the approaches in [13,17]
assume κκκ known beforehand. [14] proposes the only approach learning both θθθ
and κκκ while recommending but it still requires the κκκ(cid:96) values to be organized
in decreasing order, which we do not require. Note also that the corresponding
approach is not based, as ours, on Thompson sampling.

PB-MHB

3

While more theoretical results are obtained when applying the optimism in
face of uncertainty principle to the bandit setting, approaches based on TS are
known to deliver more accurate recommendations [1,4,6,12]. The limitation of
TS is its requirement to draw from ‘exotic’ distributions when dealing with com-
plex models. By limiting themselves to a setting where κκκ is known, [13] and [17]
face simpler distributions than the one which arises from κκκ being unknown. In
the following, we propose to use Metropolis-Hastings framework to handle this
harder distribution. [10,24] investigate a large range of distribution approxima-
tion strategies to apply TS framework to the distributions arising from the con-
textual bandit setting, and [12,21] apply approximate sampling to other settings.
Overall, these articles handle a pure bandit setting while we are in a semi-bandits
setting: for each recommendation we receive as reward a list of 1 or 0 (click or
not). As most of commercial website can track precisely on which product each
client clicks, we aim at exploiting that ﬁne-grain information.

The cascading model (CM) [9] is another popular model of user behavior.
It assumes that the positions are observed in a known order and that the user
leaves the website as soon as she clicks on an item4. More speciﬁcally, if she
clicks on the item in position (cid:96), she will not look at the following positions:
(cid:96) + 1, . . . , L. This setting has been extensively studied within the bandit frame-
work [6,8,11,15,16,20,27]. However, the assumption of CM regarding the order
of observation is irrelevant when considering items spread in a page or on a
grid (especially as reading direction of the user may varies from one country to
another).

A few approaches simultaneously handle CM and PBM [18,19,26]. Their
genericity is their strength and their weakness: they do not require the knowledge
of the behavioral model, but they cannot use that model to speed-up the process
of learning the user preferences. Moreover, these algorithms assume that the best
recommendation consists in ordering the items from the more attractive to the
less attractive ones. In the context of PBM, this is equivalent to assuming κκκ to
be sorted in decreasing order. Our algorithm does not make such assumption.
Anyhow, we also compare PB-MHB to TopRank [18] in Section 5 in order to
ensure that our model beneﬁts from the knowledge of the click model.

3 Recommendation Setting

The proposed approach handles the following online recommendations setting:
at each time-stamp t, the ORS chooses a list iii(t) = (iii1(t), . . . , iiiL(t)) of L dis-
tinct items among a set of N items. The user observes each position (cid:96) with a
probability κκκ(cid:96), and if the position is observed, the user clicks on the item iii(cid:96) with
a probability θθθiii(cid:96). We denote rrr(cid:96)(t) ∈ {0, 1} the reward in position (cid:96) obtained
by proposing iii at time t, namely 1 if the user did observe the position (cid:96) and
clicked on item iii(cid:96)(t), and 0 otherwise. We assume that each draw is independent,

4 Some reﬁned models assume a probability to leave. With these models, the user may

click on several items.

4

Gauthier et al.

meaning rrr(cid:96)(t) | iii(cid:96)(t) iid.∼ Bernoulli (θθθiii(cid:96)κκκ(cid:96)) , or in other words

(cid:40)P (rrr(cid:96)(t) = 1 | iii(cid:96)(t)) = θθθiii(cid:96)κκκ(cid:96),
P (rrr(cid:96)(t) = 0 | iii(cid:96)(t)) = 1 − θθθiii(cid:96)κκκ(cid:96).

The ORS aims at maximizing the cumulative reward, namely the total num-

ber of clicks gathered from time-stamp 1 to time-stamp T : (cid:80)T

(cid:80)L

(cid:96)=1 rrr(cid:96)(t).

t=1

Without loss of generality, we assume that max(cid:96) κκκ(cid:96) = 1.5 To keep the no-
tations simple, we also assume that θθθ1 > θθθ2 > · · · > θθθN , and κκκ1 = 1 > κκκ2 >
· · · > κκκL.6 The best recommendation is then iii∗ = (1, 2, . . . , L), which leads to
the expected instantaneous reward µ∗ = (cid:80)L

(cid:96)=1 θθθ(cid:96)κκκ(cid:96).

The pair (θθθ, κκκ) is unknown from the ORS. It has to infer the best recommen-
dation from the recommendations and the rewards gathered at previous time-
stamps, denoted D(t) = {(iii(1), rrr(1)), . . . , (iii(t − 1), rrr(t − 1))}. This corresponds
to the bandit setting where it is usual to consider the (cumulative pseudo-)regret

RT

def
=

T
(cid:88)

L
(cid:88)

t=1

(cid:96)=1

E [rrr(cid:96) | iii∗

(cid:96) ] −

T
(cid:88)

L
(cid:88)

t=1

(cid:96)=1

E [rrr(cid:96)(t) | iii(cid:96)(t)] = T µ∗ −

T
(cid:88)

L
(cid:88)

t=1

(cid:96)=1

θθθiii(cid:96)(t)κκκ(cid:96). (1)

The regret RT denotes the cumulative expected loss of the ORS w.r.t. the or-
acle recommending the best items at each time-stamp. Hereafter we aim at an
algorithm which minimizes the expectation of RT w.r.t. its choices.

4 PB-MHB Algorithm

We handle the setting presented in the previous section with the online recom-
mender system depicted by Algorithm 1 and referred to as PB-MHB (for Position
Based Metropolis-Hastings Bandit). This algorithm is based on the Thompson
sampling framework [25,2]. First, we look at rewards with a fully Bayesian point
of view: we assume that they follow the statistical model depicted in Section 3,
and we choose a uniform prior on the parameters θθθ and κκκ. Therefore the pos-
terior probability for these parameters given the previous observations D(t) is

P (θθθ, κκκ|D(t)) ∝

N
(cid:89)

L
(cid:89)

i=1

(cid:96)=1

(θθθiκκκ(cid:96))Si,(cid:96)(t) (1 − θθθiκκκ(cid:96))Fi,(cid:96)(t) ,

(2)

s=1

s=1

where Si,(cid:96)(t) = (cid:80)t−1
1iii(cid:96)(s)=i1rrr(cid:96)(s)=1 denotes the number of times the item i
has been clicked while being displayed in position (cid:96) from time-stamp 1 to t − 1,
and Fi,(cid:96)(t) = (cid:80)t−1
1iii(cid:96)(s)=i1rrr(cid:96)(s)=0 denotes the number of times the item i has
not been clicked while being displayed in position (cid:96) from time-stamp 1 to t − 1.
Second, we choose the recommendation iii(t) at time-stamp t according to its
posterior probability of being the best arm. To do so, we denote (˜θθθ, ˜κκκ) a sample
of parameters (θθθ, κκκ) according to their posterior probability, we keep the best
items given ˜θθθ, and we display them in the right order given ˜κκκ.

5 As stated in [14], we may replace (θθθ, κκκ) by (θθθ. max(cid:96) κκκ(cid:96), κκκ/ max(cid:96) κκκ(cid:96)).
6 Our algorithm and the experiments only assume κκκ1 = 1.

PB-MHB

5

Algorithm 1 PB-MHB, Metropolis-Hastings based bandit for Position-Based
Model

D(1) ← {}
for t = 1, . . . do

draw (˜θθθ, ˜κκκ) ∼ P (θθθ, κκκ|D(t)) using Algorithm 2
display the L items with greatest value in ˜θθθ, ordered by decreasing values of ˜κκκ
get rewards rrr(t)
D(t + 1) ← D(t) ∪ (iii(t), rrr(t))

end for

4.1 Sampling w.r.t. the Posterior Distribution

The posterior probability (2) does not correspond to a well-known distribution.
[13], [17] tackle this problem by considering that κκκ is known in order to manipu-
late N independent simpler distributions Pi (θθθi|θθθ−i, κκκ, D(t)). By having κκκ and θθθ
both unknown, we have to handle a law for which the components θθθ1, . . . , θθθN and
κκκ1, . . . , κκκL are correlated (see Equation 2). We handle it thanks to a carefully
designed Metropolis-Hastings algorithm [22] (cf. Algorithm 2). This algorithm
consists in building a sequence of m samples (θθθ(1), κκκ(1)), . . . , (θθθ(m), κκκ(m)) such
that (θθθ(m), κκκ(m)) follows a good approximation of the targeted distribution. It
is based on a Markov chain on parameters (θθθ, κκκ) which admits the targeted
probability distribution as its unique stationary distribution.

At iteration s, the sample (θθθ(s), κκκ(s)) moves toward sample (θθθ(s+1), κκκ(s+1))
by applying (N + L − 1) transitions: one per item and one per position except
for κκκ1. Let’s start by focusing on the transition regarding item i (Lines 5–9) and
denote (θθθ, κκκ) the sample before the transition.

The algorithm aims at sampling a new value for θθθi according to its posterior

probability given other parameters and the previous observations D(t):

Pi (θθθi|θθθ−i, κκκ, D(t)) ∝

L
(cid:89)

(cid:96)=1

θθθi

Si,(cid:96)(t) (1 − θθθiκκκ(cid:96))Fi,(cid:96)(t) ,

(3)

where θθθ−i denotes the components of θθθ except for the i-th one. This transition
consists in two steps:
1. draw a candidate value ˜θθθ after a proposal probability distribution
discussed later on;

(cid:17)
(cid:16)˜θθθ | θθθi, θθθ−i, κκκ, D(t)

q

2. accept that candidate or keep the previous sample:

θθθi ←


˜θθθ


, w. prob. min

1,

(cid:18)

Pi(˜θθθ|θθθ−i,κκκ,D(t))
Pi(θθθi|θθθ−i,κκκ,D(t))

q(θθθi|˜θθθ,θθθ−i,κκκ,D(t))
q(˜θθθ|θθθi,θθθ−i,κκκ,D(t))

(cid:19)

.



θθθi−1

, otherwise

This acceptance step yields two behaviours:

–

Pi(˜θθθ|θθθ−i,κκκ,D(t))
Pi(θθθi|θθθ−i,κκκ,D(t)) measures how likely the candidate value is compared to the
previous one, w.r.t. the posterior distribution,

6

Gauthier et al.

Algorithm 2 Metropolis-Hastings applied to the distribution of Equation (2)

√

Require: D(t): previous recommendations and rewards
Require: σ = c/
t: Gaussian random-walk steps width
Require: m: number of iterations
1: draw (θθθ, κκκ) after uniform distribution
2: κκκ1 ← 1
3: for s = 1, . . . , m do
4:
5:
6:
7:

draw ˜θθθ ∼ N (θθθi, σ)

until 0 (cid:54) ˜θθθ (cid:54) 1

for i = 1, . . . , N do

repeat

(cid:18)

Pi(˜θθθ|θθθ−i,κκκ,D(t))
Pi(θθθi|θθθ−i,κκκ,D(t))

∆Φσ (θθθi)
∆Φσ(˜θθθ)

(cid:19)

with prob. min

1,

θθθi ← ˜θθθ

end for
for (cid:96) = 2, . . . , L do

repeat

draw ˜κκκ ∼ N (κκκ(cid:96), σ)

until 0 (cid:54) ˜κκκ (cid:54) 1

with prob. min

1,

(cid:18)

P(cid:96)(˜κκκ|θθθ,κκκ−(cid:96),D(t))
P(cid:96)(κκκ(cid:96)|θθθ,κκκ−(cid:96),D(t))

∆Φσ (κκκ(cid:96))
∆Φσ (˜κκκ)

(cid:19)

8:

9:
10:
11:
12:
13:
14:

15:

κκκ(cid:96) ← ˜κκκ

end for

16:
17:
18: end for
19: return (θθθ, κκκ)

–

q(θθθi|˜θθθ,θθθ−i,κκκ,D(t))
q(˜θθθ|θθθi,θθθ−i,κκκ,D(t))

downgrades candidates easily reached by the proposal q.

(cid:17)
(cid:16)˜θθθ | θθθi, θθθ−i, κκκ, D(t)

Algorithm 2 uses a truncated Gaussian random-walk proposal for the pa-
rameter θθθi, with a Gaussian step of standard deviation σ (see Lines 5–7). Note
that due to the truncation, the probability to get the proposal ˜θθθ starting from
= φ(˜θθθ | θθθi, σ)/∆Φσ(θθθi), where φ(· | θθθi, σ) is the
θθθi is q
probability associated to the Gaussian distribution with mean θθθi and standard
deviation σ, Φ(· | θθθi, σ) is its cumulative distribution function, and ∆Φσ(θθθi) =
Φ(1 | θθθi, σ) − Φ(0 | θθθi, σ). The probability to get the proposal θθθi starting from ˜θθθ
is similar, which reduces the ratio of proposal probabilities at Line 8 to

(cid:17)
(cid:16)
θθθi | ˜θθθ, θθθ−i, κκκ, D(t)
(cid:16)˜θθθ | θθθi, θθθ−i, κκκ, D(t)

(cid:17) =

q

q

∆Φσ (θθθi)
(cid:17) .
(cid:16)˜θθθ

∆Φσ

The transition regarding parameter κκκ(cid:96) involves the same framework: the
proposal is a truncated Gaussian random-walk step and aims at the probability

P(cid:96) (κκκ(cid:96)|θθθ, κκκ−(cid:96), D(t)) ∝

N
(cid:89)

i=1

κκκ(cid:96)

Si,(cid:96)(t) (1 − θθθiκκκ(cid:96))Fi,(cid:96)(t) .

(4)

PB-MHB

7

4.2 Overall Complexity

The computational complexity of Algorithm 1 is driven by the number of random-
walk steps done per recommendation: m(N + L − 1), which is controlled by the
parameter m. This parameter corresponds to the burning period: the number
of iterations required by the Metropolis-Hastings algorithm to draw a point
(θθθ(m), κκκ(m)) almost independent from the initial one. While the requirement for
a burning period may refrain us from using a Metropolis-Hasting algorithm in
such recommendation setting, we demonstrate in the following experiments that
the required value for m remains reasonable. We drastically reduce m by start-
ing the Metropolis-Hasting call from the point used to recommend at previous
time-stamp. This corresponds to replacing Line 1 in Algorithm 2 by:
1: (θθθ, κκκ) ← (˜θθθ, ˜κκκ) used for the previous recommendation .

5 Experiments

In this section we demonstrate the beneﬁt of the proposed approach both on
two artiﬁcial and two real-life datasets. Note that whatever real-life data we are
using, we can only use them to compute the parameters θθθ and κκκ and simulate at
each time-stamp a “real” user feedback (i.e. clicks) by applying PBM with these
parameters. This is what is usually done in the literature since the recommenda-
tions done by a bandit are very unlikely to match the recommendations logged
in the ground truth data and without a good matching, it would be impossible
to compute a relevant reward for each interaction. Code and data for replicating
our experiments are available at https://github.com/gaudel/ranking bandits.

5.1 Datasets

In the experiments, the online recommender systems are required to deliver T
consecutive recommendations, their feedbacks being drawn from a PBM dis-
tribution. We consider two settings denoted purely simulated and behavioral
in the remaining. With the purely simulated setting, we choose the value of
the parameters (θθθ, κκκ) to highlight the stability of the proposed approach even
for extreme settings. Namely, we consider N = 10 items, L = 5 positions,
and κκκ = [1, 0.75, 0.6, 0.3, 0.1]. The range of values for θθθ is either close to zero
(θθθ− = [10−3, 5.10−4, 10−4, 5.10−5, 10−5, 10−6, . . . , 10−6]) or close to one (θθθ+ =
[0.99, 0.95, 0.9, 0.85, 0.8, 0.75, . . . , 0.75]).

With the behavioral setting, the values for κκκ and θθθ are obtained from true
users behavior as in [17], [14]. Two behavioral datasets are considered and pre-
sented hereafter. The ﬁrst one is KDD Cup 2012 track 2 dataset, which consists
of session logs of soso.com, a Tencent’s search engine. It tracks clicks and dis-
plays of advertisements on a search engine result web-page, w.r.t. the user query.
For each query, 3 positions are available for a various number of ads to display.
To follow previous works, instead of looking for the probability to be clicked
per display, we target the probability to be clicked per session. This amounts

8

Gauthier et al.

to discarding the information Impression. We also ﬁlter the logs to restrict the
analysis to (query, ad) couples with enough information: for each query, ads are
excluded if they were displayed less than 1,000 times at any of the 3 possible
positions. Then, we ﬁlter queries that have less than 5 ads satisfying the previous
condition. We end up with 8 queries and from 5 to 11 ads per query. Finally, for
each query q, the parameters (θθθ[q], κκκ[q]) are set from the Singular Value Decom-
position (SVD) of the matrix MMM [q] ∈ RN ×L which contains the probability to
be clicked for each item in each position. By denoting ζ [q], the greatest singular
value of MMM [q], and uuu[q] (respectively vvv[q]) the left (resp. right) singular vector as-
1 ζ [q]uuu[q] and κκκ[q] def
sociated to ζ [q], we set θθθ[q] def
1 = 1,
and θθθ[q]T
κκκ[q] = ζuuu[q]T
vvv[q]. This leads to θθθi values ranging from 0.004 to 0.149,
depending on the query.

1 , such that κκκ[q]

= vvv[q]/vvv[q]

= vvv[q]

The second behavioral dataset is Yandex7. As in [18], we select the 10 most
frequent queries, and for each query the ORS displays 5 items peeked among the
10 most attractive ones. As for KDD dataset, the parameters (θθθ[q], κκκ[q]) are set
from an SVD. This leads to θθθi values ranging from 0.070 to 0.936, depending on
the query.

5.2 Competitors

We compare the performance of PB-MHB with the performance of PMED [14],
TopRank [18], and the standard baseline εn-greedy [3].

PMED is designed to match a lower-bound on the expected regret of any
reasonable algorithm under the PBM assumption. Let us recall that it assumes
κκκ(cid:96) values to be decreasing, which means the ORS knows in advance which is the
most observed position, which is the second most observed position, and so on.
PB-MHB does not require this ordering, it learns it from interactions with users.
PMED uniform-exploration parameter α is ﬁxed to 1. Due to its very high time
complexity, experiments with PMED are stopped after 5 hours for each dataset
(which corresponds to about 105 recommendations).

TopRank handles a wider range of click models than PB-MHB, but it also
assumes the knowledge of the order on positions. TopRank hyper-parameter δ
is set to 1/T as recommended by Theorem 1 in [18].

Finally, we compare PB-MHB to (cid:15)n-Greedy. At each time-stamp t, an es-
timation (ˆθθθ, ˆκκκ) of parameters (θθθ, κκκ) is obtained applying SVD to the collected
data. Let us denote ˆiii(t) the recommendation with the highest expected reward
given the inferred values (ˆθθθ, ˆκκκ). A greedy algorithm would recommend ˆiii(t). Since
this algorithm never explores, it may end-up recommending a sub-optimal af-
fectation. (cid:15)n-Greedy counters this by randomly replacing each item of the rec-
ommendation with a probability ε(t) = c/t, where c is a hyper-parameter to be
tuned. In the following, we plot the results obtained with the best possible value

7 Yandex

challenge,
https://www.kaggle.com/c/yandex-personalized-web-search-challenge

personalized

search

web

2013.

PB-MHB

9

(a) Impact of c

(b) Impact of m and random start

√

Fig. 1: Cumulative regret w.r.t. time-stamp on Yandex data. Impact of the width
t of Gaussian random-walk steps (left). Impact of the use of the parameters
c/
from the previous time-stamp to warm-up the Metropolis-Hasting algorithm and
of the number m of Metropolis-Hastings iterations per recommendation (right).
The shaded area depicts the standard error of our regret estimates.

for c, while trying c in {100, 101, . . . , 106}. Note that the best value for c varies
a lot from a dataset to another.

In the experiments presented hereafter the requirements of each algorithm
are enforced. Namely, κκκ(cid:96) values are decreasing when running experiments with
PMED and TopRank.

5.3 Results

We compare the previously presented algorithms on the basis of the cumulative
regret (see Equation (1)), which is the sum, over T consecutive recommendations,
of the diﬀerence between the expected reward of the best possible answer and
of the answer of a given recommender system. The regret will be plotted with
respect to T on a log-scale basis. The best algorithm is the one with the lowest
regret. The regret plots are bounded by the regret of the oracle (0) and the regret
of a recommender system choosing the items uniformly at random. We average
the results of each algorithm over 20 independant sequences of recommendations
per query.

√

PB-MHB Hyper-parameters PB-MHB behavior is aﬀected by two hyper-
t of the Gaussian random-walk steps, and the num-
parameters: the width c/
ber m of Metropolis-Hastings iterations per recommendation. Overall, when
Metropolis-Hastings runs start from the couple (˜θθθ, ˜κκκ) from the previous time-
stamp, we show in Figure 1a that PB-MHB exhibits the smallest regret on Yan-
dex as soon as c > 1000 and m = 1. Note that m = 1 is also the setting which
minimizes the computation time of PB-MHB. These hyper-parameter choices
also hold for the 3 other datasets (not shown here for lack of space).

We now discuss the impact of choosing other hyper-parameter values on
Yandex data. The regret is the smallest as soon as c is large enough (c >= 100).
In Figure 1b, we illustrate the impact of m. It yields a high regret only when c and
m are both too small (full blue curve): when the random-walk steps are too small
the Metropolis-Hasting algorithm requires more iterations to get uncorrelated

100102104Time-stamp101100101102103104Cumulative Expected RegretPB-MHB, m=1, c=101PB-MHB, m=1, c=100PB-MHB, m=1, c=101PB-MHB, m=1, c=102PB-MHB, m=1, c=103100102104Time-stamp101100101102103104Cumulative Expected RegretPB-MHB, m=1,    c=100PB-MHB, m=10,  c=100PB-MHB, m=1,    c=103PB-MHB, m=10,  c=103PB-MHB, m=1,    c=103, rand. start10

Gauthier et al.

(a) KDD

(b) Yandex

(c) Simulated, θθθ− (d) Simulated, θθθ+

Fig. 2: Cumulative regret w.r.t. time-stamp on ﬁve diﬀerent settings for all com-
petitors. The plotted curves correspond to the average over 20 independant se-
quences of recommendations per query (in total: 20 sequences for simulated data,
160 sequences for KDD and 200 sequences for Yandex). The shaded area depicts
the standard error of our regret estimates. For (cid:15)n-Greedy, c is set to 104 for KDD
and Yandex settings, to 105 when θθθ is close to 0, and to 103 when θθθ is close to 1.

samples (˜θθθ, ˜κκκ). For reasonable values of c, m has no impact on the regret. Figure
1b also shows the impact of keeping the parameters from the previous time-
stamp compared to a purely random start. Starting from a new randomly drawn
set of parameters would require more than m = 10 iterations to obtain the
same result, meaning a computation budget more than 10 times higher. This
behavior is explained by the gap between the uniform law (which is used to draw
the starting set of parameters) and the targeted law (a posteriori law of these
parameters) which concentrates around its MAP. Even worse, this gap increases
while getting more and more data since the a posteriori law concentrates with
the increase of data. As a consequence, the required value for m increases along
time when applying a standard Metropolis-Hasting initialisation, which explains
why the dotted red line diverges from the solid one around time-stamp 30.

Comparison with Competitors Figure 2 compares the regret obtained by
PB-MHB and its competitors on datasets with various click and observation
probabilities. Up to time-stamp 104, PMED and PB-MHB exhibit the smallest
regret on all settings. Thereafter, PMED is by far the algorithm with the smallest
regret. Regarding computation time, apart from PMED all the algorithms re-
quire less than 20 ms per recommendation which remains aﬀordable: (cid:15)n-Greedy
is the fastest with less than 0.5ms per recommendation8; then TopRank and
PB-MHB require 10 to 40 times more computation time than (cid:15)n-Greedy; ﬁnally,

8 Computation time for a sequence of 107 recommendations vs. the ﬁrst query of Yan-
dex data, on an Intel Xeon E5640 CPU2.67GHz with 50 GB RAM. The algorithms
are implemented in Python.

101102103104105106Time-stamp100101102103104105Cumulative Expected Regret101102103104105106Time-stamp100101102103104105Cumulative Expected Regret102103104105106Time-stamp101100101102103Cumulative Expected Regret101102103104105106Time-stamp100101102103104105Cumulative Expected Regretn-greedy, c=104PB_MHB, c=103, m=1TopRankPMEDPB-MHB

11

PMED requires more than 150ms per recommendation which makes it imprac-
tical regardless of its low regret.

6 Conclusion

We have introduced a new bandit-based algorithm, PB-MHB, for online recom-
mender systems in the PBM which uses a Thompson sampling framework to
learn the κκκ and θθθ parameters of this model instead of assuming them given.
Experiments on simulated and real datasets show that our method (i) suﬀers a
smaller regret than its competitors having access to the same information, and
(ii) suﬀers a similar regret as its competitors using more prior information.

These results are still empirical but we plan to formally prove them in future
work. Indeed, [21] upper-bounded the regret of a Thompson Sampling bandit
algorithm, while using Langevin Monte-Carlo to sample posterior values of pa-
rameters. That could be a good starting point for theoretically studying the
convergence of PB-MHB. We also would like to improve our algorithm by fur-
ther working on the proposal law to draw candidates for the sampling part. The
proposal is currently a truncated random walk. By managing it diﬀerently (with
a logit transformation for instance) we could improve both the time and precision
performance. On the other hand, with a better understanding of the evolution
of the target distribution, we could also improve the sampling part. Moreover,
we would like to apply PB-MHB to environments where κκκ is evolving with time
(with new marketing trends) and where our learning setting could develop its
full potential.

References

1. Agrawal, S., Goyal, N.: Thompson sampling for contextual bandits with linear
payoﬀs. In: proc. of the 30th Int. Conf. on Machine Learning. ICML’13 (2013)
2. Agrawal, S., Goyal, N.: Near-optimal regret bounds for thompson sampling. Jour.

of the ACM, JACM 64(5), 30:1–30:24 (Sep 2017)

3. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed

bandit problem. Machine Learning 47(2), 235–256 (May 2002)

4. Chapelle, O., Li, L.: An empirical evaluation of thompson sampling. In: Advances

in Neural Information Processing Systems 24. NIPS’11 (2011)

5. Chen, W., Wang, Y., Yuan, Y.: Combinatorial multi-armed bandit: General frame-
work and applications. In: proc. of the 30th Int. Conf. on Machine Learning.
ICML’13 (2013)

6. Cheung, W.C., Tan, V., Zhong, Z.: A thompson sampling algorithm for cascading
bandits. In: proc. of the 22nd Int. Conf. on Artiﬁcial Intelligence and Statistics.
AISTATS’19 (2019)

7. Chuklin, A., Markov, I., de Rijke, M.: Click Models for Web Search. Morgan &

Claypool Publishers (2015)

8. Combes, R., Magureanu, S., Prouti`ere, A., Laroche, C.: Learning to rank: Regret
lower bounds and eﬃcient algorithms. In: proc. of the 2015 ACM SIGMETRICS
Int. Conf. on Measurement and Modeling of Computer Systems (2015)

12

Gauthier et al.

9. Craswell, N., Zoeter, O., Taylor, M., Ramsey, B.: An experimental comparison of
click position-bias models. In: proc. of the 2008 Int. Conf. on Web Search and Data
Mining. WSDM ’08 (2008)

10. Dumitrascu, B., Feng, K., Engelhardt, B.: Pg-ts: Improved thompson sampling for
logistic contextual bandits. In: Advances in Neural Information Processing Systems
31. NIPS’18 (2018)

11. Katariya, S., Kveton, B., Szepesv´ari, C., Wen, Z.: Dcm bandits: Learning to rank
with multiple clicks. In: proc. of the Int. Conf. on Machine Learning. ICML (2016)
12. Kawale, J., Bui, H.H., Kveton, B., Tran-Thanh, L., Chawla, S.: Eﬃcient thompson
sampling for online matrix factorization recommendation. In: Advances in Neural
Information Processing Systems 28. NIPS’15 (2015)

13. Komiyama, J., Honda, J., Nakagawa, H.: Optimal regret analysis of thompson
sampling in stochastic multi-armed bandit problem with multiple plays. In: proc.
of the 32nd Int. Conf. on Machine Learning. ICML’15 (2015)

14. Komiyama, J., Honda, J., Takeda, A.: Position-based multiple-play bandit prob-
lem with unknown position bias. In: Advances in Neural Information Processing
Systems 30. NIPS’17 (2017)

15. Kveton, B., Szepesv´ari, C., Wen, Z., Ashkan, A.: Cascading bandits: Learning to
rank in the cascade model. In: proc. of the 32nd Int. Conf. on Machine Learning.
ICML’15 (2015)

16. Kveton, B., Wen, Z., Ashkan, A., Szepesv´ari, C.: Combinatorial cascading bandits.
In: Advances in Neural Information Processing Systems 28. NIPS’15 (2015)
17. Lagr´ee, P., Vernade, C., Capp´e, O.: Multiple-play bandits in the position-based
model. In: Advances in Neural Information Processing Systems 30. NIPS’16 (2016)
18. Lattimore, T., Kveton, B., Li, S., Szepesvari, C.: TopRank: A practical algorithm
for online stochastic ranking. In: Advances in Neural Information Processing Sys-
tems 31. NIPS’18 (2018)

19. Li, C., Kveton, B., Lattimore, T., Markov, I., de Rijke, M., Szepesv´ari, C., Zoghi,
M.: BubbleRank: Safe online learning to re-rank via implicit click feedback. In:
proc. of the 35th Uncertainty in Artiﬁcial Intelligence Conference. UAI’19 (2019)
20. Li, S., Wang, B., Zhang, S., Chen, W.: Contextual combinatorial cascading bandits.

In: proc. of the 33rd Int. Conf. on Machine Learning. ICML’16 (2016)

21. Mazumdar, E., Pacchiano, A., Ma, Y., Bartlett, P.L., Jordan, M.I.: On thompson
sampling with langevin algorithms. In: proc. of the 37th Int. Conf. on Machine
Learning. ICML’20 (2020)

22. Neal, R.M.: Probabilistic inference using markov chain monte carlo methods. Tech.

rep., University of Zurich, Department of Informatics (09 1993)

23. Richardson, M., Dominowska, E., Ragno, R.: Predicting clicks: Estimating the
click-through rate for new ads. In: proc. of the 16th International World Wide
Web Conference. WWW ’07 (2007)

24. Riquelme, C., Tucker, G., Snoek, J.: Deep bayesian bandits showdown: An empir-
ical comparison of bayesian deep networks for thompson sampling. In: proc. of the
Int. Conf. on Learning Representations. ICLR’18 (2018)

25. Thompson, W.R.: On the likelihood that one unknown probability exceeds another
in view of the evidence of two samples. Biometrika 25(3/4), 285–294 (1933)
26. Zoghi, M., Tunys, T., Ghavamzadeh, M., Kveton, B., Szepesvari, C., Wen, Z.:
Online learning to rank in stochastic click models. In: proc. of the 34th Int. Conf.
on Machine Learning. ICML’17 (2017)

27. Zong, S., Ni, H., Sung, K., Ke, N.R., Wen, Z., Kveton, B.: Cascading bandits
for large-scale recommendation problems. In: proc. of the 32nd Conference on
Uncertainty in Artiﬁcial Intelligence. UAI ’16 (2016)

