Towards Scalable Hybrid Stores: Constraint-Based
Rewriting to the Rescue
Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, Stamatis

Zampetakis

To cite this version:

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, Stamatis Zampetakis. Towards
Scalable Hybrid Stores: Constraint-Based Rewriting to the Rescue. SIGMOD 2019 - ACM SIG-
MOD International Conference on Management of Data, Jun 2019, Amsterdam, Netherlands.
￿hal-
02070827v2￿

HAL Id: hal-02070827

https://inria.hal.science/hal-02070827v2

Submitted on 20 May 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Towards Scalable Hybrid Stores: Constraint-Based Rewriting to
the Rescue
Damian Bursztyn∗
Thales Digital Factory,France
dbursztyn@gmail.com

Rana Alotaibi
UC San Diego
ralotaib@eng.ucsd.edu

Alin Deutsch
UC San Diego
deutsch@cs.ucsd.edu

Ioana Manolescu
Inria
LIX (UMR 7161, CNRS and
Ecole polytechnique)
France
ioana.manolescu@inria.fr

Stamatis Zampetakis∗
TIBCO Orchestra Networks
zabetak@gmail.com

ABSTRACT

Big data applications routinely involve diverse datasets: relations
flat or nested, complex-structure graphs, documents, poorly struc-
tured logs, or even text data. To handle the data, application de-
signers usually rely on several data stores used side-by-side, each
capable of handling one or a few data models, and each very effi-
cient for some, but not all, kinds of processing on the data. A current
limitation is that applications are written taking into account which
part of the data is stored in which store and how. This fails to take
advantage of (i) possible redundancy, when the same data may be
accessible (with different performance) from distinct data stores;
(ii) partial query results (in the style of materialized views) which
may be available in the stores.

We present ESTOCADA, a novel approach connecting applica-
tions to the potentially heterogeneous systems where their input
data resides. ESTOCADA can be used in a polystore setting to trans-
parently enable each query to benefit from the best combination
of stored data and available processing capabilities. ESTOCADA
leverages recent advances in the area of view-based query rewrit-
ing under constraints, which we use to describe the various data
models and stored data. Our experiments illustrate the significant
performance gains achieved by ESTOCADA.

1 INTRODUCTION
Big Data applications increasingly involve diverse data sources,
such as: flat or nested relations, structured or unstructured doc-
uments, data graphs, relational databases, etc. Such datasets are
routinely hosted in heterogeneous stores. One reason is that the
fast pace of application development prevents consolidating all the
sources into a single data format and loading them into a single
store. Instead, the data model often dictates the choice of the store,
e.g., relational data gets loaded into a relational or “Big Table”-style
system, JSON documents in a JSON store, etc. Heterogeneous stores
also “accumulate” in an application along the time, e.g., at some
point one decision is made to host dataset D1 in Spark and D2 in
MongoDB, while later on another application needs to use D1 and
D2 together. Systems capable of exploiting diverse Big Data in this
fashion are usually termed polystores [6, 18, 36].

∗Work done while the author was a PhD student working at Inria, France.

Query evaluation in a polystore recalls to some extent mediator
systems [28]; in both cases, sub-queries are delegated to the un-
derlying stores when possible, while the remaining operations are
applied in the upper integration layer. Current polystores process a
query assuming that each of its input datasets is available in exactly
one store (often chosen for its support of a given data model).

We identify two limitations of such architectures. First, they do
not exploit possible data redundancy: the same data could be stored
in several stores, some of which may support a query operation
much more efficiently than others. Second, they are unable to take
advantage of the presence of partially computed query results,
which may be available in one or several stores (in the style of
materialized views), when the data model of the queried dataset
differs from the data model of the store hosting the view.

To overcome these limitations, we propose a novel approach
for allowing an application to transparently exploit data stored in a
set of heterogeneous stores, as a set of (potentially overlapping) data
fragments; further, if fragments store results of partial computations
applied on the data, we show how to speed up queries using them
as materialized views, which reduces query processing effort and
seeks to take maximum advantage of the efficient query processing
features of each store. Importantly, our approach does not require
any change to the application code. The example below illustrates
these performance-enhancing opportunities.

1.1 Motivating example

Consider the Medical Information Mart for Intensive Care III (MIMIC-
III) [33] dataset, comprising health data for more than 40.000 In-
tensive Care Unit (ICU) patients from 2001 to 2012. The total size
of 46.6 GB, and it consists of : (i) all charted data for all patients
and their hospital admission information, ICU stays, laboratory
measurements, caregivers’ notes, and prescriptions; (ii) the role of
caregivers (e.g., MD stands for “medical doctor”), (iii) lab measure-
ments (e.g., ABG stands for “arterial blood gas”) and (iv) diagnosis
related groups (DRG) codes descriptions.

Our motivation query Q1 is: “for the patients transferred into
the ICU due to “coronary artery” issues, with abnormal blood test
results, find the date/time of admission, their previous location
(e.g., emergency room, clinic referral), and the drugs of type “addi-
tive” prescribed to them”. Evaluating this query through the Aster-
ixDB [2] (Version 9.4) JSON store took more than 25 minutes; this is

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

because the system does not support full-text search by an index if
the text occurs within a JSON array. In SparkSQL (v2.3.2), the query
took more than an hour, due to its lack of indexes for selective data
access. In the MongoDB JSON store (v4.0.2), it took more than 17
minutes due to its limited join capability. Finally, in PostgreSQL
with JSON support (v9.6), denoted Postgres in the sequel, Q1 took
12.6 minutes.

Now, consider we had at our disposal three materialized views
which pre-compute partial results for Q1. SOLR is a well-known
highly efficient full-text server, also capable of handling JSON doc-
uments. Consider a SOLR server stores a view V1 storing the IDs
of patients and their hospital admission data, as well as the care-
givers’ reports, include notes on the patients’ stay (e.g., detailed
diagnosis, etc). Full-text search on V1 for ”coronary artery” allows
to efficiently retrieve the IDs of the respective patients. Further,
consider that a Postgres server stores a view V2 with the patients
meta-data information and their hospital admission information
such as admission time and patients’ location prior to admission.
Finally, assume available a view V3 which stores all drugs that are
prescribed for each patient that has "abnormal blood" test results,
as a JSON document stored in Postgres.

Now, we are able to evaluate Q1 by a full-text search on V1
followed by a BindJoin [48] with the result of filtering V3, and
projecting prescribed drugs as well as patients’ admission time
and prior location from V2. Using a simple Java-based execution
engine (implementing select, project, join, etc.) to access the views
and join them, this takes about 5.70 mins, or a speedup of 5×
w.r.t. plain JSON query evaluation in SparkSQL and AsterixDB.
This is also a speedup of 2× and speedup of 3× w.r.t. plain JSON
query evaluation in MongoDB and Postgres, respectively.
Lessons learned. We can draw the following conclusions from the
above example. (1.) Unsurprisingly, materialized views drastically
improve query performance since they pre-compute partial query
results. (2.) More interestingly: materialized views can strongly im-
prove performance even when stored across several data stores, al-
though such a hybrid scenario incurs a certain performance penalty
due to the marshaling of data from one data model/store to another.
In fact, exploiting the different strengths of each system (e.g., SOLR’s
text search, Postgres’ efficient join algorithms, etc.) is the second
reason (together with materialized view usage) for our performance
gains. (3.) Different system combinations work best for different
queries, thus it must be easy to add/remove a view in one system,
without disrupting other queries that may be currently well-served.
As known from classical data integration research [28], such flexi-
bility is attained through the “local-as-view” (LAV) approach, where
the content of each data source is described as a view. Thus, adding
or removing a data source from the system is easily implemented by
adding or removing the respective view definition. (4.) Application
data sets may come in a variety of formats, e.g., MIMIC is in JSON,
Apache log data is often represented in CSV, etc. However, while
the storage model may change as data migrates, applications should
not be disrupted. A simple way of achieving this is to guarantee
them access to the data in its native format, regardless of where it is
stored.

Observe that the combination of 2., 3. and 4. above goes well
beyond the state of the art. Most LAV systems assume both the
application data and the views are organized according to the same

Data model

Query language/API

Systems

Relational

SQL

Major vendors

JSON

JSON

JSON

Key-value

Full-text and JSON

SparkSQL

AQL/SQL++

SQLw/JSON

Redis API

Solr API

Spark [9]

AsterixDB [2]

Postgres

Redis

Solr

XML

XQuery, XPath

Saxon [3]

Table 1: Data Stores Supported by ESTOCADA

data model. Thus, their view-based query rewriting algorithms are
designed specifically within the bounds of that model, e.g., rela-
tional [39], or XML [11, 43, 46]. Different from these, some LAV
systems [16, 41] allow different data models for the stored views,
but consider only the case when the application data model is XML.
As a consequence, the query answering approach adopted in these
systems is tailored toward the XML data model and query language.
In contrast, we aim at a unified approach, supporting any data
model both at the application and at the view level.The core technical
question to be answered in order to attain such performance bene-
fits without disrupting applications is view-based query rewriting
across an arbitrary set of data models. In this paper, we introduce
a novel approach for cross-model view-based rewriting and we
report on its implementation and experimental evaluation in a poly-
store context. Our approach is currently capable of handling the
systems listed in Table 1, together with their data models and query
languages.
1.2 Outline
The remainder of this paper is organized as follows. Section 2 for-
malizes the rewriting problem we solve. Section 3 outlines our ap-
proach; at its core is a view-based query rewriting algorithm based
on an internal model, invisible to users and applications, but which
crucially supports rewriting under integrity constraints. Section 4
describes the language we use for (potentially cross-model) views
and queries. Section 5 shows how we reduce the multi-data model
rewriting problem to one within this internal pivot model, then
transform rewritings obtained there into data integration plans.
Section 6 describes a set of crucial optimizations and extensions we
contributed to the rewriting algorithm at the core of our approach,
in order to make it scale to our polystore setting; we formalize the
guarantees on this algorithm in Section 7. We present our experi-
mental evaluation in Section 8, discuss related work in Section 9
then conclude in Section 10.
2 PROBLEM STATEMENT
We assume a set of data model-query language pairs P =
{(M1, L1), (M2, L2), . . .} such that for each 1 ≤ i, an Li query eval-
uated against an Mi instance returns an answer which is also an
Mi instance. The same model may be associated to several query
languages; for instance, AsterixDB, MongoDB and Postgres have
different query languages for JSON. As we shall see, we consider
expressive languages for realistic settings, supporting conjunctive
querying, nesting, aggregation, and object creation. Without loss of
generality, we consider that a language is paired with one data
model only.

Towards Scalable Hybrid Stores

, ,

that make the same function calls, with the same arguments as the
original query.

3 OVERVIEW AND ARCHITECTURE

We outline here our approach for solving the above problem.
Our integration language: QBT XM . We devised QBT XM , a con-
crete integration language which supports queries over several data
stores, each with its own data model and query language. QBT XM
follows a block-based design, with blocks organized into a tree in
the spirit of the classical Query Block Trees (QBT) introduced in
System R [49], slightly adapted to accommodate subsequent SQL
developments, in particular, the ability to nest sub-queries within
the select clause [31]. The main difference in our setting is that each
block may be expressed in a different query language and carry
over data of a different data model (e.g., SQL for relational data,
key-based search API for key-value data, different JSON query lan-
guages etc.). We call the resulting language QBT XM , for cross-model
QBT (detailed in Section 4.2).
QBT XM views. Each materialized view V is defined by an QBT XM
query; it may draw data from one or several data sources, of the
same or different data models. Each view returns (holds) data fol-
lowing one data model, and is stored in a data store supporting that
model.
Encoding into a single pivot model. We reduce the cross-model
rewriting problem to a single-model setting, namely relational
constraint-based query reformulation, as follows (see also Figure 1;
yellow background identifies the areas where we bring contribu-
tions in this work.). First, we encode relationally the structure of
original data sets, the view specifications and the application query.
Note that the relations used in encoding are virtual, i.e., no data
is migrated into them; they are also hidden, i.e., invisible to both
the application designers and to users. They only serve to support
query rewriting via relational techniques.

The virtual relations are accompanied by integrity constraints
that reflect the features of the underlying data models (for each
model M, a set enc(M) of constraints). For instance, we describe
the organization of a JSON document data model using a small
set of relations such as Node(nID, name), Child(pID, cID), Descen-
dant(aID, dID), etc. together with the constraints specifying that
every node has just one parent and one tag, that every child is
also a descendant, etc. Such modeling had first been introduced in
local-as-view XML integration works [16, 41]. The constraints are
tuple-generating dependencies (tgds) or equality-generating depen-
dencies (egds) [5], extended with such well-researched constraints
as denial constraints [25] and disjunctive constraints [24]. We detail
the pivot model in Section 5.1.
Reduction from cross-model to single-model rewriting. Our
reduction translates the declaration of each view V to additional
constraints enc(V ) that reflect the correspondence between V ’s
input data and its output. Constraints have been used to encode
single-model views [17] and correspondences between source and
target schemas in data exchange [20]. The novelty here is the rich
collection of supported models, and the cross-model character of
the views.

Figure 1: Outline of our approach

We consider a polystore setting comprising a set of stores S =
{S1, S2, . . .} such that each store S ∈ S is characterized by a pair
(MS , LS ) ∈ P, indicating that S can store instances of the model
MS and evaluate queries expressed in LS .

D, V 2

We consider a set of data sets D = {D1, D2, . . .}, such that each
data set D ∈ D is an instance of a data model MD . A data set D
is stored as a set of (potentially overlapping) materialized views
{V 1
D is stored within the
storage system S j
D is an instance of a data model
D
supported by S j
1.
D

D . . .}, such that for every 1 ≤ j, V j
∈ S. Thus, V j

We consider an integration language IL, capable of expressing
computations to be made within each store and across all of them,
as follows:

• For every store S and query qS ∈ L′
that qS should be evaluated over S;

S , IL allows specifying

• Further, IL allows expressing powerful processing over the

results of one or several such source queries.

Such integration language has been proposed in several works [18,
36]; we use it to express computations over the views. An IL ex-
pression bears obvious similarities with an execution plan in a
wrapper-mediator system; its evaluation is split between compu-
tations pushed to the stores, and subsequent operations applied
by the mediator on top of their results. The main distinction is
that IL remains declarative, abstracting the details of each in-store
computation, which is simply specified by a query in the store’s
language.

We assume available a cost model which, given an IL expression
e, returns an estimation of the cost of evaluating e (including the
cost of its source sub-queries). The cost may reflect e.g., disk I/O,
memory needs, CPU time, transfer time between distributed sites
etc. We outline the concrete model we use in Appendix K.

We term rewriting of a query q an integration query expressed in
IL, which is equivalent to q. We consider the following rewriting
problem:

Definition 1 (Cross-model rewriting problem). Given a
query q ∈ IL over several datasets Di , 1 ≤ i ≤ n, and a set of views
V materialized over these datasets, find the equivalent rewriting r of
q using the views, which minimizes the cost estimation c(r ).

Most modern query languages include such primitives as arith-
metic operations, aggregation, and calls to arbitrary UDFs; these
suffice to preclude decidability of checking whether two queries
are equivalent. We therefore aim at finding rewritings under black-
box (uninterpreted) function semantics (UFS): we seek rewritings

1For uniformity, we describe any collection of stored data as a view, e.g., a table stored
in an RDBMS is an (identity) view over itself.

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

An incoming query Q over the original datasets DS1, . . . , DSl ,
whose data models respectively are M1, . . . , Ml , is encoded as a re-
lational query enc(Q) over the dataset’s relational encoding. enc(Q)
is centered around conjunctive queries, with extensions such as
aggregates, UDFs, nested sub-queries, disjunction and negation.

The reformulation problem is thus reduced to a purely relational
setting: given a relational query enc(Q), a set of relational integrity
constraints encoding the views, enc(V1) ∪ . . . ∪ enc(Vn ), and the
set of relational constraints obtained by encoding the data models
M1, . . . , Ml , find the queries RW i
r expressed over the relational
views, for some integer k and 1 ≤ i ≤ k, such that each RW i
r is
equivalent to enc(Q) under these constraints.

The challenge in coming up with the reduction consists in de-
signing a faithful encoding, i.e., one in which rewritings found by (i)
encoding relationally, (ii) solving the resulting relational reformu-
lation problem, and (iii) decoding each reformulation RW i
r into a
QBT XM query R = dec(RW i
r ) over the views in the polystore, corre-
spond to rewritings found by solving the original problem. That is,
R is a rewriting of Q given the views {V1, . . . , Vn } if R = dec(RW i
r ),
where RW i
r is a relational reformulation of enc(Q) under the con-
straints obtained from encoding V1, . . . , Vn, M1, . . . , Ml . The reduc-
tion is detailed in Sections 5.2 and 5.3.
Relational rewriting using constraints. To solve the single-
model rewriting problem, we need to reformulate relational queries
under constraints. The algorithm of choice is known as Chase
& Backchase (C&B, in short), introduced in [15] and improved
in [30] to yield the Provenance-Aware C&B algorithm (PACB, in
short). PACB was designed to work with relatively few views and
constraints. In contrast, in the polystore setting, each of the many
datasources is described by a view, and each view is encoded by
many constraints. For instance, the JSON views in our experiments
(Section 8) are encoded via ~ 45 tgds involving 10-way joins in
the premise. To cope with this complexity, we incorporated into
PACB novel scale-up techniques (discussed in Section 6.1). PACB
was designed for conjunctive queries under set semantics. Towards
supporting a richer class of queries, we extended it to bag semantics
(Section 6.2) and QBTs (Section 6.3).
Decoding the relational rewritings. On each relational refor-
r issued by our modified PACB rewriting, a decoding
mulation RW i
step dec(RW i
r ) is performed to:

(i) Group the reformulation atoms by the view they pertain to; for
instance, we infer that the three atoms Document(dID, “file.json”),
Root(dID, rID), Child(rID, cID), Node(cID, book) refer to a single
JSON document by following the connections among nodes and
knowledge of the JSON data model.

(ii) Reformulate each such atom group into a query which can

be completely evaluated over a single view.

(iii) If several views reside in the same store, identify the largest
subquery that can be delegated to that store, along the lines of
query evaluation in wrapper-mediator systems [26].
Evaluation of non-delegated operations. A decoded rew-riting
may be unable to push (delegate) some query operations to the store
hosting a view, if the store does not support them; for instance,
most key-value and document stores do not support joins. Similarly,
if a query on structured data requests the construction of new
nested results (e.g., JSON trees) and if the data is in a store that

does not support such result construction natively, it will have
to be executed outside of that store. To evaluate such “last-step”
operations, we rely on a lightweight execution engine [10], which
uses a nested relational model and supports bind joins [48] (with
sideways information passing) to evaluate nested subqueries. Our
engine can be easily replaced with another similar execution engine;
we exemplify this with BigDAWG [18] in Section 8.1.2.
Choice of the most efficient rewriting. Decoding may lead to
several rewritings R1, . . . , Rk ; for each Ri , several evaluation plans
may lead to different performance. The problem of choosing the
best rewriting and best evaluation plan in this setting recalls query
optimization in distributed mediator systems [44]. For each rewrit-
ing Ri , we denote by c(Ri ) the lowest cost of an evaluation plan
that we can find for Ri ; we choose the rewriting Rbest that mini-
mizes this cost. While devising cost models for polystore settings
is beyond the scope of this paper, we architected ESTOCADA to
take the cost model as configuration parameter.
4 THE QBT XM LANGUAGE
We present here the language we use for views and queries. First,
we recall the classical Query Block Trees (QBTs), then we extend
them to cross-model views and queries.
4.1 Query Block Trees (QBT)
Since many of the languages ESTOCADA supports allow for nested
queries and functions (user-defined or built-in such as aggregates),
our pivot language Query Block Trees (QBTs) also supports these
features. These are essentially the classical System R QBTs [49],
slightly adapted to accommodate subsequent SQL extensions, such
as nesting in the select clause (introduced in SQL-1999 [31]). We
first illustrate QBTs on a SQL example.

Example 1. Consider the following SQL query which computes,
for each student who is not enrolled in any course for the Spring’16
quarter, the number of Spring’16 courses she is waitlisted for (a count
of 0 is expected if the student is not waitlisted). While this query
could be written using joins, outer joins and group by operations, we
show a natural alternative featuring nesting (which illustrates how
we systematically normalize away such operations):

SELECT s. ssn , COUNT ( SELECT w. cno

FROM
WHERE

FROM
WHERE

Student s
NOT EXISTS ( SELECT

FROM
WHERE

Waitlist w
w. ssn = s. ssn
AND w. qtr = ' Spring 2016 '
) AS cnt

c. no
Course c , Enrollment e
c. no = e. cno
AND s. ssn = e. ssn
AND e. qtr = ' Spring 2016 ')

This query consists of three blocks. The outermost SELECT-FROM-
WHERE expression corresponds to the root block, call it B0. The two
nested SELECT-FROM-WHERE expressions are modeled as children
blocks of the root block, call them B00 and B01 for the block nested
in the SELECT clause, respectively the block nested in the WHERE
□
clause.

The variables occurring in a block B can be either defined by B
(in which case we call them bound in B), or by one of its ancestors
(in which case they are called free in B). We assume w.l.o.g. that the

Towards Scalable Hybrid Stores

, ,

bound variables of B have fresh names, i.e., they share no names
with variables bound in B’s ancestors.

JSON query language, and for a simple declarative key-value query
language we designed for Redis.

Example 2. In Example 1, variable s is bound in B0, but it occurs
□

free in both B00 and B01.

QBTs model select-from-where expressions as blocks, organized
into a tree whose parent-child relationship reflects block nesting.
Nested blocks always appear as arguments to functions, be they
built-in (e.g., COUNT for B00 and NOT EXISTS for B01 in Example 1)
or user-defined. While not shown in Example 1, the case of blocks
nested within the FROM clause corresponds to the identity function.
As we will show, we use QBTs to translate relationally applica-
tion queries written in other models, but also (potentially cross-
model) views and rewritings.
4.2 Integration Language: QBT XM
To specify cross-model queries and views, we adopted a block-based
design, similar to QBTs, but in which each block is expressed in
its own language, signaled by an annotation on the block. We call
the resulting language QBT XM , which stands for cross-model QBT.
QBT XM queries comprise a FOR and a RETURN clause. The FOR
clause introduces several variables and specifies their legal bindings.
The RETURN clause specifies what data should be constructed for
each binding of the variables. Variables can range over different
data models, which is expressed by possibly several successive
blocks, each pertaining to its own model. In SQL style, this defines
a Cartesian product of the variable bindings computed by each block
from the FOR clause; this Cartesian product can be filtered through
WHERE clause. We impose the restriction that variables shared by
blocks spanning different data models must be of text or numeric
type, so as to avoid dealing with conversions of complex values
across models. While there is no conceptual obstacle to handle such
conversions automatically, the topic is beyond the scope of this
paper. We describe QBT XM informally, by examples; the grammar
of QBT XM is delegated to Appendix I.

Example 3. We illustrate the QBT XM definition of views V1 and
V2 from Section 1, using AsterixDB’s SQL++ syntax (easier to read
than the JSON syntax of Postgres):
View V1 :
FOR AJ :{ SELECT

M. patientID AS patientID ,
A. admissionID AS admissionID ,
A. report AS report

FROM

MIMIC M , M. admissions A}

RETURN SJ :{ " patientID ": patientID ,

" admissionID ": admissionID ,
" report ": report }

View V2 :
FOR AJ :{ SELECT

M. patientID AS patientID ,
A. admissionID AS admissionID ,
A. admissionLoc AS admissionLoc
A. admissionTime AS admissionTime
MIMIC M , M. admissions A}

FROM

RETURN PR :{ patientID , admissionID ,

admissionLoc , admissionTime }

□

FOR clauses bind variables, while RETURN clauses specify how
to construct new data based on the variable bindings. Blocks are
delimited by braces annotated by the language whose syntax they
conform to. The annotations AJ, PR and SJ stand for the SQL++
language of AsterixDB and the languages of Postgres and Solr,
respectively. Also, below, PJ and RK stand respectively for Postgres’

5 REDUCTION FROM CROSS-MODEL TO

SINGLE-MODEL SETTING

A key requirement in our setting is the ability to reason about
queries and views involving multiple data models. This is challeng-
ing for several reasons. First, not every data model/query language
setting supported in ESTOCADA comes with a known view-based
query rewriting (VBQR, in short) algorithm: consider key-value pairs
as data model and declarative languages over them, or JSON data
and its query languages in circulation, etc. Second, even if we had
these algorithms for each model/language pair, neither would read-
ily extend to a solution of the cross-model VBQR setting in which
views may be defined over data from various models, materializ-
ing their result in yet again distinct models, and in which query
rewritings access data from potentially different models than the
original query. Third, for the feasibility of development and main-
tenance as the set of supported model/language pairs evolves, any
cross-model VBQR solution needs to be modularly extensible to
additional models/languages, in the sense that no modification to
the existing code is required and it suffices to just add code dealing
with the new model/language pair. Moreover, the developer of this
additional code should not need to understand any of the other
model/language pairs already supported in ESTOCADA.

To address these challenges, we reduce the cross-model VBQR
problem to a single-model VBQR problem. That is, we propose
a unique pivot data model (Section 5.1), on which QBT (Section 4.1)
serves as a pivot query language. Together, they allow us to capture
data models, queries and views so that cross-model rewritings can
be found by searching for rewritings in the single-model pivot set-
ting instead. Section 5.1 presents the pivot model; then, Section 5.2
presents query encoding in the pivot language, Section 5.3 shows
how to encode views as constraints, finally Section 5.4 describes
the decoding of rewriting into integration plans.
5.1 The Pivot Model
Our pivot model is relational, and it makes prominent use of ex-
pressive integrity constraints. This is sufficiently expressive to
capture the key aspects of the data models in circulation today,
including: freely nested collections and objects, object identity (e.g.,
for XML element nodes and JSON objects), limited binding pat-
terns (as required by key-value stores and full-text indexes such as
Lucene/Solr), relationships (in the E/R and ODL sense), functional
dependencies and much more. The integrity constraints we use
are tuple-generating dependencies (tgds) or equality-generating
dependencies (egds) [5], extended to denial constraints [25] and
disjunctive constraints [24].

JSON. We represent JSON documents as relational instances
conforming to the schema VREJ (Virtual Relational Encoding of
JSON) in Table 2. We emphasize that these relational instances
are virtual, i.e., the JSON data is not stored as such. Regardless
of the JSON data’s physical storage, we only use VREJ to encode
JSON queries relationally, in order to reason about them. VREJ
constraints express the fact that JSON databases are organized into
named collections of JSON values, which in turn can be objects
(consisting of a set of key-value pairs), arrays of values, and scalar

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

values (numeric or text). We model objects, arrays and values in

Collection J (name, id)
Child J (parentId, childId, key, type)
Eq J (x, y)
V alue J (x, y)
Table 2: Snippet of VREJ

an object-oriented fashion, i.e. they have identities. This is because
some JSON stores, e.g., MongoDB and AsterixDB, support query
languages that refer to identity and/or distinguish between equality
by value versus equality by identity. Our modeling supports both
the identity-aware and the identity-agnostic view of JSON, via
appropriate translation of queries and views into pivot language
expressions over the VREJ schema. Relation CollectionJ attaches
to each persistent name the id of an array. ValueJ attaches a scalar
value to a given id. ChildJ states that the value identified by childId
is immediately nested within the value identified by parentId. The
type attribute records the type of the parent (array "a" or object
"o") and determines the kind of nesting and the interpretation of
the key attribute: if the parent is an array, key holds the position
at which the child occurs; if the parent is an object, then key holds
the name of the key whose associated value is the child.

Our modeling of parent-child (immediate nesting) relationship
between JSON values provides the type of value only in conjunction
with a navigation step where this value is a parent, and the step leads
to a child value of undetermined type. This modeling reflects the
information one can glean statically (at query rewriting time, as
opposed to query runtime) from JSON query languages in circu-
lation today. Recall that JSON data is semi-structured, i.e., it may
lack a schema. Queries can, therefore, express navigation leading to
values whose type cannot be determined statically if these values
are not further navigated into. The type of a value can be inferred
only from the type of navigation step into it.

Example 4. If a query navigation starts from a named JSON
collection “coll” and proceeds to the fifth element e therein, we can
infer that “coll” is of array type, but we do not know the type of e.
Only if the query specifies an ensuing lookup of the value v associated
to key “k” in e can we infer e’s type (object). However, absent further
□
navigation steps, we cannot tell the type of the child value v.

Finally, relation EqJ is intended to model value-based equality
for JSON (id-based equality is captured directly as equality of the
id attributes).

We capture the intended meaning of the VREJ relations via con-
straints that are inherent in the data model (i.e. they would hold if
we actually stored JSON data as a VREJ instance). We illustrate a
few of these below; following convention, free variables are to be
read as universally quantified.

The fact that a persistent name uniquely refers to a value is
expressed by the egd (1) below. Egd (2) states that an object cannot
have two distinct key-value pairs sharing the same key, or that an
array cannot have two distinct elements at the same position. Tgds
(3) and (4) state that value-based equality is symmetric, respectively
transitive, and tgd (5) states that value-equal parents must have
value-equal children for each parent-child navigation step. Egd (6)

states that no two distinct scalar values may correspond to a given
id.

Collection J (n, x) ∧ Collection J (n, y) → x = y
Child J (p, c1, k, t) ∧ Child J (p, c2, k, t) → c1 = c2

Eq J (x, y) → Eq J (y, x)
Eq J (x, y) ∧ Eq J (y, z) → Eq J (x, z)

Eq J (p, p ′) ∧ Child J (p, c, k, t) →

∃c ′ Eq J (c, c ′) ∧ Child J (p ′, c ′, k, t)

(1)

(2)

(3)

(4)

(5)

V alue J (i, v1) ∧ V alue J (i, v2) → v1 = v2
(6)
Key-Value Store Model. Our interpretation of the key-value
pairs model is compatible with many current systems, in particular
Redis, supported by ESTOCADA. Calling a map a set of key-value
pairs, the store accommodates a set of persistently named maps
(called outer maps). Within each outer map, the value associated to
a key may be either itself a map (called an inner map), or a scalar
value. In case of an inner map, the value associated to a key is a
scalar value.

Given the analogy between key-value and JSON maps, we model
the former similarly to the latter, as instances of the relational
schema VREK, consisting of the relations: MapKV (name, mapId),
ChildKV (parentId, childId, key, type) and EqKV (x, y). Here, MapKV
models the association between a persistent name and (the id of)
an outer map. ChildKV reflects the immediate nesting relationship
between a map (the parent) and a value associated to one of its
keys (the child). The type attribute tells us whether the parent map
is an outer or an inner map (these are treated asymmetrically in
some systems). The EqKV relation models value-based equality,
analogously to EQJ for JSON.

The intended semantics of these relations is enforced by similar
constraints to the JSON model, e.g., in a map, there is only one
value for a key (ChildKV (p, c1, k, t), ChildKV (p, c2, k, t) → c1 = c2),
persistent map names are unique (MapKV (n, x), MapKV (n, y) →
x = y) etc.

XML. Query reformulation for XML has already been reduced
in prior work to a purely relational setting using constraints. We
refer the reader to [16, 17].

Relational. It is well known that the relational data model en-
dowed with key, uniqueness and foreign key constraints is captured
by our pivot model: key/uniqueness constraints are expressible by
egds, and foreign key constraints by tgds.

Binding Patterns. We have seen above a natural way to model
sets of key-value pairs using a relation. To simplify the discussion,
we abstract from the parentId and type attributes of relation ChildKV
above, focusing on the binary relationship between keys and values:
KV (key, value). Note that typical key-values store APIs require that
values can be looked up only given their key, but not conversely. If
we do not capture this limitation, the rewriting algorithm may pro-
duce rewritings that correspond to no executable plan. For instance,
consider a rewriting of the form: R(v) : −KV (k, v). R corresponds
to no legal plan given the access limitation of KV, because R re-
quires the direct extraction of all values stored in the store, without
looking them up by key.

This setting involving relations with limited lookup access is
well-known in the literature, and is modeled via the concept of
relations adorned with binding patterns [48].

Towards Scalable Hybrid Stores

, ,

In a relational setting, query rewriting when access to the views is
limited by binding patterns has been studied for conjunctive queries
and views [22, 42], yet complete rewriting under both integrity
constraints and binding patterns was not considered until [13]. [13]
shows how to encode binding patterns using tgd constraints, which
fit into our pivot model. The idea is to introduce a unary relation,
say A(x) to state that x is accessible, and for each binding pattern a
tgd stating that when all required input attributes of a relation are
accessible, then the output attributes are accessible as well. This
allows the chase with such tgds to determine which attributes of a
rewriting are accessible.

Nulls, Equality and Aggregation. Different stores exhibits
wide variability in the treatment of nulls [45], thus equality and
aggregation semantics may differ across stores. To account for this,
by default we use distinct null constants, distinct aggregate func-
tions and distinct equality relationships for the relational encoding
of distinct stores. For instance, the sum aggregate of language
L1 is not automatically equated to the sum of L2, since the two
may treat nulls differently. When the null treatment coincides to-
tally (e.g., for distinct but SQL-compliant RDBMS stores), we use
a single common encoding. When it coincides only partially, we
encode as much as possible with constraints. For instance the tgd
eq1(null1, x) → eq2(null2, x) states that if a value x is equal to null
in System 1’s interpretation of null and equality, this holds also in
System 2’s interpretation.
MIMIC (M),
Child J (M , patientID ," patientID " ,"o"),
Child J (M ,A ," admissions " ,"o"),
Child J (A , admissionID ," admissionID " ,"o"),
Child J (A , admissionLoc ," admissionLoc " ,"o"),
Child J (A , admissionTime ," admissionTime " ,"o"),
Child J (A , report ," reports " ,"o")->
V2 ( patientID , admissionID , admissionLoc , admissionTime )
Figure 2: Relational encoding of QBT XM View V2
5.2 Encoding QBT XM Queries into the Pivot

Language

The purpose of the pivot language is to reduce the VBQR problem
to a single-model setting. The pivot model enables us to represent
relationally queries expressed in the various languages supported
in our system (recall Table 1), and which can be combined into a
single QBT XM query. As shown in Figure 1, an incoming query Q is
encoded as a relational conjunctive enc(Q) over the relational encod-
ing of the datasets it refers to (with extensions such as aggregates,
user-defined functions, nested queries, disjunction and negation).
Figure 3 shows the QBT XM query Q1 of the motivating scenario,
and its relational encoding enc(Q1) appears in Appendix A.
5.3 Encoding QBT XM Views as Constraints
We translate each view definition V into additional relational in-
tegrity constraints enc(V ) showing how the view inputs are related
to its output. Figure 2 illustrates the relational encoding of QBT XM
view V2 from Section 4 (for space reasons, similar constraints result-
ing from V1 and V2 appear in Appendix B. Due to space limitation,
we omit the constraints for V3 from this version of the paper).
5.4 From Rewritings to Integration Plans
We translate each query rewriting into a logical plan specifying
(i) the (native) query to be evaluated within each store, and (ii) the

FOR AJ :{ SELECT M. patientID AS patientID ,

FROM

WHERE

A. admissionLoc AS admissionID ,
A. admissionTime AS admissionTime ,
P. drug AS drug
LABITEM L , MIMIC M , M. admissions A ,
A. labevents LE , A. prescriptions P
L. itemID = LE . labItemID AND
L. category = ' blood ' AND
L. flag = ' abnormal ' AND
P. drugtype = ' additive ' AND
contains ( report , ' coronary artery ')}

RETURN AJ :{ " patientID ": patientID , " drug ": drug ,

" admissionLoc ": admissionLoc ,
" admissionTime ": admissionTime }
Figure 3: Motivating scenario QBT XM Query Q1

remaining logical operations, to be executed within the integra-
tion engine. The translation first decodes RWr into QBT XM syntax.
Based on the heuristic that the store where a view resides is more
efficient than the mediator, and thus it should be preferred for all
the operations it can apply, decoding tries to delegated to the stores
as much as possible. To that end, it first partitions each rewriting
RWr into view-level subqueries, which are sets of atoms referring
to the virtual relations of a same view. If a group of view-level sub-
queries pertains to the same store and if the store supports joins,
we translate the group to a single query to be pushed to the store.

RW Q1 < patientID , drug , admissionLoc , admissionTime >: -
V1 (d1 ),
Child J V1
Child J V1
Child J V1

(d1 , patientID ," patientID " ,"o"),
(d1 , admissionID ," admissionID " ,"o"),
(d1 , report ," report " ,"o"),

V alue J ( report ," like - coronary artery "),

V2 ( patientID , admissionID , admissionLoc , admissionTime ),

V3 (d2 ),
Child J V3
Child J V3
Child J V3
Child J V3
Child J V3
Child J V3

(d2 , patientID ," patientID " ,"o"),
(d2 ,A ," admission " ,"o"),
(A , admissionID ," admissionID " ,"o"),
(A , drugs ," drugs " ,"o"),
( drugs , drug ," drug " ,"o"),
( drugs , drugtype ," drugtype " ,"o"),

V alue J ( drugtype ," additive ")

Figure 4: Rewriting RW Q1 of QBT XM query Q1

1, RW Q2
1

Example 5 (Delegation-Aware Decoding). Consider the rewrit-
ing RW Q1 of QBT XM query Q1 shown in Figure 4. First, we delimit
the view-wide subqueries (delimited by empty lines in the figure),
calling them RW Q1
in order from the top. The
and RW Q3
1
subquery heads contain variables from the head of RW Q1, as well as
join variables shared with other subqueries. For example, the head of
contains the variables admissionLoc and admissionTime (in
RW Q2
1
the head of RW Q1), and also patientI D, admissionID, needed to join
with RW Q1
. These relational subqueries are then decoded
1
to the native syntax of their respective stores, each constituting a block
of the resulting QBT XM rewriting dec(RW Q1) (shown in Appendix C).
□

, and RW Q3
1

ESTOCADA’s plan generator next translates the decoded QBT XM
rewriting to a logical plan that pushes leaf blocks to their native
store, applying last-step operators on the results. The integration
plan for RW Q1 is shown in Figure 5.

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

Π1,2,3,4,7

▷◁l .1=r .1&l .2=r .2

Π1,2,5,6

▷◁l .1=r .1&l .2=r .2

q=report:’coronary artery’
&fl=patientID,admissionID

SELECT v3-»’patientID’ AS PID2,
A-»’admissionID’ AS AID2, D-»’drug’ AS drug
FROM V3 v3,
jsonb_array_elements(v3->’admission’) A,
jsonb_array_elements(A->’drugs’) D
WHERE v3-»’patientID’=? AND
A-»’admissionID’=? AND
D-»’drugtype’=’additive’

SELECT patientID AS PID1,
admissionID AS AID1, admissionLoc,
admissionTime
FROM V2
WHERE patientID=? AND admissionID=?

Figure 5: Integration plan of the motivating scenario

6 PACB OPTIMIZATION AND EXTENSION

We detail below just enough of the PACB algorithm’s inner work-
ing to explain our optimization. Then, Section 6.1 introduces our
optimization in the original PACB setting (conjunctive relational
queries and set semantics). Section 6.2 extends this to bag semantics,
Section 6.3 extends it to QBT XM .

A key ingredient of the PACB algorithm is to capture views as
constraints, in particular tgds, thus reducing the view-based rewrit-
ing problem to constraints-only rewriting. For a given view V , the
constraint VI O states that for every match of the view body against
the input data there is a corresponding tuple in the view output; the
constraint VO I states the converse inclusion, i.e., each view tuple is
due to a view body match. Then, given a set V of view definitions,
PACB defines a set of view constraints C V = {VI O , VO I | V ∈ V}.
The constraints-only rewriting problem thus becomes: given the
source schema σ with a set of integrity constraints I, a set V of
view definitions over σ and the target schema τ which includes V,
given a conjunctive query Q expressed over σ , find reformulations
ρ expressed over τ that are equivalent to Q under the constraints
I ∪ CV .

For instance, if σ = {R, S }, I = ∅, τ = {V } and view V ma-
terializes the join of tables R and S, V (x, y) : −R(x, z), S(z, y), the
constraints capturing V are:

VI O :
VO I :

R(x, z) ∧ S(z, y) → V (x, y)
V (x, y) → ∃z R(x, z) ∧ S(z, y).

For input σ -query Q(x, y) : −R(x, z), S(z, y), PACB finds the τ -
reformulation ρ(x, y) : −V (x, y). Algorithmically, this is achieved
by:

(i) chasing Q with the set of constraints I ∪ CI O

V where CI O
V

=

{VI O | V ∈ V};

(ii) restricting the chase result to only the τ -atoms (the result is

called the universal plan) U ;

(iii) chasing U with the constraints in I ∪ CO I

=
{VO I | V ∈ V}; the result is denoted B and called the backchase;
finally:

V , where CO I
V

(iv) matching Q against B and outputting as rewritings those
subsets of U that are responsible for the introduction (during the
backchase) of the atoms in the image of Q. 2

In our example, I is empty, CI O
V

= {VI O }, and the result of
the chase in phase (i) is Q1(x, y) : −R(x, z), S(z, y), V (x, y). The
universal plan obtained in phase (ii) by restricting Q1 to τ is U (x, y) :
−V (x, y). The result of backchasing U with CO I
V in phase (iii) is
B(x, y) : −V (x, y), R(x, z), S(z, y) and in phase (iv) we find a match
from Q’s body into the R and S atoms of B, introduced during the
backchase due to U ’s atom V (x, y). This allows us to conclude that
ρ(x, y) : −V (x, y) is an equivalent rewriting of Q.
6.1 PACBopt: Optimized PACB
The idea for our optimization was sparked by the following obser-
vation. The backchase phase (step (iii)) involves the VO I constraints
for all V ∈ V. The backchase attempts to match the left-hand side
of each VO I for each V repeatedly, leading to wasted computation
for those views that have no match. In a polystore setting, the large
number of data sources and stores lead to a high number of views,
most of which are often irrelevant for a given query Q.

V with CO I

This observation leads to the following modified algorithm. De-
fine the set of views mentioned in the universal plan U as relevant
to Q under I, denoted relev I (Q). Define the optimized PACB al-
gorithm PACBopt identical with PACB except phase (iii) where
PACBopt replaces CO I
r el evI (Q ). That is, PACBopt performs
the backchase only with the OI -constraints of the views determined
in phase (i) to be relevant to Q, ignoring all others. This modifica-
tion is likely to save significant computation when the polystore
includes many views. In our example assume that, besides V , V con-
tained 1000 other views {V i }1≤i ≤1000, each irrelevant to Q. Then
the universal plan obtained by PACB would be the same as U above,
and yet {V i
}1≤i ≤1000 would still be considered by the backchase
O I
phase, which would attempt at every step to apply each of these
1000 constraints. In contrast, PACBopt would save this work.

In general, ignoring even a single constraint c during backchase
may lead to missed rewritings [47]. This may happen even when c
mentions elements of schema σ that are disjoint from those mentioned
in U , if the backchase with I exposes semantic connections between
these elements. In our setting, we prove that this is not the case:
Theorem 6.1. Algorithm PACBopt finds the exact same rewritings

as the original PACB.
This is because we only ignore some view-capturing constraints,
which can be shown (by analysing the backchase evolution) never
to apply, regardless of the constraints in I. Combined with the main
result from [30], Theorem 6.1 yields the completeness of PACBopt
in the following sense:

Corollary 6.2. Whenever the chase of input conjunctive query

Q with the constraints in I terminates3, we have:

2Recall that a chase step s with constraint c matches c’s premise against existing atoms
e and adds new atoms n corresponding to c’s conclusion. To support fast detection
of responsible atoms in Phase (iv ), s records that the e atoms are responsible for the
introduction of the n atoms [30]. Our optimization does not affect Phase (iv ).
3Termination of the chase with tgds is undecidable [14], but many sufficient conditions
for termination are known, starting with weak acyclicity [19]. Our constraints are
chosen so as to ensure termination.

Towards Scalable Hybrid Stores

, ,

(a) algorithm PACBopt without a cost model enumerates all join-

minimal4 V-based rewritings of Q under I, and

(b) algorithm PACBopt equipped with a cost model c finds the

c-optimal join-minimal rewritings of Q.

In Section 8.2, we evaluate the performance gains of PACBopt
over the original PACB, measuring up to 40x speedup when operat-
ing in the polystore regime.
6.2 Extending PACBopt to Bag Semantics
We extended PACBopt to find equivalent rewritings of an input
conjunctive query under bag semantics (the original PACB only
addresses set semantics). The extension involves a modification
to phase (iv). In its original form, the matches from Q into the
backchase result B are not necessarily injective, being based on
homomorphisms [5]. These allow multiple atoms from Q to map into
the same atom of B. To find bag-equivalent rewritings, we disallow
such matches, requiring match homomorphisms to be injective.
6.3 PACB

: Extending PACBopt to QBTs

opt
qbt

The original PACB algorithm (and our extensions thereof) have so
far been defined for conjunctive queries only. However, recall that
QBTs (Section 4.1) are nested.

We extend the PACBopt algorithm to nested QBTs as follows.
Each block B is rewritten in the context of its ancestor blocks A1, . . . , An ,
to take into account the fact that the free variables of B are instanti-
ated with the results of the query corresponding to the conjunction
of its ancestor blocks. We therefore replace B with the rewriting of
the query A1 ∧ A2 ∧ . . . ∧ An ∧ B. We call the resulting algorithm
⟨C, c⟩ to emphasise the fact
PACB
that it is parameterized by the constraints C and the cost model c.
Recalling the uninterpreted (black-box) function semantics from

, using the notation PACB

opt
qbt

opt
qbt

Section 2, Corollary 6.2 implies:

Corollary 6.3. Under uninterpreted-function semantics, Corol-
lary 6.2 still holds when we replace conjunctive queries with QBT
opt
queries and PACBopt with PACB
qbt

.

7 GUARANTEES ON THE REDUCTION
We provide the following formal guarantees for our solution to the
cross-model rewriting problem based on the reduction to single-
model rewriting. Recall from Section 3 that enc(M) are the relational
constraints used to encode M ∈ M in virtual relations, enc(Q)
encodes a QBT XM query Q as a QBT query over the virtual relations,
and dec(R) decodes a QBT query over the virtual relations into a
QBT XM query. Also recall from Section 6 that given a set V of
QBT XM views, CV are the relational constraints used to capture
V. We have:

Theorem 7.1 (Soundness of the reduction). Let Q be a QBT XM
query over a polystore over the set of data models M. Let I be a set of
integrity constraints satisfied by the data in the polystore and enc(I)
be their relational encoding.

For every rewriting R obtained via our reduction, i.e.

R = dec( PACB

opt
qbt⟨enc(I) ∪

(cid:216)

M ∈M

enc(M) ∪ CV, c⟩(enc(Q)) ),

4A rewriting of Q is join-minimal if none of its joins can be removed while preserving
equivalence to Q .

R is a c-optimal V-based rewriting equivalent to Q under I, assuming
black-box function semantics (Section 2).

In the above, enc(I) ∪ (cid:208)
straints used by the PACB

M ∈M enc(M) ∪ CV is the set of con-
algorithm; Theorem 7.1 states the

opt
qbt

correction of our approach, outlined in Figure 1.
8 EXPERIMENTAL EVALUATION

Below, we describe experiments demonstrating the efficiency and
effectiveness of our cross-model rewriting technique.
Hardware Platform. We used commodity cluster machines with
an Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 40Cores, 123GB
RAM, disk read speed 616 MB/s, and disk write speed 455 MB/s
(interesting since some systems, e.g., AsterixDB, write intermediate
results to disk).
Polystore Configuration. For our main polystore setting (called
“ESTOCADA polystore engine“ hereafter) we use [10] as a light-
weight execution engine and a set of data stores, selected for their
capabilities and popularity: an RDBMS (Postgres v9.6), JSON docu-
ment stores (Postgres v9.6, MongoDB v4.0.2) and AsterixDB v4.9,
SparkSQL v2.3.2 and a text search engine (Solr v6.1). We set a 60GB
buffer cache for all systems, we configured the number of utilizable
cores to 39, and we assigned 8GB to the working memory (the
default for Postgres). We set AsterixDB’s compiler frame size (the
page size for batch query processing) to 6MB.
Dataset. We used the real-life 46.6 GB MIMIC-III dataset [33]
described in Section 1.1.
Generating Query and View Families. We created a polystore
benchmark based on MIMIC as follows.

We defined a set QT of 25 query templates, each checking mean-
ingful conditions against a patient’s data. These are parameterized
by the patient id and by other selection constants, and they in-
volve navigation into the document structure. Each query/view
is obtained by joining a subset of QT and picking values for the
selection constants, leading to an exponential space of meaningful
queries and views. Among them, we identified those with non-
empty results: first, non-empty instantiations of QT queries, then
joins of two such queries, then three etc., in an adaptation of the
Apriori algorithm [7].

Example 6. Consider the query templates QT0, QT1 and QT2 in
Appendix H, shown directly in relationally encoded form. QT0 asks for
patients’ information including patient’s PATIENTID, DOB, GENDER
and DOD (if available). QT1 asks for all "abnormal" lab measurement
results for patients. QT2 asks for bloodwork-related lab measurements.
The natural join of QT0, QT1 and QT2 yields a query Q which seeks
□
information on patients with abnormal blood test results.
The Queries. We chose 50 queries QEX P among those described
above (we show examples in Appendixes D, E, F and G). 58% of the
queries (Q01, . . . ,Q29) contain full-text operations; all involve joins,
selections, and at least two-level-deep navigation into the JSON
document structure.
The Views. We materialized a set of views VEX P as follows.

We stored in Postgres 6 relational views VP ostдr eSQ L ⊂ VEX P of
the MIMIC-JSON dataset, comprising the uniformly structured part
of the dataset (including all patient chart data and admission under
specific services such as Cardiac Surgery, Neurologic Medical, etc).

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

Figure 6: ESTOCADA (relational and JSON views in Postgres, JSON view in Solr) vs. single-store evaluation

Figure 7: ESTOCADA (relational, resp. JSON views in Postgres) vs. single-store evaluation

We also stored in Postgres a set VPostдr es J SO N ⊂ VEX P of 21
views which are most naturally represented as nested JSON docu-
ments. These views store for instance: (i) lab tests conducted for
each patient with ”abnormal” test results (e.g., blood gas, Cere-
brospinal Fluid (CSF)); (ii) data about drugs prescribed to patients
sensitive to certain types of antibiotics (e.g., CEFEPIME); (iii) data
about drugs and lab test prescribed to each patient who underwent
specific types of procedures (e.g.,carotid endarterectomy); (iv) mi-
crobiology tests conducted for each patient; (v) lab tests for each
patient who was prescribed certain drug types (e.g. additive, base);
etc. We placed in Solr a view VSolr ∈ VEX P , storing for each patient
the caregivers’ reports (unstructured text). The usage of AsterixDB,
SparkSQL and MongoDB is detailed below.
8.1 Cross-Store Rewritings Evaluation
We study the effectiveness of ESTOCADA cross-store query rewrit-
ing: (i) compared to single-store query evaluation (Section 8.1.1);
(ii) improving performance in the pre-existing polystore engines:
BigDAWG [18] and ESTOCADA polystore engine (Section 8.1.2).
8.1.1 Single-Store Evaluation Comparison. Figure 8 sh-ows
the rewriting time for the query set and views described above.
Notice that most queries are rewritten within 100ms and the few
outliers require at most 190ms, which is negligible compared to the
query execution time (in the order of minutes, as seen in Figures 6
and 7). Figure 9 shows the distribution of rewriting time over the
same query set when we scale up the number of relevant views
(we did not materialize them) to 128 views (this is for stress-test
purposes, as 128 views relevant to the data touched by a single
query is unlikely).
Queries with Text Search Predicates. Figure 6 reports the to-
tal execution plus rewriting time of ESTOCADA using the above
views for Q01 to Q29, all of which feature a text search predicate
against the text notes in the caregiver’s report. For each query,
cross-model rewriting and evaluation significantly outperforms
direct evaluation in any single store.

Figure 8: Rewriting time (28 relevant views)

Figure 9: Rewriting time (128 relevant views)

For SparkSQL and AsterixDB, queries (Q01,. . . , Q29) took over
25 minutes (the timeout value we used). The bottleneck is the text
search operation: full-text indexing is lacking in SparkSQL, and
limited to exclude text occurring within JSON arrays in AsterixDB.
This confirms our thesis that cross-model redundant storage of data
into the stores most suited for an expected operation is worthwhile.
For MongoDB and Postgres, the execution time is correlated with
the number of JSON array unnest operators. For instance, query
Q25 has 5 unnest operators whereas query Q17 has 2. Postgres
outperforms MongoDB because the latter lacks join-reordering
optimization, and it does not natively support inner joins. These
must be simulated by left outer joins – using the $lookup operator

0510152025Q01Q02Q03Q04Q05Q06Q07Q08Q09Q10Q11Q12Q13Q14Q15Q16Q17Q18Q19Q20Q21Q22Q23Q24Q25Q26Q27Q28Q29Execution time in minutes (includes rewriting time for ESTOCADA)Mongo DBAsterixDBPostgres(JSON)SparkSQL(JSON)Cross-Store EvaluationTimeout0510152025Q30Q31Q32Q33Q34Q35Q36Q37Q38Q39Q40Q41Q42Q43Q44Q45Q46Q47Q48Q49Q50Execution time in minutes (includes rewriting time for ESTOCADA)Mongo DBAsterixDBPostgres(JSON)SparkSQL(JSON)Cross-Store EvaluationTimeoutTowards Scalable Hybrid Stores

, ,

– followed by a selection for non-null values –using the $match
operator.
Queries without Text Search Predicates. Figure 7 repeats this
experiment for queries Q30,. . . ,Q50, which do not perform text
search. These queries each feature join, selection and navigation
into the JSON document structure (at least two levels deep). The
relevant views for these queries are VPostдr eSQ L ∪VPostдr es J SO N ;
again, exploiting them outperforms single-store evaluation. Spark-
SQL has the highest query execution time; this is because it only
supports navigation into JSON arrays through the EXPLODE func-
tion, which is highly inefficient (see Appendix F for examples).
AsterixDB is outperformed because its optimizer does not support
join reordering. In single-store evaluation, Postgres is more efficient;
this is why we chose it to store VPostдr es J SO N .
Materializing All Views In a Single Store. We performed ex-
periments showing that cross-model evaluation is more efficient
than view-based single-store evaluation: on Postgres, for most of
our queries; for the other systems, for all our queries. The details
can be found in Appendix L.

8.1.2 Polystore Evaluation Comparison. We study the bene-
fits that ESTOCADA can bring to an existing polystore system by
transparently rewriting queries using materialized views stored
across different stores. We used two polystore engines: (i) ESTO-
CADA polystore engine instantiated with two Postgres servers (one
stores relational data while the other stores JSON) and Solr v6.1 for
storing text; (ii) the latest release of the BigDAWG polystore. A key
BigDAWG concept is an island, or collection of data stores accessed
with a single query language. BigDAWG supports islands for rela-
tional, array and text data, based on the Postgres, SciDB [4] and
Apache Accumulo [1] stores, respectively. BigDAWG queries use
explicit CAST operators to migrate an intermediary result from an
island to another. To work with BigDAWG, we extended it with two
new CAST operators: (i) to migrate Solr query results to Postgres;
(ii) to migrate Postgres JSON query results to a Postgres relational
instance and vice-versa. The main difference between our ESTO-
CADA polystore engine and BigDAWG is that we join subquery
results in the mediator using a BindJoin[48], whereas BigDAWG
migrates such results to an island capable of performing the join
(typically, a relational one).
Data Storage. We store the MIMIC-III dataset (in both systems) as
follows: patient metadata in Postgres relational instance; caregivers’
reports in Solr; patients’ lab events, prescriptions, microbiology
events, and procedures information in the Postgres JSON instance.
Polystore Queries. We rewrote Q01,. . . , Q29 in BigDAWG syn-
tax, referring to each part of the data from its respective island
(as BigDAWG requires); we refer to the resulting query set as
QBiдDAW G . Appendix J illustrates a BigDAWG query. We have
the same set of queries in QBT XM syntax; we call these queries
QESTOCADA Polystor e .
The Views. To the view set VEX P introduced above, we have
added a new set of views VN EW which can benefit QBiдDAW G
and QESTOCADA Polystor e queries as we detail below.

The queries vary in terms of full-text search predicates selec-
tivity. 60% of QBiдDAW G and QESTOCADA queries consist of full-
text search predicates, which are not highly selective (e.g., “low

blood pressure“). We refer to these queries as Q60%
BiдDAW G and
Q60%
ESTOCADA P olystor e . We observed that the cost of such queries
in BigDAWG is dominated by moving and ingesting the results of
Solr queries in Postgres (relational instance) to join them with the
results of other sub-queries. To alleviate this overhead, we mate-
rialized in the Postgres server storing JSON a set of 6 views VN EW ,
which join the data from Solr (using those full-text predicates in the
views definitions) with JSON data from the Postgres JSON server.

Given the Q60%

BiдDAW G queries, VP ostдr es J SO N , VPostдr esSQ L
and VN EW , our cross-model views-based rewriting approach finds
rewritings using views from Postgres (relational instance) and Post-
gres (JSON instance). The performance saving is due to the fact
that we no longer have to move data from Solr to a relational
Postgres instance (see Figure 10 for queries labeled *). Although
ESTOCADA polystore engine does not require any data movement,
it still benefits from utilizing VP ostдr es J SO N , VP ostдr esSQ L and
VN EW to answer Q60%
ESTOCADA P olystor e queries as shown in Fig-
ure 10 (queries labeled with *). In contrast, the remaining 40% of
QBiдDAW G and QESTOCADA P olystor e queries have highly selective
full-text search predicates (we refer to these as queries Q40%
BiдDAW G
and Q40%
ESTOCADA Polystor e ). The high selectivity of these queries
reduces the overhead of moving the data from Solr to Postgres (re-
lational instance) in BigDAWG, and in general the data movement
is not a bottleneck for these queries. However, both systems can
benefit from the materialized views VEX P to evaluate these queries
as shown in Figure 10 (queries labeled with +).

BiдDAW G and Q60%

As mentioned earlier, ESTOCADA polystore engine and Big-
DAWG differ in terms of multi-store join evaluation strategies,
leading to different performance variations when the queries and/or
views change. On the queries Q60%
ESTOCADA Polystor e ,
where the full-text search predicates are not very selective, Big-
DAWG execution time is dominated by moving partial results to
the join server. In contrast, ESTOCADA polystore engine performs
better since it computes the join in memory in the mediator, thus it
does not need to pay the intermediary result storage cost. For the
other 40% of queries (with very selective full-text search predicates),
BigDAWG data movement cost is negligible, thus its evaluation time
is dominated by evaluating the join between sub-queries results in
the Postgres relational island, where join algorithms and orders may
be better chosen than in the ESTOCADA polystore engine (the two
platform support different sets of join algorithms). However, the
differences are negligible. The main conclusion of this experiment,
however, is that our cross model views-based rewriting approach
improves performance in both polystore engines.

8.2 PACB VS PACBopt
This experiment demonstrates the performance gains of PACBopt
over PACB in a polystore setting. We consider the queries QEX P and
the 28 views VEX P that can be utilized to answer QEX P , introduced
above. We add to VEX P some irrelevant views Vir r el ⊆ (V −VEX P ).
We scale the size of Vir r el from 1000 to 4000. Figure 11 presents
the average rewriting time of QEX P in the presence of VEX P ∪Vir r el ;
the y axis is in log scale.

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

Figure 10: Queries and Rewritings Evaluation In Polystore Engines

queries to one store or another; data sets can also be migrated by the
users. Our LAV approach is novel and, as we have shown, enables
to improve the performance of such stores also. The integration
of “NoSQL” stores has been considered e.g., in [? ] again in a GAV
approach, without the benefits of LAV view-based rewriting.

Adaptive stores for a single data model have been studied e.g., in [8,

12, 29, 34, 35]; views have been also used in [50? ] to improve the
performance of a large-scale distributed relational store. The nov-
elty of ESTOCADA here is to support multiple data models, by
relying on powerful query reformulation techniques under con-
straints.

Data exchange tools such as Clio [19, 27] allow migrating data
between two different schemas of the same (typically relational
and sometimes XML) model (and thus not focused on cross-model
rewriting). View-based rewriting and view selection are grounded
in the seminal works [28, 39]; the latter focuses on maximally con-
tained rewritings, while we target exact query rewriting, which
leads to very different algorithms. Further setting our work apart is
the scale and usage of integrity constraints. Our pivot model recalls
the ones described in [16, 41] but ESTOCADA generalizes these
works by allowing multiple data models both at the application and
storage level.

CloudMdsQL [36] is an integration language resembling QBT XM ,
and our cross-model view-based rewriting approach could be easily
adapted to use CloudMdsQL as its integration language, just like
we adapted it to use BigDAWG’s. The polystore engine supporting
CloudMdsQL does not feature our cross-model view-based query
rewriting functionality.

Works on publishing relational data as XML [21] followed the
GAV paradigm, thus did not raise the (cross-model) view-based
rewriting problem we address here.

10 CONCLUSIONS

We have shown that multi-store architectures have the potential
to significantly speed up query evaluation by materializing views
in the systems most suited to expected workload operations, even
when these views are distributed across stores and data models. ES-
TOCADA supports this functionality by a local-as-view approach
whose immediate benefit is flexibility since it requires no work
when the underlying data storage changes. To make this approach
feasible, we had to couple modeling contributions (in designing
a reduction from multi-model view-based rewriting to relational
rewriting under constraints), with algorithmic contributions (in
optimizing and extending the state-of-the-art algorithm for rela-
tional view-based rewriting under constraints) and with systems

Figure 11: PACB vs PACBopt rewriting performance

8.3 Summary of Experimental Findings

We have shown that our cross-model views-based rewriting ap-
proach is portable across polystore engines. Moreover, it is worth-
while, as it improves their performance in natural scenarios for
both cross-model and even single-model user queries; the latter are
outperformed by rewritings using a distributed cross-store (and
cross-model) set of materialized views (even when accounting for
the time it takes to find the rewriting). We have also shown that
the time spent searching for rewritings is a small fraction of the
query execution time and hence a worthwhile investment. As we
confirm experimentally, the performance of the rewriting search
is due to our optimized PACBopt algorithm, shown to outperform
standard PACB by up to 40×.

9 RELATED WORK

Heterogeneous data integration is an old topic [26, 28, 39, 41] ad-
dressed mostly in a single-model (typically relational) setting, where
cross-model query rewriting and execution did not occur. The fed-
erated Garlic system [26] considered true multi-model settings,
but the non-relational stores did not support their own declara-
tive query language (they included for instance text documents,
spreadsheets, etc.), being instead wrapped to provide an SQL API.
Consequently works on federated databases did not need cross-
model rewriting.

The remark “one-size does not fit all” [51] has been recently
revisited [32, 40] for heterogeneous stores. [37] uses a relational
database as a “cache” for partial results of a MapReduce computa-
tion, while [38] considers view-based rewriting in a MapReduce
setting. Unlike our work, these algorithms need the data, views and
query to be in the same data model.

Polystores [6, 18] allow querying heterogeneous stores by group-
ing similar-model platform into “islands” and explicitly sending

051015Q01*Q02*Q03+Q04*Q05+Q06*Q07*Q08*Q09+Q10*Q11*Q12+Q13*Q14+Q15+Q16*Q17+Q18*Q19+Q20*Q21*Q22*Q23*Q24+Q25*Q26*Q27+Q28+Q29*Execution time in minutes (Includes rewriting time)ESTOCADA Polystore Engine (Direct Evaluation)ESTOCADA Polystore Engine (Query Rewriting)BigDAWG (Direct Evaluation)BigDAWG (Query Rewriting)3569382740484720821311652111101001000100001028202830284028Average rewriting time (ms)# of viewsPACBPACB(OPT)Towards Scalable Hybrid Stores

, ,

contributions (integrating our rewriting algorithm within [10] and
BigDAWG).

In our experiments, we achieved performance gains by simply
placing the materialized views according to a few common-sense
guidelines (e.g., place large unstructured text collections in a store
with good full-text indexing support, and place inherently nested
data in JSON document stores). As a natural future work step, we are
targeting the cost-based recommendation of optimal cross-model
view placement.

REFERENCES
[1] [n. d.]. Apache Accumulo.https://accumulo.apache.org/.
[2] [n. d.]. AsterixDB. https://asterixdb.apache.org/.
[3] [n. d.]. Saxon.http://saxon.sourceforge.net/.
[4] [n. d.]. Scidb.https://www.paradigm4.com/.
[5] Serge Abiteboul, Richard Hull, and Victor Vianu. 1995. Foundations of Databases.

Addison-Wesley.

[6] Divy Agrawal, Sanjay Chawla, Bertty Contreras-Rojas, Ahmed K. Elmagarmid,
Yasser Idris, Zoi Kaoudi, Sebastian Kruse, Ji Lucas, Essam Mansour, Mourad
Ouzzani, Paolo Papotti, Jorge-Arnulfo Quiané-Ruiz, Nan Tang, Saravanan Thiru-
muruganathan, and Anis Troudi. 2018. RHEEM: Enabling Cross-Platform Data
Processing - May The Big Data Be With You! PVLDB (2018).

[7] Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast Algorithms for Mining

Association Rules in Large Databases. In PVLDB.

[8] I. Alagiannis, S. Idreos, and A. Ailamaki. 2014. H2O: a hands-free adaptive store.

In SIGMOD.

[9] Michael Armbrust, Reynold S Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K
Bradley, Xiangrui Meng, Tomer Kaftan, Michael J Franklin, Ali Ghodsi, et al. 2015.
Spark SQL: Relational data processing in Spark. In SIGMOD. ACM.

[10] Raphaël Bonaque, Tien Duc Cao, Bogdan Cautis, François Goasdoué, Javier
Letelier, Ioana Manolescu, Oscar Mendoza, Swen Ribeiro, Xavier Tannier, and
Michaël Thomazo. 2016. Mixed-instance querying: a lightweight integration
architecture for data journalism. In VLDB. New Delhi, India. https://hal.inria.fr/
hal-01321201

[11] Ding Chen and Chee-Yong Chan. 2010. ViewJoin: Efficient view-based evaluation

of tree pattern queries. In ICDE.

[12] Debabrata Dash, Neoklis Polyzotis, and Anastasia Ailamaki. 2011. CoPhy: A
Scalable, Portable, and Interactive Index Advisor for Large Workloads. PVLDB
(2011).

[13] Alin Deutsch, Bertram Ludäscher, and Alan Nash. 2005. Rewriting Queries Using

Views with Access Patterns Under Integrity Constraints. In ICDT.

[14] Alin Deutsch, Alan Nash, and Jeffrey B. Remmel. 2008. The chase revisited. In

PODS.

[29] S. Idreos, M.L. Kersten, and S. Manegold. 2007. Database Cracking. In CIDR.
[30] Ioana Ileana, Bogdan Cautis, Alin Deutsch, and Yannis Katsis. 2014. Complete yet
practical search for minimal query reformulations under constraints. In SIGMOD.
[31] ISO/IEC 9075-1:1999. [n. d.]. SQL:1999. http://www.iso.org/iso/iso_catalogue/
catalogue_tc/catalogue_detail.htm?csnumber=26196. Accessed March 2016.
[32] A. Jindal, J.-A. Quiané-Ruiz, and J. Dittrich. 2013. WWHow! Freeing Data Storage

from Cages. In CIDR.

[33] AEW Johnson, J.J. Pollard, L. Shen, L. Lehman, M. Feng, M. Ghassemi, B. Moody,
P. Szolovits, L.A. Celi, and R.G. Mark. 2016. MIMIC-III, a freely accessible critical
care database. Scientific Data (2016). DOI: 10.1038/sdata.2016.35. Available at:
http://www.nature.com/articles/sdata201635 (2016).

[34] M. Karpathiotakis, I. Alagiannis, T. Heinis, M. Branco, and A. Ailamaki. 2015.
Just-In-Time Data Virtualization: Lightweight Data Management with ViDa. In
CIDR.

[35] Asterios Katsifodimos, Ioana Manolescu, and Vasilis Vassalos. 2012. Materialized

view selection for XQuery workloads. In SIGMOD.

[36] Boyan Kolev, Patrick Valduriez, Carlyna Bondiombouy, Ricardo Jiménez-Peris,
Raquel Pau, and José Pereira. 2016. CloudMdsQL: querying heterogeneous cloud
data stores with a common language. Distributed and parallel databases (2016).
[37] J. LeFevre, J. Sankaranarayanan, H. Hacigümüs, and al. 2014. MISO: souping up

big data query processing with a multistore system. In SIGMOD.

[38] Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigümüs, and al. 2014. Oppor-

tunistic physical design for big data analytics. In SIGMOD.

[39] A. Levy, A. Rajaraman, and J. Ordille. 1996. Querying Heterogeneous Information

Sources Using Source Descriptions. In VLDB.

[40] H. Lim, Y. Han, and S. Babu. 2013. How to Fit when No One Size Fits.. In CIDR.
[41] Ioana Manolescu, Daniela Florescu, and Donald Kossmann. 2001. Answering

XML Queries on Heterogeneous Data Sources. In VLDB.

[42] Alan Nash and Bertram Ludäscher. 2004. Processing First-Order Queries under

Limited Access Patterns. In PODS.

[43] N. Onose, A. Deutsch, Y. Papakonstantinou, and E. Curtmola. 2006. Rewriting

Nested XML Queries Using Nested Views. In SIGMOD.

[44] M. T. Özsu and P. Valduriez. 2011. Principles of Distributed Database Systems,

Third Edition. Springer.

[45] Yannis Papakonstantinou. 2016. Semistructured Models, Queries and Algebras

in the Big Data Era: Tutorial Summary. In SIGMOD.

[46] Derek Phillips, Ning Zhang, Ihab F. Ilyas, and M. Tamer Özsu. 2006. InterJoin:
Exploiting Indexes and Materialized Views in XPath Evaluation. In SSDBM.
[47] Lucian Popa, Alin Deutsch, Arnaud Sahuguet, and Val Tannen. 2000. A chase

too far?. In ACM SIGMOD Record, Vol. 29. ACM, 273–284.

[48] Anand Rajaraman, Yehoshua Sagiv, and Jeffrey D. Ullman. 1995. Answering

Queries Using Templates with Binding Patterns. In PODS.

[49] P. Griffith Selinger, M.M. Astrahan, D.D. Chamberlin, R.A. Lorie, and T.G. Price.
1979. Access Path Selection in a Relational Database Management System. In
SIGMOD.

[50] J. Shute, R. Vingralek, B. Samwel, and al. 2013. F1: A Distributed SQL Database

That Scales. In PVLDB.

[15] Alin Deutsch, Lucian Popa, and Val Tannen. 2006. Query reformulation with

constraints. SIGMOD (2006).

[16] Alin Deutsch and Val Tannen. 2003. MARS: A System for Publishing XML from

Mixed and Redundant Storage. In In VLDB.

[51] M. Stonebraker and U. Cetintemel. 2005. "One Size Fits All": An Idea Whose

Time Has Come and Gone. In ICDE.

A RELATIONAL ENCODING OF MOTIVATING

[17] Alin Deutsch and Val Tannen. 2003. Reformulation of XML Queries and Con-

straints. In ICDT.

[18] J. Duggan, A. J. Elmore, M. Stonebraker, M. Balazinska, B. Howe, J. Kepner, S.
Madden, D. Maier, T. Mattson, and S. B. Zdonik. 2015. The BigDAWG Polystore
System. SIGMOD (2015).

[19] R. Fagin, P. Kolaitis, R. Miller, and L Popa. 2003. Data Exchange: Semantics and

Query Answering. In ICDT.

[20] Ronald Fagin, Phokion G Kolaitis, Renée J Miller, and Lucian Popa. 2005. Data
exchange: semantics and query answering. Theoretical Computer Science (2005).
[21] Mary Fernández, Yana Kadiyska, Dan Suciu, Atsuyuki Morishima, and Wang-
Chiew Tan. 2002. SilkRoute: A framework for publishing relational data in XML.
TODS (2002).

[22] Daniela Florescu, Alon Y. Levy, Ioana Manolescu, and Dan Suciu. 1999. Query

Optimization in the Presence of Limited Access Patterns. In SIGMOD.

[23] Georges Gardarin, Fei Sha, and Zhao-Hui Tang. 1996. Calibrating the Query
Optimizer Cost Model of IRO-DB, an Object-Oriented Federated Database System.
In VLDB. 378–389. http://www.vldb.org/conf/1996/P378.PDF

[24] Gösta Grahne and Alberto O. Mendelzon. 1999. Tableau Techniques for Querying

Information Sources through Global Schemas. In ICDT.

[25] Gosta Grahne and Adrian Onet. 2013. Anatomy of the chase. arXiv preprint

arXiv:1303.6682 (2013).

[26] Laura Haas, Donald Kossmann, Edward Wimmers, and Jun Yang. 1997. Optimiz-

ing queries across diverse data sources. (1997).

[27] Laura M. Haas, Mauricio A. Hernández, Howard Ho, Lucian Popa, and Mary Roth.
2005. Clio grows up: from research prototype to industrial tool. In SIGMOD.
[28] Alon Y Halevy. 2001. Answering Queries Using Views: A Survey. The VLDB

Journal (2001).

QBT XMQUERY Q1

Q1 < p a t i e n t I D , drug , a d m i s s i o n L o c , a d m i s s i o n T i m e >: −
MIMIC (M) ,
Chil d J (M, p a t i e n t I D , " p a t i e n t I D " , " o " ) ,
Chil d J (M, A , " a d m i s s i o n s " , " o " ) ,
Chil d J ( A , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
Chil d J ( A , r e p o r t , " r e p o r t s " , " o " ) ,

V alue J ( r e p o r t , " l i k e −c o r o n a r y a r t e r y " ) ,
Chil d J ( A , a d m i s s i o n L o c , " a d m i s s i o n L o c " , " o " ) ,
Chil d J ( A , a d m i s s i o n T i m e , " a d m i s s i o n T i m e " , " o " ) ,
Chil d J ( A , LE , " l a b e v e n t s " , " o " ) ,
Chil d J ( LE , f l a g , " f l a g " , " o " ) ,

V alue J ( f l a g , " a b n o r m a l " ) ,
Chil d J ( LE , l a b I t e m I D , " i d " , " o " ) ,
Chil d J ( A , P , " p r e s c r i p t i o n s " , " o " ) ,
Chil d J ( P , drug , " drug " , " o " ) ,
Chil d J ( P , d r u g t y p e , " d r u g t y p e " , " o " ) ,
V alue J ( d r u g t y p e , " a d d i t i v e " ) ,

LABITEMS ( L ) ,
Chil d J ( L ,
itemID ,
Chil d J ( L , c a t e g o r y ,

" i t e m I D " , " o " ) ,

" c a t e g o r y " , " o " ) ,

V alue J ( c a t e g o r y , " b l o o d " ) ,

Eq J ( itemID , l a b I t e m I D )

B VIEWS CONSTRAINTS

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

( d1 , p a t i e n t I D , " p a t i e n t I D " , " o " ) ,
( d1 , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
( d1 , r e p o r t , " r e p o r t " , " o " )

( d1 , p a t i e n t I D , " p a t i e n t I D " , " o " ) ,
( d1 , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
( d1 , r e p o r t , " r e p o r t " , " o ") − >

V1 Forward ( IO ) C o n s t r a i n t s
MIMIC (M) ,
Chil d J (M, p a t i e n t I D , " p a t i e n t I D " , " o " ) ,
Chil d J (M, A , " a d m i s s i o n s " , " o " ) ,
Chil d J ( A , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
Chil d J ( A , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
Chil d J ( A , r e p o r t , " r e p o r t s " , " o " ) −>
V1 ( d1 ) ,
Chil d J V1
Chil d J V1
Chil d J V1
V1 ' s BACKWARD ( OI ) CONSTRAINTS :
V1 ( d1 ) ,
Chil d J V1
Chil d J V1
Chil d J V1
MIMIC (M) ,
Chil d J (M, p a t i e n t I D , " p a t i e n t I D " , " o " ) ,
Chil d J (M, A , " a d m i s s i o n s " , " o " ) ,
Chil d J ( A , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
Chil d J ( A , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
Chil d J ( A , r e p o r t , " r e p o r t s " , " o " )
V2 ' s BACKWARD ( OI ) CONSTRAINTS :
V2 ( p a t i e n t I D , a d m i s s i o n I D , a d m i s s i o n L o c , a d m i s s i o n T i m e )−>
MIMIC (M) ,
Chil d J (M, p a t i e n t I D , " p a t i e n t I D " , " o " ) ,
Chil d J (M, A , " a d m i s s i o n s " , " o " ) ,
Chil d J ( A , a d m i s s i o n I D , " a d m i s s i o n I D " , " o " ) ,
Chil d J ( A , a d m i s s i o n L o c , " a d m i s s i o n L o c " , " o " ) ,
Chil d J ( A , a d m i s s i o n T i m e , " a d m i s s i o n T i m e " , " o " ) ,
Chil d J ( A , r e p o r t , " r e p o r t s " , " o " )

C DECODING OF REWRITING RW Q1

FOR S J : { q= r e p o r t : c o r o n a r y a r t e r y &
f l = p a t i e n t I D , a d m i s s i o n I D } ,
PR : { SELECT p a t i e n t I D AS P I D1 ,

a d m i s s i o n I D AS AI D1 ,
a d m i s s i o n L o c AS a d m i s s i o n L o c ,
a d m i s s i o n T i m e AS a d m i s s i o n T i m e
V2 } ,

FROM

P J : { SELECT v3−> ' p a t i e n t I D ' AS P I D2 ,

A−> ' a d m i s s i o n I D ' AS AI D2 ,
D−> ' drug ' AS drug

FROM V3 v3 ,

j s o n b _ a r r a y _ e l e m e n t s ( v3−> ' a d m i s s i o n ' ) A ,
j s o n b _ a r r a y _ e l e m e n t s ( A−> ' d r u g s ' ) D

WHERE D−>> ' d r u g t y p e ' = ' a d d i t i v e ' }

WHERE p a t i e n t I D =P I D1 AND

a d m i s s i o n I D =AI D1 AND
P I D1 = P I D2 AND
AI D1 =AI D2

RETURN AJ : {

" p a t i e n t I D " : p a t i e n t I D ,
" a d m i s s i o n L o c " : a d m i s s i o n L o c ,
" a d m i s s i o n T i m e " : a d m i s s i o n T i m e ,
" drug " : drug }

D SAMPLE JSON QUERY IN POSTGRES

SYNTAX

Q01 :
SELECT D . PATIENT−>> ' PATIENTID ' , LI −>> ' LABEL '
FROM

MIMIC AS D , LABITEMS AS LI ,
j s o n b _ a r r a y _ e l e m e n t s ( D . PATIENT−> ' ADMISSIONS ' ) AS A ,
j s o n b _ a r r a y _ e l e m e n t s ( A−> ' LABEVENTS ' ) AS LE ,
j s o n b _ a r r a y _ e l e m e n t s ( A−> ' NOTEEVENTS ' ) AS NE
L I . LABITEM−> ' ITEMID ' =LE−> ' ITEMID ' AND
L I . LABITEM@> ' { " FLUID " : " B l o o d " } ' AND
L I . LABITEM@> ' { " CATEGORY " : " B l o o d Gas " } ' AND
LE−>> ' FLAG ' = ' a b n o r m a l ' AND
t o _ t s v e c t o r ( ' e n g l i s h ' , NE . t e x t : : t e x t ) @@
p l a i n t o _ t s q u e r y ( ' e n g l i s h ' , ' r e s p i r a t o r y f a i l u r e ' )

WHERE

E SAMPLE ASTERIXDB QUERY (SQL++)

Q01 :
USE M I M I C i i i ;
SELECT D . PATIENTID , L I . LABEL
FROM

MIMIC AS D , LABITEMS LI ,
D . ADMISSIONS AS A , LABEVENTS AS LE ,
A . NOTEEVENTS AS N
L I . ITEMID=LE . ITEMID AND

WHERE

LE . FLAG = " a b n o r m a l " AND
L I . CATEGORY= " B l o o d Gas " AND
L I . FLUID = " B l o o d " AND
c o n t a i n s ( N . t e x t , " r e s p i r a t o r y f a i l u r e " )

F SAMPLE SPARKSQL QUERY

Q01 :
SELECT D . PATIENTID , L I . LABEL
FROM

WHERE

LABITEMS AS LI , MIMIC AS M
LATERAL VIEW e x p l o d e ( ADMISSIONS ) AS A
LATERAL VIEW e x p l o d e ( A . LABEVENTS ) AS LE
LATERAL VIEW e x p l o d e ( A . NOTEEVENTS ) AS N
L I . ITEMID=LE . ITEMID AND
LE . FLAG = " a b n o r m a l " AND
L I . CATEGORY= " B l o o d Gas " AND
L I . FLUID = " B l o o d " AND
N . t e x t LIKE ' % r e s p i r a t o r y f a i l u r e % '

G SAMPLE MONGODB QUERY

Q02 :
{

{
{
{
{

}
} ,
{
{
{
{

" \ " r e s p i r a t o r y f a i l u r e \ " "

} } } ,

{ " $ s e a r c h " :
} ,

" $ t e x t " :
" $ADMISSIONS "
" $ADMISSIONS . LABEVENTS "
{
" d _ l a b i t e m s " ,

} ,

" a g g r e g a t e " :
" p i p e l i n e " :

" mimic " ,

{

[
" $match " :
" $unwind " :
" $unwind " :
" $ l o o k u p " :
" from " :
" l e t " :
" p i p e l i n e " :

{

" i t e m " :
[
" $match " :
[

{
{
" $ p r o j e c t " :

{
" $eq " :
" $eq " :
{

{

{

" $ADMISSIONS . LABEVENTS . ITEMID "

} ,

" $and " :

" $ e x p r " :

{
" $ITEMID " ,
" $FLUID " ,

[
[
" ITEMID " :

0 ,

" B l o o d "

] } ,
" $$ i t e m "
]
}
]
\ _ i d " : 0 }

"

}

} ,

}
}

] ,

" a s " :

" t "

" $match " :
" $unwind " :
" $unwind " :
" $match " :

{

{

{

" t " :

" $ne " :

} ,
" $ADMISSIONS . ICUSTAYS " } ,
" $ADMISSIONS . ICUSTAYS . PRESCRIPTIONS " } ,

[ ]

}

}

" $ e x p r " :

{

" $and " :
{

[

" $eq " :

[

" $ADMISSIONS . ICUSTAYS . PRESCRIPTIONS . DRUG_TYPE " ,
" ADDITIVE "
[

} ,

]

" $eq " :

" $ADMISSIONS . ICUSTAYS . PRESCRIPTIONS . DRUG " ,
]
" P o t a s s i u m C h l o r i d e "

} ,

" $eq " :

[

{

{

" $ADMISSIONS . LABEVENTS . FLAG " ,
" a b n o r m a l "

} ,

]

}

}

}
" t . LABEL " :

]
1 ,

" t . CATEGORY " :

1 }

}

]

{

" $ p r o j e c t " :

{

}

H QUERY TEMPLATES QT0, QT1, AND QT2
QT0 <PATIENTID , DOB , DOD, GENDER> : −
MIMIC (M) ,
Chil d J (M, PATIENTID , " PATIENTID " , " o " ) ,
Chil d J (M, DOB , " DOB " , " o " ) ,
Chil d J (M, DOD, " DOD " , " o " ) ,
Chil d J (M, GENDER , " GENDER " , " o " ) ;
QT1 <PATIENTID , ITEMID , VALUE , CHARTTIME> : −
MIMIC (M) ,
Chil d J (M, PATIENTID , " PATIENTID " , " o " ) ,
Chil d J (M, A , " ADMISSIONS " , " o " ) ,
Chil d J ( A , LE , " LABEVENTS " , " o " ) ,
Chil d J ( LE , ITEMID ,
Chil d J ( LE , VALUE ,
Chil d J ( LE , CHARTTIME ,
Chil d J ( LE , FLAG ,

" CHARTTIME " , " o " ) ,

" ITEMID " , " o " ) ,

" VALUE " , " o " ) ,

" FLAG " , " o " ) ,
V alue J ( FLAG , " a b n o r m a l " )

QT2 <ITEMID , LABEL , FLUID >: −
LABITEMS ( L ) ,
Chil d J ( L ,
ITEMID ,
Chil d J ( L , CATEGORY ,
Chil d J ( L , LABEL ,
Chil d J ( L , FLUID ,

" LABEL " , " o " ) ,

" FLUID " , " o " ) ,

" ITEMID " , " o " ) ,

" CATEGORY " , " o " ) ,

V alue J ( FLUID , " B l o o d " )

Towards Scalable Hybrid Stores

, ,

FROM

WHERE

LABITEMS LI ,

LE−>> ' LABEL ' AS LABEL
PATIENTEVENTS AS P ,
j s o n b _ a r r a y _ e l e m e n t s ( P . PATIENTEVENT−> ' ADMISSIONS ' ) AS A ,
j s o n b _ a r r a y _ e l e m e n t s ( A−> ' LABEVENTS ' ) AS LE
L I . LABITEM−> ' ITEMID ' =LE−> ' ITEMID ' AND
L I . LABITEM@> ' { " FLUID " : " B l o o d " } ' AND
L I . LABITEM@> ' { " CATEGORY " : " B l o o d Gas " } ' ) ,

TAB2 ,

' ( PATIENTID INTEGER , ADMISSIONID INTEGER , LABEL VARCHAR ) ' ,

r e l a t i o n a l ) ,

PATIENTS P
WHERE P . PATIENTID= t a b 1 . PATIENTID AND P . PATIENTID= t a b 2 . PATIENTID )

I SYNTAX OF QBT XMQUERY LANGUAGE

Bl ock −> f or P at t er n w her e P at t er n ? r e t ur nP at t er n

XM

Q BT
f or P at t er n −> FOR pat t er n ( ' , ' pat t er n ) ∗
w her e P at t er n −> WHERE cond i t ion
r e t ur nP at t er n −> RETURN cons t r uc t or
pat t er n −> annot at ion ' : '
mod el P at t er n − > A J P at t er n

' { ' mod el P at t er n ' } '

| P J P at t er n
| RK P at t er n

cond i t ion −> t er m | t er m=t er m |

' ( ' cond i t ion ' ) '

| cond i t ion AND cond i t ion

t er m −> cons t ant | v ar i abl e

| annot at ion ' : '
| f unC al l

' { ' mod el P at hEx pr

' } '

f unC al l −> f un N ame ' ( ' ar дs ?
ar дs −> t er m ( ' , ' t er m ) ∗
mod el P at hEx pr − > A J P at hEx pr | P J P at hEx pr | RK LookU p Ex pr

' ) '

cons t r uc t or −> annot at ion ' : '
mod elCons t r uc t or − > A J Ob jCons t r uc t or
| P J Ob jCons t r uc t or
| RK M apCons t r uc t or

' { ' mod elCons t r uc t or

' } '

/ / ∗ AJ Pattern ∗ / /
A J P at t er n − > A J F or P at t er n A J W her e P at t er n ?
A J F or P at t er n − > v ar i abl e IN col l ec t ion N ame

| v ar i abl e IN A J P at hEx pr
A J W her e P at t er n − > WHERE A J Cond i t ion
A J Cond i t ion − > A J t er m | A J t er m=A J t er m |

' ( ' A J Cond i t ion ' ) '

| A J Cond i t ion AND A J Cond i t ion
A J t er m − > A J P at hEx pr | v ar i abl e | cons t ant | A J F unC al l

/ / ∗ PJ Pattern ∗ / /
P J P at t er n − > P J F r omP at t er n P J W her e P at t er n ?
P J F r omP at t er n − > r el at ion N ame AS v ar i abl e

| J S O N _ARRAY _E LE M E N T S (P J P at hEx pr ) AS v ar i abl e

P J W her e P at t er n − > WHERE P J Cond i t ion
P J Cond i t ion − > P J t er m

| P J t er m=P J t er m
|
| P J Cond i t ion AND P J Cond i t ion
P J t er m − > P J P at hEx pr | v ar i abl e | cons t ant | P J F unC al l

' ( ' P J Cond i t ion ' ) '

/ / ∗ RK Pattern ∗ / /
RK P at t er n − > v ar i abl e IN RK LookU p Ex pr
/ / ∗ AJ Path Expression ∗ / /
A J P at hEx pr − > v ar i abl e ' .
/ / ∗ PJ Path Expression ∗ / /
P J P at hEx pr − > v ar i abl e ' −> ' k ey ( ' −> ' k ey ) ∗
/ / ∗ RK Look Up Expression ∗ / /
RK LookU p Ex pr − > mainM ap N ame ' [ ' k ey ' ] '

' k ey ( ' .

' k ey ) ∗

| v ar i abl e [ ' k ey ' ] '

/ / ∗ AJ Constructor ∗ / /
A J Ob jCons t r uc t or − >
' { '

( k ey ' : ' A J v alue ( ' , ' k ey ' : ' A J v alue ) ∗

' } '

A J v alue − > v ar i abl e | cons t ant | A J P at hEx pr | A J P at t er n

| A J Ob jCons t r uc t or

/ / ∗ PJ Constructor ∗ / /
P J Ob jCons t r uc t or − >

J S O N _BU I LD_O B J ECT ( k ey , P J v alue ( ' , ' k ey , P J v alue ) ∗

)

P J v alue − > v ar i abl e | cons t ant | P J P at hEx pr | P J P at t er n

| P J Ob jCons t r uc t or

/ / ∗ RK Constructor ∗ /
RK M apCons t r uc t or − >

k ey − > ' { ' k ey ' : ' v ar i abl e ( ' , ' k ey ' : ' v ar i abl e ) ∗ ' } '

annot at ion −> AJ | PR | PJ | SJ | RK
k ey − > STRING
col l ec t ion N ame − > STRING
mainM ap N ame − > STRING
r el at ion N ame − > STRING
f un N ame − > STRING
cons t ant − > STRING |

INTEGER

J SAMPLE BIGDAWG QUERY

Query01 :
b d r e l ( SELECT P . PATIENTID , TAB2 . LABEL

FROM
b d c a s t
( b d t e x t ( q= t e x t : r e s p i r a t o r y + f a i l u r e & f l =PATIENTID , ADMISSIONID ) ,

TAB1 ,

' ( PATIENTID INTEGER , ADMISSIONID INTEGER ) ' ,

r e l a t i o n a l ) ,

b d c a s t
( b d j q (

SELECT P . PATIENTID AS PATIENTID ,

( A−>> ' ADMISSIONID ' ) : : i n t AS ADMISSIONID ,

, ,

Rana Alotaibi, Damian Bursztyn, Alin Deutsch, Ioana Manolescu, and Stamatis Zampetakis

Figure 12: ESTOCADA cross-store query evaluation (relational, resp. JSON views in Postgres, JSON view in Solr) vs. single-store
query evaluation with views (in MongoDB and Postgres).

This model allows to find the cheapest plan for RW ; for a given

rewritten query Q, the cheapest rewriting is selected.

L MATERIALIZING ALL VIEWS IN A SINGLE

STORE EXPERIMENT

We now compare cross-model evaluation against view-based single-
store evaluation. To that purpose, we stored all VEX P views in each
of MongoDB, AsterixDB, SparkSQL and Postgres. As shown in Fig-
ure 12, multi-store evaluation outperforms single-store evaluation
with views in MongoDB. We note that: (i) each MongoDB query
joins three materialized views; the required use of the $unwind
(a.k.a. unnest) operator inside $lookup operators makes the latter
slower. For each document/tuple from an outer collection in the
$lookup operator, $unwind unnests the entire inner collection to
perform the join, which is inefficient; (ii) MongoDB requires to
place the full-text search predicate at the beginning of a query
pipeline, which can hurt performance if the predicate is not highly
selective. In AsterixDB and SparkSQL, all queries timed out due to
the lack of full-text indexing on collection fields. To try to help
them, we flattened the nested documents and created the full text-
search index. In this setting, AsterixDB failed with an internal error
(ArrayIndexOutOfBoundsException is thrown), while SparkSQL
still timed out since it has no full-text index support. For 40% of the
queries, single-store view-based evaluation in Postgres outperforms
cross-model evaluation, which is the most efficient for 60% of the
queries. Note that Postgres benefits from the view-based rewriting
algorithm of ESTOCADA here. On top of view-based performance
improvements, Postgres exploits more choices of join order and
algorithms.

K COST-BASED OPTIMIZATION

Optimization in the ESTOCADA polystore engine used in our ex-
periment combines heuristics and costs, as follows.

First, for each view Vi used in a rewriting RW , such that Vi resides
in the datastore DSi , RW decoding (Section 5.4) consolidates the
maximum number of atoms that can be pushed (evaluated) into
DSi , depending on the query capabilities of the latter; we call such
an operator SourceAccess, and systematically choose to evaluate it
in DSi .

The cost of SourceAccess is estimated using a linear cost model
with a fixed start-up component and one that grows linearly with
the estimated size of the returned data (see below); this model also
uses system (physical) parameters which we computed by running
by hand a few "system profiling" queries, notably the observed
disk read and write rate (for cold reads) and time to manipulate
data in memory for subsequent, hot reads. We had derived these
physical parameters for prior work on the same team cluster, thus
we just reused those numbers. In general, calibration [23] can be
used, i.e., a module of the polystore systems runs some controlled
experiments on each hardware on which it is newly deployed, and
from these experiments derives the values characterizing the system
performance. Calibration is present in many modern systems, e.g.,
RHEEM5.

The cardinality of a SourceAccess is estimated using: (i) statistics
on the views (e.g., number of tuples, or JSON trees, their average size,
etc.); these are gathered at view materialization time, together with
the minimum, maximum, and number of distinct values for fine-
granularity attributes; (ii) default constant factors for inequalities
or keyword search. Statistic estimates are rarely exact and ours
could be perfected; however, they have already helped us make
some good choices.

Optimization develops left-deep join plans in a bottom-up fash-
ion based on the SourceAccess operators. To place each join, we
consider: (i) each data source hosting one input, if it supports joins;
(ii) always, as a fall-back, the mediator. On a full join tree, we push
the remaining selection (σ ) and projections (π ) as far as possible
(taking into account source capabilities), then add possible (linear-
cost) Construct operators which structure the results in the desired
data model. The cost of σ and π are linear in the size of the input
with system-dependent constants; the constant for full-text search
is higher than for equality selections.

5https://github.com/rheem-ecosystem/rheem/tree/master/rheem-
core/src/main/java/org/qcri/rheem/core/profiling

05101520Q01Q02Q03Q04Q05Q06Q07Q08Q09Q10Q11Q12Q13Q14Q15Q16Q17Q18Q19Q20Q021Q22Q23Q24Q25Q26Q27Q28Q29Execution time in minutes (includes ABC rewriting time)MongoDBPostgres (JSON instance)ESTOCADA Polystore Engine