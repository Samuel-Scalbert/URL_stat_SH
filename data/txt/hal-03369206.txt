Study on Acoustic Model Personalization in a Context
of Collaborative Learning Constrained by Privacy
Preservation
Salima Mdhaffar, Marc Tommasi, Yannick Estève

To cite this version:

Salima Mdhaffar, Marc Tommasi, Yannick Estève. Study on Acoustic Model Personalization in a
Context of Collaborative Learning Constrained by Privacy Preservation. SPECOM 2021 - 23rd In-
ternational Conference on Speech and Computer, Sep 2021, St Petersburg, Russia. pp.426 - 436,
￿10.1007/978-3-030-87802-3_39￿. ￿hal-03369206￿

HAL Id: hal-03369206

https://hal.science/hal-03369206

Submitted on 12 Oct 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Study on acoustic model personalization in a
context of collaborative learning constrained by
privacy preservation

Salima Mdhaﬀar1[0000−0002−8472−6890], Marc Tommasi2[0000−0003−2838−4408],
and Yannick Est`eve1[0000−0002−3656−8883]

1 LIA - Avignon Universit´e, France
2 Universit´e de Lille, CNRS, Inria, Centrale Lille, UMR 9189 - CRIStAL, France
firstname.lastname@univ-avignon.fr

Abstract. This paper investigates diﬀerent approaches in order to im-
prove the performance of a speech recognition system for a given speaker
by using no more than 5 minutes of speech from this speaker, and with-
out exchanging data from other users/speakers. Inspired by the federated
learning paradigm, we consider speakers that have access to a personal-
ized database of their own speech, learn an acoustic model and collab-
orate with other speakers in a network to improve their model. Several
local personalizations are explored depending on how aggregation mech-
anisms are performed. We study the impact of selecting, in an adaptive
way, a subset of speakers’s models based on a notion of similarity. We
also investigate the eﬀect of weighted averaging of ﬁne-tuned and global
models. In our approach, only neural acoustic model parameters are ex-
changed and no audio data is exchanged. By avoiding communicating
their personal data, the proposed approach tends to preserve the privacy
of speakers.
Experiments conducted on the TEDLIUM 3 dataset show that the best
improvement is given by averaging a subset of diﬀerent acoustic models
ﬁne-tuned on several user datasets. Our approach applied to HMM/TDNN
acoustic models improves quickly and signiﬁcantly the ASR performance
in terms of WER (for instance in one of our two evaluation datasets, from
14.84% to 13.45% with less than 5 minutes of speech per speaker)

Keywords: automatic speech recognition · privacy-protection · collab-
orative learning · acoustic models · personalization

1

Introduction

User interface of modern electronic and personal devices are more and more
based on voice interaction and this tendency would probably continue increasing
during the next years. Automatic speech recognition (ASR) is the technology at
the core of voice interaction systems. To attain a satisfactory level of usability,
ASR models need to be trained with a huge amount of training data costly to
be collected and annotated. But an important issue with the data collection is

2

S. Mdhaﬀar et al.

privacy preservation. Indeed, some users are now very reluctant to use software
solutions that do not preserve their privacy. Eﬀorts towards privacy have been
made by European states and the General Data Protection Regulation (GDPR)
for instance constraints the way to realize data collection. Speech signals can be
considered as sensitive information because in addition to the linguistic content,
speech also brings information about the speaker: identity, gender, age, health,
emotion...[8]

Diﬀerent levels of privacy preservation can be deﬁned according to the private
information to preserve. Two main approaches have been proposed depending on
the information to hide. In the Interspeech VoicePrivacy Challenge [13], the aim
of privacy preservation consisted in modifying the speech representation features,
trying to remove the speaker identity without removing the linguistic content.
In this scenario, the data is ﬁrst anonymized, and then collected. Even if very
promising results have been reached with these contributions, data anonymiza-
tion is still imperfect and negatively impacts the performance of ASR systems.
Another approach consists in avoiding to share data: data is only used locally, on
the user device, to personalize the model to this user. Then the models are ex-
changed, assuming that adapted models contain less sensitive information than
the data itself. Such approaches have been used in diﬀerent works for speech
recognition [7], mainly through the use of distributed learning to speech the
acoustic model train process [16].

Instead of targeting the improvement of a single general model by sharing
anonymized data or applying a distributed learning approach, we propose in this
paper to focus on the personalization of an initial model to each user. Inspired by
widespread of powerful personal devices, we consider speakers that have access
to a personalized database of their own speech. In this scenario, closely related
to personalized federated learning [5,14], it is possible to both locally ﬁne-tune
an acoustic model and collaborate with other speakers in a network to improve
their own model.

The paper is organised along the following lines. Section 2 presents related
works. Section 3 details the model adaptation. The experimental setup are de-
scribed in Section 4. The experimental results are presented in Section 5 before
concluding and giving some perspectives in Section 6.

2 Related Work

In this study the term ’personalization’ can be interpreted as ’speaker adapta-
tion’, more used in the speech community. A nice overview of speaker adaptation
techniques for neural acoustic models has been presented in [1], that classiﬁes
adaptation techniques into three categories: embedding-based approaches that
relies to the use of auxiliary speaker-dependent features like i-vectors [3], model-
based approaches that relies to speaker data to update the neural weights, and
data augmentation approaches ’which attempt to synthetically generate addi-
tional training data with a close match to the target speaker, by transforming
the existing training data’. No approach based on the use of collaborative train-

Acoustic model adaptation in a context of collaborative learning

3

ing was mentioned in this paper for speaker adaptation, but collaborative train-
ing has already been investigated for acoustic model training. In [2,7], federated
learning was applied to improve a general shared acoustic model with the goal of
privacy preservation, but no speaker adaptation was targeted. Federated learn-
ing was also experimented in [4] to speed up the training process and improve
the shared general acoustic model performance.

3 Model adaptation

Our objective is to locally improve the acoustic model for a target speaker by
taking advantage from both local pre-existing data and from pre-existing mod-
els speciﬁc to other users. In our scenario, a global acoustic model is available,
trained on the initial corpus. This global model is distributed to all the devices,
on which it is possible to ﬁne-tune a local instance of the global model by ex-
ploiting locally the user data. These local models can be shared in order to
indirectly take beneﬁt from the local data used to their adaptation, through a
model averaging.

Since the number of speakers (i.e devices) can be very high, and so the num-
ber of adapted models, it seems relevant to propose a strategy to better select
these local models that could be use to adapt the model of a target speaker. In
a classical hybrid HMM/DNN speech recognition acoustic features like MFCC
(Mel-Frequency Cepstral Coeﬃcients) are generally augmented with additional
speaker-speciﬁc features like i-vectors [6] or x-vectors [12] that can capture infor-
mation about the speaker. In this work, we assume that this kind of information
cannot be exchanged between the diﬀerent devices since we want to avoid to
share explicit knowledge about the speaker and the linguistic content present
in the data. To select the best candidate models from the other speakers, we
suggest to consider the euclidean distance between candidate models and the
model ﬁne-tuned on the target user data.

Our study explores diﬀerent ways to adapt a HMM/DNN acoustic model
through the use of model averaging, local ﬁne-tuning, or a combination of these
approaches. The aim of this adaptation is to modify the parameters of the
(generic) neural network involved in the HMM/DNN architecture. Fine-tuning
consists in continuing the training process of the generic acoustic model on a
small dataset of the target speaker, by taking care on avoiding overﬁtting. Model
averaging consists on computing a model whose each weight is the average of
the weights extracted from a set of models that share the same neural topology.
Figure 1 illustrates the adaptation approach explored in this work. In this
framework, no user data is shared: the ﬁne-tuning is made locally, only adapted
models can be exchanged.

4 Experimental setup

This section describes the ASR system, the experimental methodology and the
datasets used for the experiments on speaker adaptation through ﬁne-tuning and

4

S. Mdhaﬀar et al.

Fig. 1. Model personnalization for a target speaker

model averaging, applied in addition to the classical use of i-vectors as auxiliary
input features for neural network speaker adaptation.

4.1 ASR system

The ASR system is based on the Kaldi toolkit [9]. Acoustic models are based
on a chain-TDNN approach [10]. The chain-TDNN setup is based on 13 layers
with dimension 512 and is trained on cepstral mean and variance normalized
40-dimensional MFCC features concatenated with 9 left and 9 right neighbor
frames. We also incorporates i-vectors as an additional input features. The acous-
tic model has about 14 million parameters. The initial and ﬁnal learning rates
were equal to 0.00025 and 0.000025 respectively. Training audio samples were
randomly perturbed in speed and volume during the training process. This ap-
proach is commonly called audio augmentation.

When ﬁne-tuning the generic model on target speaker data, we modify only
the value of learning rate (the initial and ﬁnal learning rates were equal to
0.000025 and 0.000015 respectively) and all hyperparameters (i.e. learning rate
and local epochs number) are assumed to be homogeneous among all workers.

We make available complete recipes for building the generic acoustic model

and the ﬁne-tuned models3.

As described below, the TEDLIUM 3 dataset was used to train the acoustic
models. Data used to train the model is not a part of the TEDLIUM 3 data
and is described in [11]. The language model used in our experiment is a 4-gram
model, which was pruned to 10 million n-grams.

3 https://github.com/mdhaﬀar/Acoustic model personalisation

Acoustic model adaptation in a context of collaborative learning

5

4.2 Experimental Methodology

The experiments are conducted in the following way. We start by building a
generic ASR model using a large set of utterances from many speakers. Then,
every speaker is associated with a worker that ﬁne-tunes the generic model with
a fresh set of utterances of the given speaker. We obtain a set of ﬁne-tuned
ASR models. Then we try to collaboratively improve these ﬁne-tuned models in
diﬀerent ways.

Let us consider in the following a set of n diﬀerent speakers. Let us denote by
G the generic model, Ps the ﬁne-tuned model of speaker s. We call an average
model of a given set of models, the model deﬁned by the average of model
parameters component wise. We denote by ¯Ps the per-speaker average of all
ﬁne-tuned models Ps(cid:48) except Ps: i.e ¯Ps = 1
s(cid:48)(cid:54)=s Ps(cid:48). For a given speaker s
n−1
and an integer k, we denote by ¯B≤k
the average of the k best models, measured
by the WER on the set of utterances of s used to ﬁne-tune Ps.

(cid:80)

s

We also performed a hierarchical clustering on personal models represented
by the vector of their weights of the ﬁrst layer only. This choice of the ﬁrst
layer as a representative layer comes from a preliminary study we made. This
study showed us that the word error rate obtained by using an acoustic model
ﬁne tuned on a non-target speaker and the Euclidean distance between the ﬁrst
layer of this model and the ﬁrst layer of the model ﬁne-tuned on the target
speaker data is the most correlated, in comparison to the use of other layers.
This is illustrated by Figure 2.

e
u
l
a
V
n
o
i
t
a
l
e
r
r
o
C

0.4

0.2

0

1

2

3

4

5

6

7

8

9 10 11 12 13

Layer of neural network

Fig. 2. Pearson correlation between WER and Euclidian distance in function of the
layer order on the perso1 dataset, described in Section 4.3

For the hierarchical clustering, we use the Numpy library4 with the ward
linkage function. Let us recall that the principle of the hierarchical clustering is
to build a hierarchy of clusters in bottom-up fashion. The Euclidean distance

4 https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.

hierarchy.linkage.html

6

S. Mdhaﬀar et al.

between weight vectors is used to compute the distance between neural network
models. In an iterative process, the two closest clusters are successively merged
until only one remains. The output can be represented by a dendrogram. The
Ward linkage function [15] is used to evaluate the distance between clusters. It
is based on minimum variance method and allows to minimize the total within-
cluster variance.

Using this dendrogram, for a given speaker s and an integer k, we compute
the average of k closest models to Ps (in terms of distance within the

¯D≤k
s
dendrogram). 5

We perform diﬀerent kinds of aggregation of the ﬁne-tuned models using a

weighted average combined to:

1. the generic model G,
2. or the per-speaker average of all ﬁne-tuned models ¯Ps
3. or the k-best ﬁne-tuned models ¯B≤k

s

(models that have the lowest WER on

data from speaker s)

4. or the k-nearest neighbours models ¯D≤k

s where the similarity is given by a

dendrogram of Euclidean distance between model weights

5. or the average of k models taken at random among all ﬁne-tuned models ¯Rk
s .

The weighted average of Ps with one these models M ∈ {G, ¯Ps, ¯B≤k

, Rk
s }
is computed by αPs + (1 − α)M , i.e. in a component-wise convex combination
of the weights.

, ¯D≤k
s

s

4.3 Datasets

All experiments to train acoustic models were conducted with the TEDLIUM
3 dataset, a large corpus of 452 hours of TED talks given by 2,295 speakers.
The dataset is ready for training ASR systems but also dedicated to speaker
adaptation tasks. We processed the dataset in an original way for this set of
experiments. We split it into three parts so that the sets of speakers in each
part are pairwise disjoints. Characteristics of the three parts are reported in
Table 1. The ﬁrst part is called generic and has been used to train an initial
acoustic model for ASR. The two other parts called perso1 and perso2 are used
for 2 distinct trials of model personalization and evaluation. In each part p ∈
{perso1, perso2}, for each speaker s, we consider a small subset of 5 minutes of
speech data called train s
p to ﬁne-tune a per-speaker model and the remaining is
called test s
p and used for evaluation. These datasets are never shared (or merged)
with other data. We consider them as personal and private datasets belonging
to speakers. The average duration of test s
p data is presented in the third line in
Table 1. For the reproductibility of experimental results by research community,
we give the list of the new division of the dataset6.

5 Note that this set may not be unique and we build it iteratively, starting from the

closest cluster and choosing models uniformly at random in the last iteration.

6 https://github.com/mdhaﬀar/Acoustic model personalisation

Acoustic model adaptation in a context of collaborative learning

7

generic perso1 perso2

Duration (hours)
Duration of speech (hours)
Average duration per speaker (minutes) -
Number of speakers

200
170

880

150
125
8.5
650

170
150
8.1
765

Table 1. TEDLIUM3 dataset

5 Experimental results

In our experiments, we take k equals to 50, except for ¯B≤k
, where k = 10. We
measure the average of WER of diﬀerent models on the test s
p data. In a more
formal way, if we denote by WER(M, S) the word error rate of model M on
the dataset S, then we compute averages in the following way. For each part
p ∈ {perso1, perso2}, and for a base model Ms in {G, ¯Ps, ¯B≤k
s }, we
compute the average WER on part p as 1
p).
n
The word error rate of the generic model G and the ﬁne-tuned model Ps are
given in Table 2.

s=1 WER(αPs + (1 − α) ¯Ms, test s

, ¯D≤k
s

, Rk

(cid:80)n

s

s

Generic model G 15.43 14.84
Speaker model Ps 15.04 14.63
Table 2. Word Error Rate of the generic and the ﬁne-tuned models.

perso1 perso2

The values of average WER for the diﬀerent base models computed as ex-
plained above are reported in Table 3 for the values of α = 0, when the private
data of the given speaker has not been used and α = 1/2 when the ﬁne-tuned
model and the other model equally contribute to the averaged model. A graphi-
cal representation of the results is given in Figure 3 for α varying between 0 and
1.

α = 0
Base model perso1 perso2 perso1 perso2

α = 0.5

G
¯Ps
¯D≤50
s
¯R50
s
¯B≤10
s

15.43 14.84 14.43 13.92
15.21 14.62 14.38 13.82
15.49 14.81 14.59 14.01
15.25 14.66 14.35 13.84
14.13 13.45
14.72 13.8

Table 3. Results of collaborative learning with (α = 0.5) and without (α = 0) ﬁne-
tuned model of the target speaker.

8

S. Mdhaﬀar et al.

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

¯Ps
G
¯B≤10
s
¯R50
s

¯Ps
G
¯B≤10
s
¯R50
s

15.5

15

14.5

14

13.5

15

14.5

14

13.5

)

%

(
R
E
W

)

%

(
R
E
W

13

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

α (weight for the speaker model Ps)

Fig. 3. WER according to diﬀerent weighted average of Ps with four diﬀerent aggre-
gated models: all ﬁne-tuned models ¯Ps, generic model G, 10-best (WER) ﬁne-tuned
models ¯B≤10

on perso1 (top) and perso2 (bottom).

, 50 random models ¯R≤50

s

s

Acoustic model adaptation in a context of collaborative learning

9

The signiﬁcance of our results is measured using WER and using the 95%
conﬁdence interval. Conﬁdence interval for the perso1 is 0.08 and 0.07 for perso2.
Conﬁdence interval means that if the improvement in WER exceed this value, we
can consider it as signiﬁcant improvement. The generic model G is considered as
a good model as it is trained by mixing all the training data such that training
is carried out on public dataset. WER of G evaluated on the test part of the
two datasets perso1 and perso2 are presented in line 1 in Table 2. We observe
that the values are signiﬁcantly diﬀerent. The second line in Table 2 presents
results for the speaker model (the local model trained by ﬁne-tuning using only
5 minutes of speech from the target speaker). Compared to the generic model in
Table 2, speaker model improves the WERs for the two sets perso1 and perso2.
It is also treated as a good model since it is trained using data from the target
speaker.

Table 3 shows results obtained under various aggregation of acoustic model.
Results are given for two conﬁgurations: with and without the local ﬁne-tuned
model of the target speaker.

Collaborative learning without the local ﬁne-tuned model of the target speaker
(α = 0): The second line presents results of aggregation of all ﬁne-tuned models
except the target speaker. An improvement is shown compared to the generic
model (from 15.43% to 15.21% for perso1 and from 14.84% to 14.62% for perso2).
This improvement is signiﬁcant since it exceeds the conﬁdence interval value (the
absolute gain for perso1 is 0.22 (0.22>0.08) and the same for perso2).

The third line in Table 3 presents results of aggregation of k-nearest neigh-
bours models. The selection of k-nearest neighbours models does not improve
results compared to the generic model and the model of all speakers (15.49%
WER for perso1 and 14.81% WER for perso2). This selection is compared to
a random selection. Results are presented in the line 4 in Table 3. Results of
random selection are better than the results of dendrogram-based selection. Sur-
prisingly, the beneﬁt of using close models is not empirically demonstrated. This
may be due to several factors. Either the distance is not reﬂecting a notion of
usefulness or some amount of diversity is necessary to obtain models that behave
well on new data. It should be noted that the ﬁrst layer has maybe a too large
number of parameters to compute a meaningful distance. We also tried to reduce
the dimension of this vector, but the impact on the correlations with the WER
(computed in a similar way than in Fig 2) was not observable.

Collaborative learning with the local ﬁne-tuned model of the target speaker (α =
0.5): As shown in Table 2, the ﬁne-tuned model using a small local dataset
improves WER compared to the generic model. So, we decide to take beneﬁt
from this improvement by aggregating models obtained with α = 0 with the
ﬁne-tuned model of the target speaker (note that this model is trained with a
very small dataset). Results in Table 3 shows a signiﬁcant improvement in WER
for all kinds of aggregation.

Acoustic models are prone to overﬁtting when the training dataset is limited.
This could explain why the speaker model Ps cannot get very high performance,

10

S. Mdhaﬀar et al.

since speakers’ models are trained using only 5 minutes. Averaging speakers’
models with the target speaker model allows us to produce a more accurate
model than the target speaker model. The weight value used to combine the
target speaker model Ps and an aggregated model has an inﬂuence on the re-
sulting model combination. This is illustrated in Figure 3. Results in Figure 3
show also that with a good aggregated model, there is less need of the ﬁne-tuned
model Ps to get better results. This is particularly visible when combining Ps
to the ¯B≤10
aggregated model, made by averaging the ten models ﬁne-tuned on
other speakers that got the lowest WERs when applied to the target speaker
data. This ﬁnal combined model outperforms all the other ones, but its usage
seems unrealistic since a local decoding process on the target user data is nec-
essary for all the available non-target speaker models P (cid:48)
s. However, these results
provide good indications to continue this work on acoustic model collaborative
personalization with privacy preservation constraints.

s

6 Conclusion

In this paper, we investigate a collaborative learning algorithm to locally improve
the performance of an automatic speech recognition system, without sharing
data (but models). In this purpose, we suggest to take beneﬁt from acoustic
models that have been separately ﬁne-tuned for each user, in addition to the local
ﬁne-tuning on the target speaker data. Two kinds of local personalizations are
explored, based on a ﬁne-tuning processed on local data, and model averaging.
Signiﬁcant improvements are observed when these two local personalizations are
combined through a weighted average. We also observed that a random selection
of non-target speaker models gives better results than a non-naive approach. In
a scenario where computations are not limited, a selection of non-target speaker
models based on their performances – in terms of WER – on the target speaker
data gives the best results.

7 Acknowledgements

This work was supported by the French National Research Agency under project
DEEP-PRIVACY (ANR-18-CE23-0018).

References

1. Bell, P., Fainberg, J., Klejch, O., Li, J., Renals, S., Swietojanski, P.: Adaptation al-
gorithms for neural network-based speech recognition: An overview. arXiv preprint
arXiv:2008.06580 (2020)

2. Cui, X., Lu, S., Kingsbury, B.: Federated acoustic modeling for automatic speech
recognition. In: ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). pp. 6748–6752. IEEE (2021)

Acoustic model adaptation in a context of collaborative learning

11

3. Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P., Ouellet, P.: Front-end factor
analysis for speaker veriﬁcation. IEEE Transactions on Audio, Speech, and Lan-
guage Processing 19(4), 788–798 (2010)

4. Dimitriadis, D., Kumatani, K., Gmyr, R., Gaur, Y., Eskimez, S.E.: A federated

approach in training acoustic models. In: Proc. Interspeech (2020)

5. Fallah, A., Mokhtari, A., Ozdaglar, A.: Personalized federated learning with the-
oretical guarantees: A model-agnostic meta-learning approach. In: Larochelle,
H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in
Information Processing Systems. vol. 33, pp. 3557–3568. Curran
Neural
Associates,
Inc. (2020), https://proceedings.neurips.cc/paper/2020/file/
24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf

6. Gupta, V., Kenny, P., Ouellet, P., Stafylakis, T.: I-vector-based speaker adaptation
of deep neural networks for french broadcast audio transcription. In: 2014 IEEE
international conference on acoustics, speech and signal processing (ICASSP). pp.
6334–6338. IEEE (2014)

7. Leroy, D., Coucke, A., Lavril, T., Gisselbrecht, T., Dureau, J.: Federated learning
for keyword spotting. In: ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). pp. 6341–6345. IEEE (2019)
8. Nautsch, A., Jim´enez, A., Treiber, A., Kolberg, J., Jasserand, C., Kindt, E., Del-
gado, H., Todisco, M., Hmani, M.A., Mtibaa, A., et al.: Preserving privacy in
speaker and speech characterisation. Computer Speech & Language 58, 441–480
(2019)

9. Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hanne-
mann, M., Motlicek, P., Qian, Y., Schwarz, P., et al.: The kaldi speech recognition
toolkit. In: IEEE 2011 workshop on automatic speech recognition and understand-
ing. No. CONF, IEEE Signal Processing Society (2011)

10. Povey, D., Peddinti, V., Galvez, D., Ghahremani, P., Manohar, V., Na, X., Wang,
Y., Khudanpur, S.: Purely sequence-trained neural networks for asr based on
lattice-free mmi. In: Interspeech. pp. 2751–2755 (2016)

11. Rousseau, A., Del´eglise, P., Esteve, Y.: Enhancing the ted-lium corpus with selected
data for language modeling and more ted talks. In: LREC. pp. 3935–3939 (2014)
12. Snyder, D., Garcia-Romero, D., Sell, G., Povey, D., Khudanpur, S.: X-vectors: Ro-
bust dnn embeddings for speaker recognition. In: 2018 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP). pp. 5329–5333. IEEE
(2018)

13. Tomashenko, N., Srivastava, B.M.L., Wang, X., Vincent, E., Nautsch, A., Yam-
agishi, J., Evans, N., Patino, J., Bonastre, J.F., No´e, P.G., et al.: Introducing the
voiceprivacy initiative. arXiv preprint arXiv:2005.01387 (2020)

14. Wang, K., Mathews, R., Kiddon, C., Eichner, H., Beaufays, F., Ramage, D.: Fed-

erated evaluation of on-device personalization (2019)

15. Ward Jr, J.H.: Hierarchical grouping to optimize an objective function. Journal of

the American statistical association 58(301), 236–244 (1963)

16. Zhang, W., Cui, X., Finkler, U., Kingsbury, B., Saon, G., Kung, D., Picheny, M.:
Distributed deep learning strategies for automatic speech recognition. In: ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP). pp. 5706–5710. IEEE (2019)

