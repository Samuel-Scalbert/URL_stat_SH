Differentially Private Federated Learning on
Heterogeneous Data
Maxence Noble, Aurélien Bellet, Aymeric Dieuleveut

To cite this version:

Maxence Noble, Aurélien Bellet, Aymeric Dieuleveut. Differentially Private Federated Learning on
Heterogeneous Data. Proceedings of The 25th International Conference on Artificial Intelligence and
Statistics (AISTATS), 2022, Virtual, Spain. ￿hal-03905078￿

HAL Id: hal-03905078

https://inria.hal.science/hal-03905078

Submitted on 17 Dec 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Diﬀerentially Private Federated Learning on Heterogeneous Data

Maxence Noble
Centre de Math´ematiques Appliqu´ees
Ecole Polytechnique, France
Institut Polytechnique de Paris

Aur´elien Bellet
Univ. Lille, Inria, CNRS,
Centrale Lille,
UMR 9189 - CRIStAL,
F-59000 Lille, France

Aymeric Dieuleveut
Centre de Math´ematiques Appliqu´ees
Ecole Polytechnique, France
Institut Polytechnique de Paris

Abstract

Federated Learning (FL) is a paradigm for
large-scale distributed learning which faces
two key challenges:
(i) training eﬃciently
from highly heterogeneous user data, and (ii)
protecting the privacy of participating users.
In this work, we propose a novel FL ap-
proach (DP-SCAFFOLD) to tackle these two
challenges together by incorporating Diﬀer-
ential Privacy (DP) constraints into the pop-
ular SCAFFOLD algorithm. We focus on the
challenging setting where users communicate
with a “honest-but-curious” server without
any trusted intermediary, which requires to
ensure privacy not only towards a third party
observing the ﬁnal model but also towards
the server itself. Using advanced results from
DP theory and optimization, we establish
the convergence of our algorithm for con-
vex and non-convex objectives. Our paper
clearly highlights the trade-oﬀ between util-
ity and privacy and demonstrates the supe-
riority of DP-SCAFFOLD over the state-of-
the-art algorithm DP-FedAvg when the num-
ber of local updates and the level of hetero-
geneity grows. Our numerical results conﬁrm
our analysis and show that DP-SCAFFOLD
provides signiﬁcant gains in practice.

1 INTRODUCTION

Federated Learning (FL) enables a set of users with lo-
cal datasets to collaboratively train a machine learn-
ing model without centralizing data (Kairouz et al.,

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

2021). Compared to machine learning in the cloud, the
promise of FL is to avoid the costs of moving data and
to mitigate privacy concerns. Yet, this promise can
only be fulﬁlled if two key challenges are addressed.
First, FL algorithms must be able to eﬃciently deal
with the high heterogeneity of data across users, which
stems from the fact that each local dataset reﬂects the
usage and production patterns speciﬁc to a given user.
Heterogeneous data may prevent FL algorithms from
converging unless they use a large number of communi-
cation rounds between the users and the server, which
is often considered as a bottleneck in FL (Khaled et al.,
2020; Karimireddy et al., 2020b). Second, when train-
ing data contains sensitive or conﬁdential information,
FL algorithms must provide rigorous privacy guaran-
tees to ensure that the server (or a third party) cannot
accurately reconstruct this information from model
updates shared by users (Geiping et al., 2020). The
widely recognized way to quantify such guarantees is
Diﬀerential Privacy (DP) (Dwork and Roth, 2014).

Since the seminal FedAvg algorithm proposed by
McMahan et al. (2017), a lot of eﬀort has gone into
addressing these two challenges separately. FL algo-
rithms like SCAFFOLD (Karimireddy et al., 2020b) and
FedProx (Li et al., 2020a) can better deal with het-
erogeneous data, while versions of FedAvg with Dif-
ferential Privacy (DP) guarantees have been proposed
based on the addition of random noise to the model
updates (McMahan et al., 2018; Geyer et al., 2017;
Triastcyn and Faltings, 2019). Yet, we are not aware
of any approach designed to tackle data heterogene-
ity while ensuring diﬀerential privacy, or of any work
studying the associated trade-oﬀs. This appears to be
a challenging problem: on the one hand, data hetero-
geneity can hurt the privacy-utility trade-oﬀ of DP-FL
algorithms (by requiring more communication rounds
and thus more noise). On the other hand, it is not
clear how to extend existing heterogeneous FL algo-
rithms to satisfy DP and what the resulting privacy-
utility trade-oﬀ would be in theory and in practice.

Diﬀerentially Private Federated Learning on Heterogeneous Data

Our work precisely aims to tackle the issue of data
heterogeneity in the context of FL under DP con-
straints. We aim to protect the privacy of any user’s
data against a honest-but-curious server observing all
user updates, and against a third party observing only
the ﬁnal model. We present DP-SCAFFOLD, a novel dif-
ferential private FL algorithm for training a global
model from heterogeneous data based on SCAFFOLD
(Karimireddy et al., 2020b) augmented with the ad-
dition of noise in the local model updates. Our con-
vergence analysis leverages a particular initialization
of the algorithm, and controls a diﬀerent set of quan-
tities than in the original proof.

Relying on recent tools for tightly keeping track of
the privacy loss of the subsampled Gaussian mecha-
nism (Wang et al., 2020) under R´enyi Diﬀerential Pri-
vacy (RDP) (Mironov, 2017), we formally character-
ize the privacy-utility trade-oﬀ of DP-FedAvg, consid-
ered as the state-of-the-art DP-FL algorithm (Geyer
et al., 2017), and DP-SCAFFOLD in convex and non-
convex regimes. Our results show the superiority of
DP-SCAFFOLD over DP-FedAvg when the number of lo-
cal updates is large and/or the level of heterogeneity
is high. Finally, we provide experiments on simulated
and real-world data which conﬁrm our theoretical ﬁnd-
ings and show that the gains achieved by DP-SCAFFOLD
are signiﬁcant in practice.

The rest of the paper is organized as follows. Section 2
reviews some background and related work on FL,
data heterogeneity and privacy. Section 3 describes
the problem setting and introduces DP-SCAFFOLD. In
Section 4, we provide theoretical guarantees on both
privacy and utility for DP-SCAFFOLD and DP-FedAvg.
Finally, Section 5 presents the results of our exper-
iments and we conclude with some perspectives for
future work in Section 6.

2 RELATED WORK

Federated learning & heterogeneity. The base-
line FL algorithm FedAvg (McMahan et al., 2017) is
known to suﬀer from instability and convergence issues
in heterogeneous settings, related to device variabil-
ity or non-identically distributed data (Khaled et al.,
2020). In the last case, these issues stem from a user-
drift in the local updates, which occurs even if all users
are available or full-batch gradients are used (Karim-
ireddy et al., 2020b). Several FL algorithms have been
proposed to better tackle heterogeneity. FedProx (Li
et al., 2020a) features a proximal term in the objective
function of local updates. However, it is often numer-
ically outperformed by SCAFFOLD (Karimireddy et al.,
2020b), which relies on variance reduction through
control variates. In a nutshell, the update direction of

the global model at the server (c) and the update direc-
tion of each user i’s local model (ci) are estimated and
combined in local Stochastic Gradient Descent (SGD)
steps (c − ci) to correct the user-drift (see Section 3.3
for more details).

MIME (Karimireddy et al., 2020a) also focuses on client
heterogeneity and improves on SCAFFOLD by using the
stochastic gradient evaluated on the global model as
the local variate ci and the synchronized full-batch gra-
dient as the global control variate c. However, com-
puting full-batch gradients is very costly in practice.
Similarly, incorporating DP noise into FedDyn (Acar
et al., 2021), which is based on the exact mini-
mization of a proxy function, is not straightforward.
On the other hand, the adaptation of SCAFFOLD to
DP-SCAFFOLD is more natural as control variates only
depend on stochastic gradients and thus do not de-
grade the privacy level throughout the iterations (see
details in Section 4.1).

Extension to other optimization schemes: While
Fed-Opt (Reddi et al., 2020) generalizes FedAvg by
using diﬀerent optimization methods locally (e.g.,
Adam (Kingma and Ba, 2014), AdaGrad (Duchi et al.,
2011), etc., instead of vanilla local SGD steps) or a dif-
ferent aggregation on the central server, these methods
may also suﬀer from user-drift. Their main objective
is to improve the convergence rate (Wang et al., 2021)
without focusing on heterogeneity. We thus choose to
focus on the simplest algorithm to highlight the impact
of DP and heterogeneity.

Federated learning & diﬀerential privacy. Even
if datasets remain decentralized in FL, the privacy
of users may still be compromised by the fact that
the server (which may be “honest-but-curious”) or a
third party has access to model parameters that are
exchanged during or after training (Fredrikson et al.,
2015; Shokri et al., 2017; Geiping et al., 2020). Diﬀer-
ential Privacy (DP) (Dwork and Roth, 2014) provides
a robust mathematical way to quantify the informa-
tion that an algorithm A leaks about its input data.
DP relies on a notion of neighboring datasets, which in
the context of FL may refer to pairs of datasets diﬀer-
ing by one user (user-level DP) or by one data point
of one user (record-level DP).

Deﬁnition 2.1 (Diﬀerential Privacy, Dwork and
Roth, 2014). Let (cid:15), δ > 0. A randomized algorithm
A : X n → Y is ((cid:15), δ)-DP if for all pairs of neighboring
datasets D, D(cid:48) and every subset S ⊂ Y, we have:

P[A(D) ∈ S] ≤ e(cid:15)P[A(D(cid:48)) ∈ S] + δ.

The privacy level is controlled by the parameters (cid:15) and
δ (the lower, the more private). A standard building
block to design DP algorithms is the Gaussian mech-

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

anism (Dwork and Roth, 2014), which adds Gaussian
noise to the output of a non-private computation. The
variance of the noise is calibrated to the sensitivity of
the computation, i.e., the worst-case change (measured
in (cid:96)2 norm) in its output on two neighboring datasets.
The design of private ML algorithms heavily relies on
the Gaussian mechanism to randomize intermediate
data-dependent computations (e.g. gradients). The
privacy guarantees of the overall procedure are then
obtained via composition (Dwork et al., 2010; Kairouz
et al., 2015). Recent theoretical tools like R´enyi Dif-
ferential Privacy (Mironov, 2017) (see Appendix B) al-
low to obtain tighter privacy bounds for the Gaussian
mechanism under composition and data subsampling
(Wang et al., 2020).

In the context of FL, the output of an algorithm A
in the sense of Deﬁnition 2.1 contains all informa-
tion observed by the party we aim to protect against.
Some work considered a trusted server and thus only
protect against a third party who observes the ﬁnal
model.
In this setting, McMahan et al. (2018) in-
troduced DP-FedAvg and DP-FedSGD (i.e., DP-FedAvg
with a single local update), which was also proposed
independently by Geyer et al. (2017). These algo-
rithms extend FedAvg and FedSGD by having the server
add Gaussian noise to the aggregated user updates.
Triastcyn and Faltings (2019) used a relaxation of DP
known as Bayesian DP to provide sharper privacy loss
bounds. However, these papers do not discuss the the-
oretical trade-oﬀ between utility and privacy. Some
recent work by Wei et al. (2020) has formally exam-
ined this trade-oﬀ for DP-FedSGD, providing a utility
guarantee for strongly convex loss functions. However,
they do not consider multiple local updates.

Some papers also considered the setting with a
“honest-but-curious” server, where users must ran-
domize their updates locally before sharing them. This
corresponds to a stronger version of DP, referred to as
Local Diﬀerential Privacy (LDP) (Duchi et al., 2013;
Zhao et al., 2021; Duchi et al., 2018). DP-FedAvg
and DP-FedSGD can be easily adapted to this setting
by pushing the Gaussian noise addition to the users,
which induces a cost in utility. Zhao et al. (2021) con-
sider DP-FedSGD in this setting but do not provide any
utility analysis. Girgis et al. (2021b) provide utility
and compression guarantees for variants of DP-FedSGD
in an intermediate model where a trusted shuﬄer be-
tween the server and the users randomly permutes the
user contributions, which is known to amplify privacy
(Balle et al., 2019; Cheu et al., 2019; Ghazi et al., 2019;
Erlingsson et al., 2019). However, both of these stud-
ies do not consider multiple local updates, which is
key to reduce the number of communication rounds.
Li et al. (2020b) consider the server as “honest-but-

curious” but does not ensure end-to-end privacy to the
users. Finally, Hu et al. (2020) present a personalized
DP-FL approach as a way to tackle data heterogeneity,
but it is limited to linear models.

Summary. To the best of our knowledge, there exists
no FL approach designed to tackle data heterogeneity
under DP constraints, or any study of existing DP-FL
algorithms capturing the impact of data heterogeneity
on the privacy-utility trade-oﬀ.

3 DP-SCAFFOLD

In this section, we ﬁrst describe the framework that
we consider for FL and DP, before giving a detailed
description of DP-SCAFFOLD. A table summarizing all
notations is provided in Appendix A.

3.1 Federated Learning Framework

1, ..., di

We consider a setting with a central server and M
users. Each user i ∈ [M ], holds a private local dataset
R} ⊂ X R, composed of R observations
Di = {di
living in a space X . We denote by D := D1 (cid:116) ... (cid:116) DM
the disjoint union of all user datasets. Each dataset Di
is supposed to be independently sampled from distinct
distributions. The objective is to solve the following
empirical risk minimization problem over parameter x:

minx∈Rd F (x) := 1
M

(cid:80)M

i=1 Fi(x),

(cid:80)R

where Fi(x) := 1
j=1 fi(x, di
j) is the empirical risk
R
on user i, and for all x ∈ Rd and d ∈ X , fi(x, d)
is the loss of the model x on observation d. We de-
note by ∇fi(x, di
j) the gradient of the loss fi com-
puted on a sample di
j ∈ Di, and by extension, for any
Si ⊂ Di, ∇fi(x, Si) := 1
∇fi(x, di
j) is the av-
|Si|
eraged mini-batch gradient. We note that our results
can easily be adapted to optimize any weighted average
of the loss functions and to imbalanced local datasets.

j∈Si

(cid:80)

3.2 Privacy Model

We aim at controlling the information leakage from
individual datasets Di in the updates shared by the
users. For simplicity, our analysis focuses on record-
level DP with respect to (w.r.t) the joint dataset D.
We thus consider the following notion of neighbor-
hood: D, D(cid:48) ∈ X M R are neighboring datasets (denoted
||D − D(cid:48)|| ≤ 1) if they diﬀer by at most one record,
that is if there exists at most one i ∈ [M ] such that
Di and D(cid:48)
i diﬀer by one record. We want to ensure
privacy (or quantify privacy level) (i) towards a third
party observing the ﬁnal model and (ii) towards an
honest-but-curious server. Our DP budget is set in
advance and denoted by ((cid:15), δ), and corresponds to the

Diﬀerentially Private Federated Learning on Heterogeneous Data

Algorithm 1: DP-SCAFFOLD(T, K, l, s, σg, C)

Server Input: initial x0, initial c0
i-th User Input: initial c0
i
Output: xT

1 for t = 1, ..., T do

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

User subsampling by the server:

Sample C t ⊂ [M ] of size (cid:98)lM (cid:99)

Server sends (xt−1, ct−1) to users i ∈ C t
for user i ∈ C t do

Initialize model: y0
for k = 1, ..., K do

i ← xt−1

Data subsampling by user i:
Sample Sk
for sample j ∈ Sk

i ⊂ Di of size (cid:98)sR(cid:99)
i do

Compute gradient:
gij ← ∇fi(yk−1
, di
j)
Clip gradient:
˜gij ← gij/ max (cid:0)1, ||gij||2/C(cid:1)
Add DP noise to local gradients:

i

(cid:80)

j∈Sk
i
i − ηl( ˜H k

˜gij + 2C
sR N (0, σ2
g)
i − ct−1
i + ct−1)
(xt−1 − yK
i )
i − ct−1
)
i
i , ∆ct
i)

˜H k
i ← 1
sR
i ← yk−1
yk
i ← ct−1
i − ct−1 + 1
˜ct
Kηl
i − xt−1, ˜ct
i) ← (yK
i , ∆ct
(∆yt
User i sends to server (∆yt
i ← ˜ct
ct
i
Server aggregates:
(cid:80)
(∆xt, ∆ct) ← 1
lM
xt ← xt−1 + ηg∆xt, ct ← ct−1 + l∆ct

i∈Ct(∆yt

i , ∆ct
i)

At any round t ∈ [T ], a subset C t of users with cardi-
nality (cid:98)lM (cid:99) is uniformly selected by the server, where l
is the user sampling ratio. Each user i ∈ C t downloads
the global model xt−1 held by the central server and
performs K local updates on their local copy yi of the
i = xt−1.
model (with step-size ηl ≥ 0), starting from y0
At iteration k ∈ [K], user i ∈ C t samples an inde-
pendent (cid:98)sR(cid:99)-mini-batch of data Sk
i ⊂ Di, where s
is the data sampling ratio. Given a clipping parame-
ter C > 0, for all j ∈ Sk
, di
j)
is computed and clipped at threshold C (Abadi et al.,
2016), giving ˜gij. The resulting average stochastic gra-
dient H k
) is made private w.r.t. Di using Gaus-
sian noise calibrated to the (cid:96)2-sensitivity S = 2C/sR
and to the scale σg (a parameter which will depend on
the privacy budget), giving ˜H k
i (yk−1
i

i (yk−1
i
) + SN (0, σ2

i , the gradient ∇fi(yk−1

) such that

i (yk−1
i

i (yk−1
i

) := H k

˜H k

g).

i

Finally, we update the model yk−1
(cid:0)

i

i ← yk−1
yk

i − ηl

(omitting index t):

˜H k
(cid:124)

i (yk−1
)
i
(cid:125)
(cid:123)(cid:122)
“noisy” gradient

+ ct−1 − ct−1
(cid:125)
(cid:123)(cid:122)
(cid:124)
drift correction

i

(cid:1),

(1)

using the control variates which are updated at the
end of each inner loop:

i ← ct−1
ct

i − ct−1 +

1
Kηl

(xt−1 − yK

i ) =

1
K

K
(cid:88)

k=1

˜H k

i (yk−1
i

).

i − xt−1) and (ct

After K local
iterations, each user communicates
(yK
) to the central server, and
updates the global model with step-size ηg, as de-
scribed in Step 21 of Alg. 1.

i − ct−1
i

desired level of privacy towards a third party observing
the ﬁnal model (or any model during the training pro-
cess).1 We will also report the corresponding (weaker)
DP guarantees towards the server.

3.3 Description of DP-SCAFFOLD

We now explain how our algorithm DP-SCAFFOLD is
constructed. DP-SCAFFOLD proceeds similarly as stan-
dard FL algorithms like FedAvg: all users perform
a number of local updates K, before communicating
with the central server. We denote T the number of
communication rounds. As SCAFFOLD, DP-SCAFFOLD
relies on the use of control variates that are updated
throughout the iterations of the algorithm: (i) on the
server side (c, downloaded by the users) and (ii) on
the user side ({ci}i∈[M ], uploaded to the server).

1The use of composition for analyzing the privacy guar-
antee for the ﬁnal model implies that the same guarantee
holds even if every intermediate global model is observed.

From the privacy point of view, the updates (∆yi, ∆ci)
that are transmitted to the server are private w.r.t. D
(proved in Section 4.1), thus making private (x, c) w.r.t
D by postprocessing.

The complete pseudo-code is given in Algorithm 1.
Subsampling steps, which amplify privacy (Ka-
siviswanathan et al., 2011), are highlighted in red, and
steps speciﬁcally related to DP are highlighted in yel-
low. Setting σg = 0 and C = ∞ recovers the classi-
cal SCAFFOLD algorithm, and removing control variates
(i.e., setting ct
i to 0 for all t ∈ [T ], i ∈ [M ]) recovers
DP-FedAvg, which we describe in Appendix A (Algo-
rithm 2) for completeness.

Intuition for control variates.
In SCAFFOLD, the
local control variate ci converges to the local gra-
dient ∇fi(x∗) at the optimal, while c approximates
1
i=1 ci (Karimireddy et al., 2020b, Appendix E).
M
Therefore, adding (c − ci) in the update balances the
local stochastic gradient and limits user-drift.

(cid:80)M

Warm-start version of DP-SCAFFOLD. We adapt the
warm-start strategy from Karimireddy et al. (2020b,

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

k=1

˜H k

(cid:80)K

(cid:80)M

i=1 c0

i , with c0

Appendix E) to accommodate DP constraints, lead-
ing to DP-SCAFFOLD-warm. The ﬁrst few rounds of
communication are saved to set2 the initial values of
the control variates to c0 = 1
i =
M
1
i (x0) (perturbed by DP-noise), without up-
K
dating the global model. Note that as we leverage
user sampling in the privacy analysis, the server can-
not communicate with all users at a single round and
the users have to be randomly picked to ensure pri-
vacy. We prove the convergence of DP-SCAFFOLD-warm
in Section 4.2 (assuming that every user participated
to the warm-start phase). Our experiments in Section
5 are conducted with this version of DP-SCAFFOLD.

User-level privacy. Our framework can easily be
adapted to user-level privacy, by setting S = 2C/s.

4 THEORETICAL ANALYSIS

We ﬁrst provide the analysis of the privacy level in
Section 4.1, then analyze utility in Section 4.2.

4.1 Privacy

We ﬁrst establish that the setting of our algorithms
DP-SCAFFOLD and DP-FedAvg enables a fair compari-
son in terms of privacy.
Claim 4.1. For a given noise scale σg > 0, xt has
the same level of privacy at any round t ∈ [T ] in
DP-SCAFFOLD(-warm) and DP-FedAvg after the server
aggregation.

This claim can be proved by induction, see Ap-
pendix B. Consequently, the analysis of privacy is sim-
ilar for DP-FedAvg or DP-SCAFFOLD. Theorem 4.1 gives
the order of magnitude of σg (same for DP-FedAvg and
DP-SCAFFOLD) to ensure DP towards the server or any
third party. Similar to previous work (see e.g., Girgis
et al., 2021b), the results presented below consider the
following regime, as it allows to obtain simple closed
forms for the privacy guarantees in Theorem 4.1.

Assumption 1. We consider a noise level σg, a
privacy budget (cid:15) > 0 and a data-subsampling ratio
s s.t.:
(i) s = o(1), (ii) (cid:15) < 1 and (iii) σg =
Ω(s(cid:112)K/ log(2T l/δ)) (high privacy regime).

Note that our analysis does not require Assumption 1,
but the resulting expressions and the dependency on
the key parameters are then diﬃcult to interpret. This
assumption is actually not used in our experiments,
where we compute the privacy loss numerically using
the complete formulas from our proof.

2This happens with high probability: typically, after 4/l
where l = o(1), all users have been selected at least once
with probability 1 − e−4 ≈ 0.98.

4.1

guarantee).

(Privacy
Under Assumption 1,

Let
Theorem
set σg =
(cid:15), δ > 0.
Ω(cid:0)s(cid:112)lT K log(2T l/δ) log(2/δ)/(cid:15)
for
Then,
DP-SCAFFOLD(-warm) and DP-FedAvg, xT
is (1)
(cid:0)O((cid:15)), δ(cid:1)-DP towards a third party, (2) (cid:0)O((cid:15)s), δs
(cid:1)-
DP towards the server, where (cid:15)s = (cid:15)(cid:112)M/l and
δs = δ

M (cid:1).

√

2 ( 1

l + 1).

Sketch of proof. We here summarize the main steps
of the proof. Let σg be a given DP noise level. Our
proof stands for the privacy analysis over a query func-
tion of sensitivity 1 (since calibration is made with
constant S in Section 3.2). We denote GM(σg) the
corresponding Gaussian mechanism. We ﬁrst provide
the result for any third party.

We combine the following steps:

a = 1
lM

a where σ2

• Data-subsampling with R´enyi DP. Let t ∈ [T ] be
an arbitrary round. We ﬁrst estimate an upper DP
bound (cid:15)a (w.r.t. D) of the privacy loss after the ag-
gregation by the server of lM individual contributions
(Step 21 in Alg. 1). Those are private w.r.t. to the cor-
responding local datasets, say (α, (cid:15)i)-RDP w.r.t. Di
where i ∈ C t stands for the i-th user, each one be-
ing the result of the composition of K adaptative s-
subsampled GM(σg). For any α > 1, we know that
GM(σg) is (α, α/2σ2
g)-RDP (Mironov, 2017). Wang
et al. (2020) proves that the s-subsampled GM(σg)
is (α, O(s2α/σ2
g))-RDP under Assumption 1-(i). By
itera-
the RDP composition rule over the K local
tions, we have (cid:15)i(α) ≤ O(Ks2α/σ2
g). Therefore, the
aggregation over all users considered in C t
is pri-
vate w.r.t. D with a corresponding Gaussian noise
σ2
of variance S2σ2
g
Ks2 (mean of in-
dependent Gaussian noises). Yet, making the whole
aggregation private w.r.t. D only requires a l2 cali-
bration equal to S(cid:48) = S/lM (by triangle inequality)
which means we can quantify the gain of privacy as
(α, O(Ks2α/lM σ2
g))-RDP. After converting this result
into a DP bound (Mironov, 2017), we get that for any
δ(cid:48) > 0, the whole mechanism is ((cid:15)a(α, δ(cid:48)), δ(cid:48))-DP where
(cid:15)a(α, δ(cid:48)) = O(cid:0) Ks2α
+ log(1/δ(cid:48))
α−1
• User-subsampling with DP. In order to get explicit
bounds (that may not be optimal), we then use classi-
cal DP tools to estimate an upper DP bound (cid:15)T after
T rounds. By combining ampliﬁcation by subsampling
results (Kasiviswanathan et al., 2011) over users and
strong composition (Kairouz et al., 2015) (with As-
sumption 1-(ii)) over communication rounds, we ﬁnally
get that, for any δ(cid:48)(cid:48) > 0, xT is ((cid:15)T (α, δ(cid:48), δ(cid:48)(cid:48)), T lδ(cid:48) +δ(cid:48)(cid:48))-
DP where (cid:15)T (α, δ(cid:48), δ(cid:48)(cid:48)) = O(l(cid:15)a(α, δ(cid:48))(cid:112)T log(1/δ(cid:48)(cid:48))).
• Fixing parameters. Considering our ﬁnal privacy
budget δ for any third party, we ﬁx δ(cid:48) := δ/2T l and
δ(cid:48)(cid:48) := δ/2. Following the method of the Moments Ac-
countant (Abadi et al., 2016), we minimize the bound

lM σ2
g

(cid:1).

Diﬀerentially Private Federated Learning on Heterogeneous Data

on (cid:15)T w.r.t. α > 1, which gives that (cid:15)T = O(˜(cid:15)) where

˜(cid:15) = l(cid:112)T log(2/δ)

(cid:18) s(cid:112)K log(2T l/δ)
√

σg

lM

+

(cid:19)

.

Ks2
lM σ2
g

Finally, under Assumption 1-(iii), the second term is
bounded by the ﬁrst one. We then invert the formula
of this upper bound of ˜(cid:15) to express σg as a function of
a given privacy budget (cid:15), proving the ﬁrst statement.

To prove the second statement, we recall that the
server has access to individual contributions before ag-
gregation (which prevents a reduction by a factor lM
of the variance) and that it knows the selected users
at each round, which cancels the user-sampling eﬀect
(factor l). We refer to Appendix B for the full proof
as well as the non-asymptotic (tighter) formulas.

Remarks on privacy accounting. The RDP anal-
ysis conducted to handle data subsampling allows to
limit the impact of K in the expression of σg. A
standard analysis would require a noise level increased
by an extra factor O((cid:112)log(T Kls/δ)). On the other
hand, we tracked the privacy loss over the communica-
tion rounds using standard strong composition (Dwork
et al., 2010), which gives a closed-form expression but
is often sub-optimal in practice. In our experiments,
we use RDP upper bounds to calibrate σg more tightly.
We refer to Appendix B for more details.

Extension to other frameworks.
Instead of the
Gaussian mechanism, other randomizers could be ap-
plied, possibly to the per-example gradients. The pri-
vacy analysis would be then similar to ours as long as
a tight RDP bound on the subsampling of this mech-
anism is provided (see the work of Wang et al., 2020,
for more details). Otherwise, classic DP results for
composition and subsampling must be used instead.
Besides this, our analysis could be extended to the use
of a shuﬄer between the users and the server to am-
plify privacy guarantees. For instance, one could use
a recent RDP result for shuﬄed subsampled pure DP
mechanisms (Girgis et al., 2021a, Theorem 1).

4.2 Utility

We denote by (cid:107) · (cid:107) the Euclidean (cid:96)2-norm. We assume
that F is bounded from below by F ∗ = F (x∗), for an
x∗ ∈ Rd. Furthermore, we make standard assumptions
on the functions (Fi)i∈[M ].

i

di
j

).

i.e., E

is condi-
] =

, di
j)
j)|yk−1
, di
i

1. the stochastic gradient ∇fi(yk−1
i
[∇fi(yk−1
tionally unbiased,
∇Fi(yk−1
2. the stochastic gradient has bounded variance, i.e.,
for any y ∈ Rd, E
3. there exists a clipping constant C independent of i, j
such that (cid:107)∇fi(yk−1
j)(cid:107) ≤ C.

j) − ∇Fi(y)(cid:107)2] ≤ ς 2.

[(cid:107)∇fi(y, di

, di

di
j

i

i

The ﬁrst condition is naturally satisﬁed when di
j is
uniformly sampled in [R]. The second condition is
classical in the literature, and can be relaxed to only
assume that the noise is bounded at the optimal
point x∗ (Gower et al., 2019). Remark that conse-
quently, the variance of a mini-batch of size sR uni-
formly sampled over Di is upper bounded by ς 2/sR.
Finally, the third point ensures that we can safely ig-
nore the impact of gradient clipping.

to obtain a convergence guarantee

Lastly,
for
DP-FedAvg (but not for DP-SCAFFOLD), we use As-
sumption 4 on the data-heterogeneity, which bounds
gradients ∇fi towards ∇f .
Assumption 4 (Bounded Gradient dissimilarity).
There exist constants G ≥ 0 and B ≥ 1 such that:

∀x ∈ Rd, 1
M

(cid:80)M

i=1 ||∇Fi(x)||2 ≤ G2 + B2||∇F (x)||2.

√

g := s(cid:112)lT K log(2T l/δ) log(2/δ)/(cid:15)

Quantifying the heterogeneity between users by con-
trolling the diﬀerence between the local gradients
and the global one is classical
in federated opti-
mization (e.g. Kairouz et al., 2021). We can now
state a utility result in the convex case, by consider-
ing σ∗
M (order of
magnitude of noise scale to approximately ensure end-
to-end ((cid:15), δ)-DP w.r.t. D according to Theorem 4.1).
This result is extended to the strongly convex and non-
convex cases in Appendix C.
Theorem 4.2 (Utility result - convex case). Assume
that for all i ∈ [M ], Fi is convex. Let x0 ∈ Rd and
denote D0 := ||x0 − x∗||. Under Assumptions 2 and
3, we consider the sequence of iterates (xt)t≥0 of Algo-
rithm 1 (DP-SCAFFOLD) and Algorithm 2 (DP-FedAvg),
starting from x0, and with DP noise σg := σ∗
g . Then
there exist step-sizes (ηg, ηl) and weights (wt)t∈[T ] such
that the expected excess of loss E[F (xT )] − F ∗, where
xT = (cid:80)T

t=1 wtxt, is bounded by:

• For DP-FedAvg, under Assumption 4:

(cid:18) D0C(cid:112)d log(T l/δ) log(1/δ)
(cid:15)M R
(cid:123)(cid:122)
privacy bound

(cid:124)

(cid:125)

+

√

1 − l

lM T

Assumption 2. For all i ∈ [M ], Fi is diﬀerentiable
and ν-smooth (i.e., ∇Fi is ν-Lipschitz).

O

We also make the following assumption on the stochas-
tic gradients and data sampling.

Assumption 3. For any iteration t ∈ [T ], k ∈ [K],

ςD0
sRlM KT

√

(cid:124)

+

B2νD2
0
T

+

GD0
√

(cid:123)(cid:122)
optimization bound

+

D4/3

0 ν1/3G2/3
T 2/3

(cid:19)

.

(cid:125)

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

• For DP-SCAFFOLD-warm:

O

(cid:18) D0C(cid:112)d log(T l/δ) log(1/δ)
(cid:15)M R
(cid:123)(cid:122)
privacy bound

(cid:124)

(cid:125)

+

√

νD2
ςD0
0
l2/3T
sRlM KT
(cid:124)
(cid:125)
(cid:123)(cid:122)
optimization bound

+

(cid:19)

.

The two bounds given in Theorem 4.2 consist of three
and two terms respectively:

√

√

1−l

ςD0
sRlM KT

1. A classical convergence rate resulting from (non-
private) ﬁrst order optimization, highlighted in green.
The dominant part, as T → ∞, is
. This
term is inversely proportional to the square root of
the total number of iterations T K times the average
number of gradients computed per iteration lM × sR,
and increases proportionally to the stochastic gradi-
ents’ standard deviation ς and the initial distance to
the optimal point D0.
2. An extra term, in blue, showing that heterogeneity
hinders the convergence of DP-FedAvg, for which As-
sumption 4 is required. Here, as T → ∞, the dominant
term in it is GD0
, except if the user sampling ratio
√
l = 1, then the dominating term becomes D4/3
0 ν1/3G2/3
.
T 2/3
Both these terms do not decrease with the number
of local iterations K, and increase with heterogeneity
constant G. This extra term for DP-FedAvg highlights
the superiority of DP-SCAFFOLD over DP-FedAvg
under data heterogeneity.
3. Lastly, an additional term showing the impact of
DP appears. This term is diverging with the num-
ber of iterations T , which results in the privacy-utility
trade-oﬀ on T . Moreover, this term decreases propor-
tionally to the whole number of data records M R. It
also outlines the cost of DP since it sublinearly grows
with the size of the model d and dramatically increases
inversely to the DP budget (cid:15).

lM T

Take-away messages. Our analysis highlights that:
(i) DP-SCAFFOLD improves on DP-FedAvg in the pres-
ence of heterogeneity; and (ii) increasing the number of
local updates K is very proﬁtable to DP-SCAFFOLD, as
it improves the dominating optimization bound with-
out degrading the privacy bound. These aspects are
numerically conﬁrmed in Section 5.

Sketch of proof and originality. To establish The-
orem 4.2, we adapt the proof of Theorems V and
VII in Karimireddy et al. (2020b). However, we
consider a weakened assumption on stochastic gra-
dients due the addition of Gaussian noise in the lo-
cal updates. Consequently, in order to limit the im-
pact of this additional noise, we change the quan-
tity (Lyapunov function) that is controlled during the
proof: we combine the squared distance to the optimal
point (cid:107)xt − x∗(cid:107)2 to a control of the lag at iteration
i,k−1 − xt(cid:107)2; where we ensure
t,

E(cid:107)αt

(cid:80)M

(cid:80)K

1
KM

k=1

i=1

that our control variates (ct
i)i∈[M ] at iteration t corre-
spond to noisy stochastic gradients measured at points
(αt
.3
This proof is detailed in Appendix C.

i,k−1)i∈[M ], that is, ct

i = 1
K

˜H k
i

(cid:80)K

i,k−1

αt

k=1

(cid:16)

(cid:17)

Relying directly on the result in Karimireddy et al.
(2020b) would require to devote a large fraction (e.g.,
half) of the privacy budget to the initialization phase
to obtain a reasonable bound. Such a strategy did not
perform well in experiments.

(cid:80)K

i = 1
K

On the warm-start strategy. To obtain the utility
result, we have to ensure that initial users’ controls c0
i
i (x0) (notations of
are set as follows: c0
Alg. 1). Our theoretical result thus only holds for the
DP-SCAFFOLD-warm version. However, we observed in
our experiments that DP-SCAFFOLD (which uses initial
user control variates equal to 0) led to the same results
as DP-SCAFFOLD-warm.

˜H k

k=1

Extension to other local randomizers. Our utility
analysis would easily extend to any unbiased mecha-
nism with explicit variance (see Appendix C).

5 EXPERIMENTS

Experimental setup. In our experiments,4 we per-
form federated classiﬁcation with two models: (i) lo-
gistic regression (LogReg) for synthetic and real-world
data, and (ii) a deep neural network with one hidden
layer (DNN ) (see Appendix D.1 for the precise archi-
tecture) on real-world data. We ﬁx the global step-size
ηg = 1, local step-size ηl = η0/sK where η0 is carefully
tuned (see Appendix D.1), and use a (cid:96)2-regularization
parameter set to 5.10−3. Regarding privacy, we ﬁx
δ = 1/M R in all experiments. Then, for each setting,
once the parameters related to sampling and number
of iterations are ﬁxed, we calculate the correspond-
ing privacy bound (cid:15) by using non-asymptotic upper
bounds from RDP theory (see Section 4.1). Details on
the clipping heuristic are given in Appendix D.1. We
report average results over 3 random runs.

Datasets. For synthetic data, we follow the data gen-
eration setup of Li et al. (2020a), which enables to
control heterogeneity between users’ local models and
between users’ data distributions, respectively with
parameters α and β (the higher, the more heteroge-
neous). Note that the setting (α, β) = (0, 0) still cre-
ates heterogeneity and does not lead to i.i.d. data
across users. Our data is generated from a logistic re-
gression design with 10 classes, with input dimension
d(cid:48) = 40. We consider M = 100 users, each holding

3In contrast, the proof in the convex case in Karim-

ireddy et al. (2020b) controls 1
M
4Code available on Github.

(cid:80)M

i=1

E(cid:107)ct

i − ∇fi(x∗)(cid:107)2.

Diﬀerentially Private Federated Learning on Heterogeneous Data

R = 5000 samples. We compare three levels of hetero-
geneity: (α, β) ∈ {(0, 0), (1, 1), (5, 5)}. Details on data
generation are given in Appendix D.2.

We also conduct experiments on the EMNIST-
‘balanced’ dataset (Cohen et al., 2017), which con-
sists of 47 balanced classes (letters and digits) con-
taining all together 131, 600 samples. The dataset is
divided into M = 40 users, who each have R = 2500
data records. Heterogeneity is controlled by parame-
ter γ. For γ% similar data, we allocate to each user
γ% i.i.d. data and the remaining (100 − γ)% by sort-
ing according to the label (Hsu et al., 2019), which
corresponds to ‘FEMNIST’ (Federated EMNIST ). For
experiments involving DNN, we rather use the sem-
inal MNIST dataset, which features 60, 000 samples
labeled by one of the 10 balanced classes. All of the
samples are allocated between M = 60 users (thus
R = 1000). For both datasets, we consider hetero-
geneity levels γ ∈ {0%, 10%, 100%}.

We split each dataset in train/test sets with proportion
80%/20%. Features are ﬁrst standardized, then each
data point is normalized to have unit L2 norm.

Superiority of DP-SCAFFOLD. We ﬁrst study the per-
formance of diﬀerent algorithms under varying levels
of data heterogeneity and number of local updates.
We set subsampling parameters to l = 0.2 and s = 0.2
for all of the datasets and ﬁx the noise level σg = 60
for synthetic data, and σg = 30 for real-world data.
We compare 6 algorithms: FedAvg, FedSGD (FedAvg
with Ks = 1), SCAFFOLD(-warm), with and without
DP. The results for LogReg (convex objective) with
T = 400 are shown in Figure 1 for synthetic data
and Figure 2 (top row) for FEMNIST. Figure 2 (bot-
tom row) shows results for DNN (non-convex objec-
tive) with T = 100 on MNIST data. We report in the
ﬁgure caption the corresponding privacy bound (cid:15) for
the last iterate with respect to a third party.

In both convex and non-convex settings, DP-SCAFFOLD
clearly outperforms DP-FedAvg and DP-FedSGD under
data heterogeneity. The performance gap also in-
creases with the number K of local updates, see Fig-
ure 1. These results conﬁrm our theoretical results:
they show that the control variates of DP-SCAFFOLD
are robust to noise, and allow to overcome the lim-
itations of DP-FedAvg under high heterogeneity and
many local updates.

Trade-oﬀs between parameters. In DP-SCAFFOLD,
a ﬁxed guarantee (cid:15) can be achieved by diﬀerent com-
binations of values for K, T and σg, as shown in
Theorem 4.2). We propose to empirically observe
these trade-oﬀs on synthetic data under a high privacy
regime ((cid:15) = 3). The sampling parameters are ﬁxed to
l = 0.05, s = 0.2. Given σg and K, we calculate the

maximal value of T such that the privacy bound is still
maintained after T communication rounds. Table 1
shows the test accuracy obtained after these iterations
for a high heterogeneity setting (α, β) = (5, 5).

Our results highlight the trade-oﬀ between T and K
(which relates to hardware and communication con-
straints in real deployments) to achieve some given
performance. Indeed, if K is too large, T has to be
chosen very low to ensure the desired privacy, lead-
ing to poor accuracy. For instance, with K = 40,
T cannot exceed 90, and the resulting accuracy thus
barely reaches 22%, even with low private noise. On
the other hand, if we set K too low, DP-SCAFFOLD does
not converge despite a high value of T , since it does
not take advantage of the local updates. Moreover,
we can observe another dimension of the trade-oﬀ in-
volving σg. It seems that better performance can be
achieved by setting σg relatively low, although it im-
plies to choose a smaller T . This trade-oﬀ is evidenced
by the fact that the accuracy achieved in the ﬁrst two
rows (σg = 10 and σg = 20) is quite similar, showing
that σg and T compensate each other.

Other results. Appendix D.3 shows results with
other metrics and heterogeneity levels, higher privacy
regimes, and presents additional experiments on the
eﬀect of sampling parameters l and s (and the trade-
oﬀ with T ) on privacy and convergence.

6 CONCLUSION

Our paper
introduced a novel FL algorithm,
DP-SCAFFOLD, to tackle data heterogeneity under DP
constraints, and showed that it improves over the base-
line DP-FedAvg from both the theoretical and empiri-
cal point of view. In particular, our theoretical analy-
sis highlights an interesting trade-oﬀ between the pa-
rameters of the problem, involving a term of hetero-
geneity in DP-FedAvg which does not appear in the rate
of DP-SCAFFOLD. As future work, we aim at providing
additional experiments with deep learning models and
various sizes of local datasets across users, for more
realistic use-cases. Besides, our paper opens other
perspectives. DP-SCAFFOLD may be improved by in-
corporating other ML techniques such as momentum.
On the experimental side, a larger number of sam-
ples and a more precise tuning of the trade-oﬀ between
T , K and subsampling parameters may dramatically
improve the utility for real-world cases under a given
privacy budget. From a theoretical perspective, inves-
tigating an adaptation of our approach to a personal-
ized FL setting (Fallah et al., 2020; Sattler et al., 2020;
Marfoq et al., 2021), where formal privacy guarantees
have seldom been studied (at the exception of Bellet
et al., 2018; Hu et al., 2020), is a direction of interest.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Table 1: Test Accuracy (%) For DP-SCAFFOLD on Synthetic Data, With (cid:15) = 3, l = 0.05, s = 0.2, (α, β) = (5, 5).

σg

10
20
40
80
160

K = 1

K = 5

K = 10

K = 20

K = 40

27.41±0.71
27.34±1.31
21.05±2.27
17.61±2.62
13.97±1.70

T = 542
T = 545
T = 546
T = 546
T = 546

45.53±0.99
44.39±0.46
34.50±0.65
24.41±0.81
15.99±0.30

T = 488
T = 502
T = 505
T = 506
T = 506

43.52±1.52
43.47±1.74
36.85±0.85
27.33±0.37
19.27±1.65

T = 428
T = 451
T = 457
T = 458
T = 458

42.51±0.80
42.33±0.77
33.24±0.41
19.42±0.51
14.86±0.75

T = 324
T = 352
T = 360
T = 362
T = 362

21.80±3.28
20.14±2.67
14.85±0.95
14.08±0.14
14.17±0.06

T = 72
T = 83
T = 86
T = 87
T = 87

Figure 1: Train Loss On Synthetic Data ((cid:15) = 13). First Row: K = 50; Second Row: K = 100.

Figure 2: Test Accuracy With K = 50. First Row: FEMNIST (LogReg), (cid:15) = 11.4; Second Row: MNIST (DNN),
(cid:15) = 7.2.

Diﬀerentially Private Federated Learning on Heterogeneous Data

Acknowledgments

We thank Baptiste Goujaud and Constantin Philip-
penko for interesting discussions. We thank anony-
mous reviewers for their constructive feedback. The
work of A. Dieuleveut is partially supported by ANR-
19-CHIA-0002-01 /chaire SCAI, and Hi! Paris. The
work of A. Bellet is supported by grants ANR-16-
CE23-0016 (Project PAMELA) and ANR-20-CE23-
0015 (Project PRIDE).

References

Martin Abadi, Andy Chu, Ian Goodfellow, H Bren-
dan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with diﬀerential privacy. In
Proceedings of the 2016 ACM SIGSAC conference
on computer and communications security, pages
308–318, 2016.

Durmus Alp Emre Acar, Yue Zhao, Ramon
Matas, Matthew Mattina, Paul Whatmough, and
Venkatesh Saligrama. Federated learning based on
dynamic regularization. In International Conference
on Learning Representations, 2021.

Galen Andrew, Om Thakkar, H. Brendan McMahan,
and Swaroop Ramaswamy. Diﬀerentially private
learning with adaptive clipping. Advances in Neural
Information Processing Systems, 34, 2021.

Borja Balle, James Bell, Adri`a Gasc´on, and Kobbi Nis-
sim. The privacy blanket of the shuﬄe model. In
Annual International Cryptology Conference, pages
638–667. Springer, 2019.

Aur´elien Bellet, Rachid Guerraoui, Mahsa Taziki, and
Marc Tommasi. Personalized and private peer-to-
peer machine learning. In International Conference
on Artiﬁcial Intelligence and Statistics, pages 473–
481. PMLR, 2018.

Albert Cheu, Adam Smith, Jonathan Ullman, David
Zeber, and Maxim Zhilyaev. Distributed diﬀerential
privacy via shuﬄing. In Annual International Con-
ference on the Theory and Applications of Crypto-
graphic Techniques, pages 375–403. Springer, 2019.

Gregory Cohen, Saeed Afshar, Jonathan Tapson, and
Andre Van Schaik. Emnist: Extending mnist to
handwritten letters. In 2017 international joint con-
ference on neural networks (IJCNN), pages 2921–
2926. IEEE, 2017.

John C. Duchi, Elad Hazan, and Yoram Singer. Adap-
tive subgradient methods for online learning and
stochastic optimization. Journal of machine learn-
ing research, 12(7), 2011.

John C. Duchi, Michael I. Jordan, and Martin J. Wain-
wright. Local privacy and statistical minimax rates.

In 2013 IEEE 54th Annual Symposium on Founda-
tions of Computer Science, pages 429–438. IEEE,
2013.

John C. Duchi, Michael I. Jordan, and Martin J. Wain-
wright. Minimax optimal procedures for locally pri-
vate estimation. Journal of the American Statistical
Association, 113(521):182–201, 2018.

Cynthia Dwork and Aaron Roth. The algorithmic
foundations of diﬀerential privacy. Foundations and
Trends® in Theoretical Computer Science, 9(3-4):
211–407, 2014.

Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan.
In 2010 IEEE
Boosting and diﬀerential privacy.
51st Annual Symposium on Foundations of Com-
puter Science, pages 51–60. IEEE, 2010.

´Ulfar Erlingsson, Vitaly Feldman,

Ilya Mironov,
and
Ananth Raghunathan, Kunal Talwar,
Abhradeep Thakurta.
Ampliﬁcation by shuf-
ﬂing: From local to central diﬀerential privacy via
anonymity. In Proceedings of the Thirtieth Annual
ACM-SIAM Symposium on Discrete Algorithms,
pages 2468–2479. SIAM, 2019.

Alireza Fallah, Aryan Mokhtari,

and Asuman
Ozdaglar.
Personalized federated learning with
theoretical guarantees: A model-agnostic meta-
learning approach. Advances in Neural Information
Processing Systems, 33:3557–3568, 2020.

Matt Fredrikson, Somesh Jha, and Thomas Risten-
part. Model inversion attacks that exploit conﬁ-
dence information and basic countermeasures.
In
Proceedings of the 22nd ACM SIGSAC conference
on computer and communications security, pages
1322–1333, 2015.

Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge,
and Michael Moeller. Inverting gradients-how easy
is it to break privacy in federated learning? Ad-
vances in Neural Information Processing Systems,
33:16937–16947, 2020.

Robin C. Geyer, Tassilo Klein, and Moin Nabi. Dif-
ferentially private federated learning: A client level
perspective. arXiv preprint arXiv:1712.07557, 2017.

Badih Ghazi, Rasmus Pagh, and Ameya Velingker.
Scalable and diﬀerentially private distributed ag-
arXiv preprint
gregation in the shuﬄed model.
arXiv:1906.08320, 2019.

Antonious M. Girgis, Deepesh Data, and Suhas Dig-
gavi. Renyi diﬀerential privacy of the subsampled
shuﬄe model in distributed learning. Advances in
Neural Information Processing Systems, 34, 2021a.

Antonious M. Girgis, Deepesh Data, Suhas Diggavi,
Peter Kairouz, and Ananda Theertha Suresh. Shuf-
ﬂed model of federated learning: Privacy, accuracy

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

and communication trade-oﬀs. IEEE Journal on Se-
lected Areas in Information Theory, 2(1):464–478,
2021b.

Robert Mansel Gower, Nicolas Loizou, Xun Qian, Al-
ibek Sailanbayev, Egor Shulgin, and Peter Richt´arik.
Sgd: General analysis and improved rates.
In In-
ternational Conference on Machine Learning, pages
5200–5209. PMLR, 2019.

Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown.
Measuring the eﬀects of non-identical data distri-
bution for federated visual classiﬁcation.
arXiv
preprint arXiv:1909.06335, 2019.

Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and
Yanmin Gong. Personalized federated learning with
diﬀerential privacy. IEEE Internet of Things Jour-
nal, 7(10):9530–9539, 2020.

Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
The composition theorem for diﬀerential privacy. In
International conference on machine learning, pages
1376–1385. PMLR, 2015.

Peter Kairouz, H. Brendan McMahan, Brendan Avent,
Aur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cor-
mode, Rachel Cummings, Rafael G. L. D’Oliveira,
Hubert Eichner, Salim El Rouayheb, David Evans,
Josh Gardner, Zachary Garrett, Adri`a Gasc´on,
Badih Ghazi, Phillip B. Gibbons, Marco Gruteser,
Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan
Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi,
Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub
Koneˇcn´y, Aleksandra Korolova, Farinaz Koushan-
far, Sanmi Koyejo, Tancr`ede Lepoint, Yang Liu,
Prateek Mittal, Mehryar Mohri, Richard Nock,
Ayfer ¨Ozg¨ur, Rasmus Pagh, Mariana Raykova,
Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn
Song, Weikang Song, Sebastian U. Stich, Ziteng
Sun, Ananda Theertha Suresh, Florian Tram`er, Pra-
neeth Vepakomma, Jianyu Wang, Li Xiong, Zheng
Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen
Zhao. Advances and open problems in federated
learning. Foundations and Trends® in Machine
Learning, 14(1–2):1–210, 2021.

Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale,
Mehryar Mohri, Sashank J. Reddi, Sebastian U.
Stich, and Ananda Theertha Suresh. Mime: Mim-
icking centralized stochastic algorithms in federated
learning. arXiv preprint arXiv:2008.03606, 2020a.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar
Mohri, Sashank Reddi, Sebastian Stich,
and
Ananda Theertha Suresh. Scaﬀold: Stochastic con-
trolled averaging for federated learning.
In Inter-
national Conference on Machine Learning, pages
5132–5143. PMLR, 2020b.

Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi
Nissim, Sofya Raskhodnikova, and Adam Smith.
What can we learn privately?
SIAM Journal on
Computing, 40(3):793–826, 2011.

Ahmed Khaled, Konstantin Mishchenko, and Peter
Richt´arik. Tighter theory for local sgd on identical
and heterogeneous data.
In International Confer-
ence on Artiﬁcial Intelligence and Statistics, pages
4519–4529. PMLR, 2020.

Diederik P. Kingma and Jimmy Ba. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and
Patrick Haﬀner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86
(11):2278–2324, 1998.

Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar
Sanjabi, Ameet Talwalkar, and Virginia Smith. Fed-
erated optimization in heterogeneous networks. Pro-
ceedings of Machine Learning and Systems, 2:429–
450, 2020a.

Yiwei Li, Tsung-Hui Chang, and Chong-Yung Chi. Se-
cure federated averaging algorithm with diﬀerential
privacy. In 2020 IEEE 30th International Workshop
on Machine Learning for Signal Processing (MLSP),
pages 1–6. IEEE, 2020b.

Othmane Marfoq, Giovanni Neglia, Aur´elien Bellet,
Laetitia Kameni, and Richard Vidal. Federated
multi-task learning under a mixture of distribu-
tions. Advances in Neural Information Processing
Systems, 34, 2021.

H. Brendan McMahan, Eider Moore, Daniel Ram-
age, Seth Hampson, and Blaise Aguera y Arcas.
Communication-eﬃcient learning of deep networks
from decentralized data.
In Artiﬁcial intelligence
and statistics, pages 1273–1282. PMLR, 2017.

H. Brendan McMahan, Daniel Ramage, Kunal Talwar,
and Li Zhang. Learning diﬀerentially private recur-
rent language models. In International Conference
on Learning Representations, 2018.

Ilya Mironov. R´enyi diﬀerential privacy.

In 2017
IEEE 30th computer security foundations sympo-
sium (CSF), pages 263–275. IEEE, 2017.

Yurii Nesterov et al. Lectures on convex optimization,

volume 137. Springer, 2004.

Sashank J. Reddi, Zachary Charles, Manzil Zaheer,
Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, San-
jiv Kumar, and H. Brendan McMahan. Adaptive
federated optimization. In International Conference
on Learning Representations, 2020.

Felix Sattler, Klaus-Robert M¨uller, and Wojciech
Clustered federated learning: Model-

Samek.

Diﬀerentially Private Federated Learning on Heterogeneous Data

agnostic distributed multitask optimization under
privacy constraints. IEEE Transactions on Neural
Networks and Learning Systems, 2020.

Reza Shokri, Marco Stronati, Congzheng Song, and
Vitaly Shmatikov. Membership inference attacks
against machine learning models.
In 2017 IEEE
Symposium on Security and Privacy (SP), pages 3–
18. IEEE, 2017.

Aleksei Triastcyn and Boi Faltings. Federated learning
with bayesian diﬀerential privacy.
In 2019 IEEE
International Conference on Big Data (Big Data),
pages 2587–2596. IEEE, 2019.

Tim Van Erven and Peter Harremos. R´enyi divergence
and kullback-leibler divergence. IEEE Transactions
on Information Theory, 60(7):3797–3820, 2014.

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri
Joshi, H. Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine
Daly, Deepesh Data, et al. A ﬁeld guide to federated
optimization.
arXiv preprint arXiv:2107.06917,
2021.

Yu-Xiang Wang, Borja Balle,

and Shiva Ka-
siviswanathan. Subsampled R´enyi Diﬀerential Pri-
vacy and Analytical Moments Accountant. Journal
of Privacy and Conﬁdentiality, 10(2), 2020.

Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H.
Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek,
and H. Vincent Poor. Federated learning with dif-
ferential privacy: Algorithms and performance anal-
ysis. IEEE Transactions on Information Forensics
and Security, 15:3454–3469, 2020.

Yang Zhao, Jun Zhao, Mengmeng Yang, Teng Wang,
Ning Wang, Lingjuan Lyu, Dusit Niyato, and Kwok-
Yan Lam. Local Diﬀerential Privacy based Feder-
ated Learning for Internet of Things. IEEE Internet
of Things Journal, 8(11):8836–8853, 2021.

Supplementary Material:
Diﬀerentially Private Federated Learning on Heterogeneous Data

ORGANIZATION OF THE APPENDIX

This appendix is organized as follows. Appendix A summarizes the main notations and provides the detailed
DP-FedAvg algorithm for completeness. Appendix B provides details on our privacy analysis. Appendix C gives
the full proofs of our utility results for the convex, strongly convex and non-convex cases. Finally, Appendix D
provides more details on the experiments of Section 5, as well as additional results.

A ADDITIONAL INFORMATION

A.1 Table of Notations

Table 2 summarizes the main notations used throughout the paper.

Table 2: Summary of the main notations.

Symbol

Description

[n]
M , i ∈ [M ]
T , t ∈ [T ]
K, k ∈ [K]
Di
R
D
fi(x, d)
Fi
F
xt ∈ Rd
i ∈ Rd
yk
ct ∈ Rd
i ∈ Rd
ct
l ∈ (0, 1)
s ∈ (0, 1)
(cid:15), δ
σg
C
ν
µ
ς 2

1, . . . , di
R

j=1 fi(·, di

j))

i=1 Di)

set {1, 2, ..., n} for any n ∈ N
number and index of users
number and index of communication rounds
number and index of local updates (for each user)
local dataset held by the i-th user, composed of points di
size of any local dataset Di
joint dataset ((cid:70)M
loss of the i-th user for model x on data record d
local empirical risk function of the i-th user ( 1
R
global objective function ( 1
M
server model after round t
model of i-th user after local update k
server control variate after round t
control variate of the i-th user after round t
user sampling ratio
data sampling ratio
diﬀerential privacy parameters
standard deviation of Gaussian noise added for privacy
gradient clipping threshold
Lipschitz-smoothness constant
strong convexity parameter
variance of stochastic gradients

i=1 Fi)

(cid:80)M

(cid:80)R

A.2

DP-FedAvg Algorithm

The code of DP-FedAvg is given in Algorithm 2.

Diﬀerentially Private Federated Learning on Heterogeneous Data

Algorithm 2: DP-FedAvg(T, K, l, s, σg, C)

Server Input: initial x0
Output: xT

1 for t = 1, ..., T do

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

User subsampling by the server: C t ⊂ [M ] of size (cid:98)lM (cid:99)
Server communicates xt−1 to users i ∈ C t
for user i ∈ C t do

Initialize model: y0
for k = 1, ..., K do

i ← xt

Data subsampling by user: Sk
for sample j ∈ Sk

i do

i ⊂ Di of size (cid:98)sR(cid:99)

Compute gradient: gij ← ∇fi(yk−1
Clip gradient: ˜gij ← gij/ max (cid:0)1, ||gij||2/C(cid:1)
i ← 1
sR

, di
j)

i

i − ηl ˜H k

Add DP noise to local gradients: ˜H k
i ← yk−1
yk
i ← yK

∆yt
User i communicates to server: ∆yt
i
i∈Ct ∆yt
i

i − xt−1

(cid:80)

Server aggregates: ∆xt ← 1
lM
xt ← xt−1 + ηg∆xt

i

(cid:80)

j∈Sk
i

˜gij + 2C

sR N (0, σ2
g)

B DETAILS ON PRIVACY ANALYSIS

In this section, we provide the proof of our privacy results. We start by recalling standard diﬀerential privacy
results on composition and ampliﬁcation by subsampling in Section B.1. Section B.2 reviews recent results in
R´enyi Diﬀerential Privacy (RDP) which allow to obtain tighter privacy bounds. We then formally state and
prove Claim 4.1 in Section B.3. Finally, we provide the proof of our main result (Theorem 4.1) in Section B.4.

B.1 Reminders on Diﬀerential Privacy

In the following, we denote by D ∈ X n to a dataset of size n. Two datasets D, D(cid:48) ∈ X n are said to be neighboring
(denoted by ||D − D(cid:48)|| ≤ 1) if they diﬀer by at most one element.

Composition. Let M1(·; A1), ..., MT (·; AT ) be a sequence of T adaptive DP mechanisms where At stands for
the auxiliary input to the t-th mechanism, which may depend on the outputs of previous mechanisms (Mt(cid:48))t(cid:48)<t.
The ability to choose the sequences of mechanisms adaptively is crucial for the design of iterative machine learning
algorithms. DP allows to keep track of the privacy guarantees when such a sequence of private mechanisms is
run on the same dataset D. Simple composition (Dwork et al., 2010, Theorem III.1.) states that the privacy
parameters grow linearly with T . Dwork et al. (2010) provide a strong composition result where the (cid:15) parameter
grows sublinearly with T . This result is recalled in Lemma B.1.
Lemma B.1 (Strong adaptive composition, Dwork et al., 2010). Let M1, ..., MT be T adaptive ((cid:15), δ)-DP mech-
anisms. Then, for any δ(cid:48) > 0, the mechanism M = (M1, ..., MT ) is ((cid:15), δ)-DP where:

(cid:15) = (cid:15)(cid:112)2T log(1/δ(cid:48)) + T (cid:15)(e(cid:15) − 1) and δ = T δ + δ(cid:48).

Remark. When stating theoretical results, (cid:15) is typically approximated by O((cid:15)(cid:112)T log(1/δ(cid:48))) when (cid:15) << 1.

Privacy ampliﬁcation by subsampling. A key result in DP is that applying a private algorithm on a
random subsample of the dataset ampliﬁes privacy guarantees (Kasiviswanathan et al., 2011). In this work, we
are interested in subsampling without replacement.
Deﬁnition B.1 (Subsampling without replacement). The subsampling procedure Sampn,m : X n → X m (where
m ∈ N, with m ≤ n) takes D as input and chooses uniformly among its elements a subset D of m elements. We
may also denote Sampn,m as Sampq where q = m/n in the rest of the paper.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Lemma B.2 quantiﬁes the associated privacy ampliﬁcation eﬀect.
Lemma B.2 (Ampliﬁcation by subsampling, Kasiviswanathan et al., 2011). Let M (cid:48) : X m → Y be a ((cid:15), δ)-DP
mechanism w.r.t. a given dataset D ∈ X m. Then, mechanism M : X n → Y deﬁned as M := M (cid:48) ◦ Sampn,m is
((cid:15)(cid:48), δ(cid:48))-DP w.r.t. to any dataset D ∈ X n such that D = Sampn,m(D), where:

Remark. In theoretical results, (cid:15)(cid:48) is often approximated by O(q(cid:15)) when (cid:15) << 1.

(cid:15)(cid:48) = log(1 + q(e(cid:15) − 1)), δ(cid:48) = qδ, q = m/n.

B.2 R´enyi Diﬀerential Privacy

Abadi et al. (2016) demonstrated in practice that the privacy bounds provided by standard ((cid:15), δ)-DP theory
(see Section B.1) often overestimate the actual privacy loss. In order to better express inequalities on the tails
of the output distributions of private algorithms, we introduce the privacy loss random variable (Dwork and
Roth, 2014; Abadi et al., 2016; Wang et al., 2020). Given a random mechanism M , let M (D) and M (D(cid:48)) be the
distributions of the output when M is run on D and D(cid:48) respectively. The privacy loss LM

D,D(cid:48) is deﬁned as:

LM

D,D(cid:48)(θ) := log

(cid:18) M (D)(θ)
M (D(cid:48))(θ)

(cid:19)

where θ ∼ M (D).

(2)

The interpretation of this quantity is easy to understand: ((cid:15), δ)-DP ensures that the absolute value of the privacy
loss is bounded by (cid:15) with probability at least (1 − δ) for all pairs of neighboring datasets D and D(cid:48) (Dwork and
Roth, 2014, Lemma 3.17).

We will reason on the Cumulant Generating Function (CGF) of the privacy loss, denoted KM , rather than on
the privacy loss LM itself. This CGF is expressed as follows for any λ > 0:

KM (D, D(cid:48), λ) := Eθ∼M (D)

(cid:2)eλLM

D,D(cid:48) (θ)(cid:3) = Eθ∼M (D)

(cid:20)(cid:18) M (D)(θ)
M (D(cid:48))(θ)

(cid:19)λ(cid:21)
,

which is also equivalent to:

KM (D, D(cid:48), λ) = Eθ∼M (D(cid:48))

(cid:20)(cid:18) M (D)(θ)
M (D(cid:48))(θ)

(cid:19)λ+1(cid:21)
.

(3)

By the property of the moment generating function, KM (D, D(cid:48), ·) fully determines the distribution of the privacy
loss random variable LM
D,D(cid:48). We also deﬁne KM (λ) := sup||D−D(cid:48)||≤1 KM (D, D(cid:48), λ), which is the upper bound on
the CGF for any pair of neighboring datasets.

We can now introduce R´enyi Diﬀerential Privacy (RDP), which generalizes DP using the R´enyi divergence Dα.
Deﬁnition B.2 (R´enyi Diﬀerential Privacy, Mironov, 2017). For any α ∈ (1, ∞) and any (cid:15) > 0, a mechanism
M : X n → Y is said to be (α, (cid:15))-RDP, if for all neighboring datasets D and D(cid:48),

Dα(M (D)||M (D(cid:48))) :=

1
α − 1

log Eθ∼M (D(cid:48))

(cid:20)(cid:18) M (D)(θ)
M (D(cid:48))(θ)

(cid:19)α(cid:21)

≤ (cid:15).

(4)

Given a mechanism M and a RDP parameter α, we can thus determine from Deﬁnition B.2 the lowest value of
the (cid:15)-RDP bound, denoted (cid:15)M (α), such that M is (α, (cid:15)M (α))-RDP. Indeed, (cid:15)M (α) is such that:

(cid:15)M (α) = inf

(cid:15)∈ε(M )

(cid:15) where

ε(M ) := {(cid:15) > 0 :

sup
||D−D(cid:48)||≤1

Dα(M (D)||M (D(cid:48))) ≤ (cid:15)}.

The obvious similarity between Eq. (3) and Eq. (4) shows the link between the CGF and the notion of RDP.
Indeed, for any α ∈ (1, ∞), it is easy to see that (α − 1)(cid:15)M (α) is equal to KM (λ) where λ + 1 = α (restated in
Lemma B.3).

Lemma B.3 (Equivalence RDP-CGF). Any mechanism M is (λ + 1, KM (λ)/λ)-RDP for all λ > 0.

Diﬀerentially Private Federated Learning on Heterogeneous Data

We now recall how we can convert RDP guarantees into standard DP guarantees.

Lemma B.4 (RDP to DP conversion, Mironov, 2017). If M is ((cid:15), α)-RDP, then M is ((cid:15)+log(1/δ)/(α−1), δ)-DP
for any 0 < δ < 1.

Given Lemma B.4 and Lemma B.3, it is possible to ﬁnd the smallest (cid:15) from some ﬁxed parameter δ or the
smallest δ from some ﬁxed parameter (cid:15) so as to achieve ((cid:15), δ)-DP:

(cid:15)(δ) = min
λ>0

δ((cid:15)) = min
λ>0

log(1/δ) + KM (λ)
λ
eKM (λ)−λ(cid:15).

,

(5)

(6)

Moreover, λ → KM (λ)/λ is monotonous (Van Erven and Harremos, 2014, Theorem 3) and λ → KM (λ) is convex
(Van Erven and Harremos, 2014, Theorem 11). This last property enables to bound KM by a linear interpolation
between the values of KM evaluated at integers, as stated below:

∀λ > 0, KM (λ) ≤ (1 − λ + (cid:98)λ(cid:99))KM ((cid:98)λ(cid:99)) + (λ − (cid:98)λ(cid:99))KM ((cid:100)λ(cid:101)).

(7)

Therefore, Problem (5) is quasi-convex and Problem (6) is log-convex, and both can be solved if we know the
expression of KM (λ) for any λ > 0.

We provide below other useful results from RDP theory, which we will use in our privacy analysis.

Lemma B.5 (RDP Composition, Mironov, 2017). Let α ∈ (1, ∞). Let M1 and M2 be two mechanisms such that
M1 is (α, (cid:15)1)-RDP and M2, which takes the output of M1 as auxiliary input, is (α, (cid:15)2)-RDP. Then the composed
mechanism M2 ◦ M1 is (α, (cid:15)1 + (cid:15)2)-RDP.
Lemma B.6 (RDP Gaussian mechanism, Mironov, 2017). If f : X n → Rd has (cid:96)2-sensitivity 1, then the Gaussian
mechanism Gf (·) := f (·) + N (0, σ2
Lemma B.7 (RDP for subsampled Gaussian mechanism, Wang et al., 2020). Let α ∈ N with α ≥ 2 and
0 < q < 1 be a subsampling ratio. Suppose f : X n → Rd has (cid:96)2-sensitivity equal to 1. Let G(cid:48)
f (·) := Gf ◦ Sampq(·)
be a subsampled Gaussian mechanism. Then G(cid:48)

g)-RDP for any α > 1.

gId) is (α, α/2σ2

f is (α, (cid:15)(cid:48)(α, σ2

g))-RDP where

(cid:15)(cid:48)(α, σ2

g) ≤

1
α − 1

(cid:18)

log

1 + 2q2

(cid:19)

(cid:18)α
2

min{2(e1/σ2

g − 1), e1/σ2

g } +

α
(cid:88)

j=3

2qj

(cid:19)

(cid:18)α
j

ej(j−1)/2σ2

g

(cid:19)

.

Remark. By considering q = o(1), the dominant term in the upper bound of (cid:15)(cid:48)(α, σ2
sum of the order of q2. In particular, when σ2
simpliﬁes to 2(e1/σ2

g. This thus simpliﬁes the whole upper bound to O(αq2/σ2

g is large (i.e. high privacy regime), the term min{2(e1/σ2

g) comes from the term of the
g −1), e1/σ2
g }

g − 1) ≤ 4/σ2

g).

B.3 Proof of Claim 4.1

We restate below a more formal version of Claim 4.1 along with its proof. For any t ∈ [T ], we deﬁne subversions
of algorithms DP-SCAFFOLD (Alg. 1) and DP-FedAvg (Alg. 2), which stop at round t and reveal an output, either
to the server or to a third party:

• To the server. We assume that the sampling of users C t is known by the server. Formally, we deﬁne
i }i∈Ct (those quantities

DP-SCAFFOLD, which outputs (reveals) {yt

DP-FedAvg, which outputs {yt

i}i∈Ct, and At

i , ct

DP-SCAFFOLD, which outputs (xt, ct) and ˜At

DP-FedAvg, which outputs xt (those

At
being private w.r.t. {Di}i∈Ct).
• To a third party. We deﬁne ˜At
quantities being private w.r.t. D).

In both privacy models, DP-SCAFFOLD and DP-FedAvg can be seen as T adaptive compositions of these sub-
algorithms.

Claim B.1 (Formal version of Claim 4.1). For any t ∈ [T ], the following holds:

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

• At

DP-SCAFFOLD and At

DP-FedAvg have the same level of privacy (towards the server),

• ˜At

DP-SCAFFOLD and ˜At

DP-FedAvg have the same level of privacy (towards a third party).

Proof. We prove the claim by reasoning by induction on the number of communication rounds t. We only give
the proof for the ﬁrst statement (including the DP-SCAFFOLD-warm version). The second one can be proved in a
similar manner.

First, consider t = 1. For any i ∈ C t, control variates c0
as private as y1
of privacy of {y1

i (DP-SCAFFOLD-warm). The level of privacy for A1
i }i∈Ct, which is the same as A1

i are either all set to 0 (DP-SCAFFOLD), or c0

i are at least
DP-SCAFFOLD is thus fully determined by the level

DP-FedAvg. Therefore the claim is true for t = 1.

Then, let t ∈ [T ] and suppose that the claim is veriﬁed for all t(cid:48) < t. Let i ∈ C t and ﬁrst consider At
DP-SCAFFOLD.
The update of the i-th user model (see Eq. 1) at round t shows that an additional information leakage may come
from the correction (ct−1 − ct−1
since ct−1 is known by the server. By assumption
of induction, ct−1
i as
updated in DP-SCAFFOLD is as private w.r.t. Di as the yt
i as updated in DP-FedAvg. Besides this, the update of
the i-th control variate fully depends on the local updates of yt
i through the average of the DP-noised stochastic
gradients calculated over the local iterations. Therefore, considering all the contributions from C t, At
DP-FedAvg
and At

is also known by the server. Therefore, using the post-processing property of DP, the yt

), or more precisely from ct−1

DP-SCAFFOLD have the same level of privacy.

i

i

i

B.4 Proof of Theorem 4.1

Preliminaries. Lemma B.7 only gives an upper bound of the RDP privacy for a subsampled Gaussian mecha-
nism when α ∈ N with α ≥ 2. However we will need to optimize our privacy bound w.r.t. α ∈ R with α > 1. We
thus use Lemma B.3 and the convexity of the CGF (see Eq. 7) to generalize this upper bound to the following
result.

Let α ∈ R with α > 1. Under the same assumptions as in Lemma B.7, G(cid:48)

f is (α, (cid:15)(cid:48)(cid:48)(α, σ2

g))-RDP with

(cid:15)(cid:48)(cid:48)(α, σ2

g) ≤ (1 − α + (cid:98)α(cid:99))

(cid:98)α(cid:99) − 1
α − 1

(cid:15)(cid:48)((cid:98)α(cid:99), σ2

g) + (α − (cid:98)α(cid:99))

(cid:100)α(cid:101) − 1
α − 1

(cid:15)(cid:48)((cid:100)α(cid:101), σ2

g),

(8)

where (cid:15)(cid:48)(·, σ2

g) admits the upper bound given in Lemma B.7.

Details of the proof. Our privacy analysis assumes that the query function has sensitivity 1, since the calibra-
tion of the Gaussian noise is locally adjusted in our algorithms with the constant S = 2C/sR (see Section 3.2).
We simply denote by G the Gaussian mechanism with variance σ2
g)-RDP (Lemma B.6).
Below, we ﬁrst prove privacy guarantees towards a third party observing only the ﬁnal result, and then deduce
the guarantees towards the honest-but-curious server.

g, which is (α, α/2σ2

Step 1: data subsampling. Let t ∈ [T ] be an arbitrary round. We ﬁrst provide an upper bound (cid:15)a for the
privacy loss after the aggregation by the server of the lM individual contributions (line 20 in Alg. 1), thanks to
the local addition of noise.

Let i ∈ C t, α > 1. We denote by (cid:15)i(α) the α-RDP budget (w.r.t. Di) used to “hide” the individual contribution
of the i-th user from the server. This contribution is the result of the composition of K adaptative s-subsampled
mechanisms G:

• We ﬁrst obtain an upper RDP bound for the s-subsampled mechanism with Lemma B.7. Suppose ﬁrst
α ∈ N and α ≥ 2, which is the case covered by Lemma B.7. Under Assumption 1-(i) and Assumption 1-(iii),
the resulting mechanism is (α, O(s2α/σ2
g))-RDP. To extend this result to α > 1, we use the result provided
in (8): by factoring by s2/σ2
g), and bounding the rest of the inequality (a
convex combination between (cid:98)α(cid:99)((cid:98)α(cid:99) − 1)/(α − 1) and (cid:100)α(cid:101)((cid:100)α(cid:101) − 1)/(α − 1)) by α + 1, we also obtain that
this mechanism is (α, O(s2(α + 1)/σ2

g in the upper bound of (cid:15)(cid:48)(cid:48)(α, σ2

g))-RDP.

• We then use the result of Lemma B.5 for the RDP composition rule over the K local iterations, which gives

that (cid:15)i(α) ≤ O(Ks2(α + 1)/σ2

g).

Diﬀerentially Private Federated Learning on Heterogeneous Data

We now consider the aggregation step. Taking into account all the contributions of the users from C t, we get a
Gaussian noise of variance S2σ2
g. Note that the sensitivity of the aggregation (w.r.t. the joint
dataset D) is lM times smaller than when considering an individual contribution. Therefore, with the previous
approximation, the aggregated contributions satisfy (α, O(Ks2(α + 1)/lM σ2

a where σ2

a = 1

lM σ2

g))-RDP w.r.t. D.

After converting this result into a DP bound (Lemma B.4), we get that for any 0 < δ(cid:48) < 1, the aggregation at
line 20 in Alg. 1 is ((cid:15)a(α, δ(cid:48)), δ(cid:48))-DP w.r.t. D where (cid:15)a(α, δ(cid:48)) = O(cid:0) Ks2(α+1)

(cid:1).

+ log(1/δ(cid:48))
α−1

lM σ2
g

Without approximation: we would obtain at this step an exact upper bound (cid:15)a(α, δ(cid:48)) = K(cid:15)(cid:48)(cid:48)(α, lM σ2

g) + log(1/δ(cid:48))

α−1

.

Step 2: user subsampling.
In order to get explicit bounds, we then use classical DP tools to estimate an
upper DP bound after T rounds taking into account the ampliﬁcation by subsampling from the set of users.
Remark that these tools are however sub-optimal for practical implementations (Abadi et al., 2016, Section
5.1.).

• Using Lemma B.2, the subsampling of users enables a gain of privacy of the order of l, which gives

(cid:0)O(l(cid:15)a(α, δ(cid:48))), lδ(cid:48)(cid:1)-DP.

• Using Lemma B.1, we compose this mechanism over T iterations, which under Assumption 1-(ii) gives for

any δ(cid:48)(cid:48) > 0, (cid:0)O((cid:112)T log(1/δ(cid:48)(cid:48))l(cid:15)a(α, δ(cid:48))), T lδ(cid:48) + δ(cid:48)(cid:48)(cid:1)-DP.

Without approximation: the mechanism is (cid:0)(cid:15)∗(α, δ(cid:48))(cid:112)2T log(1/δ(cid:48)(cid:48)) + T (cid:15)∗(α, δ(cid:48))(e(cid:15)∗(α,δ(cid:48)) − 1), T lδ(cid:48) + δ(cid:48)(cid:48)(cid:1) where
(cid:15)∗(α, δ(cid:48)) = log(1 + l(e(cid:15)a(α,δ(cid:48)) − 1)).

Step 3: setting parameters. We denote (cid:15)T (α, δ(cid:48), δ(cid:48)(cid:48)) = l(cid:112)T log(1/δ(cid:48)(cid:48))(cid:0) Ks2(α+1)
stated above, the ﬁnal output of the algorithm is (O((cid:15)T ), T lδ(cid:48) + δ(cid:48)(cid:48))-DP.
Considering our ﬁnal privacy budget δ, we arbitrarily ﬁx δ(cid:48) := δ/2T l and δ(cid:48)(cid:48) := δ/2. We now aim to ﬁnd an
expression of σg such that the privacy bound is minimized. By considering the approximated bound, this gives
the following minimization problem:

(cid:1). Given what is

+ log(1/δ(cid:48))
α−1

lM σ2
g

(cid:15)T (α) := l(cid:112)T log(2/δ)

(cid:18) Ks2(α + 1)
lM σ2
g

+

log(2T l/δ)
α − 1

(cid:19)

.

min
α>1

Using DP rather than RDP enables to solve this minimization problem pretty easily since only the second factor
in (cid:15)T (α) depends on α, that is:

min
α>1

˜(cid:15)T (α) :=

Ks2(α + 1)
lM σ2
g

+

log(2T l/δ)
α − 1

.

By omitting constants, we obtain the expression for the minimum value of (cid:15)T (α):

˜(cid:15) = l(cid:112)T log(2/δ)

(cid:18) s(cid:112)K log(2T l/δ)
√

σg

lM

+

(cid:19)

.

Ks2
lM σ2
g

Under Assumption 1-(iii), we can bound the second term by the ﬁrst one, which gives:

˜(cid:15) = O

(cid:18) s(cid:112)lT K log(2/δ) log(2T l/δ)
√
σg

M

(cid:19)

.

We then invert the formula of this upper bound of ˜(cid:15) to express σg as a function of a given privacy budget (cid:15):

σg = Ω(cid:0)s(cid:112)lT K log(2T l/δ) log(2/δ)/(cid:15)

√

M (cid:1),

which proves that the algorithm is (O((cid:15)), δ)-DP towards a third party observing its ﬁnal output.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Without approximation: the minimization problem is much more complex and has to be solved numerically
(cid:15)∗(α, δ(cid:48))(cid:112)2T log(1/δ(cid:48)(cid:48)) + T (cid:15)∗(α, δ(cid:48))(e(cid:15)∗(α,δ(cid:48)) − 1)

s.t. δ = T lδ(cid:48) + δ(cid:48)(cid:48),

min
α>1,δ(cid:48)>0,δ(cid:48)(cid:48)>0

or:

min
α>1,x∈(0,1)

(cid:15)∗(α, xδ/T l)(cid:112)2T log(1/(1 − x)δ) + T (cid:15)∗(α, xδ/T l)(e(cid:15)∗(α,xδ/T l) − 1).

Extension to privacy towards the server. The crucial diﬀerence with the third party case is that the server
observes individual contributions and knows which users are subsampled at each step. Removing the privacy
ampliﬁcation eﬀect of the l-subsampling of users and the aggregation step, the minimization problem becomes

(cid:15)T (α) := (cid:112)T log(2/δ)

min
α>1

(cid:18) Ks2α
σ2
g

+

log(2T /δ)
α − 1

(cid:19)

,

where the minimizing value can be approximated by:

˜(cid:15) = (cid:112)T log(2/δ)

(cid:18) s(cid:112)K log(2T /δ)
σg

+

(cid:19)
.

Ks2
σ2
g

Under Assumption 1-(iii), we can bound the second term by the ﬁrst one:

˜(cid:15) = O

(cid:18) s(cid:112)T K log(2/δ) log(2T /δ)
σg

(cid:19)

,

which proves that we obtain (O((cid:15)s), δs)-DP towards the server where (cid:15)s = (cid:15)

(cid:113) M

l and δs = δ

2 ( 1

l + 1).

Finer results for ampliﬁcation by subsampling. To establish privacy towards a third party, it is actually
possible to combine the subsampling ratios (user and data) to determine a bound upon the subsampling of data
directly from D and thus to quantify a more precise gain in privacy (Girgis et al., 2021b). The diﬃculty in this
setup is that this combined subsampling is not uniform overall, which requires extending the proof of Lemma B.2
as done by Girgis et al. (2021b) in the case of classical diﬀerential privacy.

Implementation.
In practice, we determine a RDP upper bound at Step 2, by using the theorem proved by
Wang et al. (2020) (which is not restricted to Gaussian mechanisms) with the exact RDP bound obtained at Step
1 and sampling parameter l. This result being accurate only for α ∈ N \ {0, 1}, we obtain an natural extension
of the bound for any α > 1 with Eq (7). Then, we invoke Lemma B.5 to obtain the ﬁnal RDP bound after T
communication rounds, for any α > 1. Under ﬁxed privacy parameter δ (chosen as 1/M R in our experiments),
we ﬁnally obtain the minimal value for (cid:15) w.r.t. α ∈ (1, ∞), which is determined by Eq (5). The last step is done
by using a ﬁne grid search over parameter α.

C PROOF OF UTILITY

In this section, we provide the proof of our utility results. We ﬁrst establish in Section C.1 some preliminary
results about the impact of DP noise over stochastic gradients. In Section C.2, we provide the complete version
of our utility result for DP-SCAFFOLD-warm (Theorem C.1), from which Theorem 4.2 is an immediate corollary.
We prove this theorem for convex local loss functions in Section C.3 and non-convex loss functions in Section C.4.
We ﬁnally state in Section C.5 our complete result for DP-FedAvg (Theorem C.2).

2dσg/sR. We recall that we assume that F is bounded from below

For any C, σg > 0, we deﬁne Σg(C) := 2C
by F ∗ = F (x∗), for an x∗ ∈ Rd.

C.1 Preliminaries

√

Properties of DP-noised stochastic gradients. Let i ∈ [M ], x ∈ Rd, Si ⊂ Di and C, σg > 0. Suppose
Assumptions 2 and 3.3 are veriﬁed (the last assumption ensures that the clipping on per-example local gradients
with threshold C is not eﬀective).

Diﬀerentially Private Federated Learning on Heterogeneous Data

We recall below the expression of ˜Hi(x) from Section 3.3, which is the noised version of the local gradient Hi(x)
of the i-th user over Si evaluated at x (omitting index k):

˜Hi(x) := Hi(x) +

2C
sR

N (0, σ2

g), where Hi(x) :=

1
sR

(cid:88)

di
j ∈Si

∇fi(x, di

j).

We recall that the (cid:96)2-sensitivity of Hi(x) w.r.t. Si is upper bounded by 2C/sR, which explains the scaling of
the Gaussian noise in the expression of ˜Hi(x). Since the variance of N (0, Id) is 2d, the following statement holds
directly:

E(cid:2) ˜Hi(x)(cid:3) = Hi(x) and E(cid:2)|| ˜Hi(x) − Hi(x)||2(cid:3) ≤

8C2dσ2
g
s2R2 = Σg(C)2.

By combining our utility assumptions with the result stated above, we can deduce the following lemma.

Lemma C.1 (Regularity of DP-noised stochastic gradients). Under Assumptions 2 and 3, for any iteration
t ∈ [T ], k ∈ [K],

1. E(cid:2) ˜H k

i (yk−1
i

)|yk−1
i

(cid:3) = ∇Fi(yk−1

i

),

2. E(cid:2)|| ˜H k

i (yk−1
i

) − ∇Fi(yk−1

i

)||2|yk−1
i

(cid:3) ≤

ς 2
sR

+ Σ2

g(C).

The proof of Lemma C.1 is easily obtained by conditioning on the two sources of randomness (i.e., mini-batch
sampling and Gaussian noise) which are independent, thus the variance is additive. This result can be seen as
a degraded version of Assumption 3 due to the local injection DP noise, a fact that we will strongly leverage to
derive convergence rates.

We now enumerate several statements that will be used in the utility proof. First, Lemma C.2 enables to
control ||∇F ||2 using the assumption of smoothness over the local loss functions. Second, Lemma C.3 provides
separation inequalities of mean and variance (Karimireddy et al., 2020b, Lemma 4), which enables to state a
result on quantities of interest in Corollary C.1.

Lemma C.2 (Nesterov inequality). Suppose Assumption 2 is veriﬁed and assume that for all i ∈ [M ], Fi is
convex. Then,

∀x ∈ Rd, ||∇F (x)||2 ≤ 2ν(F (x) − F ∗).

Proof. Let x ∈ Rd.

||∇F (x)||2 = ||∇F (x) − ∇F (x∗)||2 = ||

1
M

M
(cid:88)

i=1

∇Fi(x) − ∇Fi(x∗)||2

M
(cid:88)

||∇Fi(x) − ∇Fi(x∗)||2

(Jensen inequality)

≤

1
M

i=1
≤ 2ν(F (x) − F ∗)

(Nesterov et al., 2004, Theorem 2.1.5)

Lemma C.3 (Separating mean and variance). Let (A1, ..., An) be n random variables in Rd not necessarily
independent.

1. Suppose that their mean is E[Ai] = ai and their variance is uniformly bounded, i.e.

E[||Ai − ai||2] ≤ σ2

A. Then,

for all i ∈ [n],

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ai

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ai

+ n2σ2
A.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

2. Suppose that their conditional mean is E[Ai|Ai−1, ...A1] = ai and their variance is uniformly bounded, i.e. for
all i ∈ [n], E[||Ai − ai||2] ≤ σ2

A. Then,

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ai

≤ 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ai

+ 2nσ2
A.

Corollary C.1. Let t ∈ [T ]. In the following statements, the expectation is taken w.r.t. the randomness from
their local data sampling and from the Gaussian DP noise, conditionally to the users’ sampling C t and initial
value of variables yi, that is y0 = xt−1 (same for all users). We have:

• E





(cid:13)
(cid:13)
(cid:13)

1
KlM

(cid:88)

(cid:88)

( ˜H k

i (yk−1
i

) − ∇Fi(yk−1

i

i∈Ct

k∈[K]

(cid:13)
(cid:13)
(cid:13)

2(cid:12)
(cid:12)
C t, y0
(cid:12)
(cid:12)

))



 ≤

Σ2

g(C) + ς 2/sR
KlM

,

• E

(cid:20)(cid:13)
(cid:13)
(cid:13)

1
lM

(cid:88)

i∈Ct

(cid:13)
2
i − E[ct
ct
(cid:13)
i]
(cid:13)

|C t, y0

(cid:21)

≤

Σ2

g(C) + ς 2/sR
KlM

,

• E

(cid:20)(cid:13)
(cid:13)
2
(cid:13)ct − E[ct]
(cid:13)
(cid:13)
(cid:13)

(cid:21)

|y0

≤

Σ2

g(C) + ς 2/sR
KlM

.

).

i (yk−1
i

Proof. First inequality. We deﬁne a random variable A such as A := 1
˜H k
From Lemma C.1, we have that for all i ∈ C t, k ∈ [K]: E[Ai,k|yk−1
(cid:104)
|| ˜H k
Furthermore, by Lemma C.1, E

i
≤ Σ2

) − ∇Fi(yk−1

)||2|yk−1
i

i (yk−1
i

(cid:105)

i

KlM

] = E[ ˜H k
g(C) + ς 2/sR.

(cid:80)

i∈Ct

(cid:80)

k∈[K] Ai,k, with Ai,k :=

i (yk−1
i

)|yk−1
i

] = ∇Fi(yk−1

i

).

Furthermore, (a) for i, j ∈ C t, (cid:80)
ditionally to y0; (b) for any i ∈ C t, (Ai,k − ∇Fi(yk−1
∇Fi(yk−1

k∈[K] Ai,k − ∇Fi(yk−1

i }k(cid:48)∈[k−1])] = 0. Consequently:

)|σ({yk(cid:48)

i

i

) and (cid:80)
))k∈[K]

i

k∈[K] Aj,k − ∇Fj(yk−1

j

is a martingale increment,

) are independent con-
i.e., E[Ai,k −





(cid:13)
(cid:13)
(cid:13)

E

1
KlM

(cid:88)

(cid:88)

i∈Ct

k∈[K]

Ai,k − ∇Fi(yk−1

i

)

(cid:13)
(cid:13)
(cid:13)

2(cid:12)
(cid:12)
(cid:12)
(cid:12)

C t, y0





(a)
=

1
(KlM )2

(cid:88)

E

i∈Ct





(cid:13)
(cid:13)
(cid:13)

(cid:88)

k∈[K]


Ai,k − ∇Fi(yk−1

i

)



y0



(cid:13)
(cid:13)
(cid:13)

2(cid:12)
(cid:12)
(cid:12)
(cid:12)

(b)
=

1
(KlM )2

(cid:88)

(cid:88)

E

i∈Ct

k∈[K]







E

(cid:124)

≤

Σ2

g(C) + ς 2/sR
KlM

.

(cid:20)(cid:13)
(cid:13)Ai,k − ∇Fi(yk−1
(cid:13)
(cid:123)(cid:122)

i

(cid:13)
(cid:13)
)
(cid:13)

2(cid:12)
(cid:12)
(cid:12)
(cid:12)

σ(cid:0){yk(cid:48)

i }k(cid:48)∈[k]

≤Σ2

g (C)+ς2/sR









(cid:21)
(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

y0

(cid:125)

To prove the second equality, we need to “iteratively” expand the squared norm and take the conditional
expectation w.r.t. σ(cid:0){yk(cid:48)
(cid:1) for k = K, K − 1, . . . , 1 and use the martingale property to obtain that the
scalar products are equal to 0.

i }k(cid:48)∈[k]

Second inequality. We recall that for any i ∈ C t, ct
directly use the results from the ﬁrst inequality.
(cid:80)M

Third inequality. We recall that ct = 1
M
we can use the previous results and take the expectation over C t, which gives:

i=1 ct

i (even if local control variates are not updated). Therefore,

i = 1
K

(cid:80)K

k=1

˜H k

i (yk−1
i

). Thus 1
lM

(cid:80)

i∈Ct ct

i = A and we can

E

(cid:20)(cid:13)
(cid:13)
2
(cid:13)ct − E[ct]
(cid:13)
(cid:13)
(cid:13)

(cid:21)

|y0

≤

ς 2/sR + Σ2

g(C)

KM

≤

ς 2/sR + Σ2

g(C)

KlM

.

Diﬀerentially Private Federated Learning on Heterogeneous Data

C.2 Theorem of Convergence for DP-SCAFFOLD-warm

Theorem C.1 (Utility rates for DP-SCAFFOLD-warm, σg chosen arbitrarily). Let σg, C > 0, x0 ∈ Rd. Suppose
we run DP-SCAFFOLD-warm(T, K, l, s, σg, C) with initial local controls such that c0
i (x0) for any
i ∈ [M ]. Under Assumptions 2 and 3, we consider the sequence of iterates (xt)t≥0 of the algorithm, starting
from x0.

i = 1
K

(cid:80)K

˜H k

k=1

1. If Fi are µ-strongly convex (µ > 0), ηg =

lM , ηl = min (cid:0)
there exist weights {wt}t∈[T ] such that the averaged output of DP-SCAFFOLD-warm(T, K, l, s, σg, C), deﬁned by
xT = (cid:80)

t∈[T ] wtxt, has expected excess of loss such that:

(cid:1), and T ≥ max( 108

l
54µKηg

, 48ν
µl

), then,

2
3

,

l

2
l
3
24νKηg

√

E[F (xT )] − F (x∗) ≤ O

(cid:18) ς 2/sR + Σ2
µT KlM

g(C)

+ µD2

0 exp(− min (cid:0) l
108

(cid:19)

(cid:1)T )

,

3

µl 2
48ν

,

2. If Fi are convex, ηg =

√

lM , ηl = l

2
3
24νKηg

averaged output of DP-SCAFFOLD-warm(T, K, l, s, σg, C), deﬁned by xT = (cid:80)
loss such that:

and T ≥ 1,then, there exist weights {wt}t∈[T ] such that the
t∈[T ] wtxt, has expected excess of

E[F (xT )] − F (x∗) ≤ O

√

(cid:18) ς/

sR + Σg(C)
√
T KlM

D0 +

(cid:19)

,

D2
0

ν
l 2
3 T

3. If Fi are non-convex, ηg =

and T ≥ 1, then there exist weights {wt}t∈[T ] such that the
randomized output of DP-SCAFFOLD-warm(T, K, l, s, σg, C), deﬁned by {xT = xt with probability wt for all t},
has expected squared gradient of the loss such that:

lM , ηl = l

√

2
3
24νKηg

E||∇F (xT )||2 ≤ O

where D0 = ||x0 − x∗|| and F0 := F (x0) − F ∗.

√

(cid:18) ς/

sR + Σg(C)
√
T KlM

(cid:112)

F0 +

(cid:19)

F0

,

ν
l 2
3 T

We recover the result of Theorem 4.2 for DP-SCAFFOLD-warm where Fi are convex by setting σg = σ∗
g := s(cid:112)lT K log(2T l/δ) log(2/δ)/(cid:15)
σ∗
numerical constants omitted for the asymptotic bound).

M , which gives Σg(C) = 2Cd(cid:112)2lT K log(2T l/δ) log(2/δ)/(cid:15)R

√

√

g where
M (with

C.3 Proof of Theorem C.1 (Convex case)

In this section, we give a detailed proof of convergence of DP-SCAFFOLD-warm with convex local loss functions.
Our analysis is adapted from the proof given by Karimireddy et al. (2020b) without DP noise, but requires
original modiﬁcations (see below). Throughout this part, we re-use the notations from Section 3.3.

Summary of the main steps. Let t ∈ [T ] be an arbitrary communication round of the algorithm. We detail
below the updates that occur at this round.

• Let i ∈ C t. Starting from y0

i := yk−1
yk

i − ηlvt

i,k where vt

i = xt−1, the random variable yi is updated at local step k ∈ [K] such that
i,k = ˜H k

i + ct−1.

i (yk−1
i

) − ct−1

• Then we deﬁne the local control variate ˜ct

i for this user by:

i := ct−1 − ct−1
˜ct

i +

1
Kηl

(xt−1 − yK

i ) =

1
K

K
(cid:88)

k=1

˜H k

i (yk−1
i

).

• For any i ∈ [M ], we update the control variate ct

i such that:

– ct

i := ˜ct

i if i ∈ C t,

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

– ct

i := ct−1
i

otherwise.

• Finally, the global update is computed as:

xt = xt−1 +

ηg
lM

(cid:88)

i∈Ct

(yK

i − xt−1) and ct =

1
M

(cid:0) (cid:88)

ct
i +

(cid:88)

ct−1
i

(cid:1).

i∈Ct

i /∈Ct

To keep track of the lag in the update of ct
k ∈ [K] by:

i, we introduce αt

i,k−1 deﬁned for any i ∈ [M ], any t ∈ [T ] and any

αt

i,k−1 =

(cid:26) yk−1
i
αt−1

i,k−1

if i ∈ C t
otherwise

with α0

i,k−1 = x0.

We hence have the following property for any i ∈ [M ] and any t ∈ [T ]: ct

i = 1
K

(cid:80)K

k=1

˜H k

i (αt

i,k−1).

Additional deﬁnitions.

• Model gap: ∆xt := xt − xt−1,
• Global step-size: ˜η := Kηlηg which gives ∆xt = −

˜η
KlM

(cid:88)

k∈[K],i∈Ct

˜H k

i (yk−1
i

) + ct−1 − ct−1

i

,

• User-drift: Et :=

1
KM

K
(cid:88)

M
(cid:88)

k=1

i=1

E||yk

i − xt−1||2,

• Control lag: Ft :=

1
KM

K
(cid:88)

M
(cid:88)

k=1

i=1

E||αt

i,k−1 − xt||2 with F0 = 0.

Originality of the proof. The proof substantially diﬀers form the proof by Karimireddy et al. (2020b) in the
convex case. Indeed, Karimireddy et al. (2020b) control a combination of the quadratic distance to the optimum
i − ∇Fi(x∗)(cid:107).
and a control of the deviation between the controls and the gradients at the optimal point (cid:107)ct
Leveraging such a quantity in our proof would result in a worse upper bound on the utility than the one we get,
as either the noise added to ensure DP (if c0
i = 0)
would also appear in the initial condition (cid:107)ct
i − ∇Fi(x∗)(cid:107). On the other hand, in our approach, we combine
the quadratic distance to the optimum to a control of the lag and user-drift. In some sense this resembles some
aspects of the proof in the non-convex regime in (Karimireddy et al., 2020b), in which the excess risk (F (xt)−F ∗)
is combined with the lag. Nevertheless, our result (in the convex case), strongly leverages the convexity of the
function in the proof.

i is deﬁned w.r.t. a noised gradient) or the heterogeneity (if c0

√

Details of the proof. The idea of the proof is to ﬁnd a contraction inequality involving ||xt−x∗||2, E[F (xt−1)]−
F (x∗), Ft and ς/
sR + Σg(C). To do so, we will ﬁrst bound the variance of the server’s update. Then we will
see how the control lag evolves through the communication rounds. We will also bound the user drift. To make
the proof more readable, the index t may be omitted on random variables when the only communication round
that is considered is the t-th one.

Lemma C.4 (Variance of the server’s update). ∀˜η ∈ [0, 1/ν]

E||xt − xt−1||2 ≤ 4˜η2ν2Et + 8ν2 ˜η2Ft−1 + 8ν ˜η2E(cid:0)F (xt−1) − F (x∗)(cid:1) +

9˜η2
KlM

(ς 2/sR + Σ2

g(C)).

Proof. We consider the model gap ∆xt = xt − xt−1.

(cid:13)
E||∆xt||2 = ˜η2E
(cid:13)
(cid:13)

(cid:18) 1

KlM

(cid:124)

(cid:88)

˜Hi(yk−1
i

)

k∈[K],i∈Ct

(cid:123)(cid:122)
A1

(cid:19)

(cid:125)

−

+ ct−1
(cid:124)(cid:123)(cid:122)(cid:125)
A2

1
lM

(cid:124)

(cid:88)

i∈Ct
(cid:123)(cid:122)
A3

ct−1
i

(cid:13)
2
(cid:13)
(cid:13)

(cid:125)

Diﬀerentially Private Federated Learning on Heterogeneous Data

We combine Lemma C.3-1 on A1, A2, A3 with Corollary C.1 which controls their individual variance (conditionally
to the users’ sampling and the local parameters) by
. We ﬁrst get rid of the terms related to the
variance of the data sampling and the DP noise, before bounding the quantities of interest. It leads to:

ς 2/sR+Σ2
KlM

g(C)

E||∆xt||2 = ˜η2E


E





(cid:13)
(cid:18) 1
(cid:13)
(cid:13)
(cid:13)

KlM

(cid:88)

˜Hi(yk−1
i

(cid:19)
)

+ ct−1 −

k∈[K],i∈Ct

1
lM

(cid:88)

i∈Ct

ct−1
i

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:12)
(cid:12)
(cid:12)
(cid:12)





C t, y0





≤ ˜η2E





(cid:13)
(cid:13)
(cid:13)

1
KlM

(cid:88)

k∈[K],i∈Ct

E[ ˜Hi(yk−1

i

)|y0] + E[ct−1|y0] − E[ct−1

i



 +

|y0]

(cid:13)
2
(cid:13)
(cid:13)

9˜η2
KlM

(ς 2/sR + Σ2

g(C)),

where the inequality is given by Lemma C.3-1.

For any i ∈ C t, k ∈ [K], we have E[ ˜Hi(yk−1
Then,

i

)|y0] = E

(cid:104)
E[ ˜Hi(yk−1

i

)|yk−1
i

(cid:12)y0(cid:105)
](cid:12)

= E (cid:2)∇Fi(yk−1

i

)|y0(cid:3) = ∇Fi(yk−1

i

).

E||∆xt||2 ≤ ˜η2E





1
KlM

(cid:88)

(cid:13)
(cid:13)∇Fi(yk−1
(cid:13)

i

(cid:88)

k∈[K],i∈Ct
(cid:104)(cid:13)
(cid:13) ∇Fi(yk−1
(cid:13)
(cid:124)

E

i

k∈[K],i∈[M ]

= ˜η2 1
KM

) + E[ct−1|y0] − E[ct−1

i



 +

|y0]

(cid:13)
2
(cid:13)
(cid:13)

9˜η2
KlM

(ς 2/sR + Σ2

g(C))

(convexity of ||.||2)

2(cid:105)

(cid:13)
(cid:13)
(cid:13)

+

9˜η2
KlM

|y0]
(cid:125)

(ς 2/sR + Σ2

g(C))

(deﬁnition of C t)

) + E[ct−1|y0] − E[ct−1

i

(cid:123)(cid:122)
∇Fi(yk−1
)−∇Fi(xt−1)
+E[ct−1|y0]−∇F (xt−1)
−E[ct−1
|y0]+∇Fi(xt−1)
i

i

+∇F (xt−1)

= ˜η2 1
KM

(cid:88)

E

(cid:20)(cid:13)
(cid:13)
(cid:13)

E

(cid:104)
∇Fi(yk−1

i

k∈[K],i∈[M ]

) − ∇Fi(xt−1) + ct−1 − ∇F (xt−1) − ct−1

(cid:12)
i + ∇Fi(xt−1) + ∇F (xt−1)
(cid:12)

2(cid:21)

(cid:12)y0(cid:105) (cid:13)

(cid:13)
(cid:13)

(ς 2/sR + Σ2

g(C))
(cid:104)

E

E

(all variables are measurable wrt y0)

(cid:104)(cid:13)
(cid:13)∇Fi(yk−1
(cid:13)

i

) − ∇Fi(xt−1) + ct−1 − ∇F (xt−1) − ct−1

(cid:13)
i + ∇Fi(xt−1) + ∇F (xt−1)
(cid:13)
(cid:13)

2(cid:12)
(cid:12)

(cid:12)y0(cid:105)(cid:105)

(cid:88)

g(C))
(cid:20)(cid:13)
(cid:13)∇Fi(yk−1
(cid:13)

i

E

(Jensen inequality)

(cid:13)
) − ∇Fi(xt−1)
(cid:13)
(cid:13)

2(cid:21)

+

8˜η2
KM

(cid:88)

E

k∈[K],i∈[M ]

(cid:20)(cid:13)
(cid:13)∇Fi(αt−1
(cid:13)

(cid:13)
i,k−1) − ∇Fi(xt−1)
(cid:13)
(cid:13)

2(cid:21)

k∈[K],i∈[M ]
(cid:13)
(cid:13)
2
+ 4˜η2E
(cid:13)∇F (xt−1)
(cid:13)
(cid:13)
(cid:13)

+

9˜η2
KlM

(ς 2/sR + Σ2

g(C)).

The last inequality is obtained by deﬁnition of c and ci and by applying Jensen inequality. With Lemma C.2,
this leads to the result.

Lemma C.5 (Lag in the control variate). ∀α ∈ [1/2, 1], ∀˜η ≤ 1

24ν lα,

(cid:18)

Ft ≤

1 −

(cid:19)

17
36

l

Ft−1 +

1
24ν

l2α−1E(cid:0)F (xt−1) − F (x∗)(cid:1) +

97
48

l2α−1Et +

l
ν2

ς 2/sR + Σ2
32KlM

g(C)

.

Proof. We adapt the original proof made in the non-convex case (Karimireddy et al., 2020b, Lemma 16) and use
Lemma C.1 and Lemma C.2.

Lemma C.6 (Bounding the user drift). ∀ηg ≥ 1, ∀ηl ≤ 1/24νKηg,

9
2

ν2 ˜ηEt ≤

9
2

ν3 ˜η2Ft−1 +

9
40

˜ην
η2
g

E(cid:0)F (xt−1) − F (x∗)(cid:1) +

27
40

˜η2ν
Kη2
g

(cid:0)ς 2/sR + Σ2

g(C)(cid:1).

Proof. We once again adapt the original proof made in the non-convex case (Karimireddy et al., 2020b, Lemma
17), use Lemma C.2 and multiply on each side of the inequality by 9

2 ν2 ˜η.

9˜η2
+
KlM
≤ ˜η2 1
KM

+

≤

9˜η2
KlM
4˜η2
KM

k∈[K],i∈[M ]

(ς 2/sR + Σ2

(cid:88)

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Lemma C.7 (Progress made at each round). ∀ηg ≥ 1, ∀ηl ≤ min (cid:0)

1

24Kηgν l2/3,

l
54µKηg

(cid:1),

E||xt − x∗||2 + 27ν2 ˜η2 1
l

(cid:18)

Ft ≤

1 −

µ˜η
2

(cid:19) (cid:20)

(cid:21)

Ft−1

E||xt−1 − x∗||2 + 27ν2 ˜η2 1
l
lM
η2
g

10˜η2
KlM

1 +

(cid:18)

−

˜η
2

E(cid:0)F (xt−1) − F (x∗)(cid:1) +

(cid:19)

(ς 2/sR + Σ2

g(C)).

Proof. We recall that ∆xt = −

˜η
KlM

(cid:88)

k∈[K],i∈Ct

˜H k

i (yk−1
i

) + ct−1 − ct−1

i

. Then,

E[∆xt|y0] = E[∆xt|xt−1] = −˜ηE[ct−1|y0] = −

˜η
KM

(cid:88)

E[∇Fi(yk−1

i

)|y0].

k∈[K],i∈[M ]

(9)

We denote Et−1[.] as the expectation conditioned on randomness generated (strictly) prior to round t, i.e.
conditionally to σ(xτ , τ ≤ t − 1). We ﬁrst bound the quantity Et−1||xt − x∗||2 = Et−1||xt−1 + ∆xt − x∗||2,

Et−1||xt − x∗||2 = Et−1||xt−1 − x∗||2 + Et−1||∆xt||2 + 2

(cid:20)(cid:28)

Et−1[∆xt|y0], xt−1 − x∗

(cid:29)(cid:21)

= ||xt−1 − x∗||2 + Et−1||∆xt||2 + 2

(cid:20)(cid:28)

−

˜η
KM

(cid:124)

(cid:88)

E[∇Fi(yk−1

i

)|y0]

, xt−1 − x∗

(cid:29)(cid:21)

k∈[K],i∈[M ]
(cid:123)(cid:122)
by (9)

(cid:125)

≤ Et−1||xt−1 − x∗||2 + 4˜η2ν2Et + 8ν2 ˜η2Ft−1 + 8ν ˜η2 (cid:0)F (xt−1) − F (x∗)(cid:1)

+

9˜η2
KlM

(ς 2/sR + Σ2

g(C)) +

2˜η
KM

Et−1





(cid:124)

(cid:88)

(cid:104)∇Fi(yk−1

i

), x∗ − xt−1(cid:105)

k∈[K],i∈[M ]

(cid:123)(cid:122)
A



,



(cid:125)

(Lemma C.4)

where

E[A] ≤

2˜η
KM

E





(cid:88)

k∈[K],i∈[M ]

Fi(x∗) − Fi(xt−1) + ν||yk−1

i − xt−1||2 −

µ
4

||xt−1 − x∗||2





(cid:16)

= −2˜η

E(F (xt−1)) − F (x∗) +

E||xt−1 − x∗||

(cid:17)

+ 2ν ˜ηEt,

µ
4

where the inequality comes from the convexity and ν-smoothness property.

Hence, by taking the expectation:

E||xt − x∗||2 ≤ E||xt−1 − x∗||2 − 2˜η

(cid:16)

E(F (xt−1)) − F (x∗) +

µ
4

+ 4˜η2ν2Et + 8ν2 ˜η2Ft−1 + 8ν ˜η2E (cid:0)F (xt−1) − F (x∗)(cid:1) +

(ς 2/sR + Σ2

g(C)).

+ 2ν ˜ηEt

E||xt−1 − x∗||2(cid:17)
9˜η2
KlM

By combining all terms and multiplying by ν on each side of the inequality, it comes:

νE||xt − x∗||2 ≤

(cid:18)

1 −

(cid:19)

µ˜η
2

νE||xt−1 − x∗||2 + (8ν2 ˜η2 − 2˜ην) (cid:0)E(F (xt−1)) − F (x∗)(cid:1)

(10)

+

9˜η2ν
KlM

(ς 2/sR + Σ2

g(C)) + (2ν2 ˜η + 4ν3 ˜η2)Et + 8ν3 ˜η2Ft−1.

We now consider α ∈ [1/2, 1], ηl ≤
multiplied by 27ν3 ˜η2 1

l to obtain:

1
24Kνηg

lα and ηg ≥ 1. We use the result of Lemma C.5 where each side is

Diﬀerentially Private Federated Learning on Heterogeneous Data

27ν3 ˜η2 1
l

µ˜η
2

Ft ≤ (cid:0)1 −
9
8
27
32

(cid:1)27ν3 ˜η2 1
l

Ft−1 + 27(cid:0) µ˜η
2l

17
36
l2α−2ν2 ˜η2(cid:0)E(F (xt−1)) − F (x∗)(cid:1) +
ν ˜η2 ς 2/sR + Σ2
KlM

g(C)

−

+

+

.

(cid:1)ν3 ˜η2Ft−1
873
16

l2α−2ν3 ˜η2Et

Since we have ηl ≤

1
24Kνηg

, we recall the result from Lemma C.6:

9
2

ν2 ˜ηEt ≤

9
2

ν3 ˜η2Ft−1 +

9
40

˜ην
η2
g

E(cid:0)F (xt−1) − F (x∗)(cid:1) +

27
40

˜η2ν
Kη2
g

(cid:0)ς 2/sR + Σ2

g(C)(cid:1).

By summing inequalities (10), (11), (12), we obtain:

νE||xt − x∗||2 + 27ν3 ˜η2 1
l

l2α−2ν2 ˜η2 +

+ 8ν2 ˜η2 − 2˜ην(cid:1)(cid:0)E(F (xt−1)) − F (x∗)(cid:1)

µ˜η
2

Ft ≤ (cid:0)1 −
+ (cid:0) 9
8
+ (cid:0) 315
32
5
+ (cid:0) −
2
(cid:18)
27(cid:0) µ˜η
2l

+

+

(cid:1)

Ft−1

(cid:1)(cid:0)νE||x − x∗||2 + 27ν3 ˜η2 1
l
9
˜ην
η2
40
g
(cid:1) ˜η2ν
KlM

(cid:0)ς 2/sR + Σ2

27
40

lM
η2
g

g(C)(cid:1)

ν ˜η + 4ν2 ˜η2 +

873
16
(cid:1)ν2 ˜η2 +

l2α−2ν2 ˜η2(cid:1)νEt
(cid:19)

ν2 ˜η2

νFt−1.

25
2

−

17
36

(11)

(12)

(13)

(14)

(15)

(16)

We now consider ηl ≤ l/54µKηg. Then ˜η ≤ l/54µ and we recall that ν ˜η ≤ 1/24. We ﬁx α = 2/3 (then 2−2α = α).

In this part, we aim at simplifying the terms on the right side of the last inequality.

Simplifying (13):

9
8

l2α−2ν2 ˜η2 +

9
40

˜ην
η2
g

+ 8ν2 ˜η2 − 2˜ην ≤ (cid:0)

+

9
40

+

− 2(cid:1)ν ˜η

8
24
ν ˜η
2

.

9
8 × 24
1339
960
(cid:124) (cid:123)(cid:122) (cid:125)
∼1.39

= −

ν ˜η ≤ −

Simplifying (14):

315
32

+

27
40

lM
η2
g

≤ 10(cid:0)1 +

(cid:1).

lM
η2
g

Simplifying (15):

Since l2α−2ν ˜η = ν ˜η(cid:0) 1

l

(cid:1)2/3

≤ 1/24,

−

5
2

ν ˜η + 4ν2 ˜η2 +

873
16

l2α−2ν2 ˜η2 ≤ (cid:0) −

+

4
24

+

873
16

1
24

(cid:1)ν ˜η

ν ˜η ≤ 0.

5
2
23
384

= −

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Simplifying (16):

Since µ˜η

2l ≤ 1/108,

27(cid:0) µ˜η
2l

−

17
36

(cid:1)ν2 ˜η2 +

25
2

ν2 ˜η2 ≤ (cid:0)27(

1
108

−

17
36

) +

25
2

(cid:1)ν2 ˜η2 = 0.

We then obtain the ﬁnal result by dividing by ν on each side of the inequality.

Lemma C.8 (Convergence of DP-SCAFFOLD-warm with convex loss functions). There exist weights {wt} such
that xT = (cid:80)

t∈[T ] wtxt and:

If fi are µ-strongly convex (µ > 0), ηg ≥ 1, ηl ≤ min (cid:0)

2
l
3
24νKηg

,

l
54µKηg

(cid:1), and T ≥ max( 108

l

, 48ν
µl

2
3

),

E[F (xT )] − F (x∗) ≤ O

(cid:18) ς 2/sR + Σ2
µT KlM

g(C)

(cid:18)

1 +

(cid:19)

lM
η2
g

+ µD2

0 exp

(cid:18)

− min

(cid:18) l
108

,

3

µl 2
48ν

(cid:19)

(cid:19)(cid:19)

T

,

If fi are convex, ηg ≥ 1, ηl ≤ l

2
3
24νKηg

and T ≥ 1,

E[F (xT )] − F (x∗) ≤ O

(cid:18)

D0

where D0 := ||x0 − x∗||.

√

ς/

sR + Σg(C)
√
T KlM

(cid:115)

1 +

+

D2
0

(cid:18) 1
l

ν
T

(cid:19) 2

3 (cid:19)

,

lM
η2
g

Proof. The result of Lemma C.8 is obtained by combining the contraction inequality from Lemma C.7 and the
results from technical contraction results (Karimireddy et al., 2020b, Lemmas 1 and 2).

We then obtain the result of Theorem C.1 by setting ηg :=

√

lM ≥ 1 and ηl as low as possible.

C.4 Proof of Theorem C.1 (Non-Convex case)

To state this result, we adapt the original proof in the case with a larger variance for DP-noised stochastic
gradients (see Lemma C.1), which gives the following result.
Lemma C.9 (Convergence of DP-SCAFFOLD-warm with non-convex loss functions). There exist weights {wt}
such that xT = (cid:80)

t∈[T ] wtxt and:

If fi are non-convex, ηg ≥ 1, ηl ≤ l

2
3
24νKηg

and T ≥ 1,

E||∇F (xT )||2 ≤ O

(cid:18)(cid:112)

F0

ς/

√

sR + Σg(C)
√
T KlM

(cid:115)

1 +

(cid:18) 1
l

F0

ν
T

(cid:19) 2

3 (cid:19)
,

+

lM
η2
g

where F0 := F (x0) − F (x∗).

We obtain the result of Theorem C.1 by setting ηg :=

√

lM ≥ 1 and ηl as low as possible.

C.5 Theorem of Convergence for DP-FedAvg

Theorem C.2 (Utility rates of DP-FedAvg(T, K, l, s, σg, C), σg chosen arbitrarily). Let σg, C > 0, x0 ∈ Rd.
Suppose we run DP-FedAvg(T, K, l, s, σg, C) (see Algorithm 2). Under Assumptions 2 and 3, we consider the
sequence of iterates (xt)t≥0 of the algorithm, starting from x0.

1. If Fi are µ-strongly convex (µ > 0), ηg =

√

lM , ηl =

1
8(1+B2)νKηg

and T ≥ 8(1+b2)ν

µ

weights {wt}t∈[T ] such that the averaged output of DP-FedAvg(T, K, l, s, σg, C), deﬁned by xT = (cid:80)
has expected excess of loss such that:

, then there exist
t∈[T ] wtxt,

Diﬀerentially Private Federated Learning on Heterogeneous Data

E[F (xT )] − F (x∗) ≤ O

(cid:18) ς 2/sR + Σ2
µT KlM

g(C)

+ (1 − l)

G2
µT lM

+

νG2
µ2T 2 + µD2

0 exp

(cid:18)

−

µ
16(1 + B2)ν

T

(cid:19)(cid:19)

,

2. If Fi are convex, ηg =

√

lM , ηl =

1
8(1+B2)νKηg

the averaged output of DP-FedAvg(T, K, l, s, σg, C), deﬁned by xT = (cid:80)
such that:

and T ≥ 1, then there exist weights {wt}t∈[T ] such that
t∈[T ] wtxt, has expected excess of loss

E[F (xT )] − F (x∗) ≤ O

(cid:18)

D0

ς/

√

sR + Σg(C)
√
T KlM

GD0
√

+

√

1 − l

T lM

+

D4/3

0 ν1/3G2/3
T 2/3

+

B2νD2
0
T

(cid:19)

,

3. If Fi are non-convex, ηg =

and T ≥ 1, then there exist weights {wt}t∈[T ] such
that the randomized output of DP-SCAFFOLD-warm(T, K, l, s, σg, C), deﬁned by {xT = xt with probability wt
for all t}, has expected squared gradient of the loss such that:

lM , ηl =

1
8(1+B2)νKηg

√

E||∇F (xT )||2 ≤ O

√

(cid:18)

ν

F

ς/

√

sR + Σg(C)
√
T KlM

+

νG(cid:112)F (1 − l)
T lM

√

+

F 2/3ν1/3G2/3
T 2/3

+

B2νF
T

(cid:19)

,

where D0 := ||x0 − x∗|| and F := F (x0) − F (x∗).

Proof. To state the result of Theorem C.2, we combine the original result (Karimireddy et al., 2020b, Theorem
V) provided for any type of loss functions with the result of Lemma C.1.

D ADDITIONAL EXPERIMENTS DETAILS AND RESULTS

In this section, we give additional details on our experimental setup (Section D.1) and synthetic data generation
process (Section D.2), and provide additional results (Section D.3). All results are summarized in Table 3.

Table 3: Summary of the experiments of the paper.

Model Reference
LogReg Figs 1,3-6
LogReg Figs 2 (ﬁrst row),7-8
DNN
LogReg Tables 1,4

Take-away message
Dataset
Superiority of DP-SCAFFOLD over DP-FedAvg
Synthetic
Superiority of DP-SCAFFOLD over DP-FedAvg
FEMNIST
Superiority of DP-SCAFFOLD over DP-FedAvg
MNIST
Tradeoﬀs between K, T and σg under ﬁxed (cid:15)
Synthetic
Tradeoﬀs between T and l under ﬁxed (cid:15)
Synthetic, FEMNIST LogReg Fig 9 (ﬁrst, second rows)
MNIST
Tradeoﬀs between T and l under ﬁxed (cid:15)
Synthetic, FEMNIST LogReg Fig 10 (ﬁrst, second rows) Tradeoﬀs between T and s under ﬁxed (cid:15)
Tradeoﬀs between T and s under ﬁxed (cid:15)
MNIST

Fig 2 (second row)

Fig 10 (third row)

Fig 9 (third row)

DNN

DNN

D.1 Algorithms Setup

Hyperparameter tuning. We tuned the step-size hyperparameter η0 for each dataset, each algorithm and
each version (with or without DP) over a grid of 10 values with the lowest level of heterogeneity (5-fold cross
validation conducted on the training set). We then kept the same η0 for experiments with higher heterogeneity.

Clipping heuristic. Setting a good clipping threshold C while preserving accuracy can be diﬃcult (McMahan
et al., 2018). Indeed, if C is too small, the clipped gradients may become biased, thereby aﬀecting the convergence
rate. On the other hand, if C is too large, we have to add more noise to stochastic gradients to ensure diﬀerential
privacy (since the variance of the Gaussian noise is proportional to C2).
In practice, we follow the strategy
proposed by Abadi et al. (2016), which consists in setting C as the median of the norms of the unclipped
gradients over each stage of local training. Throughout the iterations, C will then decrease. However, we are
aware that locally setting C may leak information to the server about the magnitude of stochastic gradients. We
here consider this leak as minor and neglect its impact on privacy guarantees. Adaptive clipping (Andrew et al.,
2021) could be used to mitigate these concerns.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Deep neural network. To prove the advantage of DP-SCAFFOLD with non-convex objectives, we perform
experiments on MNIST data with a deep neural network. Its architecture is inspired by the network used by
Abadi et al. (2016) for DP-SGD. We use a feedforward neural network with ReLU units and softmax of 10 classes
(corresponding to the 10 digits of MNIST) with cross-entropy loss. Our network combines a 60-dimensional
Principal Component Analysis (PCA) projection layer and a hidden layer with 200 hidden units. Since the error
bound for DP-FL algorithms grows linearly with the dimension of the parameters for non-convex objectives
(see Theorems C.1,C.2), the PCA layer is actually necessary to prevent the curse of dimensionality due to the
addition of noise for privacy. Note that neural networks with more layers would also suﬀer from the curse of
dimensionality in the DP-FL context. Using a batch size of 500, we can reach a test accuracy higher than 98%
with this architecture in 100 epochs under the centralized setting. This result is consistent with what can be
achieved with a vanilla neural network (LeCun et al., 1998). In our framework, the PCA procedure is applied
as preprocessing to all the samples without diﬀerential privacy. To avoid privacy leakage at this step, it would
need to include a private mechanism, whose privacy loss should be added to that of the training phase (see the
discussion in Abadi et al., 2016, Section 4).

D.2 Synthetic Data Generation

Each ground-truth model for user i consists in weights Wi ∈ Rd(cid:48)×10 and bias bi ∈ R10, which are sampled
from the following distributions: Wi|ui ∼ Nd(cid:48)×10(ui, Id) and bi|u(cid:48)
i, Id) where ui ∼ Nd(cid:48)×10(0, αId) and
u(cid:48)
i ∼ N10(0, αId). The data matrix Xi of user i is sampled according to Xi|vi ∼ Nd(cid:48)(vi, Σ) where Σ is the
covariance matrix deﬁned by its diagonal Σj,j = j−1.2 and vi|Bi ∼ Nd(cid:48)(Bi, Id) where Bi ∼ Nd(cid:48)(0, νId). The
labels are obtained by independently changing the labels given by the ground truth model with probability 0.05.

i ∼ N10(u(cid:48)

D.3 Additional Experimental Results

We provide below more results on the experiments described in Section 5, including additional metrics and more
extensive choices of heterogeneity levels. We also present additional experiments with higher privacy, including
a study on the eﬀect of sampling parameters l and s (and the trade-oﬀ with T ) on privacy and convergence.

Metrics. To measure the convergence and performance of the algorithms at any communication round t ∈ [T ],
we consider the following metrics:

• Accuracy(t): the average test accuracy of the model over all users,
• Train Loss(t)= log10(F (xt) − F (x∗)): the log-gap between the objective function evaluated at parameter xt
and its minimum,
• Train Gradient Dissimilarity(t)= 1
i=1 ||∇Fi(xt)||2 − ||∇F (xt)||2, and similarly the Train Gradient Log-
M
Dissimilarity(t)= log (cid:0) 1
i=1 ||∇Fi(xt)||2(cid:1) − log (cid:0)||∇F (xt)||2(cid:1), which measure how the local gradients diﬀer
from the global gradient (i.e., the average across users) when evaluated at xt, and hence quantify the user-drift
over the rounds of communication.

(cid:80)M

(cid:80)M

M

D.3.1 Results with other metrics and diﬀerent heterogeneity levels

We provide below some additional results which complement those provided in Section 5.

• Synthetic data. We plot in Fig. 3 the evolution of the accuracy over the rounds, which is consistent with
the evolution of the train loss in Fig. 1. While the variance of the accuracy for DP-FedAvg grows with the
heterogeneity, the results of DP-SCAFFOLD-warm are not aﬀected. We can observe an average diﬀerence of
10% in the accuracy for these two algorithms over the various heterogeneity settings. We provide in Fig. 4
the evolution of the gradient dissimilarity for the same settings as in Fig. 1 and Fig. 3, which once again
shows a better convergence of DP-SCAFFOLD-warm compared to DP-FedAvg for the same privacy level. We
also provide the evolution of the train loss when varying a single heterogeneity parameter: either α (which
controls model heterogeneity across users) in Fig. 5 or β (which controls data heterogeneity across users) in
Fig. 6. In both of these settings, DP-SCAFFOLD-warm performs consistently better.

• FEMNIST data. In Fig. 7, we put in perspective the accuracy observed with K = 50 (see Fig. 2, ﬁrst
row) with the one observed with K = 100. We also show the evolution of the gradient dissimilarity in Fig. 8.
These results on real data again show the superior performance of DP-SCAFFOLD-warm, consistently with
our observations on synthetic data.

Diﬀerentially Private Federated Learning on Heterogeneous Data

Figure 3: Test Accuracy On Synthetic Data ((cid:15) = 13). First Row: K = 50; Second Row: K = 100.

Figure 4: Train Gradient Dissimilarity On Synthetic Data ((cid:15) = 13). First Row: K = 50; Second Row: K = 100.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Figure 5: Model Heterogeneity (Varying α): Train Loss On Synthetic Data ((cid:15) = 13). First Row: K = 50; Second
Row: K = 100.

Figure 6: Data Heterogeneity Varying β): Train Loss On Synthetic Data ((cid:15) = 13). First Row: K = 50; Second
Row: K = 100.

Diﬀerentially Private Federated Learning on Heterogeneous Data

Figure 7: Test Accuracy On FEMNIST Data (LogReg) with (cid:15) = 11.5. First Row: K = 50; Second Row:
K = 100.

Figure 8: Train Gradient Dissimilarity On FEMNIST Data (LogReg) with (cid:15) = 11.5. First Row: K = 50; Second
Row: K = 100.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

Table 4: Test Accuracy (%) For DP-SCAFFOLD On Synthetic Data ((cid:15) = 3, l = 0.05, s = 0.2, (α, β) = (0, 0)).

σg

10
20
40
80
160

K = 1

K = 5

K = 10

K = 20

K = 40

31.29±0.50
28.31±1.15
21.07±0.41
17.09±1.31
15.24±1.63

T = 542
T = 545
T = 546
T = 546
T = 546

44.37±0.15
41.97±0.77
33.01±1.20
21.84±0.96
15.37±0.28

T = 488
T = 502
T = 505
T = 506
T = 506

44.17±0.56
43.27±0.90
35.96±0.84
25.92±0.59
20.09±1.51

T = 428
T = 451
T = 457
T = 458
T = 458

41.71±0.36
41.13±0.55
32.82±1.11
24.02±1.27
18.09±1.86

T = 324
T = 352
T = 360
T = 362
T = 362

25.92±0.90
28.20±2.23
23.49±1.70
17.95±1.14
15.38±0.87

T = 72
T = 83
T = 86
T = 87
T = 87

D.3.2 Additional results on the trade-oﬀs between K, T and σg

In Section 5 of the main text, we presented these trade-oﬀs for DP-SCAFFOLD under a high level of heterogeneity
(see Table 1). We provide below the results of these trade-oﬀs for DP-SCAFFOLD with a lower level of heterogeneity
(α, β) = (0, 0) (see Table 4). We consider again synthetic data with (cid:15) = 3 towards any third party. To report
the test accuracy which is obtained at the end of the iterations, we proceed in two steps: (i) we compute the
average test accuracy over the last 10% of the iterations for each random run, (ii) we calculate the mean and
the standard deviation over the 3 runs and report them in the tables. We highlight in bold for each row (i.e.,
for each value of σg) in Tables 1,4 the best accuracy score obtained over all values of K.

We observe the same trends as the ones described for Table 1 in the main text. Indeed, our results clearly show
a trade-oﬀ between T and K for DP-SCAFFOLD under a ﬁxed privacy budget. If K is set too low or too large,
the performance of the algorithm is sub-optimal either because T has to be chosen too low or because control
variates are ineﬃcient under few local updates.

Moreover, we observe that setting σg to a high value does not necessarily improve the gain in the number of
communication rounds. In particular, for high values of σg, the calculation of the privacy bound does not allow
to obtain a large increase in T (T does not change between σg = 40 and σg = 160 for any value of K), which thus
leads to poor performance. This can be explained by the fact that the upper bound for the subsampled Gaussian
Indeed, given a subsampling
mechanism given in Lemma B.7 does not converge to 0 when σg is very large.
(cid:1)), which
j=3 qj(cid:0)α
1
ratio q < 1, we can observe that this bound in the asymptotic regime becomes
is positive. Hence, by increasing the value of σg, we cannot hope to inconditionally increase the number of
compositions of the mechanism under a ﬁxed privacy budget, if q is taken too large. This artefact thus proves
how small subsampling ratios have to be chosen to compute diﬀerential privacy in practice. In our experiments,
we can clearly notice that this asymptotic regime is reached as soon as σg is greater than 80. One would have
to consider lower subsampling ratios (for instance l = 10−4, s = 10−4), to obtain diﬀerent values for T when
σg = 80 and σg = 160. We investigate such trade-oﬀs in the next section.

α−1 log(1 + 2 (cid:80)α

j

Diﬀerentially Private Federated Learning on Heterogeneous Data

D.3.3 Experiments under higher privacy regime and role of sampling parameters

In Section 5 of the main text, given private parameter σg, we conducted experiments for (i) convex objective on
FEMNIST (σg = 30) and synthetic data (σg = 60) and (ii) non-convex objective on MNIST data (σg = 30).
Considering the sampling parameters we used (l = 0.2, s = 0.2), this setting allows to reach, towards any
third party, (11.4, δ)-DP for FEMNIST data and (13, δ)-DP for synthetic data. In the case of MNIST data, we
obtain (7.2, δ)-DP towards any third party for (K, T ) = (50, 100). Although these experiments only allow “low
privacy”, stronger DP guarantees can be obtained by simply decreasing the subsampling ratios (thus amplifying
the privacy). For instance, setting l(cid:48) = l/4 = 0.05 in case of synthetic data would provide (4.2, δ)-DP for
(K, T ) = (50, 400). We present below some results on numerical trade-oﬀs under a higher privacy regime.

As observed in the experiments of the main text and consistently with previous work (Karimireddy et al.,
2020b), we observe the superiority of SCAFFOLD over FedAvg and FedSGD under heterogeneous data, but most
importantly our results show that this hierarchy is preserved in our DP-FL framework with privacy constraints:
this is especially clear with growing heterogeneity and with growing number K of local updates. Besides this,
the results provided for logistic regression in the privacy regime numerically demonstrate that DP-SCAFFOLD
sometimes even outperforms (non-private) FedAvg despite the local injection of Gaussian noise, see for
instance Fig. 1-10 and Fig. 2 (bottom row), and to a lesser extent Fig. 9. Therefore, our results are quite
promising with respect to obtaining eﬃcient DP-FL algorithms under heterogeneous data for higher privacy
regimes.

Maxence Noble, Aur´elien Bellet, Aymeric Dieuleveut

In this section, we compare the robustness of DP-FedAvg and DP-SCAFFOLD w.r.t.
Trade-oﬀs between l and T .
user sampling ratio l, under a ﬁxed privacy bound (cid:15) towards a third party. We ﬁx parameters s = 0.2, K = 10
and report in Figure 9 the evolution of the test accuracy of these algorithms for l ∈ {0.08, 0.1, 0.12} over the
communication rounds. For each value of l, the number of communication rounds Tl is determined to be maximal
w.r.t. to the privacy bound, so that the desired privacy level is achieved for the output after Tl rounds. These
values of Tl are represented on Figure 9 with red vertical lines (note that the higher l, the lower Tl). We conduct
this experiment on the following datasets: synthetic data with (cid:15) = 5 (Fig 9, ﬁrst row), FEMNIST data with
LogReg model, (cid:15) = 5 (Fig 9, second row) and MNIST data with DNN model, (cid:15) = 3 (Fig 9, third row).

Our results ﬁrst show that DP-SCAFFOLD achieves much better performance than DP-FedAvg in these high privacy
regimes. The superiority of DP-SCAFFOLD towards DP-FedAvg is especially strong under high heterogeneity: we
notice a gap of 20% in the accuracy score with synthetic data for (α, β) = (5, 5) and FEMNIST data for γ = 0%
with logistic regression model, a gap of 10% in the accuracy score for MNIST data with γ = 0% and DNN.
Furthermore, DP-SCAFFOLD is robust to a low value of user sampling parameter l. We observe that we obtain the
best performance by choosing l = 0.08 (the lowest value considered), which allows to set Tl to a high value. Note
that the evolution of the accuracy is similar for all values of l. Therefore, setting a low l provides an eﬀective
way to achieve good accuracy with higher privacy.

Figure 9: Test Accuracy With Various Values For l Under Fixed Privacy Budget. First Row: Synthetic data
((cid:15) = 5); Second Row: FEMNIST data (LogReg, (cid:15) = 5); Third Row: MNIST (DNN, (cid:15) = 3).

Diﬀerentially Private Federated Learning on Heterogeneous Data

In this section, we compare the behavior of DP-FedAvg and DP-SCAFFOLD w.r.t.
Trade-oﬀs between s and T .
data sampling ratio s under a ﬁxed privacy bound (cid:15) towards a third party. We ﬁx parameters l = 0.1, K = 10
and report in Figure 10 the evolution of the test accuracy of these algorithms for s ∈ {0.05, 0.1, 0.2} over the
communication rounds. For each value of s, the number of communication rounds Ts is determined to be maximal
w.r.t. to the privacy bound, so that the desired privacy level is achieved for the output after Ts rounds. These
values of Ts are represented on Figure 10 with red vertical lines (note that the higher s, the lower T ). We conduct
this experiment for the following datasets: synthetic data with (cid:15) = 5 (Fig 10, ﬁrst row), FEMNIST data with
LogReg model, (cid:15) = 5 (Fig 10, second row) and MNIST data with DNN model, (cid:15) = 3 (Fig 10, third row).

Our results conﬁrm that DP-SCAFFOLD leads to better performance than DP-FedAvg with any value of s ∈
{0.2, 0.1, 0.05} under heterogeneity (as expected, we obtain very similar accuracy scores for these two algorithms
with γ = 100% for FEMNIST and MNIST data). However, this superiority decreases as s decreases. Consider
for instance FEMNIST data with 10% similarity: the gap in accuracy drops from 30% with s = 0.2, to less than
20% with s = 0.1. Our results seem to show that we obtain better performance with a high value of s, although
it implies to set Ts to a lower value. This is contrast to the eﬀect of l shown in Fig 9.

Figure 10: Test Accuracy With Various Values For s Under Fixed Privacy Budget. First Row: Synthetic Data
((cid:15) = 5); Second Row: FEMNIST Data (LogReg, (cid:15) = 5); Third Row: MNIST Data (DNN, (cid:15) = 3).

