Abstra: Toward Generic Abstractions for Data of Any
Model
Nelly Barret, Ioana Manolescu, Prajna Upadhyay

To cite this version:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay. Abstra: Toward Generic Abstractions for Data
￿hal-
of Any Model. BDA 2022 - informal publication only, Oct 2022, Clermont-Ferrand, France.
03774599￿

HAL Id: hal-03774599

https://inria.hal.science/hal-03774599

Submitted on 11 Sep 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Abstra: Toward Generic Abstractions for Data of Any Model

Nelly Barret, Ioana Manolescu, Prajna Upadhyay
Inria & Institut Polytechnique de Paris
nelly.barret@inria.fr,ioana.manolescu@inria.fr,prajna-devi.upadhyay@inria.fr

ABSTRACT

Digital data sharing leads to unprecedented opportunities to de-
velop data-driven systems for supporting economic activities (e.g.,
e-commerce or maps for tourism), the social and political life, and
science. Many open-access datasets are RDF graphs, but others are
CSV files, Neo4J property graphs, JSON or XML documents, etc.

Potential users need to understand a dataset in order to decide if
it is useful for their goal. While some datasets come with a schema
and/or documentation, this is not always the case. Data summaries
or schema can be derived from the data, but their technical features
may be hard to understand for non-IT specialist users, or they may
overwhelm users with information.

We propose to demonstrate Abstra, a dataset abstraction system,
which (𝑖) applies on a large variety of data models; (𝑖𝑖) computes a
description meant for humans (as opposed to a schema meant for
a parser), akin to an Entity-Relationship diagram; (𝑖𝑖𝑖) integrates
Information Extraction data profiling to classify dataset content
among a set of categories of interest to the user.

1 INTRODUCTION
Open-access data being shared over the Internet has enormous
positive impact. It enables the development of new businesses, eco-
nomic opportunities and applications; it also leads to circulating
knowledge on a variety of topics, from health to education, envi-
ronment, the arts, science, news, etc.

The World Wide Web Consortium’s recommended data shar-
ing format is RDF graphs, and many datasets are shared this way.
However, in practice, other formats are also widely used. For
instance, CSV files are shared on portals such as Kaggle or the
French public portal data.gouv.fr; hundreds of millions of biblio-
graphic notices on PubMed, a leading medical scientific site, are
available in XML; JSON is increasingly used, e.g., to document the
activity of the French parliament on the websites NosDeputes.fr and
NosSenateurs.fr, on Twitter, etc. Relational databases are sometimes
shared as dumps, including schema constraints such as primary
and foreign keys, etc., or as CSV files; property graphs (PGs, in
short, such as pioneered by Neo4J) are used to share Offshore leaks,
a journalistic database of offshore companies.

Users who must decide whether to use a dataset in an application
need a basic understanding of its content and the suitability
to their need.

Towards this goal, schemas may be available to describe the
data structure, yet they have some limitations: (𝑖) schemas are
often unavailable for semistructured datasets (XML, JSON, RDF,
PGs). Even when a schema is supplied with or extracted from the
data, e.g., [4, 9, 16, 19]: (𝑖𝑖) schema syntactic details, such as regular
expressions, etc., are hard to interpret for non-expert users; (𝑖𝑖𝑖) a
schema focuses primarily on the dataset structure, not on its content.
It does not exploit the linguistic information encoded in node names,
in the string values the dataset may contain, etc.; (𝑖𝑣) schemas

employ the data producer’s terminology, not the categories of interest
to users; (𝑣) schemas do not quantitatively reflect the dataset, whereas
knowing “what is the main content of a dataset” can be very helpful
for a first acquaintance with it. Data summarization has been studied
for semistructured data models, e.g., [7, 11]. In the particular case
when the dataset is RDF, it may come with an ontology describing
the semantics of the dataset, which is a step toward lifting limitation
(𝑖𝑖𝑖) above; however, all the others still apply. Mining for patterns
in the dataset [13] allows to find popular motifs, e.g., items often
purchased together, or small groups of strongly connected nodes
in a graph, etc. This avoids shortcomings (𝑖) and (𝑣), but not the
others. Dataset documentation, when well-written, is most helpful
for users. However, it still suffers from limitations (𝑖) and (𝑖𝑣) above:
it is often lacking, and it reflects the producer’s view.

We propose to demonstrate Abstra, a all-in-one system for
abstracting any relational, CSV, XML, JSON, RDF or PG dataset.
Abstra is based on the idea that any dataset comprises some records,
typically grouped in collections (which we view as sets). Records
describe entities or relationships in the classical conceptual data-
base design sense [17]; Abstra entities can have deeply nested
structure. When several collection of entities co-exist in a dataset,
relationships typically connect them. To identify the entities and
relationships, Abstra proceeds as follows.
(1). Given any dataset, Abstra models it as a graph, and identi-
fies collections of equivalent nodes, leveraging graph structural
summarization, as we describe in Section 2.
(2). Among the collections, Abstra detects the main ones, that is:
a small number of collections, each comprising records that may
be simple or very complex (i.e., with deeply nested structure), and
such that these collections, together, hold a large part of the dataset
contents. The challenge here is to detect, in the data graph, the nodes
and edges that are “part of” each main collection record, and to do
so efficiently even if the graph has complex, cyclic structure. This
is addressed by introducing a notion of data weight and exploiting
it as we describe in Section 3.
(3). Abstra attempts to classify each collection of entities into
a given semantic category, such as Person, Product, Geographi-
calPosition, etc., using a set of semantic properties, some of which
we collect from well-known knowledge bases, while others can be
elicited from users. The classification leverages Information Extrac-
tion to detect the presence of entities in the text fields of the data,
then exploit them in our semantic properties (Section 4). It also uses
language models to detect proximity between the dataset and the
target categories.

Abstra outputs a natural-language, compact description of
the main, classified collections of entities, together with the possible
relationships in which they participate; this description is free of
any data model-specific details. For instance, given an XMark [18]
XML document describing an online auctions site, containing 2.3M
nodes, which, together, have 80 different labels, and are organized

{authors: [{name: "Alice", cities: [”Paris”]}]}

authors

𝜖

𝜖

𝜖

𝜖

name

"Alice"

𝜖

𝜖

authors

𝜖

authors.

cities
𝜖

𝜖

"Paris"

name

"Alice"

cities
cities.
𝜖

"Paris"

Figure 1: JSON fragment (top), its direct tree model (bottom
left), and the model used in Abstra (bottom right).

on 124 labeled paths, Abstra returns: “A collection of Person en-
tities, a collection of Product, and a collection of category” (the
latter are used to describe the items for sale). Users can explore
them and inspect their internal structure through an interactive
GUI (see Section 5).

Below, we describe the abstraction steps and outline the demon-
stration scenarios, before concluding. Abstra examples and a video
can be found at: https://team.inria.fr/cedar/projects/abstra/.
2 BUILDING A COLLECTION GRAPH
We first explain how any dataset is converted in a graph representa-
tion (Section 2.1), before partitioning it and constructing the central
tool of our method, the collection graph (Section 2.2).

2.1 Graph representation of any dataset

The graph representation we start from has been introduced in
ConnectionLens [2, 8], a graph-based heterogeneous data integra-
tion system, to which we bring some modifications. Any relational,
XML, JSON, RDF, or PG dataset is turned into a directed graph
𝐺0 = (𝑁0, 𝐸0, 𝜆0) where 𝐸0 ⊆ 𝑁0 × 𝑁0 is a set of directed edges,
and 𝜆0 is a function labeling each node and edge with a string label,
that could in particular be 𝜖 (the empty label).

XML trees and RDF graphs naturally map into this modeling.
JSON documents are modeled as trees. A common model, also
used in ConnectionLens, turns maps and arrays into unlabeled
nodes. Figure 1 shows a sample JSON fragment and, at left, this
tree model: the 𝜖-labeled nodes, from the top down, correspond
respectively to the outermost map, the outermost array, the inner-
most map, and the innermost array. In Abstra, we are interested
in recognizing groups of nodes that play similar roles in the dataset
(see Section 2.2), and we facilitate that by attaching them more
meaningful node names. As illustrated in Figure 1 at the bottom
right, we (𝑖) move the labels of edges which connect a map parent
to its children, on the child nodes; (𝑖𝑖) label the children of an array
node with label of their parent, to which we concatenate . (a dot).
In general, a node’s label is computed from the closest non-empty
label among its ancestors nodes, and concatenating a dot whenever
going from an array to one of its children. This process ensures
that every node but the root has a non-empty label.

A CSV file leads to a tree, whose root has an edge going to a
node for each tuple; in turn, such a node has edges (labeled with the
possible CSV attribute names) going toward each attribute value.

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 2: Sample normalized graph.

A relational database is similarly modeled; in the presence of a
primary key-foreign key constraint of the form “𝑅.𝑎 is a foreign
key referencing 𝑆.𝑏”, each node 𝑛𝑟 corresponding to an 𝑅 tuple has
an outgoing edge labeled 𝑎 pointing to the respective 𝑆 tuple node.
From a PG, we create a node for each PG node and for each of
its attributes, with labeled edges connecting them. We also create
a node for each PG edge, having one child node for each edge
attribute (if any). Whenever the PG contains an edge 𝑒 from 𝑛𝑝1 to
𝑛𝑝2, our graph has an edge from 𝑛𝑝1 to the node 𝑛𝑒 representing 𝑒,
and one from 𝑛𝑒 to 𝑛𝑝2.

In 𝐺0, some edges have empty (𝜖) labels, e.g., parent-child edges
in XML, while other edges are labeled. For uniformity, Abstra
transforms 𝐺0 into a normalized graph 𝐺, copying all the nodes
of 𝐺0 and all its 𝜖-label edges, and replacing each 𝐺0 edge of the form
𝑙
𝑛1
−→ 𝑛2 where 𝑙 ≠ 𝜖 by two unlabeled edges 𝑛1 → 𝑥𝑙 , 𝑥𝑙 → 𝑛2
where 𝑥𝑙 is a new intermediary node labeled 𝑙. All subsequent
Abstra steps apply on the normalized graph 𝐺.

Figure 2 shows a sample bibliographic data graph 𝐺. It depicts
three papers (one partially shown), which are published in (pIn)
conferences. The papers are written by (wb) authors, described by
their name and email. Note the inverse “has written” (hW) edges
going from authors to their papers. Author 21 is invited (inv) by
the conference organizers. As Figure 2 shows, the graph may con-
tain: (𝑖) nodes such as papers, whose information content is deeply
nested, and (𝑖𝑖) several cycles (in-cycle edges are shown in red).
Within each leaf (value) node, ConnectionLens extracts named
entities such as: persons (highlighted in yellow), dates (pink high-
light), emails (light blue), etc. Abstra leverages these in order to
classify the main entity collections (Section 4).
2.2 Partitioning nodes into collections
To leverage structural information present in 𝐺, we build a partition
P = {𝐶𝑖 }𝑖 of the graph nodes 𝑁 , such that (cid:208)𝑖 𝐶𝑖 = 𝑁 and the 𝐶𝑖
are pairwise disjoint. We say the nodes from a given set 𝐶𝑖 are
equivalent, and call 𝐶𝑖 an equivalence class. Many node partitioning
schemes, a.k.a. quotient summaries, exist [7]. We need a method
that is robust to heterogeneity, i.e., it can recognize the various
papers in Figure 2 even though they have heterogeneous structure,
and efficiently computed (ideally in linear time in the size of 𝐸).
For RDF graphs, we use Type Strong summarization [10], which
satisfies these requirements; it leverages RDF types when available,
but can also identify interesting equivalence classes without them.
We extend it also to PGs and graphs derived from CSV files and

Abstra: Toward Generic Abstractions for Data of Any Model

(c) In a greedy fashion, we select the main entities by repeating

the following steps:
(i) Select the collection node 𝐶𝐸 currently having the high-

est score, as a root of a main entity;

(ii) Determine the boundary of the entity 𝐶𝐸 : this is a con-
nected subgraph of the collection graph, containing 𝐶𝐸 .
We consider all this subgraph as part of 𝐶𝐸 , which will
be reported to users including all its boundary;
(iii) Update the collection graph to reflect the selection of 𝐶𝐸
and its boundaries, and recompute the collection scores;
until a certain maximum number 𝐸𝑚𝑎𝑥 of entities have
been selected, or these entities together cover a sufficient
fraction 𝑐𝑜𝑣𝑚𝑖𝑛 of the data.

(2) Selecting relationships between the main entity col-
lections. These relationships will also be reported as part
of the abstraction (Section 3.3).

3.2 Main entity selection
We assign to each leaf node in 𝐺 an own data weight (𝑜𝑤) equal to
the number of edges incoming that node. In tree data formats,
𝑜𝑤 is 1; in RDF, for instance, a literal that is the value of many
triples may have 𝑜𝑤 > 1. We leverage this to define the 𝑜𝑤 of a
leaf collection as the sum of the 𝑜𝑤 of its nodes, e.g., in Figure 3,
𝑜𝑤 (title#) = 2, 𝑜𝑤 (name#) = 5 etc.

For each edge 𝐶𝑖 → 𝐶 𝑗 in the collection graph, we define the
edge transfer factor 𝑓𝑗,𝑖 as the fraction of nodes in 𝐶 𝑗 having a
parent node in 𝐶𝑖 ; 0 < 𝑓𝑗,𝑖 ≤ 1. Intuitively, 𝑓𝑖,𝑗 of 𝐶 𝑗 ’s weight can
also be seen as belonging to its parent 𝐶𝑖 . For instance, there are 5
name nodes, but two belong to conferences, thus the transfer factor
from name to conf is 𝑓𝑛𝑎𝑚𝑒,𝑐𝑜𝑛𝑓 = 2/5.

We experiment with two weight propagation methods.

• We run the PageRank [6] algorithm on the collection graph
with the edge direction inverted, so that each child node trans-
fers 𝑓𝑖,𝑗 of its weight to the parent. Initially, only leaf collec-
tions have non-zero 𝑜𝑤, but successive PageRank iterations
spread their weights across the graph. We denote this method
PR𝑜𝑤.

• Our second method propagates weights still backwards, but
only outside of the collection graph cycles. Specifically, we
assign to each collection a data weight 𝑑𝑤, which on leaf
collection is initialized to 𝑜𝑤, and on others, to 0. Then,
for each non-leaf 𝐶𝑖 , and non cyclic path from 𝐶𝑖 to a leaf
collection 𝐶𝑘 , we increase 𝑑𝑤 (𝐶𝑖 ) by 𝑓𝑘,𝑖 ·𝑜𝑤 (𝐶𝑘 ). We denote
this method prop𝑑𝑤.

For instance, using the second method, the collection author
in Figure 3 obtains 𝑑𝑤 = 6, corresponding to 3 transferred from
mail, and 3 transferred from name. The intuition behind prop𝑑𝑤 is
that edges that are part of cycles may have a meaning closer to
“symmetric relationships between entities”, than to “including a
collection in another collection’s boundary”.

To determine entity boundaries, we proceed as follows:

• When using prop𝑑𝑤, we consider part of the boundary of an
entity 𝐶𝑖 , any entity 𝐶𝑘 that transferred some weight to 𝐶𝑖 ,
and all the edges along which such transfers took place. For
instance, in Figure 3, mail and name are within the boundary
of author.

Figure 3: Sample collection graph corresponding to Figure 2.

relational databases. For graphs derived from XML or JSON as
discussed in Section 2.1, we simply partition the nodes by their
labels. On the graph in Figure 2, the equivalence classes are: {1, 40};
{6, 8, 38}; {20, 21, 23}, etc.; there is one class for each distinct label
of non-leaf nodes, and one class for each set of leaf nodes whose
parents are equivalent.

We call collection graph the graph whose nodes are the collections
𝐶𝑖 , and having an edge 𝐶𝑖 → 𝐶𝑘 if and only if for some nodes
𝑛𝑖 ∈ 𝐶𝑖, 𝑛 𝑗 ∈ 𝐶 𝑗 , 𝑛𝑖 → 𝑛 𝑗 ∈ 𝐸. Figure 3 shows the collection graph
corresponding to the graph in Figure 2. Here, in each collection, all
nodes have the same label, shown in the collection; this does not
hold in general, e.g., in a collection of RDF nodes, each node has a
different label. The label year# is used to denote the collection of
text children of the nodes from the collection with the label year
and similarly for the others whose label end in #. The dw attributes
will be discussed in Section 3.

3 IDENTIFYING THE MAIN ENTITIES TO
REPORT AND THEIR RELATIONSHIPS
Among the collections C, some are clearly better abstractions of the
dataset than others, e.g., in Figure 3, paper seems a better candidate
than its child collection year. However, one cannot simply return
“the parent (or root) collection(s)”: the collection graph may have no
root at all, if it is cyclic as in Figure 3 (red edges are part of cycles).
Even if a root collection exists, it may not be the best choice. For
instance, consider XHTML search results grouped in pages, of the
form ⟨top⟩ ⟨page⟩ ⟨result⟩...⟨/result⟩ ⟨result⟩... ⟨/result⟩ ⟨/page⟩ ⟨page⟩...
⟨/page⟩ ... ⟨/top⟩. Here, the top collection is that of pages, but the
actual data is in the results, thus, "a collection of results" is a better
abstraction.

3.1 Overview of the method

A high-level view of our method is the following (concrete details
will be provided below):

(1) Selecting the main entities (Section 3.2):

(a) We assign to each collection a weight, and to each edge in

the collection graph, a transfer factor.

(b) We propagate weights in the collection graph, based on the
weights and transfer factors, to assign to each collection
a score that reflects not only its own weight, but also its
position in the graph.

RDF type 𝜏 (if
typed RDF nodes)

𝐶

𝐶 properties

𝑑𝑝𝑖

P

K

𝑝
𝑑𝑜𝑚𝑎𝑖𝑛(𝑝)
𝑟𝑎𝑛𝑔𝑒 (𝑝)

l

compare

𝑘

𝐶 node
labels

profile
𝜎𝐶,𝑑𝑝𝑖

vote to
classify 𝐶
as 𝑘

compare

Figure 4: Entity classification outline.

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

classify 𝐶
as 𝜏

classify 𝐶
as winner
class 𝑘∗

set P of semantic properties, which have known domain and range
constraints connecting them to the classes in K.

We bootstrapped our K and P by selecting six common classes
(Person, Place, Organization, Creative Work, Event and Product),
and associating them, based on WikiData and YAGO, properties
they are likely to have, or to be values of. For instance, a Person
has property birthPlace, while the value of birthPlace is a Place).
Next, we rely on GitTables [14], a repository of 1.5M tables extracted
from Github. For each attribute name encountered in a table, it pro-
vides candidate properties from DBPedia [3] and/or schema.org1; it
also provides the domain and range triples corresponding to these
properties. For instance, GitTable’s entry for gender is:

• When using PR𝑜𝑤, to determine the boundary of 𝐶𝑖 , we
traverse the graph edges starting from 𝐶𝑖 and include its
neighbor node 𝐶 𝑗 if and only if (𝑖) the edge 𝐶𝑖 → 𝐶 𝑗 has a
transfer factor of at least 𝑓𝑚𝑖𝑛, or (𝑖𝑖) each node from 𝐶𝑖 has
at most one child in 𝐶 𝑗 . The intuition for (𝑖𝑖) is that such
a child 𝐶 𝑗 “can be assimilated to an attribute of 𝐶𝑖 ”, rather
than being “independent of it”.

Finally, to update the graph after selecting one main entity 𝐶𝐸 ,
each leaf collection in the boundary of 𝐶𝐸 subtracts from its own
weight 𝑜𝑤 the fraction (at most 1.0) that it propagated to 𝐶𝐸 . For
instance, once author is selected with name in its boundary, the 𝑜𝑤
of name decreases to 2. Then, the scores of all graph collections
(𝑑𝑤, respectively, PageRank score based on 𝑜𝑤) are recomputed.

𝐸, . . . , 𝐶𝑚𝑎𝑥𝐸

3.3 Relationship selection
Having selected the main entities {𝐶1
} and their bound-
aries, every oriented path in the collection graph that goes from a
given 𝐶𝑖
𝐸 is reported as a relationship. For instance,
in Figure 3, if the main entities are author (with mail and name in its
boundary) and paper (with year, title and abstract in its boundary),
hW
−−−→ paper,
the relationships reported are: paper

wB
−−−→ author, author

𝐸 to another 𝐶 𝑗

𝐸

and paper

pIn.conf.inv
−−−−−−−−−→ author.

If the scores lead to reporting three main entities, the two above
entities and also conf (with name and inv in its boundary), the rela-

tionships are: paper
inv
−−→ author.
and conf

wB
−−−→ author, author

hW
−−−→ paper, paper

3.4 Discussion

pIn
−−−→ conf,

Abstra may return different results on a given dataset, depending
on the scoring method used (prop𝑑𝑤 or PR𝑜𝑤), as well as the pa-
rameters: 𝐸𝑚𝑎𝑥 and 𝑐𝑜𝑣𝑚𝑖𝑛 (Section 3.1), and 𝑓𝑚𝑖𝑛 (Section 3.2). Em-
pirically, we have used 𝐸𝑚𝑎𝑥 ∈ {3, 5}, 𝑐𝑜𝑣𝑚𝑖𝑛 = 0.8 and 𝑓𝑚𝑖𝑛 = 0.3.
More generally, classical Entity-Relationship (E-R) modeling is
known to include a subjective factor, and for a given database,
several E-R models may be correct. Our focus is on not missing
any essential component of the dataset, while allowing users to limit
the amount of information through 𝐸𝑚𝑎𝑥 , and classifying the main
entities into semantic categories, to make them as informative as
possible, as we explain below.

4 MAIN ENTITY CLASSIFICATION
To each main entity 𝐶 thus identified, we want to associate a cate-
gory from a predefined set K of semantic classes, and using also a

"id":"schema:gender", "label":"gender",
"description":"Gender of something, typically a Person,
"domain":["schema:Person","schema:SportsTeam"],
"range": ["schema:GenderType","schema:Text"]

GitTables have been populated using SHERLOCK [15], a state-of-
the-art deep learning semantic annotation technique. From GitTa-
bles, we derive 4.187 P properties; 3.687 among them have domain
information, and 3.898 have range statements.

The overall classification process is outlined in Figure 4; solid
arrows connect associated data items and trace the classification
process, while dotted arrows go from a set to one of its elements.
At the top of the figure, if the collection contains RDF resources
considered equivalent by RDFQuotient due to a common type 𝜏,
we return that type, considering it is the most precise.

Otherwise, we exploit two kinds of information attached to 𝐶:
(1) We consider each data property 𝑑𝑝𝑖 that 𝐶 has, such as mail
for the author collection in Figure 3. Out of all the values that
𝑑𝑝𝑖 takes on a node from 𝐶, we compute an entity profile
𝜎𝐶,𝑑𝑝𝑖 , reflecting the entities extracted from these values. For
instance, 𝜎author, mail states that each value of the property
mail contains an email (blue highlight in Figure 2), and that
overall, 100% of the length of these values is part of an email
entity. In general, a profile may reflect the presence of entities
of several types, which may span over only a small part of
the property values.
We compare 𝑑𝑝𝑖 and 𝜎𝐶,𝑑𝑝𝑖 to each property 𝑝 ∈ P and
the classes in the range of 𝑝. If the property name 𝑑𝑝𝑖 is
sufficiently similar (through word embeddings) with some
property 𝑝 ∈ P, and that 𝜎𝐶,𝑑𝑝𝑖 is similarly sufficiently simi-
lar to a class 𝑘𝑖 ∈ K, e.g., EmailAddress, that is in the range of
𝑝, this leads to a vote of 𝑑𝑝𝑖 for classifying 𝐶, in every classes
𝑘 ∈ K such that 𝑘 is in the domain of 𝑝. Each property 𝑑𝑝𝑖
may “vote” in favor of several classes, via different domain
constraints; the higher the similarity between 𝑑𝑝𝑖 and 𝑝, the
more frequent 𝑑𝑝𝑖 is on 𝐶 nodes, the fewer domain and range
constraints 𝑝 has, the stronger the vote is.

(2) The labels of 𝐶 nodes may also “vote” toward classifying
collection 𝐶. All 𝐶 nodes may have the same label, e.g., author,
which may resemble the name of a class, e.g., Author. Or,
𝐶 nodes may all have different names, e.g., RDF URIs of
the form http://ns.com/Author123, from which we extract the
component after the last /, eliminate all but alphabet letters,

1https://schema.org/

Abstra: Toward Generic Abstractions for Data of Any Model

Figure 5: Sample screenshots of the Abstra GUI.

and use the result(s), weighted by their support among 𝐶
nodes, in a similar fashion.

Finally, 𝐶 is classified with the class 𝑘∗ ∈ K having received
the highest sum of votes. The process resembles domain inference
using RDF Schema ontology constraints, with the difference that
our “votes” are quantified by similarity and support, and, to keep
things simple for the users, we select a single class, the one having
the strongest support.
5 SYSTEM AND SCENARIOS
Abstra is implemented in Java, leveraging the graph creation (in-
cluding entity extraction) and Postgres-based store of Connection-
Lens [2]. All Abstra steps scale up linearly in the data size, which
we experimentally verified on datasets of up to tens of millions
of edges. The main memory needs are in TypedStrong partition-
ing, namely 𝑂 (|𝑁 |); other operations are implemented in SQL and
benefit from Postgres’ optimizations.

We have computed abstractions of dozens of synthetic bench-
mark datasets used in the data management literature, such as
XMark [18], BSBM [5], LUBM [12], and real-life datasets about
cooking, NASA flights, clean energy production, Nobel prizes, CSV

datasets from Kaggle, etc. varying the data model, the number of
collections and entity complexity, the presence of relations, etc.
During the demonstration, users will be able to: (𝑖) change the sys-
tem parameters, and see the impact on the classification results;
(𝑖𝑖) edit the semantic information K and P to influence the entity
classification; (𝑖𝑖𝑖) edit a set of small RDF, JSON and XML examples,
then abstract them to see the impact.

Abstractions are shown both as HTML text and as a light-
weight E-R diagram of the main entity collections and their re-
lationships. Clicking on a collection launches a node-link GUI
(JavaScript) enabling users to see records from the respective col-
lection, navigate to neighbors, etc. (see Figure 5).

6 CONCLUSION
Abstra extracts the main entities and relationships from heterogeneous-
structure datasets, leveraging Information Extraction, language
models, knowledge bases, and user input to classify the main col-
lections, so as to get close to the user’s interest. The goal of our
tool is to generate, as automatically as possible, compact dataset de-
scriptions. Abstra complements schema extraction [4, 9, 16, 19] or
data profiling [1], aimed at more technical uses and users; Abstra
aims to help novice, first-time users discover and start interacting
with the data, through compact, graphical abstractions.

REFERENCES
[1] Z. Abedjan, L. Golab, F. Naumann, and T. Papenbrock. Data Profiling. Synthesis

Lectures on Data Management. Morgan & Claypool Publishers, 2018.

[2] A. C. Anadiotis, O. Balalau, C. Conceicao, H. Galhardas, M. Y. Haddad,
I. Manolescu, T. Merabti, and J. You. Graph integration of structured, semistruc-
tured and unstructured data for data journalism. Information Systems, July 2021.
[3] S. Auer et al. Dbpedia: A nucleus for a web of open data. In The Semantic Web,

pages 722–735, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.

[4] M. A. Baazizi, C. Berti, D. Colazzo, G. Ghelli, and C. Sartiani. Human-in-the-loop

schema inference for massive JSON datasets. In EDBT, 2020.

[5] C. Bizer and A. Schultz. The Berlin SPARQL benchmark. IJSWIS, 5(2):1–24, 2009.
[6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine.

Comput. Networks, 30(1-7):107–117, 1998.

[7] S. Cebiric, F. Goasdoué, H. Kondylakis, D. Kotzinos, I. Manolescu, G. Troullinou,
and M. Zneika. Summarizing Semantic Graphs: A Survey. The VLDB Journal,
28(3), June 2019.

[8] C. Chanial, R. Dziri, H. Galhardas, J. Leblay, M. L. Nguyen, and I. Manolescu.
ConnectionLens: Finding connections across heterogeneous data sources (demon-
stration). PVLDB, 11(12), 2018.

[9] D. Colazzo, G. Ghelli, and C. Sartiani. Schemas for safe and efficient XML

processing. In ICDE. IEEE Computer Society, 2011.

[10] F. Goasdoué, P. Guzewicz, and I. Manolescu. RDF graph summarization for

first-sight structure discovery. The VLDB Journal, 29(5), Apr. 2020.

[11] R. Goldman and J. Widom. Dataguides: Enabling query formulation and opti-

mization in semistructured databases. In VLDB, 1997.

[12] Y. Guo, Z. Pan, and J. Heflin. Lubm: A benchmark for owl knowledge base

systems. Journal of Web Semantics, 3(2-3):158–182, 2005.

[13] J. Han, M. Kamber, and J. Pei. Data mining concepts and techniques, third edition.

Morgan Kaufmann Publishers, 2012.

[14] M. Hulsebos, Ç. Demiralp, and P. Groth. Gittables: A large-scale corpus of

relational tables. CoRR, abs/2106.07258, 2021.

[15] M. Hulsebos, K. Hu, M. Bakker, E. Zgraggen, A. Satyanarayan, T. Kraska, Ç. Demi-
ralp, and C. A. Hidalgo. Sherlock: A Deep Learning Approach to Semantic Data
Type Detection. SIGKDD explorations : newsletter of the Special Interest Group
(SIG) on Knowledge Discovery & Data Mining, May 2019.

[16] H. Lbath, A. Bonifati, and R. Harmer. Schema inference for property graphs. In

EDBT. OpenProceedings.org, 2021.

[17] R. Ramakhrishnan and J. Gehrke. Database Management Systems (3rd edition).

McGraw-Hill, 2003.

[18] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse. Xmark:

A benchmark for XML data management. In PVLDB, 2002.

[19] W. Spoth, O. A. Kennedy, Y. Lu, B. Hammerschmidt, and Z. H. Liu. Reducing

ambiguity in JSON schema discovery. In SIGMOD, 2021.

