Stochastic smoothing of the top-K calibrated hinge loss
for deep imbalanced classification
Camille Garcin, Maximilien Servajean, Alexis Joly, Joseph Salmon

To cite this version:

Camille Garcin, Maximilien Servajean, Alexis Joly, Joseph Salmon. Stochastic smoothing of the top-K
calibrated hinge loss for deep imbalanced classification. ICML 2022 - 39th International Conference
on Machine Learning, Jul 2022, Baltimore, United States. pp.7208-7222. ￿hal-03828747￿

HAL Id: hal-03828747

https://inria.hal.science/hal-03828747

Submitted on 25 Oct 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Stochastic smoothing of the top-K calibrated hinge loss
for deep imbalanced classification

Camille Garcin 1 2 Maximilien Servajean 3 4 Alexis Joly 2 Joseph Salmon 1 5

Abstract
In modern classification tasks, the number of la-
bels is getting larger and larger, as is the size
of the datasets encountered in practice. As the
number of classes increases, class ambiguity and
class imbalance become more and more problem-
atic to achieve high top-1 accuracy. Meanwhile,
Top-K metrics (metrics allowing K guesses) have
become popular, especially for performance re-
porting. Yet, proposing top-K losses tailored for
deep learning remains a challenge, both theoreti-
cally and practically. In this paper we introduce a
stochastic top-K hinge loss inspired by recent de-
velopments on top-K calibrated losses. Our pro-
posal is based on the smoothing of the top-K oper-
ator building on the flexible ”perturbed optimizer”
framework. We show that our loss function per-
forms very well in the case of balanced datasets,
while benefiting from a significantly lower compu-
tational time than the state-of-the-art top-K loss
function. In addition, we propose a simple variant
of our loss for the imbalanced case. Experiments
on a heavy-tailed dataset show that our loss func-
tion significantly outperforms other baseline loss
functions.

1. Introduction

Fine-grained visual categorization (FGVC) has recently at-
tracted a lot of attention (Wang et al., 2022), in particular
in the biodiversity domain (Horn et al., 2018; Garcin et al.,
2021; Van Horn et al., 2015). In FGVC, one aims to classify
an image into subordinate categories (such as plant or bird
species) that contain many visually similar instances. The

1IMAG, Univ Montpellier, CNRS, Montpellier, France
2Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France
3LIRMM, Univ Montpellier, CNRS, Montpellier, France 4AMIS,
Paul Valery University, Montpellier, France 5Institut Univer-
sitaire de France (IUF). Correspondence to: Camille Garcin
<camille.garcin@umontpellier.fr>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

intrinsic ambiguity among the labels makes it difficult to
obtain high levels of top-1 accuracy as is typically the case
with standard datasets such as CIFAR10 (Krizhevsky, 2009)
or MNIST (LeCun et al., 1998). For systems like Merlin
(Van Horn et al., 2015) or Pl@ntNet (Affouard et al., 2017),
due to the difficulty of the task, it is generally relevant to
provide the user with a set of classes in the hope that the true
class belongs to that set. In practical applications, the dis-
play limit of the device only allows to give a few labels back
to the user. A straightforward strategy consists in returning
a set of K classes for each input, where K is a small integer
with respect to the total number of classes. Such classifiers
are called top-K classifiers, and their performance is eval-
uated with the well known top-K accuracy (Lapin et al.,
2015; Russakovsky et al., 2015). While such a metric is
very popular for evaluating applications, common learning
strategies typically consist in learning a deep neural network
with the cross-entropy loss, neglecting the top-K constraint
in the learning step.

Yet, recent works have focused on optimizing the top-K
accuracy directly. Lapin et al. (2015) have introduced the
top-K hinge loss and a convex upper-bound, following tech-
niques introduced by Usunier et al. (2009) for ranking. A
limit of this approach was raised by Berrada et al. (2018),
as they have shown that the top-K hinge loss by Lapin et al.
(2015) can not be directly used for training a deep neural
network. The main arguments put forward by the authors to
explain this practical limitation are: (i) the non-smoothness
of the top-K hinge loss and (ii), the sparsity of its gradi-
ent. Consequently, they propose a smoothed alternative
adjustable with a temperature parameter. However, their
smoothing procedure is computationally costly when K in-
creases (as demonstrated in our experiments), despite the
efficient algorithm they provide to cope with the combina-
torial nature of the loss. Moreover, this approach has the
drawback to be specific to the top-K hinge loss introduced
by Lapin et al. (2015).

In contrast, we propose a new top-K loss that relies on the
smoothing of the top-K operator (the operator returning the
K-th largest value of a vector). The smoothing framework
we consider, the perturbed optimizers (Berthet et al., 2020),
can be used to smooth variants of the top-K hinge loss but

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

could independently be considered for other learning tasks
such as K-nearest neighbors or top-K recommendation (He
et al., 2019; Covington et al., 2016). Additionally, we intro-
duce a simple variant of our loss to deal with imbalanced
datasets. Indeed, for many real-world applications, a long-
tailed phenomenon appears (Reed, 2001): a few labels enjoy
a lot of items (e.g., images), while the vast majority of the la-
bels receive only a few items, see for instance a dataset like
Pl@ntnet-300k (Garcin et al., 2021) for a more quantitative
overview. We find that the loss by Berrada et al. (2018) fails
to provide satisfactory results on the tail classes in our exper-
iments. On the contrary, our proposed loss based on uneven
margins outperforms the loss from Berrada et al. (2018)
and the LDAM loss (Cao et al., 2019), a loss designed for
imbalance cases known for its very good performance in
fine-grained visual classification challenges (Wang et al.,
2021). To the best of our knowledge, our proposed loss is
the first loss function tackling both the top-K classification
and class imbalance problems jointly.

2. Related work

Several top-K losses have been introduced and experi-
mented with in (Lapin et al., 2015; 2016; 2017). However,
the authors assume that the inputs are features extracted
from a deep neural network and optimize their losses with
SDCA (Shalev-Shwartz & Zhang, 2013). Berrada et al.
(2018) have shown that the top-K hinge loss from Lapin
et al. (2015) could not be directly used in a deep learning op-
timization pipeline. Instead, we are interested in end-to-end
deep neural network learning. The state-of-the art top-K
loss for deep learning is that of Berrada et al. (2018), which
is a smoothing of a top-K hinge loss by Lapin et al. (2015).
The principle of the top-K loss of Berrada et al. (2018) is
based on the rewriting the top-K hinge loss of Lapin et al.
(2015) as a difference of two maxes on a combinatorial num-
ber of terms, smooth the max with the logsumexp, and use
a divide-and-conquer approach to make their loss tractable.
Instead, our approach relies on smoothing the top-K oper-
ator and using this smoothed top-K operator on a top-K
calibrated loss recently proposed by Yang & Koyejo (2020).
Our approach could be used out-of-the box with other top-K
hinge losses. In contrast, the smoothing method of Berrada
et al. (2018) is tailored for the top-K hinge loss of Lapin
et al. (2015), which is shown to be not top-K calibrated in
(Yang & Koyejo, 2020).

For a general theory of smoothing in optimization, we re-
fer the reader to Beck & Teboulle (2012); Nesterov (2005)
while for details on perturbed optimizers, we refer the reader
to Berthet et al. (2020) and references therein. In the lit-
erature, other alternatives have been proposed to perform
top-K smoothing. Xie et al. (2020) formulate the smooth
top-K operator as the solution of a regularized optimal trans-

port problem between well-chosen discrete measures. The
authors rely on a costly optimization procedure to compute
the optimal plan. Xie & Ermon (2019) propose a smoothing
of the top-K operator through K successive softmax. Be-
sides the additional cost with large K, the computation of
K successive softmax brings numerical instabilities.

Concerning imbalanced datasets, several recent contribu-
tions have focused on architecture design (Zhou et al., 2020;
Wang et al., 2021). Instead, we focus here on the design
of the loss function and leverage existing popular neural
networks architectures. A popular loss for imbalanced clas-
sification is the focal loss (Lin et al., 2017) which is a modi-
fication of the cross entropy where well classified-examples
induce a smaller loss, putting emphasis on difficult exam-
ples. Instead, we use uneven margins in our formulation,
requiring examples of the rarest classes to be well classi-
fied by a larger margin than examples of the most common
classes. Uneven margin losses have been studied in the
binary case in (Scott, 2012; Li & Shawe-Taylor, 2003; Iran-
mehr et al., 2019). For the multi-class setting, the LDAM
loss (Cao et al., 2019) is a widely used uneven margin loss
which can be seen as a cross entropy incorporating uneven
margins in the logits. Instead, our imbalanced top-K loss
relies on the smoothing of the top-K operator.

3. Proposed method

3.1. Preliminaries

X

to [L] ≜

Following classical notation, we deal with multi-class clas-
sification that considers the problem of learning a clas-
based on n pairs of
sifier from
(input, label) i.i.d. sampled from a joint distribution P:
(x1, y1), . . . , (xn, yn)
[L], where
is the input
is the space of RGB images of a given size in
data space (
our vision applications) and the y’s are the associated labels
among L possible ones.

1, . . . , L
{

∈ X ×

X

X

}

∈

For a training pair of observed features and label (x, y),
RL refers to the associated score vector (often referred
s
to as logits). From now on, we use bold font to represent
vectors. For k
[L], sk refers to the score attributed to the
k-th class while sy refers to the score of the true label and
s(k) refers to the k-th largest score1, so that s(1) ≥ · · · ≥
[L], we define topK and
s(k) ≥ · · · ≥
∈
topΣK, functions from RL to R as:

s(L). For K

∈

topK : s

topΣK : s

(cid:55)→

(cid:55)→

s(K)
(cid:88)

k∈[K]

s(k) .

(1)

(2)

We write 1L = (1, . . . , 1)⊤
gradient

RL, the
topK(s) is a vector with a single one at the

RL. For s

∈

∈

∇

1Ties are broken arbitrarily.

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

(a) Top-K:
ℓ = ℓK .

(b) Cross-entropy:
ℓ = ℓCE.

(c) Multi-class hinge:
ℓ = ℓK

Hinge.

(d) Calibrated hinge:
ℓ = ℓK
Cal. Hinge.

(e) Convexified hinge:
ℓ = ℓK
CVXHinge.

(f) Smoothed hinge
ℓK,0.1
Smoothed Hinge.

(g) Smoothed hinge
ℓK,1
Smoothed Hinge.

(h) Noised balanced:
ℓK,0.3,30
Noised bal..

(i) Noised balanced:
ℓK,1,30
Noised bal..

(j) Noised imbalanced:
ℓK,1,30,5
Noised Imbal..

Figure 1. Level sets of the function s
ℓ(s, y) for different losses described in Table 1, for L = 3 classes, K = 2 and a true label y = 3
(corresponding to the upper corner of the triangles). For visualization the loss are rescaled between 0 and 1, and the level sets are restricted
to vector s
∆3. The losses have been harmonized to display a margin equal to 1. For our proposed loss, we have averaged the level
sets over 100 replications to avoid meshing artifacts.

(cid:55)→

∈

2

·

K-th largest coordinate of s and 0 elsewhere (denoted as
arg topK(s)). Similarly,
topΣK(s) is a vector with K
ones at the K largest coordinates of s and 0 elsewhere (de-
noted as arg topΣK(s)).

∇

The top-K loss (a 0/1 loss) can now be written

ℓK(s, y) = 1{topK (s)>sy} .

(3)

This loss reports an error when the score of the true label y
is not among the K-th largest scores. One would typically
seek to minimize this loss. Yet, being a piece-wise constant
function w.r.t. to its first argument2, numerical difficulties
make solving this problem particularly hard in practice.
In what follows we recall some popular surrogate top-K
losses from the literature before providing new alternatives.
We summarize such variants in Table 1 and illustrate their
differences in Figure 1 for L = 3, K = 2 (see also Figure 6
in Appendix, for L = 3, K = 1).

A first alternative introduced by Lapin et al. (2015) is a
relaxation generalizing the multi-class hinge loss introduced
by Crammer & Singer (2001) to the top-K case:
Hinge(s, y) = (cid:0)1 + topK(s\y)
ℓK

sy

(4)

(cid:1)

+ ,

−

where s\y is the vector in Rd−1 obtained by removing the
). The authors
y-th coordinate of s, and (
propose a convex loss function ℓK
CVXHinge (see Table 1) which
upper bounds the loss function ℓK
Hinge.

)+ ≜ max(0,
·

·

Berrada et al. (2018) have proposed a smoothed counterpart
of ℓK
Hinge, relying on a recursive algorithm tailored for their
combinatorics smoothed formulation. Yet, a theoretical

{

∈

∈

∈

π

≥

as

∈ X

RL and π

Hinge and ℓK

k∈[L] πk = 1, πk

limitation of ℓK
CVXHinge was raised by Yang &
Koyejo (2020) showing that they are not top-K calibrated.
Top-K calibration is a property defined by Yang & Koyejo
(2020). We recall some technical details in Appendix A
and the precise definition of top-K calibration is given in
Definition A.2.
RL : (cid:80)
We let ∆L ≜
0
}
denote the probability simplex of size L. For a score
∆L representing the conditional dis-
s
tribution of y given x, we write the conditional risk at
Rℓ|x(s, π) = Ey|x∼π(ℓ(s, y)) and the (inte-
x
ℓ(f ) ≜ E(x,y)∼P[ℓ(f (x), y)] for a scoring
grated) risk as
RL. The associated Bayes risks are
function f :
Rℓ|x(s, π) and
defined respectively by
≜ inf f :X →RL
ℓ(f ). The following result by (Yang &
R
Koyejo, 2020) shows that a top-K calibrated loss is top-K
consistent, meaning that a minimizer of such a loss would
also lead to Bayes optimal classifiers:
Theorem 3.1. (Yang & Koyejo, 2020, Theorem 2.2). Sup-
pose ℓ is a nonnegative top-K calibrated loss function. Then,
ℓ is top-K consistent, i.e., for any sequence of measurable
RL, we have:
functions f (n) :
(cid:16)
f (n)(cid:17)

ℓ|x(π) ≜ inf s∈RL

R
X →

f (n)(cid:17)

X →

R

R

(cid:16)

∗
ℓ

∗

∗
ℓ =
→ R

⇒ RℓK

∗
ℓK .
→ R

ℓ

R

In their paper, Yang & Koyejo (2020) propose a slight mod-
ification of the multi-class hinge loss ℓK
Hinge and show that it
is top-K calibrated:

ℓK
Cal. Hinge(s, y) = (1 + topK+1(s)

sy)+ .

(5)

−

2See for instance Figure 1a for a visualization.

The loss ℓK

Cal. Hinge thus has an appealing theoretical guaran-

s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

Table 1. Summary of standard top-K losses: vanilla top-K ℓK ; Cross Entropy ℓCE; hinge top-K ℓK
Cal. Hinge; Log-sum Smoothed hinge top-K ℓK,τ
Calibrated hinge top-K ℓK
Noised imbalanced hinge top-K ℓ

K,ϵ,B,my
Noised Imbal. (proposed).

Smoothed Hinge; Noised balanced hinge top-K ℓK,ϵ,B

Hinge; Convexified hinge top-K ℓK

CVXHinge;
Noised bal. (proposed);

Expression

Param.

Loss : ℓ(s, y)

ℓK (s, y)

ℓCE(s, y)

ℓ

my
LDAM(s, y)
ℓγ
focal(s, y)
ℓK
Hinge(s, y)
ℓK
CVXHinge(s, y)

ℓK
Cal. Hinge(s, y)

1{topK (s)>sy }
(cid:16)
esy / (cid:80)

k∈[L] esk

(cid:17)

− ln

(cid:16)

− ln

esy −my /(cid:2)esy −my + (cid:80)
(cid:17)γ

(1 − log [ℓCE(s, y)]

k∈[L],k̸=y esk (cid:3)(cid:17)
ℓCE(s, y)

(cid:0)1 + topK (s\y) − sy

(cid:1)

+

(cid:80)

k∈[K] topk(1L − δy + s) − sy

(cid:16) 1
K

(cid:17)

+

1

(1 + topK+1(s) − sy)+
sj
Kτ (cid:105)

+ (cid:80)

{y /∈A}
τ

j∈A

− τ ln

(cid:104) (cid:88)

Reference

Equation (3)

(Lin et al., 2017)

(Cao et al., 2019)

Equation (4), (Lapin et al., 2015)

(Lapin et al., 2015)

Equation (17), (Yang & Koyejo, 2020)

K

—

my

γ

K

K

K

ℓK,τ
Smoothed Hinge(s, y)

τ ln

(cid:104) (cid:88)

e

sj
Kτ (cid:105)

(cid:80)
j∈A

e

K, τ

(Berrada et al., 2018)

A⊂[L],|A|=K

A⊂[L],|A|=K

ℓK,ϵ,B
Noised bal.(s, y)
K,ϵ,B,my
Noised Imbal. (s, y)

ℓ

(1 + (cid:100)topK+1,ϵ,B (s) − sy)+, where (cid:100)topK+1,ϵ,B (s) is the noisy top-K + 1

K, ϵ, B

Equation (8), (proposed)

(my + (cid:100)topK+1,ϵ,B (s) − sy)+, where (cid:100)topK+1,ϵ,B (s) is the noisy top-K + 1 K, ϵ, B, my

Equation (13), (proposed)

tee that ℓK
as the starting point of our smoothing proposal.

Hinge does not have. Therefore we will use ℓK

Cal. Hinge

3.2. New loss for balanced top-K classification

Berrada et al. (2018) have shown experimentally that a
deep learning model trained with ℓK
Hinge does not learn. The
authors claim that the reason for this is the non smoothness
of the loss and the sparsity of its gradient.

We also show in Table 2 that a deep learning model trained
with ℓK
Cal. Hinge yields poor results. The problematic part
stems from the top-K function which is non-smooth and
whose gradient has only one non-zero element (that is equal
to one). In this paper we propose to smooth the top-K
function with the perturbed optimizers method developed
by Berthet et al. (2020). We follow this strategy due to
its flexibility and to the ease of evaluating associated first
order information (a crucial point for deep neural network
frameworks).
Definition 3.2. For a smoothing parameter ϵ > 0, we define
for any s

RL the ϵ-smoothed version of topΣK as:

∈
topΣK,ϵ(s) ≜ EZ[topΣK(s + ϵZ)] ,

where Z is a standard normal random vector, i.e., Z

(0, IdL).

N
Proposition 3.3. For a smoothing parameter ϵ > 0,

(6)

∼

• The function topΣK,ϵ : RL

R is strictly convex,

twice differentiable and √K-Lipschitz continuous.

→

stopΣK,ϵ is

•

∇

√

KL
ϵ

-Lipschitz.

• When ϵ

→

0, topΣK,ϵ(s)

topΣK(s).

→

All proofs are given in the appendix.

The smoothing strategy introduced leads to a natural
smoothed approximation of the top-K operator, leveraging
topΣK−1(s) for any score
the link topK(s) = topΣK(s)
RL):
s
Definition 3.4. For any s

RL (where we use the convention topΣ0 = 0L

[L], we define

RL and k

−

∈

∈

∈

∈

topK,ϵ(s) ≜ topΣK,ϵ(s)

topΣK−1,ϵ(s) .

−

This definition leads to a smooth approximation of the topK
function, in the following sense:
Proposition 3.5. For a smoothing parameter ϵ > 0,

√
• topK,ϵ is 4

-smooth.

KL
ϵ
RL,
topK,ϵ(s)
where CK,L = K√2 log L.

• For any s

∈

|

topK(s)

ϵ

·

| ≤

−

CK,L,

Observe that the last point implies that for any s
topK,ϵ(s)

topK(s) when ϵ

0.

RL,

∈

→

→

We can now define an approximation of the calibrated top-
K hinge loss ℓK
Cal. Hinge using topK,ϵ in place of topK (see
Figures 1h and 1i for level sets with K = 2)3.
Definition 3.6. We define ℓK,ϵ
top-K hinge loss as:

Noised bal. the noised balanced

• The gradient of topΣK,ϵ reads:

ℓK,ϵ
Noised bal.(s, y) = (1 + topK+1,ϵ(s)

sy)+ .

(8)

3For illustrations with K = 1, see Figures 6h and 6j

−

stopΣK,ϵ(s) = E[arg topΣK(s + ϵZ)] .

(7)

∇

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

We call the former balanced as the margin (equal to 1) is the
same for all L classes. The parameter ϵ controls the variance
of the noise added to the score vectors. When ϵ = 0, we
recover the top-K calibrated loss of Yang & Koyejo (2020),
ℓK
Cal. Hinge.
Proposition 3.7. For a smoothing parameter ϵ > 0 and
a label y
, y) is continuous, differen-
Noised bal.(
·
tiable almost everywhere, with continuous derivative. • The
gradient of ℓ(
Noised bal.(

, y) is given by:
·
ℓ(s, y) = 1{1+topK+1,ϵ(s)≥sy }

, y) ≜ ℓK,ϵ
·

[L], • ℓK,ϵ

topK+1,ϵ(s)

δy),

(9)

∈

(

∇
RL is the vector with 1 at coordinate y and 0

−

·

∇

where δy
elsewhere.

∈

Practical implementation: As is, the proposed loss can not
be used directly to train modern neural network architec-
tures due to the expectation and remains a theoretical tool.
Following (Berthet et al., 2020), we simply rely on a Monte
Carlo method to estimate the expectation for both the loss
and its gradient: we draw B noise vectors Z1, . . . , ZB, with
i.i.d.
Zb
Noised bal. is then
∼ N
estimated by:

[B]. The loss ℓK,ϵ

(0, IdL) for b

∈

to the first coordinate). Assume the three noise vectors
sampled are:

Z1 =

(cid:21)

(cid:20) 0.2
−0.1
0.1
0.3

, Z2 =

(cid:21)

(cid:20) 0.1
0.1
−0.1
0.1

, Z3 =

(cid:21)

.

(cid:20) −0.1
−0.1
0.1
−0.1

The perturbed vectors are now:

s + ϵZ1 =

(cid:21)

(cid:20) 2.6
2.5
2.4
0.8

, s + ϵZ2 =

(cid:21)

(cid:20) 2.5
2.7
2.2
0.6

, s + ϵZ3 =

(cid:21)

.

(cid:20) 2.3
2.5
2.4
0.4

The induced perturbation may provoke a change in both
topK and arg topK. For the perturbed vector s + ϵZ2, the
added noise changes the top-2 value but it is still achieved
at coordinate 1: topK(s + ϵZ2) = 2.5 and arg topK(s +

(cid:21)
. However, for s + ϵZ1 and s + ϵZ3, the added

ϵZ2) =

(cid:20) 1
0
0
0

noise changes the coordinate at which the top-2 is achieved:
(cid:21)
,

and arg topK(s + ϵZ3) =

arg topK(s + ϵZ1) =

(cid:21)

(cid:20) 0
1
0
0

(cid:20) 0
0
1
0

with topK(s + ϵZ1) = 2.5 and topK(s + ϵZ3) = 2.4,
giving:

(cid:100)topK,ϵ,B(s) = (2.5 + 2.5 + 2.4)/3 = 2.47 ,

ℓK,ϵ,B
Noised bal.(s, y) = (1 + (cid:100)topK+1,ϵ,B(s)
where (cid:100)topK+1,ϵ,B(s) ≜ (cid:91)topΣK+1,ϵ,B(s)
is a Monte Carlo estimate with B samples:

−

−

sy)+ ,

(10)

(cid:91)topΣK,ϵ,B(s)

(cid:91)
∇

topK,ϵ,B(s) =

(cid:21)

1
3

(cid:18) (cid:20) 0
1
0
0

+

(cid:21)

(cid:20) 1
0
0
0

+

(cid:20) 0
0
1
0

(cid:21) (cid:19)

=



 .





1
3
1
3
1
3
0

(cid:91)topΣK,ϵ,B(s) =

1
B

B
(cid:88)

b=1

topΣK(s + ϵZb) .

(11)

We approximate

∇

sℓK,ϵ

Noised bal.(s, y) by G, with:

G = 1

{1+(cid:100)topK+1,ϵ,B (s)≥sy}·

( (cid:91)
∇

topK+1,ϵ,B(s)

−

δy),

(12)

where the Monte Carlo estimate

(cid:91)
topK+1,ϵ,B(s) ≜ (cid:92)arg topK+1,ϵ,B(s)
∇
is given by:

(cid:92)arg topK+1,ϵ,B(s) =

1
B

B
(cid:88)

b=1

arg topK+1(s + ϵZb) .

We train our loss with SGD. Hence, B repetitions are drawn
each time the loss is evaluated. The iterative nature of this
process helps amplify the smoothing power of the approach,
explaining why even small values of B can lead to good
performance (see Section 4).

Illustration. Consider the case L = 4, K = 2, B = 3, ϵ =
(cid:20) 2.4
2.6
2.3
0.5

. We have topK(s) = 2.4

1.0 with a score vector s =

(cid:21)

and arg topK(s) =

(the top-2 value of s corresponds

(cid:21)

(cid:20) 1
0
0
0

We see the added noise results in giving weight to the gra-
dient coordinates k whose associated score sk is close to
topK(s) (in this example the first and third coordinates).
Note that if we set ϵ to a smaller value, e.g., ϵ = 0.1,
the added perturbation is not large enough to change the
arg topK in the perturbed vectors, leading to the same gradi-
ent as the non-smoothed top-K operator: (cid:91)
topK,0.1(s) =
(cid:20) 1
0
0
0

. Hence, ϵ acts as a parameter which allows exploring

coordinates k whose score values sk are close to the top-K
score (provided that ϵ and/or B are large enough).

∇

(cid:21)

3.3. New loss for imbalanced top-K classification

In real world applications, a long-tailed distribution between
the classes is often present, i.e., few classes receive most
of the annotated labels. This occurs for instance in datasets
such as Pl@ntNet-300K (Garcin et al., 2021) and Inaturalist
(Horn et al., 2018), where a few classes represent the vast
majority of images. For these cases, the performance of
deep neural networks trained with the cross entropy loss is
much lower for classes with a small numbers of images, see
(Garcin et al., 2021).

We present an extension of the loss presented in Section 3.2
to the imbalanced case. This imbalanced loss is based on
uneven margins (Scott, 2012; Li & Shawe-Taylor, 2003;

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

Table 2. Influence of ϵ on CIFAR-100 best validation top-5 accu-
racy obtained by training a DenseNet 40-40 with loss ℓK=5,ϵ,B=10
.
The training procedure is the same as in Section 4.4.

Noised bal.

ϵ

0.0

1e-4

1e-3

1e-2

1e-1

1.0

10.0

100.0

Top-5 acc.

19.38

14.84

11.4

93.36

94.46

94.24

93.78

93.12

Iranmehr et al., 2019; Cao et al., 2019). The underlying idea
is to require larger margins for classes with few examples,
which leads to a higher incurred loss for mistakes made on
examples of the least common classes.

Imposing a margin my parameter per class in Equation (10)
leads to the following formulation:

ℓK,ϵ,B,my
Noised Imbal.(s, y) = (my + (cid:100)topK+1,ϵ,B(s)

sy)+ . (13)

−

Here, we follow Cao et al. (2019) and set my = C/n1/4
,
with ny the number of samples in the training set with class
y, and C a hyperparameter to be tuned on a validation set.

y

3.4. Comparisons of various top-K losses

In Table 1, we synthesize the various top-K loss functions
evoked above. To better understand their differences, Fig-
ure 1 provides a plot of the losses for s in the 2-simplex, for
K = 2 and L = 3. The correct label is set to be y = 3 and
corresponds to the vertex on top and in red. Figure 1a shows
the classical top-K that we would ideally want to optimize.
It has 0 error when s3 is larger than the smallest coordinate
of s (i.e., is in the top-2) and 1 otherwise. Figure 1b shows
the cross-entropy, by far the most popular (convex) loss
used in deep learning. As mentioned by Yang & Koyejo
(2020), the cross-entropy happens to be top-K calibrated
for all K. Figure 1c shows the top-K hinge loss proposed
by Lapin et al. (2015) and Figure 1e is a convex upper re-
laxation. Unfortunately, Yang & Koyejo (2020) have shown
that such losses are not top-K calibrated and propose an
alternative, illustrated in Figure 1d. We show that the loss
of Yang & Koyejo (2020) performs poorly when used for
optimizing a deep neural network. Figure 1f and Figure 1g
show the smoothing proposed by Berrada et al. (2018) of
the loss in Figure 1c, while Figure 1h and Figure 1i show
our proposed noised smoothing of the loss in Figure 1d. The
difference with Berrada et al. (2018) is that we start with a
top-K calibrated hinge loss, and our smoothing consists in
smoothing only the top-K operator, which mostly affects
classes whose scores are close to the top-K score, while
the method from Berrada et al. (2018) results in a gradient
where all coordinates are non-zero.

Finally, Figure 1j shows our noised imbalanced top-K loss.
Additional visualizations of our noised top-K loss illustrat-
ing the effect of B and ϵ can be found in the appendix, see
Figures 4 and 5.

Figure 2. Average number of non-zero gradient coordinates as a
function of ϵ (loss ℓK,ϵ,3
Noised bal., CIFAR-100 dataset, DenseNet 40-40
model, 1st epoch). The gradient dimension is 100. We see that
the gradient remains sparse even for large values of ϵ. Together
with Table 2, this shows that having a non-sparse gradient is not
a necessary condition for successful learning, contrary to what is
suggested in (Berrada et al., 2018)

4. Experiments

The Pytorch (Paszke et al., 2019) code for our top-K loss
and experiments can be found at: https://github.
com/garcinc/noised-topk.

4.1. Influence of ϵ and gradient sparsity

Noised bal. coincides with ℓK

Table 2 shows the influence of ϵ on CIFAR-1004 top-5 accu-
racy. When ϵ = 0, ℓK,ϵ,B
Cal. Hinge. We
see that a model trained with this loss fails to learn properly.
This resonates with the observation made by Berrada et al.
(2018) that a model trained with ℓK
Hinge, which is close to
ℓK
Cal. Hinge, also fails to learn. Table 2 also shows that when
ϵ is too small (ϵ = 10−4 or ϵ = 10−3), the optimization
remains difficult and the learned models have very low per-
formance. For sufficiently high values of ϵ, in the order of
10−2 or greater, the smoothing is effective and the learned
models achieve a very high top-5 accuracy. Table 2 also
shows that although the optimal value of ϵ appears to be
around 10−1, the optmization is robust to high values of ϵ.

Hinge. Indeed, for any s

Berrada et al. (2018) argue that a reason a model trained with
ℓK
Hinge fails to learn is because of the sparsity of the gradient
of ℓK
Hinge(s, y)
∈
has at most two non-zero coordinates. This is one of the
main reasons put forward by the authors to motivate the
smoothing of ℓK
Smoothed Hinge, whose gradient coor-
dinates are all non-zero.

Hinge into ℓK,τ

RL, y

sℓK

[L],

∇

∈

We investigate the behaviour of the gradient of our loss
Noised bal.(s, y) for each training example
by computing
during the first epoch. We then compute the average number
of non-zero coordinates in the gradient. We repeat this

sℓK,ϵ,B

∇

4For experiments with CIFAR-100, we consider a DenseNet 40-
40 model (Huang et al., 2017), similarly as Berrada et al. (2018).

10−410−310−210−1100101102(cid:15)0.00.51.01.52.02.53.03.54.0Av.numberofnon-zerogradientcoordinatesK=3K=5K=10Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

Table 3. Influence of B hyper-parameter on the best validation top-
5 accuracy (loss ℓ5,0.2,B
Noised bal., CIFAR-100 dataset, DenseNet 40-40
model. The training procedure is the same as in Section 4.4.)

B

1

2

3

5

10

50

100

Top-5 acc

94.28

94.2

94.46

94.52

94.24

94.64

94.52

process for several values of ϵ and report the results in
Figure 2. There are two points to highlight:

• The number of non-zero gradients coordinates increases
with ϵ. This is consistent with our illustration example
in Section 3.2: high values of ϵ allow putting weights on
gradient coordinates whose score is close to the topK score.

CE and ℓK,τ

• Even when ϵ is large, the number of non-zero gradient
coordinates is small: on average, 4 out of 100. In com-
parison, for ℓK
Smoothed Hinge, all gradient coordinates
are non-zero. Yet, even with such sparse gradient vectors,
we manage to reach better top-5 accuracies than ℓK
CE and
ℓK,τ
Smoothed Hinge (see Table 4). Therefore, one of the main take-
away is that a non-sparse gradient does not appear to be a
necessary condition for successful learning contrary to what
is suggested in (Berrada et al., 2018). A sufficiently high
probability (controlled by ϵ) that each coordinate is updated
at training is enough to achieve good performance.

4.2. Influence of B

Table 3 shows the influence of the number of sampled stan-
dard normal random vectors B on CIFAR-100 top-5 accu-
racy for a model trained with our balanced loss with K = 5.
B appears to have little impact on top-5 accuracy, indicating
that there is no need to precisely estimate the expectation
in Equation (11). As increasing B comes with computation
overhead (see next section) and does not yield an increase
of top-K accuracy, we advise setting it to a small value
(e.g., 3

10).

B

≤

≤

4.3. Computation time

Smoothed Hinge, and our balanced loss ℓK,ϵ,B

In Figure 3, we plot the average epoch duration of a model
trained with the cross entropy ℓCE, the loss from Berrada
et al. (2018) ℓK,τ
Noised bal.
(for several values of B) as a function of K. For standard
training values, i.e., K = 5, B = 3, the average epoch time
is 65s for ℓCE, 68s for ℓK,ϵ,B
Noised bal. (+4.6% w.r.t. ℓCE) and 81s
for ℓK,τ
Smoothed Hinge (+24.6% w.r.t. ℓCE). Figure 3 further shows
that while the average epoch duration of ℓK,τ
Smoothed Hinge seems
to scale linearly in K, our loss does not incur an increased
epoch duration when K increases. Thus, for K = 10, the
average epoch time for ℓK,τ
Smoothed Hinge is 90s versus 60s for
ℓK,ϵ,B
Noised bal. with B = 3. While for most classical datasets
(Russakovsky et al., 2015; Krizhevsky, 2009) small values

Figure 3. Average epoch time as a function of K for different
losses (CIFAR-100 dataset, DenseNet 40-40 model). The proposed
loss ℓK,ϵ,B
Noised bal. is not sensitive to the parameter K contrarily to
ℓK,τ
Smoothed Hinge introduced by Berrada et al. (2018).

of K are enough to achieve high top-K accuracy, for other
applications high values of K may be used (Covington
et al., 2016; Cole et al., 2020), making our balanced loss
computationally attractive.

4.4. Comparisons for balanced classification

The CIFAR-100 (Krizhevsky, 2009) dataset contains 60,000
images (50,000 images in the training set and 10,000 images
in the test set) categorized in 100 classes. The classes are
grouped into 20 superclasses (e.g., fish, flowers, people),
each regrouping 5 classes. Here we compare the top-K ac-
curacy of a deep learning model trained on CIFAR-100 with
either our balanced loss ℓK,ϵ,B
Noised bal., the cross entropy ℓCE,
or the loss from Berrada et al. (2018), ℓK,τ
Smoothed Hinge, which
is the current state-of-the art top-K loss for deep learning.
We repeat the experiment from Section 5.1 of Berrada et al.
(2018) to study how ℓK,ϵ,B
Noised bal. reacts when noise is intro-
duced in the labels. More precisely, for each training image,
its label is sampled randomly within the same super-class
with probability p. Thus, p = 0 corresponds to the original
dataset and p = 0.5 corresponds to a dataset where all the
training examples have a label corresponding to the right
superclass, but half of them (on average) have a different
label than the original one. With such a dataset, a perfect
top-5 classifier is expected to have a 100% top-5 accuracy.
As in Berrada et al. (2018) we extract 5000 images from the
training set to build a validation set. We use the same hyper-
parameters as Berrada et al. (2018): we train a DenseNet
40-40 (Huang et al., 2017) for 300 epochs with SGD and a
Nesterov momentum of 0.9. For the learning rate, our policy
consists in starting with value of 0.1 and dividing it by ten
at epoch 150 and 225. The batch size and weight decay
are set to 64 and 1.10−4 respectively. Following Berrada
et al. (2018), the smoothing parameter τ of ℓK,τ

Smoothed Hinge

51020304050K100200Av.epochduration(s.)‘K,(cid:15),BNoisedbal.,B=3‘K,(cid:15),BNoisedbal.,B=10‘K,(cid:15),BNoisedbal.,B=50‘K,τSmoothedHinge‘KCEStochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

Table 4. Top-5 accuracy for different losses as a function of the
label noise probability p within the superclasses of CIFAR-100
(DenseNet 40-40 model).

Label noise p

ℓK
CE

ℓ5,1.0
Smoothed Hinge

ℓ5,0.2,10
Noised bal.

0.0
0.1
0.2
0.3
0.4

94.2±0.1
90.3±0.2
87.6±0.1
85.7±0.4
83.6±0.2

93.3±0.0
92.2±0.3
90.4±0.2
88.8±0.1
87.4±0.1

94.4±0.1
91.9±0.1
90.7±0.5
89.7±0.1
87.8±0.6

is set to 1.0. For ℓK,ϵ,B
Noised bal., we set the noise parameter ϵ
to 0.2 and the number of noise samples B at 10. We keep
the models with the best top-5 accuracies on the validation
set and report the top-5 accuracy on the test set in Table 4.
The results are averaged over four runs with four different
random seeds (we give the 95% confidence inverval). They
show that the models trained with ℓ5,0.2,10
Noised bal. give the best
top-5 accuracies except when p = 0.1 where it is slightly
below ℓ5,1.0
Smoothed Hinge. We also observe that the performance
gain over the cross entropy is significant in the presence of
label noise (i.e., for p > 0).

We provide additional experiments on ImageNet (Rus-
sakovsky et al., 2015) in Appendix D.

4.5. Comparison for imbalanced classification

4.5.1. PL@NTNET-300K

We consider Pl@ntNet-300K5, a dataset of plant images
recently introduced in (Garcin et al., 2021).
It consists
of 306,146 plant images distributed in 1,081 species (the
classes). The particularities of the dataset are its long-tailed
distribution (80% of the species with the least number of
images account for only 11% of the total number of images)
and the class ambiguity: many species are visually simi-
lar. For such an imbalanced dataset, accuracy and top-K
accuracy mainly reflect the performance of the model on the
few classes representing the vast majority of images. Often
times, we also want the model to yield satisfactory results
on the classes with few images. Therefore, for this dataset
we report macro-average top-K accuracy, which is obtained
by computing top-K accuracy for each class separately and
then taking the average over classes. Thus, the class with
only a few images contributes the same as the class with
thousands of images to the overall result.

In this section we compare the macro-average top-K
accuracy of a deep neural network trained with either
Noised bal., ℓK,ϵ,B,my
ℓCE, ℓK,τ
the
loss from Cao et al. (2019) based on uneven margins pro-
viding state-of-the-art performance in Fine-Grained Visual

Noised imbal. and ℓmy

Smoothed Hinge, ℓK,ϵ,B

LDAM,

5For the experiments with Pl@ntNet-300K, we consider a

ResNet-50 model (He et al., 2016).

Categorization tasks.

Smoothed Hinge is set to 0.1.

Setup: We train a ResNet-50 (He et al., 2016) pre-trained
on ImageNet (Russakovsky et al., 2015) for 30 epochs with
SGD with a momentum of 0.9 with the Nesterov acceler-
ation. We use a learning rate of 2.10−3 divided by ten at
epoch 20 and epoch 25. The batch size and weight decay are
set to 32 and 1.10−4 respectively. The smoothing parameter
τ for ℓK,τ
To tune the margins of ℓmy
Noised imbal. more easily,
we follow (Wang et al., 2018; Cao et al., 2019): we normal-
ize the last hidden activation and the weight vectors of the
last fully-connected layer to both have unit ℓ2-norm, and
we multiply the scores by a scaling constant, tuned for both
losses on the validation set, leading to 40 for ℓmy
LDAM and 60
for ℓK,ϵ,B,my

LDAM and ℓK,ϵ,B,my

Noised imbal..

Finally, we tune the constant C by tuning the largest mar-
gin maxy∈[L] my for both ℓmy
Noised imbal.. We find
that for both losses, the optimal largest margin is 0.2. For
ℓK,ϵ,B,my
Noised imbal., we set ϵ = 10−2 and B = 5. We further discuss
hyperparameter tuning in Appendix E.

LDAM and ℓK,ϵ,B,my

∈ {

∈ {

train

1, 3, 5

the
Noised imbal. for K

the
network with
Noised bal. and ℓK,ϵ,B,my

top-K losses
We
ℓK,τ
Smoothed Hinge, ℓK,ϵ,B
1, 3, 5
.
}
For all losses, we perform early stopping based on the best
macro-average top-K accuracy on the validation set (for
K
). We report the results on the test set in
}
Table 5 (three seeds, 95% confidence interval). We find that
the loss from Berrada et al. (2018) fails to generalize to
the tail classes in such an imbalanced setting. In contrast,
ℓK,ϵ,B
Noised bal. gives results similar to the cross-entropy while
ℓK,ϵ,B,my
Noised imbal. provides the best results (regardless of the value
of K). Noticeably, it outperforms ℓmy
LDAM (Cao et al., 2019)
for all cases.

4.5.2. IMAGENET-LT
We test ℓK,ϵ,B,my
Noised imbal. on ImageNet-LT (Liu et al., 2019), a
dataset of 115,836 training images obtained by subsampling
images from ImageNet with a pareto distribution. The re-
sulting class imbalance is much less pronounced than for
Pl@ntNet-300K.

∈ {

1, 3, 5
We train a ResNet34 with several losses for K
}
for 100 epochs with a learning rate of 1.10−2 divided by 10
at epoch 60 and 80. We use a batch size of 128 and set the
weight decay to 2.10−3. All hyperparameters are tuned on
the 20,000 images validation set from Liu et al. (2019). The
results on the test set (four seeds, 95% confidence interval)
are reported in Table 6. They show that ℓK,ϵ,B,my
Noised Imbal. performs
very well on few shot classes compared to the other losses.
Since ImageNet-LT is much less imbalanced than Pl@ntNet-
300K, there are fewer such classes, hence the overall gain is
less salient than for Pl@ntNet-300K.

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

Table 5. Macro-average top-K accuracy (on test set) for different losses measured on Pl@ntNet-300K, a heavy-tailed dataset with high
ambiguity (ResNet-50 model). The three numbers in parentheses represent respectively the mean top-K accuracies of 1) few shot classes
(< 20 training images) 2) medium shot classes (20

100 training images) 3) many shot classes (> 100 training images).

.

K

1
3
5

ℓCE

ℓK,0.1
Smoothed Hinge

focal (γ = 2.0)

ℓmax my=0.2
LDAM

ℓK,0.01,5,max my=0.2
Noised imbal.

36.3
58.8
68.7

±
±
±

0.3 (12.6/42.9/71.7)
0.4 (32.4/75.3/92.0)
0.2 (45.1/86.3/95.4)

35.7
50.3
50.9

±
±
±

0.2 (13.1/41.5/71.1)
0.2 (16.7/69.8/92.7)
0.3 (12.1/78.1/95.7)

35.8
58.7
66.4

±
±
±

0.3 (12.4/42.1/72.1)
0.4 (32.2/73.8/88.8)
0.5 (42.0/82.5/95.5)

37.6
60.4
69.7

±
±
±

0.3 (15.5/43.4/71.4)
0.3 (35.9/74.8/92.0)
0.2 (47.5/84.8/95.8)

40.6
63.3
71.9

±
±
±

0.1 (20.9/45.8/71.2)
0.3 (43.0/74.1/90.0)
0.3 (54.0/83.0/94.0)

42.4
64.9
73.2

±
±
±

0.3 (23.9/46.3/72.1)
0.4 (44.8/74.5/92.1)
0.5 (55.3/84.2/95.3)

≤

≤
ℓK,1.0,5
Noised bal.

Table 6. Top-K accuracy (on test set) for different losses measured on ImageNet-LT (ResNet-34 model). The three numbers in parentheses
represent respectively the mean top-K accuracies of 1) few shot classes (< 20 training images) 2) medium shot classes (20
100
training images) 3) many shot classes (> 100 training images).

≤

≤

.

K

1
3
5

ℓCE

ℓK,0.1
Smoothed Hinge

focal (γ = 1.0)

ℓmax my=0.4
LDAM

ℓK,0.1,5,max my=0.4
Noised imbal.

37.0
55.5
63.2

±
±
±

0.1 (1.5/28.2/60.6)
0.1 (8.2/53.0/75.3)
0.1 (15.8/63.1/80.1)

37.3
42.0
39.0

±
±
±

0.1 (1.3/28.6/60.7)
0.1 (0.0/29.1/72.9)
0.1 (0.0/20.7/75.5)

37.7
56.2
63.8

0.0 (2.4/29.8/59.9)
±
0.0 (10.2/54.0/75.1)
±
0.1 (17.8/63.6/80.3)
±

39.3
56.0
63.1

±
±
±

0.2 (10.5/33.1/57.1)
0.2 (24.1/52.0/72.3)
0.2 (32.9/60.1/77.7)

38.7
56.5
63.5

±
±
±

0.0 (7.6/32.3/57.6)
0.1 (27.0/52.6/71.7)
0.1 (37.0/60.2/77.0)

5. Conclusion and perspectives

functions for deep top-k classification. In ICLR, 2018.

We propose a novel top-K loss as a smoothed version of
the top-K calibrated hinge loss of Yang & Koyejo (2020).
Our loss function is well suited for training deep neural
networks, contrarily to the original top-K calibrated hinge
loss (e.g., the poor performance of the case ϵ = 0 in Table 2,
that reduces to their loss). The smoothing procedure we pro-
pose applies the perturbed optimizers framework to smooth
the top-K operator. We show that our loss performs well
compared to the current state-of-the-art top-K losses for
deep learning while being significantly faster to train when
K increases. At training, the gradient of our loss w.r.t. the
score is sparse, showing that non-sparse gradients are not
necessary for successful learning. Finally, a slight adapta-
tion of our loss for imbalanced datasets (leveraging uneven
margins) outperforms other baseline losses. Studying deep
learning optimization methods for other set-valued classi-
fication tasks, such as average size control or point-wise
error control (Chzhen et al., 2021) are left for future work.

Acknowledgements

The work by CG and JS was supported in part by the French
National Research Agency (ANR) through the grant ANR-
20-CHIA-0001-01 (Chaire IA CaMeLOt).

References

Affouard, A., Go¨eau, H., Bonnet, P., Lombardo, J.-C., and
Joly, A. Pl@ntnet app in the era of deep learning. In
ICLR - Workshop Track, 2017.

Beck, A. and Teboulle, M. Smoothing and first order meth-
ods: A unified framework. SIAM J. Optim., 22(2):557–
580, 2012.

Berrada, L., Zisserman, A., and Kumar, M. P. Smooth loss

Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J.-
P., and Bach, F. Learning with differentiable perturbed
optimizers. In NeurIPS, 2020.

Boucheron, S., Lugosi, G., and Massart, P. Concentration
Inequalities: A Nonasymptotic Theory of Independence.
Oxford University Press, second edition, 2013.

Cao, K., Wei, C., Gaidon, A., Arechiga, N., and Ma,
T. Learning imbalanced datasets with label-distribution-
aware margin loss. In NeurIPS, volume 32, pp. 1565–
1576, 2019.

Chzhen, E., Denis, C., Hebiri, M., and Lorieul, T. Set-
valued classification – overview via a unified framework.
arXiv preprint arXiv:2102.12318, 2021.

Cole, E., Deneu, B., Lorieul, T., Servajean, M., Botella, C.,
Morris, D., Jojic, N., Bonnet, P., and Joly, A. The geo-
lifeclef 2020 dataset. arXiv preprint arXiv:2004.04192,
2020.

Covington, P., Adams, J., and Sargin, E. Deep neural net-
works for youtube recommendations. In RecSys, pp. 191–
198, 2016.

Crammer, K. and Singer, Y. On the algorithmic implementa-
tion of multiclass kernel-based vector machines. J. Mach.
Learn. Res., 2(Dec):265–292, 2001.

Garcin, C., Joly, A., Bonnet, P., Affouard, A., Lombardo,
J., Chouet, M., Servajean, M., Lorieul, T., and Salmon,
J. Pl@ntNet-300K: a plant image dataset with high label
In NeurIPS
ambiguity and a long-tailed distribution.
Datasets and Benchmarks 2021, 2021.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In CVPR, pp. 770–778,
2016.

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

He, X., Wang, P., and Cheng, J. K-nearest neighbors hashing.

Reed, W. J. The Pareto, Zipf and other power laws. Eco-

In CVPR, pp. 2839–2848, 2019.

nomics letters, 74(1):15–19, 2001.

Helgason, R. V., Kennington, J. L., and Lall, H. S. A
polynomially bounded algorithm for a singly constrained
quadratic program. Math. Program., 18(1):338–343,
1980.

Horn, G. V., Aodha, O. M., Song, Y., Cui, Y., Sun, C.,
Shepard, A., Adam, H., Perona, P., and Belongie, S. J.
The inaturalist species classification and detection dataset.
In CVPR, pp. 8769–8778, 2018.

Huang, G., Liu, Z., van der Maaten, L., and Weinberger,
K. Q. Densely connected convolutional networks. In
CVPR, pp. 2261–2269, 2017.

Iranmehr, A., Masnadi-Shirazi, H., and Vasconcelos, N.
Cost-sensitive support vector machines. Neurocomputing,
343:50–64, 2019.

Krizhevsky, A. Learning multiple layers of features from
tiny images. Master’s thesis, University of Toronto, 2009.

Lapin, M., Hein, M., and Schiele, B. Top-k multiclass SVM.

In NeurIPS, pp. 325–333, 2015.

Lapin, M., Hein, M., and Schiele, B. Loss functions for top-
k error: Analysis and insights. In CVPR, pp. 1468–1477,
2016.

Lapin, M., Hein, M., and Schiele, B. Analysis and optimiza-
tion of loss functions for multiclass, top-k, and multilabel
classification. IEEE Trans. Pattern Anal. Mach. Intell.,
40(7):1533–1554, 2017.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

Li, Y. and Shawe-Taylor, J. The SVM with uneven margins
and Chinese document categorization. In PACLIC, pp.
216–227, 2003.

Lin, T., Goyal, P., Girshick, R. B., He, K., and Doll´ar, P.
Focal loss for dense object detection. In ICCV, pp. 2999–
3007, 2017.

Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., and Yu, S. X.
Large-scale long-tailed recognition in an open world. In
CVPR, pp. 2537–2546, 2019.

Nesterov, Y. Smooth minimization of non-smooth functions.

Math. Program., 103(1):127–152, 2005.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. In NeurIPS, pp. 8026–8037, 2019.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale
visual recognition challenge. Int. J. Comput. Vision, 115
(3):211–252, 2015.

Scott, C. Calibrated asymmetric surrogate losses. Electron.

J. Stat., 6:958–992, 2012.

Shalev-Shwartz, S. and Zhang, T. Stochastic dual coordi-
nate ascent methods for regularized loss minimization. J.
Mach. Learn. Res., 14(Feb):567–599, 2013.

Usunier, N., Buffoni, D., and Gallinari, P. Ranking with or-
dered weighted pairwise classification. In ICML, volume
382, pp. 1057–1064, 2009.

Van Horn, G., Branson, S., Farrell, R., Haber, S., Barry,
J., Ipeirotis, P., Perona, P., and Belongie, S. Building a
bird recognition app and large scale dataset with citizen
scientists: The fine print in fine-grained dataset collection.
In CVPR, pp. 595–604, 2015.

Wang, F., Cheng, J., Liu, W., and Liu, H. Additive margin
softmax for face verification. IEEE Trans. Signal Process.
Lett., 25(7):926–930, 2018.

Wang, J., Tu, Z., Fu, J., Sebe, N., and Belongie, S. Guest ed-
itorial: Introduction to the special section on fine-grained
visual categorization. IEEE Trans. Pattern Anal. Mach.
Intell., 44(02):560–562, 2022.

Wang, X., Lian, L., Miao, Z., Liu, Z., and Yu, S. X. Long-
tailed recognition by routing diverse distribution-aware
experts. In ICLR, 2021.

Xie, S. M. and Ermon, S. Reparameterizable subset sam-
pling via continuous relaxations. In IJCAI, pp. 3919–
3925. ijcai.org, 2019.

Xie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H.,
Wei, W., and Pfister, T. Differentiable top-k with optimal
transport. In NeurIPS, 2020.

Yang, F. and Koyejo, S. On the consistency of top-k sur-
rogate losses. In ICML, volume 119, pp. 10727–10735,
2020.

Zhou, B., Cui, Q., Wei, X., and Chen, Z. BBN: bilateral-
branch network with cumulative learning for long-tailed
visual recognition. In CVPR, pp. 9716–9725, 2020.

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

A. Reminder on Top-K calibration

Here we provide some elements introduced by Yang & Koyejo (2020) on top-K calibration.

Definition A.1. (Yang & Koyejo, 2020, Definition 2.3). For a fixed K
is top-K preserving w.r.t. ˜y, denoted PK(y, ˜y), if for all k

[L],

[L], and given y

∈

RL and ˜y

∈

∈

RL, we say that y

∈
˜yk > topK+1(˜y) =
˜yk < topK(˜y) =

⇒

⇒

yk > topK+1(y)
yk < topK(y) .

(14)

(15)

The negation of this statement is

Pk(y, ˜y).

¬

denote the probability simplex of size L. For a score s

RL : (cid:80)

{

∈

π

k∈[L] πk = 1, πk

We let ∆L ≜
representing the conditional distribution of y given x, we write the conditional risk at x
ℓ(f ) ≜ E(x,y)∼P[ℓ(f (x), y)] for a scoring function f :
and the (integrated) risk as
R
ℓ|x(π) ≜ inf s∈RL
∗
are defined respectively by
R

Rℓ|x(s, π) (respectively by

∗
ℓ
R

0
}

Definition A.2. (Yang & Koyejo, 2020, Definition 2.4). A loss function ℓ : RL
π

∆L and all x

≥

:

∈ X
X →
≜ inf f :X →RL

× Y →

ℓ(f )) .

R

RL and π

∆L
Rℓ|x(s, π) = Ey|x∼π(ℓ(s, y))
as
RL. The associated Bayes risks

∈

∈

R is top-K calibrated if for all

∈

∈ X

inf

s∈RL:¬Pk(s,π) Rℓ|x(s, π) >

∗
ℓ|x(π) .
R

(16)

In other words, a loss is calibrated if the infimum can only be attained among top-K preserving vectors w.r.t. the conditional
probability distribution.

Theorem A.3. Suppose ℓ is a nonnegative top-K calibrated loss function. Then ℓ is top-K consistent, i.e., for any sequence
of measurable functions f (n) :

X →

RL, we have:
f (n)(cid:17)

(cid:16)

ℓ

R

∗
ℓ =
→ R

⇒ RℓK

(cid:16)

f (n)(cid:17)

∗
ℓK .
→ R

In their paper, Yang & Koyejo (2020) propose a slight modification of the multi-class hinge loss ℓK
top-K calibrated:

Hinge and show that it is

ℓK
Cal. Hinge(s, y) = (1 + topK+1(s)

sy)+ .

−

(17)

B. Proofs and technical lemmas

B.1. Proof of Proposition 3.3

Proof. We define
∈
arg topΣK(s) = arg maxz∈CK ⟨

C

z

K ≜

(cid:110)

k∈[L] zk = K, 0

RL, (cid:80)
1,
. Recall that Z is a standard normal random vector, i.e., Z
z, s
⟩

For s

[L]

zk

≤

≤

∈

∈

∀

k

(cid:111)
.

RL, one can check that

(0, IdL).

∼ N

•

K is a convex polytope and the multivariate normal has positive differentiable density. So, we can apply (Berthet
C
et al., 2020, Proposition 2.2). For that, it remains to determine the constant RCK and Mµ. First, RCK
.
∥
z
For simplicity, let us compute maxz∈CK ∥
max

≜ maxz∈CK ∥
z

2, i.e.,

(18)

∥

2

z
∥
∥
(cid:88)

k∈[L]

s.t.

zk = K,

k

∀

∈

[L], zk

∈

[0, 1].

(19)

Note that this corresponds to the well known quadratic knapsack problem. A numerical solution can be obtained, see
for instance (Helgason et al., 1980). To obtain our bound, note that for z

K one can check that

∈ C

2 =

z
∥

∥

(cid:88)

k∈[L]

z2
k ≤

(cid:88)

k∈[L]

zk

(since

k

∀

∈

[L], zk

∈

[0, 1]).

Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

(a) B = 1

(b) B = 5

(c) B = 10

(d) B = 50

(e) B = 100

(f) B = 1

(g) B = 5

(h) B = 10

(i) B = 50

(j) B = 100

Figure 4. Impact of the sampling parameter B on the loss ℓK,1,B

Noised bal. (top part: K = 1, bottom part: K = 2)

2
z
Hence, we have
∀
∥
(1, . . . , 1, 0, . . . , 0)⊤
Proposition 2.2) this guarantees that topΣK,ϵ is √K-Lipschitz.

K,
∈ C
RL with K non-zeros values, yielding maxz∈CK ∥
z
∈

K. Now, one can check that this equality is achieved when choosing z =
= √K. Following (Berthet et al., 2020,

z
∥

≤

∥

Let us show now that Mµ ≜
computed as:

(cid:113)

EZ[

2] = √L with ν(Z) = 1
Zν(Z)
∥

2 ∥

Z

∥∇

2 and Z
∥

∼ N

(0, IdL). Hence, Mµ can be

(cid:114)

EZ

(cid:114)

EZ

(cid:104)

(cid:104)

Mµ =

=

= √L ,

2(cid:105)
Zν(Z)
∥

∥∇

2(cid:105)
∥

Z
∥

(0, IdL). Following (Berthet et al., 2020, Proposition 2.2) this guarantees

∼ N

where the last equality comes from Z
√
that

-Lipschitz.

topΣK,ϵ is

KL
ϵ

∇

• The last bullet of our proposition comes derives now directly from of (Berthet et al., 2020, Proposition 2.3).

B.2. Proof of Proposition 3.5

Proof.

• From the triangle inequality and Proposition 3.3 we get for any s, s′

RL:

∈

(cid:13)
(cid:13)

∇

topK,ϵ(s)

− ∇

topK,ϵ(s′)(cid:13)

(cid:13) =

[
∇
∥

topΣK,ϵ(s)
topΣK,ϵ(s)

− ∇

topΣK,ϵ(s′)]
topΣK,ϵ(s′)

[
∇

topΣK−1,ϵ(s)
topΣK−1,ϵ(s)

−
+

≤ ∥∇
(cid:32)
2

≤

√KL
ϵ

+ 2

− ∇
(cid:112)(K

(cid:33)

1)L

−
ϵ

∥

∥∇

s′

s
∥

−

∥ ≤

4

√KL
ϵ

s
∥

−

− ∇

− ∇
s′

∥

topΣK−1,ϵ(s′)]
∥
topΣK−1,ϵ(s′)
∥

• Using the notation from (Berthet et al., 2020, Appendix A), with F (s) = topΣK(s) and Fϵ(s) = topΣK,ϵ(s), we get

the following bounds:

topΣK,ϵ(s)
−
topΣK−1,ϵ(s)

0

0

≤

≤

topΣK(s)

ϵ
·
topΣK−1(s)

≤

−

topΣK,1(0)

ϵ

·

≤

topΣK−1,1(0) .

(20)

(21)

s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

(a) ϵ = 0.01

(b) ϵ = 0.1

(c) ϵ = 1

(d) ϵ = 10

(e) ϵ = 100

(f) ϵ = 0.01

(g) ϵ = 0.1

(h) ϵ = 1

(i) ϵ = 10

(j) ϵ = 100

Figure 5. Impact of the smoothing parameter ϵ on the loss ℓK,ϵ,50

Noised bal. (top part: K = 1, bottom part: K = 2)

Additionally, using the maximal inequality for i.i.d. Gaussian variables, see for instance (Boucheron et al., 2013,
Section 2.5), leads to:

topΣK,1(0) = E





(cid:88)

k∈[K]



Z(k)



≤

KE(cid:2)Z(1)

(cid:3)

≤

K(cid:112)2 log L

Subtracting (21) to (20), and reminding that topK(s) = topΣK(s)
topΣK,ϵ(s)

topΣK−1,ϵ(s)) gives:

−

topΣK−1(s) (and similarly topK,ϵ(s) =

−

(K

ϵ

−

·

−

1)(cid:112)2 log L

topK,ϵ(s)

topK(s)

ϵ

·

≤

−

≤

K(cid:112)2 log L ,

thus leading to:

with CK,L = K√2 log L.

B.3. Proof of Proposition 3.7

topK,ϵ(s)

|

topK(s)

ϵ

·

| ≤

−

CK,L ,

(22)

Proof.

• First, note that s
(cid:55)→
differentiable wherever ψ : s

ℓK,ϵ
Noised bal.(s, y) is continuous as a composition and sum of continuous functions. It is
sy is non-zero. From Definition 3.4 and Proposition 3.3 we get

1 + topK+1,ϵ(s)

sψ(s) = E[arg topΣK+1(s + ϵZ)]

(cid:55)→

−

δy. The formula of the gradient follows from the chain rule.

∇

−

C. Illustrations of the various losses encountered

In Figures 1 and 6, we provide a visualization of the loss landscapes for respectively K = 1 and K = 2 with L = 3 labels.
With L = 3 labels, we display the visualization as level-sets restricted to a rescaled simplex: 2 . . . ∆3. Moreover, we
have min/max rescaled all the losses so that they fully range the interval [0, 1]. Note that as we are mainly interested in
minimizing the losses, this post-processing would not modify the learned classifiers.

We provide also two additional figures illustrating the impact on our loss of the two main parameters: ϵ and B.

D. Additional experiments

CIFAR-100: Table 7 reports the top-1 accuracy obtained by the models corresponding to the results of Table 4. Hence, we
show here a misspecified case: we optimized our balanced loss for K = 5, seeking to optimize top-5 accuracy, which is

s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

(a) Top-K:
ℓ = ℓK .

(b) Cross-entropy:
ℓ = ℓCE.

(c) Multi-class hinge:
ℓ = ℓK

Hinge.

(d) Calibrated hinge:
ℓ = ℓK
Cal. Hinge.

(e) Convexified hinge:
ℓ = ℓK
CVXHinge.

(f) Smoothed hinge
ℓK,0.1
Smoothed Hinge.

(g) Smoothed hinge
ℓK,1
Smoothed Hinge.

(h) Noised balanced:
ℓK,0.3,30
Noised bal..

(i) Noised balanced:
ℓK,1,30
Noised bal..

(j) Noised imbalanced:
ℓK,1,30,5
Noised imbal..

Figure 6. Level sets of the function s
ℓ(s, y) for different losses described in Table 1, for L = 3 classes, K = 1 and a true label y = 2
(corresponding to the upper corner of the triangles). For visualization the loss are rescaled between 0 and 1, and the level sets are restricted
to vector s
∆3. The losses have been harmonized to display a margin equal to 1. For our proposed loss, we have averaged the level
sets over 100 replications to avoid meshing artifacts.

(cid:55)→

∈

2

·

Table 7. top-1 accuracy cifar100

Label noise

ℓK
CE

ℓ5,1.0
Smoothed Hinge

ℓ5,0.2,10
Noised bal.

0.0
0.1
0.2
0.3
0.4

76.6
71.0
68.1
65.5
61.8

0.1
0.2
0.1
0.3
0.4

±
±
±
±
±

69.2
71.2
71.3
70.8
70.6

0.1
0.4
0.3
0.6
0.2

±
±
±
±
±

68.7
68.3
69.4
69.3
69.1

±
±
±
±
±

0.3
0.5
0.5
0.3
0.4

reported in Table 4, but report top-1 information. Table 7 shows that when there is no label noise, as expected cross entropy
gives better top-1 accuracy than our top-5 loss. When the label noise is high, however, our loss leads to better top-1 accuracy.

Pl@ntNet-300K: Table 8 reports the top-K accuracy obtained by the models corresponding to the results of Table 5. The
top-K accuracies are much higher than the macro-average top-K accuracies reported in Table 5 because of the long-tailed
distribution of Pl@ntNet-300K. The models perform well on classes with a lot of examples which leads to high top-K
accuracy. However, they struggle on classes with a small number of examples (which is the majority of classes, see (Garcin
et al., 2021)). Thus, for Pl@ntNet-300K top-K accuracy is not very relevant as it mainly reflects the performance of the few
classes with a lot of images, ignoring the performance on challenging classes (the one with few labels) (Affouard et al.,
2017). We report it for completeness and make a few comments: First, our balanced noise loss gives better top-K accuracies
than the cross entropy or the loss from Berrada et al. (2018) for all K. Then, ℓK,ϵ,B,my
Noised bal. produce the best
top-1 accuracy, respectively 81.0 and 80.8.

Noised imbal. and ℓK,ϵ,B

Table 8. regular top-K accuracy corresponding to the models in Table 5

ℓCE

ℓK,τ
Smoothed Hinge

ℓK,ϵ,B
Noised bal.

focal

LDAM ℓK,ϵ,B,my
Noised imbal.

80.1
93.1
95.7
97.5

0.1
0.0
0.0
0.0

±
±
±
±

79.8
93.2
95.0
95.5

±
±
±
±

0.0
0.0
0.1
0.0

80.8
93.5
95.8
97.5

0.1
0.1
0.0
0.0

±
±
±
±

79.8
93.5
96.0
97.7

0.1
0.0
0.0
0.0

±
±
±
±

79.6
92.3
95.2
97.2

0.1
0.1
0.2
0.1

±
±
±
±

81.0
93.5
95.8
97.5

0.1
0.1
0.1
0.0

±
±
±
±

K

1
3
5
10

s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000s=(2,0,0)>s=(0,2,0)>s=(0,0,2)>0.0000.1430.2860.4290.5710.7140.8571.000Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification

Table 9. ImageNet test top-K accuracy, ResNet-18.

K

1
5

ℓCE

ℓK,0.1
Smoothed Hinge

ℓK,0.5,10
Noised bal.

72.24
90.60

±
±

0.15
0.05

71.43
90.71

0.14
0.06

±
±

72.46
90.52

0.15
0.07

±
±

However, we insist that in such an imbalanced setting the macro-average top-K accuracy reported in Table 5 is much more
informative than regular top-K accuracy. We see significant differences between the losses in Table 5 which are hidden in
Table 8 because of the extreme class imbalance.
ImageNet: We test ℓK,ϵ,B
Noised bal. on ImageNet. We follow the same procedure as Berrada et al. (2018): we train a ResNet-18
with SGD for 120 epochs with a batch size of 120 epochs. The learning rate is decayed by ten at epoch 30, 60 and 90. For
ℓK,τ
Smoothed Hinge, the smoothing parameter τ is set to 0.1, the weight decay parameter to 0.000025 and the initial learning rate
to 1.0, as in Berrada et al. (2018). For ℓCE, the weight decay parameter is set to 0.0001 and the initial learning rate to 0.1,
following Berrada et al. (2018). For ℓK,ϵ,B
Noised bal., we use the same validation set as in Berrada et al. (2018) to set ϵ to 0.5, the
weight decay parameter to 0.00015 and the initial learning rate to 0.1. B is set to 10. We optimize all losses for K = 1 and
K = 5. We perform early stopping based on best top-K accuracy on the validation set and report the results on the test set
(the official validation set of ImageNet) in Table 9 (3 seeds, 95% confidence interval). In the presence of low label noise,
with an important number of training examples per class and for a nearly balanced dataset, all three losses give similar
results. This is in contrast with Table 4 and Table 5, where significant differences appear between the different losses in the
context of label noise or class imbalance.

E. Hyperparameter tuning

Balanced case: For both experiments on CIFAR-100 and ImageNet, we follow the same learning strategy and use the same
hyperparameters for ℓK,τ
Noised bal., we refer the reader for the choice of ϵ and B
respectively to Section 4.1 and Section 4.2: ϵ should be set to a sufficiently large value so that learning occurs and B should
be set to a small value for computational efficiency.

Smoothed Hinge than Berrada et al. (2018). For ℓK,ϵ,B

for the parameter γ
0.5, 1.0, 2.0, 5.0
Imbalanced case: For our experiments on imbalanced datasets, we use the grid
}
{
LDAM and ℓK,ϵ,B,max my
Smoothed Hinge. For ℓmax my
0.1, 1.0
, the hyperparameter
of the focal loss and
}
max my is searched in the grid
and we find in our experiments that the best working values of max my
happen to be the same for both losses. For the scaling constant for the scores, we find that 30 and 50 are good default values
for respectively ℓmax my

for the parameter τ of ℓK,τ
0.2, 0.3, 0.4, 0.5
}
{

. Finally, for ℓK,ϵ,B,max my

, ϵ is searched in the grid

LDAM and ℓK,ϵ,B,max my

Noised imbal.

Noised imbal.

Noised imbal.

{

0.01, 0.05, 0.1
{

.
}

