Fast Rate Learning in Stochastic First Price Bidding
Juliette Achddou, Olivier Cappé, Aurélien Garivier

To cite this version:

Juliette Achddou, Olivier Cappé, Aurélien Garivier. Fast Rate Learning in Stochastic First Price
Bidding. ACML 2021 - Proceedings of Machine Learning Research 157, 2021, Nov 2021, SIngapore,
Singapore. ￿hal-03277164v2￿

HAL Id: hal-03277164

https://hal.science/hal-03277164v2

Submitted on 19 Nov 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Proceedings of Machine Learning Research 157, 2021

ACML 2021

Fast Rate Learning in Stochastic First Price Bidding

Juliette Achddou
DIENS, INRIA, Universit´e PSL, 1000mercis Group

Olivier Capp´e
DIENS, CNRS, INRIA, Universit´e PSL,

Aur´elien Garivier
UMPA,CNRS, INRIA, ENS Lyon

Editors: Vineeth N Balasubramanian and Ivor Tsang

juliette.achdou@gmail.com

olivier.cappe@cnrs.fr

aurelien.garivier@ens-lyon.fr

Abstract
First-price auctions have largely replaced traditional bidding approaches based on Vickrey
auctions in programmatic advertising. As far as learning is concerned, ﬁrst-price auctions
are more challenging because the optimal bidding strategy does not only depend on the
value of the item but also requires some knowledge of the other bids. They have already
given rise to several works in sequential learning, many of which consider models for which
the value of the buyer or the opponents’ maximal bid is chosen in an adversarial manner.
T with
Even in the simplest settings, this gives rise to algorithms whose regret grows as
respect to the time horizon T . Focusing on the case where the buyer plays against a
stationary stochastic environment, we show how to achieve signiﬁcantly lower regret: when
the opponents’ maximal bid distribution is known we provide an algorithm whose regret
can be as low as log2(T ); in the case where the distribution must be learnt sequentially,
a generalization of this algorithm can achieve T 1/3+(cid:15) regret, for any (cid:15) > 0. To obtain
these results, we introduce two novel ideas that can be of interest in their own right. First,
by transposing results obtained in the posted price setting, we provide conditions under
which the ﬁrst-price bidding utility is locally quadratic around its optimum. Second, we
leverage the observation that, on small sub-intervals, the concentration of the variations of
the empirical distribution function may be controlled more accurately than by using the
classical Dvoretzky-Kiefer-Wolfowitz inequality. Numerical simulations conﬁrm that our
algorithms converge much faster than alternatives proposed in the literature for various bid
distributions, including for bids collected on an actual programmatic advertising platform.
Keywords: multi-armed bandits; sequential bidding; auctions

√

1. Introduction

We consider the problem of setting a bid in repeated ﬁrst-price auctions. First-price auc-
tions are widely used in practice, partly because they constitute the most natural and
simple type of auctions. In particular, they have been largely adopted in the ﬁeld of pro-
grammatic advertising, where they have progressively replaced second-price auctions (Sluis.,
2017; Slefo., 2019). This recent transition took place for various reasons. First, whereas
second-price auctions have the advantage of being dominant-strategy incentive-compatible
and hence allow for simple bidding strategies (Vickrey, 1961), they were made obsolete by
the widespread use of header bidding, a technology that puts diﬀerent ad-exchange plat-

© 2021 J. Achddou, O. Capp´e & A. Garivier.

Achddou Capp´e Garivier

forms in competition. With this technology, every participating ad-exchange has to provide
the winning bid of the auction organized on its platform; a second-level auction is then or-
ganized between all the winners to determine which bidder earns the right of displaying its
banner. Second price auctions would hence jeopardize the fairness of the attribution of the
placement at sale with header bidding. Second, sellers have beneﬁted from the transition,
since many bidders continued to bid as in second-price auctions and despite the automated
implementation of so-called bid shading by demand-side platforms, meant to adjust their
bids to this new situation (Sluis., 2019). The transition to ﬁrst price auctions raises ques-
tions for advertisers who need new bidding strategies. In general, bidders participating in
auctions in the context of programmatic advertising do not know the bidding strategies
of the other contestants in advance, or anything about the valuations that other bidders
attribute to the advertisement slot. Not only do they have to learn other bidders’ behavior
on the go, but they also need to understand how valuable the placement is for their own use
(how many clicks or actions the display of their ad on this placement will lead to), which is
usually not the same for all bidders.

In this work, we model the problem faced by a single bidder in repeated stochastic ﬁrst-
price auctions, that is, when the contestants’ bids are drawn from a stationary distribution.
We consider that the learner’s bids will not inﬂuence the others’ bidding strategies. This
approximation is sensible in contexts where the major part of the stakeholders do not have
an elaborate bidding strategy. More precisely, many stakeholders never modify their bids
or do so at a very low frequency. Moreover, the poll of bidders is very large and each bidder
only participates in a fraction of the auctions, which argues in favor of the assumption that
the inﬂuence of one bidder on the rest of the participants can be neglected.

Model We consider that similar items are sold in T sequential ﬁrst price auctions. For
t = 1, . . . , T , the auction mechanism unfolds in the following way. First, the bidder submits
her bid Bt for the item that is of unknown value Vt. The other players submit their bids,
the maximum of which is called Mt. If Mt ≤ Bt (which includes the case of ties), the bidder
observes and receives Vt and pays Bt. If Bt < Mt, the bidder loses the auction and does
not observe Vt.

We make the following additional assumptions: {Vt}t≥1 are independent and identically
distributed random variables in the unit interval [0, 1]; their expectation is denoted by
v := E(Vt). The {Mt}t≥1 are independent and identically distributed random variables in
the unit interval [0, 1] with a cumulative distribution function (CDF) F , independent from
the {Vt}t≥1. When applicable, we denote by f = F (cid:48) the associated probability density
function.

Due to the stochastic nature of the setting, we study the ﬁrst-price utility of the bidder:

Uv,F (b) := E(cid:2)(Vt − b)1{Mt ≤ b}(cid:3) = (v − b)F (b). The (pseudo-)regret is deﬁned as

Rv,F

T = T max
b∈[0,1]

Uv,F (b) −

T
(cid:88)

t=1

E[Uv,F (Bt)] .

v,F = max (cid:8) arg maxb∈[0,1] Uv,f (b)(cid:9) the (highest) optimal bid. In the rest of
We denote by b∗
the paper, we will abuse notation and speak about regret although rigorously this quantity
should be termed pseudo-regret. Note that the outer max is required as the utility may
in that case, we deﬁne the optimal bid as
have multiple maxima (see Section 2 below):

Fast Rate Learning in Stochastic First Price Bidding

the one that has the largest winning rate. In the sequel, we exclude the particular case
where F (b∗
v,F ) = 0, since in this hopeless situation the contestants always bid above the
value of the item and the best strategy is not to bid at all (Bt ≡ 0): we thus assume that
F (b∗

v,F ) > 0.
In Section 3, we will ﬁrst assume that F is known to the learner. This setting bears
some similarities with the case of second-price auctions considered by (Weed et al., 2016;
Achddou et al., 2021): the truthfulness of second-price auctions makes it suﬃcient for the
bidder to learn the value of v and the valuation of the item is the only parameter to estimate
in that case. However, an important feature of the second-price auction mechanism is that
the utility of the bidder is quadratic in v under very mild assumptions on the bidding
distribution F . In the case of ﬁrst-price auctions, the utility is no longer guaranteed to be
unimodal, neither is the optimal bid b∗

v,F a regular function of v.

We treat the case, in Section 4, where the CDF F of the opponents’ maximal bid is
initially unknown to the learner, assuming that the maximal bid Mt is observed for each
auction. Note that in this more realistic setting, the bidder could not infer the optimal bid
b∗
v,F even if she had perfect knowledge of the item value v. The bidder consequently needs to
estimate F and v simultaneously, which makes it a clearly harder task. This second setting
bears some similarities with the task of ﬁxing a price in the posted price problem (Huang
et al., 2018; Kleinberg and Leighton, 2003; Bubeck et al., 2017; Cesa-Bianchi et al., 2019),
in which a seller needs to estimate the distribution of the valuations of buyers, in order
to set the optimal price in terms of her revenue. However, in contrast to the posted-price
setting, there is an additional unknown parameter v that also impacts the utility function.
In both of these settings, the learner is faced with a structured continuously-armed ban-
dit problem with censored feedback. Indeed, the bidder only observes the reward associated
with the chosen bid, but she observes the value only when she wins. This introduces a spe-
ciﬁc exploitation/exploration dilemma, where exploitation is achieved by bidding close to
one of the optimal bids but exploration requires that the bids are not set too low. This
structure seems to call for algorithms that bid above the optimal bid with high probability,
as in (Weed et al., 2016; Achddou et al., 2021) for the second-price case, but we will see in
the following that it is not necessarily true.

Related Works A major line of research in the ﬁeld of online learning in repeated auc-
tions is devoted to ﬁxing a reserve price for second-price auctions or a selling price in posted
price auctions, see (Nedelec et al., 2020) for a general survey. In the posted price setting,
arbitrarily bad distributions of bids give rise to very hard optimization problems (Rough-
garden and Schrijvers, 2016). That is why regularity assumptions are often used, like e.g.
the monotonic hazard rate (MHR) condition. Most notably, Huang et al. (2018); Cole and
Roughgarden (2014); Dhangwatnotai et al. (2015) use this assumption to bound the sample
complexity of ﬁnding the monopoly price. Regarding online learning in the posted price
setting, Kleinberg and Leighton (2003) and Cesa-Bianchi et al. (2019) introduce algorithms
for the stochastic case, respectively in the cases where the distribution of the prices are
continuous and discrete. Bubeck et al. (2017) study the adversarial counterpart. Blum
et al. (2004); Cesa-Bianchi et al. (2014) study online strategies that aim at setting the opti-
mal reserve price in second-price auctions while learning the distribution of the buyer’s bids.
Cesa-Bianchi et al. (2014) assume that bidders are symmetric, but that the bids distribution

Achddou Capp´e Garivier

is not necessarily MHR. They introduce an optimistic algorithm based on two ideas. Firstly
they observe that exploitation is achieved by submitting a price smaller than the optimal
reserve price, and secondly they use the fact that the utility can be bounded in inﬁnite
norm, thanks to the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality (Massart, 1990).

The problem of learning in repeated auctions from the point of view of the buyer was
originally addressed in the setting of second-price auctions. For the stochastic setting, Weed
et al. (2016) propose an algorithm that overbids with high probability, and that is shown
to have a regret of the order of log2 T under mild assumptions on the distribution of the
bids. They also provide algorithms for the adversarial case, that have a regret scaling in
√
T . Achddou et al. (2021) extend their work by proposing tighter optimistic strategies that
show better worst case performances. They also analyze non-overbidding strategies, proving
that such strategies can perform well on a large class of second-price auctions instances.
Flajolet and Jaillet (2017) consider the contextual set-up where the value associated to an
item is linear with respect to a context vector associated to the item, and revealed before
each action.

Learning in repeated stochastic ﬁrst price auctions is a diﬃcult problem that has given
rise to a number of very diﬀerent though equally interesting modelizations. Feng et al.
(2020) consider auctions in which the values of all the bidders are revealed as a context
before each turn, proving that the bids of bidders who use no regret contextual learning
strategies in ﬁrst price auctions converge to Bayes Nash equilibria. Han et al. (2020) also
consider the case where the values are assumed to be revealed as an element of context
before each auction takes place and the highest bid among others’ bids is only shown to the
learner when she loses. This setting interestingly introduces a censoring structure that is
opposed to the one we consider: in this context, exploitation is achieved by not bidding too
high. Han et al. (2020) provide new algorithms for this setting which have a regret of the
order of
T . A setting somewhat closer to ours is studied by Feng et al. (2018). This work
deals with the setting of a bid in an adversarial fashion, when the other bids are revealed
at each time step and the value is revealed only upon winning an auction. However the
proposed algorithm is based on a discretization of the bidding space which relies on the
prior knowledge of the smallest gap between two distinct bids. With this knowledge, the
proposed algorithm achieves an adversarial regret of the order of

T .

√

√

Contributions The highlights of Sections 2–4 are the following. In Section 2 we stress
the hardness of the ﬁrst-price bid optimization task, showing that in general it necessarily
leads to high minimax regret rates. We however transplant ideas introduced in the case of
posted prices to exhibit natural assumptions ensuring that the ﬁrst-price utility is smooth,
paving the way for faster learning. In Section 3, we consider the case where the learner can
assume knowledge of F and propose a new UCB-type algorithm called UCBid1 for learning
the optimal bid with low regret. UCBid1 is adaptive to the diﬃculty of the problem in
T ) in diﬃcult cases, but comes down to O(log2 T ) when the
the sense that its regret is O(
ﬁrst-price utility is smooth. We also provide lower-bound results suggesting that these rates
are nearly optimal. In Section 4, we consider the more general setting where F is initially
unknown to the learner. By leveraging the structure of the ﬁrst-price bidding problem,
we are able to propose an algorithm, termed UCBid1+, which is a direct generalization of
UCBid1. Interestingly, this algorithm is not optimistic anymore: it does not submit bids

√

Fast Rate Learning in Stochastic First Price Bidding

√

which are with high probability above the (unknown) optimal bid. However, it can still be
proved to achieve a regret rate of O(
T ) in the most general case and, more importantly, a
regret rate upper bounded by O(T 1/3+(cid:15)) for every (cid:15) > 0 when the ﬁrst-price utility satisﬁes
the regularity assumptions mentioned in Section 2. The latter result relies on an original
proof notably based on the use of a local concentration inequality on the empirical CDF.
All the proofs corresponding to these three sections are presented in appendix. Section 5
closes the paper with numerical simulations where we compare the proposed algorithms
with continuously-armed bandit strategies and tailored strategies from the literature, both
using simulated and real-world data.

2. Properties of Stochastic First-Price auctions

Figure 1: An example with two maximizers

There are two important diﬃculties with ﬁrst price auctions. The ﬁrst one lies in
the fact that the utility can have multiple maximizers (or multiple modes with arbitrarily
close values) and thus lead to arbitrarily hard optimization problems. To illustrate this, we
provide in Figure 1 an example of value v and discrete distribution, supported on two values
m0, m1, that leads to a utility having two global maximizers. Note that the utility Uv,F (b)
is the area of the rectangle with vertices (b, F (b)), (b, 0), (v, F (b)), (v, 0). This observation
makes it easy to build examples with multiple maxima. Discrete examples like the one in
Figure 1 are intuitive because the utility is decreasing between two successive points of the
support, but there also exist similar cases with continuous distributions (see for example
Appendix A.3). This example also shows that there exist combinations of bids distributions
and values for which the utility is not regular around its maximum.

The second diﬃculty comes from the fact that the mapping from v to the largest maxi-
mizer, ψF : v (cid:55)→ b∗
v,F may also lack regularity. Indeed, keeping the distribution in Figure 1
but setting the value to v(cid:48) = v + ∆, with a positive ∆ (resp. to v(cid:48) = −∆) yields that the
set of maximizers is {m1} (resp. {m0}). Even though ψF can not be proved to be regular
in all generality, it always holds that ψF is increasing. This is intuitive: the optimal bid
grows with the private valuation.

Lemma 1 For any cumulative distribution F , ψF : v (cid:55)→ b∗

v,F is non decreasing.

Achddou Capp´e Garivier

The two aforementioned diﬃculties contribute to making the problem at hand particu-
larly hard. In the following theorem, we show that any algorithm is bound to have a worst
case regret growing at least like

T .

√

Theorem 2 Let C denote the class of cumulative distribution functions on [0, 1]. Any
strategy, whether it assumes knowledge of F or not, must satisfy

lim inf
T →∞

maxv∈[0,1],F ∈C Rv,F
T
√
T

≥

1
64

,

Theorem 2 corresponds to Theorem 6 in Han et al. (2020). For completeness, we prove it
in Appendix B. The proof relies on speciﬁcally hard instances of CDF that are perturbations
of the example of Figure 1. It illustrates the complexity of bidding in ﬁrst-price auctions,
when F and v are arbitrary. This complexity stems from speciﬁcally hard instances of F
and v. We present a natural assumption that avoids these pathological cases.

Assumption 1 F is continuously diﬀerentiable and is strictly log-concave.

This assumption is reminiscent of the monotonic hazard rate (MHR) condition (see e.g.
Cole and Roughgarden (2014)), that appears in the analysis of the posted price prob-
lem. While MHR requires f /(1 − F ) to be increasing, Assumption 1 requires f /F to be
decreasing.
In particular, this condition is satisﬁed by truncated exponentials and Beta
distributions with f of the form Cxα−1 where α > 1 or C(1 − x)β−1 where β > 1, or Beta
distributions in which α + β < αβ (see Lemma 15 in Appendix A). Assumption 1 plays
roughly the same role for ﬁrst price auctions than MHR for the posted price setting. It guar-
antees in particular that there is a unique optimal bid. Note that if F satisﬁes Assumption
1, F is increasing, and admits an inverse which we denote by F −1.

Lemma 3 Under Assumption 1, for any v ∈ [0, 1] the mapping b (cid:55)→ Uv,F (b) has a unique
maximizer.

As does the MHR assumption for the posted-prices setting, Assumption 1 ensures that
the utility is strictly concave when expressed as a function of the quantile q = F (b) associated
with the bid b. Another important consequence of Assumption 1 is that the mapping from
v to the optimal bid b∗

v,F is guaranteed to be regular.

Lemma 4 If Assumption 1 is satisﬁed and f is continuously diﬀerentiable, then ψF : v (cid:55)→
b∗
v,F is Lipschitz continuous with a Lipschitz constant 1.

Indeed, if f is continuously diﬀerentiable and if f does not vanish on [0, 1[ (which is
implied by Assumption 1), ψF is invertible and it inverse φF writes φF : b (cid:55)→ b + F (b)/f (b).
Assumption 1 ensures that φF admits a derivative that is lower-bounded by φ(cid:48)

F (b) > 1.

Assumption 1 also implies the important property that the probability of winning the

auction at the optimal bid F (b∗

v,F ) cannot be arbitrarily small when compared to F (v).

Lemma 5 If Assumption 1 is satisﬁed, then

F (b∗

v,F ) ≥

F (v)
e

.

Fast Rate Learning in Stochastic First Price Bidding

We conclude this section by additional properties that are essential for obtaining low re-
gret rates: the utility is second-order regular, when expressed as a function of the quantiles.
Let Wv,F denote the utility expressed as a function of the quantile, Wv,F : q (cid:55)→ Uv,F (F −1(q)),
and let q∗
v,F ) be its maximizer. Under Assumption 1, the deviations of Wv,F from
its maximum are lower-bounded by a quadratic function.

v,F := F (b∗

Lemma 6 Under Assumption 1, for any q ∈ [0, 1],

Wv,F (q∗

v,F ) − Wv,F (q) ≥

1
4

(q∗

v,F − q)2Wv,F (q∗

v,F ).

This property relies, among other arguments, on the observation that

W (cid:48)

v,F (q) = v − φF (F −1(q)) = φF (F −1(q∗

v,F )) − φF (F −1(q))

and that φ(cid:48)
F is lower-bounded by 1 under Assumption 1 (see discussion of Lemma 4 above).
Similarly, in order to obtain a quadratic lower bound on Wv,F (q), one needs to show that
φ(cid:48)
F may be upper bounded. This is the purpose of the following regularity assumption.

Assumption 2 F admits a density f such that cf < f (b) < Cf , ∀b ∈ [b∗
v,F + ∆]
and φF : b (cid:55)→ b + F (b)/f (b) admits a derivative that is upper-bounded by a constant λ ∈ R+
on [b∗

v,F − ∆, b∗

v,F , b∗

v,F + ∆].

Assumption 2 holds, in particular, when F is twice diﬀerentiable, f is lower-bounded
by a positive constant and f (cid:48) is upper-bounded by a positive constant on a neighborhood
of b∗
v,F . Note that in the ﬁeld of auction theory, it is common to assume that the utility is
approximately quadratic around the maximum, which is a far stronger assumption, as stated
in (Nedelec et al., 2020) (see (Kleinberg and Leighton, 2003) for example). Assumption 2
implies the following lower bound for the utility expressed as a function of the quantiles.

Lemma 7 Under Assumption 2, for any q ∈ [q∗

v,F , q∗

v,F + Cf ∆],

Wv,F (q∗

v,F ) − Wv,F (q) ≤

1
cf

λ(q∗

v,F − q)2.

3. Known Bid Distribution

In this section we address the online learning task in the setting where the bid distribution
F is known to the learner from the start. In order to set the bid Bt at time t, the available
information consists in Nt := (cid:80)t−1
s=1 1{Ms ≤ Bs}, the number of observed values before time
s=1 Vs1(Ms ≤ Bs) the average of those values. Let (cid:15)t := (cid:112)γ log(t − 1)/2Nt
(cid:80)t−1
t, and ˆVt := 1
Nt
denote a conﬁdence bonus depending on a parameter γ > 0 to be speciﬁed below.

Algorithm 1 (UCBid1) Initially set B1 = 1 and, for t ≥ 2, bid according to

(cid:110)

Bt = max

(cid:111)
( ˆVt + (cid:15)t − b)F (b)
.

arg max
b∈[0,1]

Achddou Capp´e Garivier

This algorithm, strongly inspired by UCB-like methods designed for second-price auc-
tions by Weed et al. (2016); Achddou et al. (2021), is a natural approach to ﬁrst-price
auctions. The idea behind this kind of method is that one should rather overestimate the
optimal bid, so as to guarantee a suﬃcient rate of observation. As an UCB-like algorithm,
UCBid1 submits an (high probability) upper bound ψF ( ˆVt + (cid:15)t) of b∗
v,F , thanks to Lemma
1 and since ψF is non decreasing. In practice, the algorithm requires a line search at each
step as the utility maximization task is usually non-trivial, as discussed in Section 1.

In the most general case, the regret of UCBid1 admits an upper bound of the order of

(cid:112)T log(T ).

Theorem 8 When γ > 1, the regret of UCBid1 is upper-bounded as

Rv,F

T ≤

√

2γ
F (b∗
v,F )

(cid:112)T log T + O(log T ) .

√

Note that

T is the order of the regret of UCB strategies designed for second-price
auctions in the absence of regularity assumptions on F (Weed et al., 2016). However, under
the regularity assumptions introduced in Section 2, it is possible to achieve faster learning
rates.

Theorem 9 If F satisﬁes Assumption 1 and 2, then, for any γ > 1,

Rv,F

T ≤

2γλC2
f
v,F )cf

F (b∗

log2(T ) + O(log T ).

The log2(T ) rate of the regret comes from the Lipschitz nature of ψF , that makes it
possible to bound the gap Bt − b∗
v,F , and from the obervation that the utility is quadratic
around its optimum. This explains the similarity with the order of the regret of UCBID
in (Weed et al., 2016), when the distribution of the bids admits a bounded density. In-
deed, in second-price-auctions, when the distribution of the bids admits a bounded density,
the utility is locally quadratic around its maximum and the equivalent of ψF is the iden-
tity, meaning that the optimal bid is just the value v of the item. The presence of the
multiplicative constant 1/F (b∗
v,F ) is also expected: it is the average time between two suc-
cessive observations under the optimal policy. This similarity between the structures of
second and ﬁrst price auctions under Assumptions 1 and 2 also suggest that the constants
in the regret may be further improved by using a tighter conﬁdence interval for v based on
Kullback-Leibler divergence, proceeding as in (Achddou et al., 2021).

Under Assumption 1, the regret of any optimistic strategy can be shown to satisfy the

following lower bound.

Theorem 10 Consider all environments where Vt follows a Bernoulli distribution with
expectation v and F satisﬁes Assumption 1 and is such that φ(cid:48) ≤ λ, and there exists cf
and Cf such that 0 < cf < f (b) < Cf , ∀b ∈ [0, 1]. If a strategy is such that, for all such
environments, Rv,F
T ≤ O(T a), for all a > 0, and there exists γ > 0 such that P(Bt < b∗) <
t−γ, then this strategy must satisfy:

lim inf
T →∞

Rv,F
T
log T

≥ c2

f λ2

(cid:18) v(1 − v)(v − b∗

v,F )

32

(cid:19)

.

Fast Rate Learning in Stochastic First Price Bidding

The ﬁrst assumption, Rv,F

T ≤ O(T a), is a common consistency constraint that is used
when proving the lower bound of Lai and Robbins (1985) in the well-established theory of
multi-armed bandits. The second assumption, P(Bt < v) < t−γ, restricts the validity of the
lower bound to the class of strategies that overbid with high probability. By construction,
this assumption is satisﬁed for UCBid1.

Note that there is a gap between the rates log T in the lower bound (Theorem 10) and
log2 T in the performance bound of UCBid1 (Theorem 9), which we believe is mostly due
to the mathematical diﬃculty of the analysis. The v(1 − v) factor may be interpreted as
an upper bound on the variance of the value distribution with expectation v. Theorem 10
displays a dependence on v of the order of v2 when v tends to 0. However this has to be put
in perspective with the fact that the value of the optimal utility Uv,F (b∗
v,F ) is also quadratic
in v, when v tends to zero under the assumptions of Theorem 10 (from Lemma 6).

4. Unknown Bid Distribution

We now turn to the more realistic, but harder, setting where both the parameter v and
and the function F need to be estimated simultaneously. For this setting, we propose
the following algorithm, which is a natural adaptation of UCBid1, simply plugging in the
empirical CDF in place of the unknown F .

It may come as a surprise that we do not add any optimistic bonus to the estimate ˆFt:
it is not necessary to be optimistic about F since the observation Mt drawn according to
F is observed at each time step whatever the bid submitted.

Algorithm 2 (UCBid1+) Submit a bid equal to 1 in the ﬁrst round, then bid:

(cid:110)

Bt = max

arg max
b∈[0,1]

( ˆVt + (cid:15)t − b) ˆFt(b)

(cid:111)
,

where ˆFt(b) := 1
t−1

(cid:80)t−1

s=1 1{Ms < b} and (cid:15)t := (cid:112)γ log(t − 1)/2Nt.

Although Bt produced by Algorithm 2 could, in principle, be arbitrarily small, it is
possible to show that there is no extinction of the observation process. Indeed, after a time
that only depends on v and F , F (Bt) is guaranteed to be higher than a strictly positive
fraction of F (b∗
v,F ) with high probability (see Lemma 28 in Appendix E). This result implies
that the number of successful auctions Nt asymptotically grows at a linear rate (with high
probability), making it possible to bound the expected diﬀerence between ˆVt + (cid:15)t and v.
Combined with the DKW inequality (Massart, 1990), this allows to bound the diﬀerence
between the utility and ( ˆVt + (cid:15)t − b) ˆFt(b) in inﬁnite norm and hence the diﬀerence between
Bt and b∗
v,F . Putting all the pieces together (see the complete proof in Appendix E) yields
the following upper bound on the regret of UCBid1+.

Theorem 11 UCBid1+ incurs a regret bounded by

Rv,F

T ≤ 12

(cid:115) γv
Uv,F (b∗

v,F )

(cid:112)T log T + O(log T ),

provided that γ > 2.

Achddou Capp´e Garivier

Note that computing the bid Bt for UCBid1+ is easy, as ( ˆVt +(cid:15)t −b) ˆFt(b) necessarily lies
among the observed bids because this function is linearly decreasing between observed bids.
More precisely, ( ˆVt + (cid:15)t − b) ˆFt(b)) = ˆFt(M (i))( ˆVt + (cid:15)t − b), for b ∈ [M (i), M (i+1)[, where M (i)
is the i-th order statistic of the observed bids (obtained by sorting the bids in ascending
order). However, as there is no obvious way to update Bt sequentially, this results in a
complexity of UCBid1+ that grows quadratically with the time horizon T .

The proof of Theorem 11 relies on the DKW inequality to bound the diﬀerence between
Bt and b∗. This happens to be very conservative and a little misleading in practice. Indeed,
what really matters is the local behavior of the empirical utility, and hence, of ˆFt around
b∗. As illustrated by Figure 2, locally, ˆFt is roughly a translation of F plus a negligible
perturbation which can be bounded in inﬁnite norm. This intuition is formalized in Lemma
12, a localized version of the DKW inequality. The fact that ˆFt is locally almost parallel to
F imposes a constraint on Bt that may be used to bound its distance from b∗, yielding an
improved regret rate under Assumptions 1 and 2, as shown by Theorem 13.

Lemma 12 For any a, b ∈ [0, 1], if F is increasing,

| ˆFt(x) − F (x) − ( ˆFt(a) − F (a))|

sup
a≤x≤b

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

≤

2(F (b) − F (a)) log

(cid:18)

√

η

t

with probability 1 − η.

√

e

t
2(F (b)−F (a))

(cid:19)

+

log(

t
2(F (b)−F (a)η2 )

6t

,

Figure 2: Local behavior of the empirical CDF

Theorem 13 If F satisﬁes Assumptions 1 and 2, UCBid1+ incurs a regret bounded by

for any (cid:15) > 0, provided that γ > 2.

T ≤ O(cid:0)T 1/3+(cid:15)(cid:1),
Rv,F

UCBid1+ thus retains the adaptivity of UCBid1. In general, its regret is of the order
√
T (omitting logarithmic terms), matching the lower bound of Theorem 2. But it is
of
reduced to T 1/3+(cid:15), for any (cid:15) > 0, in the smooth case deﬁned by Assumptions 1 and 2. In
T -regret algorithms is huge, as shown in the next
practice, the improvement over other
section.

√

0.00.20.40.60.81.00.00.20.40.60.81.0cdfEmpiricalcdf0.350.360.370.380.390.400.460.480.500.520.54cdfEmpiricalcdfFast Rate Learning in Stochastic First Price Bidding

5. Numerical simulations

5.1. Benchmark Algorithms

Methods pertaining to black box optimization. Sequential black box optimization
algorithms, also known as continuously-armed bandits (Kleinberg et al., 2008; Bubeck et al.,
2011; Munos, 2011; Valko et al., 2013), are algorithms designed to ﬁnd the optimum of an
unknown function by receiving noisy evaluations of that function at points that are chosen
sequentially by the learner. They rely on prior assumptions on the smoothness of the
unknown function. For ﬁrst-price bidding, we may consider that the reward (v − Bt)1(Mt ≤
Bt) is a noisy observation of the utility Uv,F (Bt), with a noise bounded by 1. Moreover,
when F admits a density f and f (b) < Cf , then −1 < U (cid:48)
v,F (b) = (v − x)f (b) − F (b) < Cf ,
which implies that Uv,F is Lipschitz with constant max(1, Cf ). As a consequence, all black-
box optimization algorithms that consider an objective function with Lispchitz regularity
may be used for learning in stochastic ﬁrst price auctions. HOO (Bubeck et al., 2011) has
a parameter ρ related to the level of smoothness of the objective function which we can
set to 1/2, corresponding to the observation that the ﬁrst-price utility is Lipschitz under
the assumptions discussed above. This immediately leads to a ﬁrst baseline approach with
O(
T log T ) regret rate. Setting the parameter related to the Lipschitz constant of HOO
so that it is larger than Cf is not possible in practice without prior knowledge on F . More
generally, knowing the smoothness is considered a challenge most of the time in black-
box optimization, so that several methods have been introduced that are adaptive to the
smoothness, e.g. stoSOO (Valko et al., 2013).

√

UCB on a smartly chosen discretization. Combes and Proutiere (2014) prove that
when the reward function is unimodal, a discretization based on the smoothness level of this
function suﬃces to achieve a regret of the order of
T . If F satisﬁes Assumption 1, Uv,F is
unimodal, as shown by the proof of Lemma 3. Hence, using the right discretization while
applying UCB, one can achieve a O(
T ) regret. In particular if the utility is quadratic, the
advised discretization is a grid of O(T 1/4) values.

√

√

O-UCBID1. We also implement the following algorithm, that is reminiscent of the method
used by (Cesa-Bianchi et al., 2014) to learn reserve prices.

Algorithm 3 (O-UCBid1) Submit a bid equal to 1 in the ﬁrst round, then bid:

Bt = max{b ∈ [0, ˆVt + (cid:15)t], ˆUt(b) ≥ max
b∈[0,1]

ˆUt(b) − 2(cid:15)t},

where ˆUt(b) = ( ˆVt − b) ˆFt(b).

This algorithm overbids with high probability, by construction. Thanks to the DKW
inequality, one can control the diﬀerence between the true bid distribution F and its empir-
ical version ˆFt in inﬁnite norm. Because we observe Mt at each round, (cid:107)F − ˆFt(cid:107)∞ is at most
(cid:15)t with high probability. It is easy to show that (cid:107)Uv,F − ˆUt(cid:107)∞ is bounded by a multiple of (cid:15)t
showing that Bt is (again with high probability) larger than the unknown optimal bid b∗
v,F .
O-UCBid1 is very close to the method used by (Cesa-Bianchi et al., 2014) to set a reserve
price in second-price auctions. While in ﬁrst-price auctions, a bidder needs to overbid in

Achddou Capp´e Garivier

order to favor exploration, sellers in second-price auctions are encouraged to oﬀer a lower
price than the optimal one, as they can only observe the second highest bid if their re-
serve price is set lower than the latter. The approach of Cesa-Bianchi et al. (2014) requires
successive stages as sellers in second-price auctions can only observe the second-price and
need to estimate the distribution of all bids based on this information. In our setting, we
have direct access to the opponents’ highest bid and successive stages are not required any
longer. We prove that the regret incurred by O-UCBid1 is of the order of log T
T when
γ > 1, which makes it an interesting baseline algorithm, that has guarantees similar to
those of black box optimization algorithm, without the need of knowing the smoothness or
the horizon. We refer to Theorem 23 in Appendix E for further details.

√

Methods for discrete distributions We run UCBid1+ on discrete examples. In this
case, we compare it to UCB on a discretization of [0, 1] and to WinExp, a generalization of
Exp3 for the problem of learning to bid (Feng et al., 2018).

5.2. Experiments On Simulated Data

Figure 3: Two choices of F ; associated utilities for v = 1/2.

(a) Regret plots under the ﬁrst instance of

(b) Regret plots under the second instance

the problem

of the problem

Figure 4: Regret plots for known F

0.00.20.40.60.81.0b0.00.20.40.60.81.0cdf−0.5−0.4−0.3−0.2−0.10.00.10.2utilityF1(b)F2(b)Uv,F1(b)Uv,F2(b)0200040006000800010000Time020406080100120140160RegretUCBid1BalancedGreedy0200040006000800010000Time020406080100120140160RegretUCBid1BalancedGreedyFast Rate Learning in Stochastic First Price Bidding

(a) Regret plots under the ﬁrst instance

(b) Regret plots under the second in-

of the problem

stance of the problem

Figure 5: Regret plots for unknown F

In this section we focus on two particular instances of the ﬁrst price auction learning
problem. The ﬁrst instance is characterized by a value distribution set to a Bernoulli
distribution of average 0.5, and a distribution of the highest contestants’ bids set to a
Beta(1,6). The second instance only diﬀers by the distribution of the highest contestants’
bids, which is set to a mixture of two Beta distributions: 0.55 × Beta(500, 2500) + 0.45 ×
Beta(1000, 2000). This distribution is very close to that used in the proof of Theorem 2,
but is continuous. The cumulative distribution and the matching utility of each instance are
plotted on Figure 3. Both distributions are smooth but the ﬁrst one satisﬁes Assumption
1, while it is not clear that the second one does.

Figures 4(a)subﬁgure and 4(b)subﬁgure show the regret of various strategies when F
is known. The ﬁrst (respectively second) ﬁgure represents the regrets of these strategies
under the ﬁrst (respectively second) instance of the problem described above. The horizon
is set to 10000 and the results of 720 Monte Carlo trials are aggregated. The plots represent
the average regret over time (shaded areas correspond to the interquartile range). The
strategy termed Greedy is a naive strategy that bids max arg max ˆUt(b), whenever it has
made more than three observations. It shows a linear regret, which comes from the fact
that when it only observes value samples equal to zero during the ﬁrst three observations,
it bids 0 indeﬁnitely, and thus incurs the regret Uv,F (b∗
v,F ) − Uv,F (0) at each time step.
Observing only 0 three times in a row is not very likely: the third quartile is very small,
but the consequences are so terrible that the average is many orders of magnitude higher.
The strategy termed Balanced consists in bidding the median of the highest contestants’
bids. It guarantees that the learner is able to win half of the rounds. As expected, this
strategy, which does not adapt to the instance at hand, shows poor performances in both
cases. However, it is a better solution than bidding 0 or 1. Finally, we also plot the
regret of UCBid1. Note that in order to implement UCBid1 we would have to compute
arg maxb∈[0,1]( ˆVt + (cid:15)t − b)F (b) at each round; instead we only use an approximation of this
quantity by computing the argmax of the function over a grid of 10000 values. UCBid1
outperforms the naive baseline strategies in both cases. Under the more complex second

0200040006000800010000Time050100150200250300350400RegretUCBid1+OUCBid1HOO,ρ=0.5ν=1HOO,ρ=0.25ν=1stoSOO,δ=0.01,k=12UCBdiscretestep=0.10200040006000800010000Time050100150200250300350400RegretUCBid1+OUCBid1HOO,ρ=0.5ν=1HOO,ρ=0.25ν=1stoSOO,δ=0.01,k=12UCBdiscretestep=0.1Achddou Capp´e Garivier

instance of the problem, it shows a larger regret than under the ﬁrst one. However, even in
this more complex case, the rate of growth of the regret stays very low.

In
In Figure 5, we analyze the regrets of diﬀerent algorithms when F is unknown.
this setting, we compare UCB on a discretization of [0, 1] with 10 arms, HOO (Bubeck
et al., 2011) with various parameters, O-UCBid1 and UCBid1+ with γ = 1 and stoSOO
(Valko et al., 2013) with the parameters recommended in the latter paper. For eﬃciency
reasons, we also do not allow the tree built by HOO and stoSOO to have a depth larger
than log2 T . The various versions of HOO, UCB, as well as stoSOO show regret plots that
could correspond to a
T behavior. UCBid1+ shows a dramatically improved regret plot
compared to the black box optimization strategies.

√

(a) Utility with v = 1/2

(b) Regret plots

Figure 6: An example with discrete bids

Figure 6 shows a diﬀerent example where the distribution of bids is discrete with a
probability mass of 0.51 on 0.1 and equal probability masses on i/50, ∀i ∈ [1 . . . 4, 6, . . . , 50].
We compare UCBid1+ with UCB, having operated a discretization into 10 arms and with
Winexp with a discretization into 50 arms. UCBid1+ again yields a regret at least 5 times
smaller than the other algorithms.
In addition, it is important to stress that UCBid1+
and O-UCBid1 are anytime algorithms, while all the alternatives shown on Figures 5 and 6
require, at least, the knowledge of the time horizon.

5.3. Experiments On a Real Bidding Dataset

We also experiment on a real-world bidding dataset representing the highest bids from the
contestants of one advertiser on a certain campaign. Thanks to Numberly, a media trading
agency, Adverline, an advertising network, and Xandr, a supply and demand-side platform,
we collected a set of 56607 bids that were made on a speciﬁc placement on Adverline’s
inventory on auctions that Numberly participated to, for a speciﬁc campaign. We keep
only the bids smaller than the 90% quantile and we normalize them to get data between 0
and 1 (see Figure 10 in Appendix F for a histogram). The regret plots are represented in
Figure 7(b)subﬁgure. As earlier, with discrete simulated data, we compare UCBid1+ with
UCB, having operated a discretization into 10 arms and with Winexp with a discretization
into 100 arms. Unsurprisingly, the regret plots are similar to those with simulated data,

0.00.20.40.60.81.0b0.00.20.40.60.81.0F(b)−0.5−0.4−0.3−0.2−0.10.00.10.2U(b)0200040006000800010000Time050100150200250300350RegretUCBid1+UCBdiscretestep=0.1Winexpstep=0.02Fast Rate Learning in Stochastic First Price Bidding

(a) Utility with v = 1/2

(b) Regret plots with bidding data

Figure 7: Experiment with real bidding data

since the distributions at hand are similar. UCBid1+ still largely outperforms the baseline
algorithms.

Acknowledgments

We would like to thank Adverline for accepting to provide us with the bidding data on
their inventories and Xandr for making this data transaction possible. We are very grateful
to them for their support on this project. Aur´elien Garivier acknowledges the support of
the Project IDEXLYON of the University of Lyon, in the framework of the Programme In-
vestissements d’Avenir (ANR-16-IDEX-0005), and Chaire SeqALO (ANR-20-CHIA-0020).

References

Juliette Achddou, Olivier Capp´e, and Aur´elien Garivier. Eﬃcient algorithms for stochastic repeated

second-price auctions. In Algorithmic Learning Theory, pages 99–150. PMLR, 2021.

A. Blum, V. Kumar, A. Rudra, and F. Wu. Online learning in online auctions. Theoretical Computer

Science, 324(2-3):137–146, 2004.

S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv´ari. X-armed bandits. Journal of Machine Learning

Research, 12(5), 2011.

S. Bubeck, N. Devanur, Z. Huang, and R. Niazadeh. Multi-scale online learning and its applications

to online auctions. arXiv preprint arXiv:1705.09700, 2017.

O. Capp´e, A. Garivier, O. Maillard, R. Munos, G. Stoltz, et al. Kullback–Leibler upper conﬁdence

bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516–1541, 2013.

N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price

auctions. IEEE Transactions on Information Theory, 61(1):549–564, 2014.

N. Cesa-Bianchi, T. Cesari, and V. Perchet. Dynamic pricing with ﬁnitely many unknown valuations.

In Algorithmic Learning Theory, pages 247–273. PMLR, 2019.

R. Cole and T. Roughgarden. The sample complexity of revenue maximization. In Proceedings of

the forty-sixth annual ACM symposium on Theory of computing, pages 243–252, 2014.

0.00.20.40.60.81.0b0.00.20.40.60.81.0F(b)−0.5−0.4−0.3−0.2−0.10.00.1U(b)0200040006000800010000Time0255075100125150175200RegretUCBid1+UCBdiscretestep=0.1Winexpstep=0.01Achddou Capp´e Garivier

R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In

International Conference on Machine Learning, pages 521–529. PMLR, 2014.

P. Dhangwatnotai, T. Roughgarden, and Q. Yan. Revenue maximization with a single sample.

Games and Economic Behavior, 91:318–333, 2015.

Z. Feng, C. Podimata, and V. Syrgkanis. Learning to bid without knowing your value. In Proceedings

of the 2018 ACM Conference on Economics and Computation, pages 505–522, 2018.

Z. Feng, G. Guruganesh, C. Liaw, A. Mehta, and A. Sethi. Convergence analysis of no-regret bidding

algorithms in repeated auctions. arXiv preprint arXiv:2009.06136, 2020.

A. Flajolet and P. Jaillet. Real-time bidding with side information. In Advances in Neural Infor-

mation Processing Systems, pages 5168–5178, 2017.

A. Garivier, P. M´enard, and G. Stoltz. Explore ﬁrst, exploit next: The true shape of regret in bandit

problems. Mathematics of Operations Research, 44(2):377–399, 2019.

Y. Han, Z. Zhou, and T. Weissman. Optimal no-regret learning in repeated ﬁrst-price auctions.

arXiv preprint arXiv:2003.09795, 2020.

Z. Huang, Y. Mansour, and T. Roughgarden. Making the most of your samples. SIAM Journal on

Computing, 47(3):651–674, 2018.

R. Kleinberg and T. Leighton. The value of knowing a demand curve: Bounds on regret for online
posted-price auctions. In 44th Annual IEEE Symposium on Foundations of Computer Science,
2003. Proceedings., pages 594–605. IEEE, 2003.

R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings of

the fortieth annual ACM symposium on Theory of computing, pages 681–690, 2008.

T.L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in applied

mathematics, 6(1):4–22, 1985.

P. Massart. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality. The annals of Proba-

bility, pages 1269–1283, 1990.

R. Munos. Optimistic optimization of deterministic functions without the knowledge of its smooth-

ness. In Advances in neural information processing systems, 2011.

T. Nedelec, C. Calauz`enes, N. El Karoui, and V. Perchet. Learning in repeated auctions. arXiv

preprint arXiv:2011.09365, 2020.

T. Roughgarden and O. Schrijvers. Ironing in the dark. In Proceedings of the 2016 ACM Conference

on Economics and Computation, pages 1–18, 2016.

G. Slefo. Google’s ad manager will move to ﬁrst-price auction., 2019. press.

S. Sluis. Big changes coming to auctions, as exchanges roll the dice on ﬁrst-price., 2017. press.

S. Sluis. Everything you need to know about bid shading., 2019. press.

M. Valko, A. Carpentier, and R. Munos. Stochastic simultaneous optimistic optimization. In Inter-

national Conference on Machine Learning, pages 19–27. PMLR, 2013.

W. Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of Finance,

16(1):8–37, 1961.

Fast Rate Learning in Stochastic First Price Bidding

J. Weed, V. Perchet, and P. Rigollet. Online learning in repeated auctions.

In Conference on

Learning Theory, pages 1562–1583. PMLR, 2016.

Achddou Capp´e Garivier

Supplementary Material

Outline. We prove in Appendix A all the results pertaining to Section 2 apart from Theo-
rem 2, which is proved separately in Appendix B. In Appendix C, we introduce preliminary
results necessary to analyze the regrets of the algorithms presented in main body of the
paper. Appendix D contains all the proofs of the results of Section 3, while the theorems of
Section 4 are proved in Appendix E. A ﬁgure related to Section 5 is presented in Appendix
F.

Notation.

• In the following we write U instead of Uv,F (respectively W instead of Wv,F ; b∗ instead

of b∗

v,F ; q∗ instead of q∗

v,F and RT instead of Rv,F

T ) when there is no ambiguity.

• b(q) denotes F −1(q).

• ˆV (n) := 1/n (cid:80)n

s=1 V (s) is the mean of the n ﬁrst observed values.

• We set V (cid:48)

s = Vs if Ms ≤ Bs, and V (cid:48)

s = ∅ otherwise.

• We set Ft = σ((Ms, V (cid:48)

s )s≤t) be the σ-algebra generated by the the bid maxima and

the values observed up to time t.

• St := (Vt − b∗)1(Mt < b∗) − (Vt − Bt)1(Mt < Bt) represents the instantaneous regret.

Appendix A. Properties of ﬁrst-price auctions

A.1. General properties

Lemma 1 For any cumulative distribution function F , ψF is non decreasing.

Proof Let 0 < v1 < v2 < 1. We have Uv2,F (b∗
Uv1,F (b∗
v2,F .
v2,F )−Uv1,F (b∗

v1,F and b∗
By summing these two inequalities, Uv2,F (b∗

v2,F ) ≥ 0, by deﬁnition of b∗

v2,F ) − Uv2,F (b∗

v1,F ) ≥ 0 and Uv1,F (b∗

v1,F ) −

v2,F )−(Uv2,F (b∗

v1,F )−Uv1,F (b∗

v1,F )) ≥

0. Hence

v1,F )) ≥ 0.
We then prove the result by contradiction, by assuming that b∗
F (b∗

v2,F ), since F is non decreasing. In this case,

(v2 − v1)(F (b∗

v2,F ) − F (b∗

v1,F > b∗

v2,F . Then F (b∗

v1,F ) =

Uv1,F (b∗

v1,F ) = (v1 − b∗

v1)F (b∗

v1,F ) < (v1 − b∗

v2)F (b∗

v2,F ) = Uv1,F (b∗

v2,F ).

This is impossible, since b∗

v1,F is an optimizer of Uv1,F . In conclusion, b∗

v1,F ≤ b∗

v2,F

Fast Rate Learning in Stochastic First Price Bidding

A.2. Properties under regularity assumptions

Lemma 3 If Assumption 1 is satisﬁed, then for any v ∈ [0, 1], Uv,F has a unique maxi-
mizer.

Proof If F satisﬁes Assumption 1 then f
and f does not vanish on ]0, 1[.
(cid:16)
f (b). So U (cid:48)(b) = 0 if and only if v = b + F (b)
The derivative of U is U (cid:48)(b) =
f (b) .
Since φF is increasing, this can only be satisﬁed by a single b ∈ [0, 1]. Also, since f does
not vanish, U is unimodal (increasing then decreasing).

F is decreasing and φF : b (cid:55)→ b + F (b)

f (b) is increasing

v − b − F (b)
f (b)

(cid:17)

Lemma 14 If Assumption 1 is satisﬁed, then Wv,F is strongly concave.

If F satisﬁes Assumption 1 then f
does not vanish on ]0, 1[.
The derivative of U is U (cid:48)(b) =
(cid:16)
v − b − F (F −1(q))
f (F −1(q))
creasing, and U (cid:48) is strongly concave.

= v − φ(cid:48)

(cid:17)

(cid:16)

F is decreasing and φF : b (cid:55)→ b + F (b)

f (b) is increasing and f

v − b − F (b)
f (b)

(cid:17)

f (b). The derivative of W is W (cid:48)(q) =

F (F −1(q)), since φF is increasing. Consequently, U (cid:48) is de-

Lemma 4 If Assumption 1 is satisﬁed and f is diﬀerentiable, then ψF : v (cid:55)→ b∗(v, F ) is
Lipschitz continuous with a Lipschitz constant 1.

If b∗ is the optimum of the utility U , then it satisﬁes (v − b∗)f (b∗) − F (b∗) = 0. It

F (b∗)
f (b∗)
F (b∗) > 1 thanks to Assumption 1, φF is invertible and (φF )−1 = ψF is Lipschitzian

φF (b∗) := b∗ +

= v.

Proof
satisﬁes

Since φ(cid:48)
with constant 1 .

Lemma 5 If Assumption 1 is satisﬁed, then

F (b∗) ≥ e−1F (v)

Proof We know that b∗ < v and

Hence

log

(cid:19)

(cid:18) F (v)
F (b∗)

=

(cid:90) v

b∗

f (u)
F (u)

du.

F (v)
F (b)

= exp

(cid:18)(cid:90) v

b∗

(cid:19)

du

.

f (u)
F (u)

Achddou Capp´e Garivier

Since f (u)

F (u) is decreasing, thanks to Assumption 1,

F (v)
F (b)

(cid:18)

(v − b∗)

≤ exp

(cid:19)

.

f (b∗)
F (b∗)

We have v − b∗ = F (b∗)

f (b∗) , by deﬁnition of b∗. Hence exp

(cid:16)

v − b∗) f (b∗)
F (b∗)

(cid:17)

= exp(1) and

F (b∗) ≥ exp(−1)F (v).

Lemma 6 If Assumption 1 is satisﬁed, for any 0 ≤ q(cid:48) ≤ 1,

W (q∗) − W (q(cid:48)) ≤

(q∗ − q(cid:48))2W (q∗)

1
4

Proof Note that this proof is an adaptation of the proof of Lemma 3.2 in Huang et al.
(2018). In this proof, we denote by b(q) F −1(q).

First of all, let us observe that U (cid:48)(b) = (v−φF (b))f (b). We have W (cid:48)(q) = v−φF (F −1(q)).
Assumption 1 implies that φ(cid:48)
To prove Lemma 6, we will apply case-based reasoning. There are three cases depending
on the relation between q(cid:48) and q∗: q(cid:48) > q∗, q(cid:48) = q∗, and q(cid:48) < q∗. The second case, i.e.,
q(cid:48) = q∗, is trivial.

F (b) > 1, ∀b ∈ [0, 1].

First, consider the case when q(cid:48) > q∗. It holds

W (q∗) − W (q(cid:48)) =

(cid:90) q(cid:48)

q∗

−W (cid:48)(q)dq =

(cid:90) q(cid:48)

(cid:16)

q∗

φF (b(q)) − v

(cid:17)

dq.

We therefore need to bound φF (b(q), ∀q ∈ [q∗, q(cid:48)]. By deﬁnition of q∗, for any q s.t. q∗ ≤
q ≤ q(cid:48), we have

q(v − b(q)) ≤ q∗(v − b(q∗)).

By rewriting this equation,

b(q) ≥

qv − q∗v + q∗b(q∗)
q

= v

(cid:19)

(cid:18) q − q∗
q

+

q∗
q

b(q∗)

(1)

Secondly, by the intermediate value theorem, there exists b ∈ [b(q∗), b(q)], such that

φF (b(q)) − φF (b(q∗)) = φ(cid:48)

(cid:16)
F (b)

b(q) − b(q∗)

(cid:17)

≥ b(q) − b(q∗),

for any q∗ ≤ q ≤ q(cid:48), where the second inequality follows from Assumption 1 that dφF (b)
and F being increasing thanks to Assumption 1. This in turn yields

db ≥ 1

φF (b(q)) ≥ v + b(q) − b(q∗),

Fast Rate Learning in Stochastic First Price Bidding

since by deﬁnition, W (cid:48)(q∗) = φF (b(q∗)) = v. Combining with Inequality 1, we get that

φF (b(q)) − v ≥ v(

q − q∗
q

) +

q∗
q

b(q∗) − b(q∗) ≥ (v − b(q∗))(

q − q∗
q

) =

W (q∗)
q∗

(

q − q∗
q

)

Therefore, we get that

W (q∗) − W (q(cid:48)) =

(cid:90) q(cid:48)

−W (cid:48)(q)dq =

(cid:90) q(cid:48)

(cid:16)

q∗
W (q∗)
q∗

≥

(cid:90) q(cid:48)

q(cid:48)+q∗
2

q∗
q − q∗
q

dq,

φF (b(q)) − v

(cid:17)

dq ≥

W (q∗)
q∗

(cid:90) q(cid:48)

q∗

q − q∗
q

dq

since q−q∗
1 − 2q∗

q ≥ 0 for any q(cid:48) ≤ q ≤ q∗. Moreover, for any q ≥ q(cid:48)+q∗
q(cid:48)+q∗ . Hence, we can derive the following inequality

q(cid:48)+q∗ ≥ q(cid:48)−q∗

2

, we have q−q∗

q = 1 − q∗

q ≥

W (q∗) − W (q(cid:48)) ≥

(cid:90) q(cid:48)

q(cid:48)+q∗
2

q(cid:48) − q∗
q(cid:48) + q∗

W (q∗)
q∗

dq =

(q(cid:48) − q∗)2
2(q(cid:48) + q∗)

W (q∗)

q∗ =

(q(cid:48) − q∗)2
2q∗(q(cid:48) + q∗)

W (q∗) .

The lemma then follows from the fact that 0 ≤ q(cid:48), q∗ ≤ 1.

The second case, q(cid:48) > q∗ has to be treated a little diﬀerently than the ﬁrst, partly
because we now need to upper bound b(q) instead of lower-bounding it. We achieve this by
using the concavity of W (proved in Lemma 14).

By concavity of the revenue curve, for any q(cid:48) ≤ q ≤ q∗, we have

W (q) ≥

q − q(cid:48)
q∗ − q(cid:48) W (q∗) +

q∗ − q
q∗ − q(cid:48) W (q(cid:48)) ,

because W lies above the segment that connects (q(cid:48), W (q(cid:48))) and (q∗, W (q∗)), between q(cid:48)
and q∗. Hence

(v −b(q))q ≥

q − q(cid:48)
q∗ − q(cid:48) (v −b(q∗))q∗ +

q∗ − q

q∗ − q(cid:48) (v −b(q(cid:48)))q(cid:48) ≥ qv −b(q∗)q∗ q − q(cid:48)

q∗ − q(cid:48) −b(q(cid:48))q(cid:48) q∗ − q
q∗ − q(cid:48) ,

And

which yields

−qb(q) ≥

q∗q(cid:48)
(q∗ − q(cid:48))

(cid:16)

(cid:17)
b(q∗) − b(q(cid:48))

+ q

q(cid:48)b(q(cid:48)) − q∗b(q∗)
q∗ − q(cid:48)

,

qb(q) ≤

(cid:16)

q∗q(cid:48)
(q∗ − q(cid:48))

b(q(cid:48)) − b(q∗)

(cid:17)

+ q

q∗b(q∗) − q(cid:48)b(q(cid:48))
q∗ − q(cid:48)

,

Dividing both sides by q, we have

b(q) ≤

q∗q(cid:48)
q(q∗ − q(cid:48))

(cid:16)

b(q(cid:48)) − b(q∗)

(cid:17)

+

q∗b(q∗) − q(cid:48)b(q(cid:48))
q∗ − q(cid:48)

,

(2)

Further, by the intermediate value theorem, there exists b ∈ [b(q∗), b(q)], such that

φF (b(q)) − φF (b(q∗)) = φ(cid:48)

F (b)

(cid:16)

(cid:17)
b(q) − b(q∗)

,

Achddou Capp´e Garivier

for any q∗ ≤ q ≤ q(cid:48). Further, by Assumption 1 that dφF (b)
thanks to Assumption 1, for any q(cid:48) ≤ q ≤ q∗,

db ≥ 1, and because b is increasing

φF (b(q)) − φF (b(q∗)) ≤ b(q) − b(q∗)

and

φF (b(q)) ≤ v + b(q) − b(q∗) = v + b(q) − b(q∗),

Combining with Inequality 2, we get that

φF (b(q)) ≤ v +

= v +

q∗q(cid:48)
q(q∗ − q(cid:48))
q(cid:48)(q∗ − q)
q(q∗ − q(cid:48))

(cid:16)

b(q(cid:48)) − b(q∗)

(cid:17)

+

− b(q∗)

q∗b(q∗) − q(cid:48)b(q(cid:48))
q∗ − q(cid:48)
q(cid:48)(q∗ − q)
q∗(q∗ − q(cid:48))

(cid:0)b(q(cid:48)) − b(q∗)(cid:1) ≤ v +

(cid:0)b(q(cid:48)) − b(q∗)(cid:1) ,

where the last inequality is due to q ≤ q∗ and b(q(cid:48)) − b(q∗) < 0. Hence, we have

W (q∗) − W (q(cid:48)) =

(cid:90) q∗

W (cid:48)(q)dq

q(cid:48)
(cid:90) q∗

q(cid:48)
(cid:90) q∗

=

≥

=

v − φF (b(q))dq

q(cid:48)(q∗ − q)
q∗(q∗ − q(cid:48))

(cid:0)b(q∗) − b(q(cid:48))(cid:1)dq

q(cid:48)
q(cid:48)
2q∗ (q∗ − q(cid:48))(cid:0)b(q∗) − b(q(cid:48))(cid:1).

On the other hand, we have

W (q∗) − W (q(cid:48)) = (q∗ − q(cid:48))v + q(cid:48)b(q(cid:48)) − q∗b(q∗).

Taking the linear combination 2q∗

3q∗−q(cid:48) · 3 + q∗−q(cid:48)

3q∗−q(cid:48) · 4, we have

W (q∗) − W (q(cid:48)) ≥ v

−

(q∗ − q(cid:48))2
3q∗ − q(cid:48) b(q∗)
(q∗ − q(cid:48))2W (q∗)

(q∗ − q(cid:48))2
3q∗ − q(cid:48))
1
q∗(3q∗ − q(cid:48))
1
3

=

≥

(q∗ − q(cid:48))2W (q∗) ,

(3)

(4)

where the last inequality holds because 0 ≤ q∗, q(cid:48) ≤ 1.

Lemma 7 If Assumption 2 is satisﬁed, for any F −1(b∗) ≤ q(cid:48) ≤ F −1(b∗ + ∆) ≤ b∗ + Cf ∆),

W (q∗) − W (q(cid:48)) ≤

1
cf

λ(q∗ − q(cid:48))2,

Fast Rate Learning in Stochastic First Price Bidding

Proof

W (q∗) − W (q(cid:48)) =

(cid:90) q(cid:48)

q∗

−W (cid:48)(q)dq =

(cid:90) q(cid:48)

(cid:16)

q∗

φF (b(q)) − v

(cid:17)

dq.

by the intermediate value theorem, there exists b ∈ [b(q∗), b(q)], such that

φF (b(q)) − φF (b(q∗)) = φ(cid:48)

F (b)

(cid:16)

b(q) − b(q∗)

(cid:17)

≥ λ(b(q) − b(q∗)),

so that φF (b(q))−v ≤ λ(b(q)−b(q∗)) when q∗ ≤ q ≤ q(cid:48) and φF (b(q))−v ≥ λ(b(q)−b(q∗))
when q(cid:48) ≤ q ≤ q∗. Since f is bounded from below by cf , and since by the intermediate
value theorem ∃u ∈ [q, q∗], b(q) − b(q∗) = b(cid:48)(u)(q − q∗) ≥ 1

f (u) (q − q∗), this yields

W (q∗) − W (q(cid:48)) ≤ λ

1
cf

(q(cid:48) − q∗)2

in both cases.

Lemma 15 Beta distributions such that

α + β < αβ

satisfy Assumption 1.

Proof The density of a Beta distribution satisﬁes

f (x) =

xα−1(1 − x)β−1
B(α, β)

And

f (cid:48)(x) =

(α − 1)xα−2(1 − x)β−1 − (β − 1)xα−1(1 − x)β−2
B(α, β)

,

where B(α, β) = Γ(α+β)
(cid:16) f
F

1 if and only if

(cid:17)(cid:48)

Γ(α)Γ(β) when Γ denotes the Gamma function. F satisﬁes assumption
(x) = F (x)f (cid:48)(x)−f 2(x)

< 0, ∀x ∈]0, 1[, which is equivalent to:

F 2(x)

f (cid:48)(x)F (x) − f 2(x) < 0, ∀x ∈]0, 1[ ⇐⇒

f (cid:48)(x)
f (x)

F (x) < f (x), ∀x ∈]0, 1[)

⇐⇒ F (x)B(α, β) [(α − 1)(1 − x) − (β − 1)x] < xα(1 − x)β,

∀x ∈]0, 1[.

Therefore we study the function G : x (cid:55)→ F (x)B(α, β) [(α − 1)(1 − x) − (β − 1)x] −

xα(1 − x)β. First of all, we observe that G(0) = 0. Next, we note that

G(cid:48)(x) = − F (x)(α + β − 2)B(α, β) + ((α − 1) − (α + β − 2)x)xα−1(1 − x)β−1

(cid:16)

(α(1 − x) − βx) xα−1(1 − x)β−1(cid:17)

−

Achddou Capp´e Garivier

and G(cid:48)(0) = 0. Now, we compute the second derivative of G:

G(cid:48)(cid:48)(x) = − (α + β − 2)xα−1(1 − x)β−1 + ((α − 1) − (α + β − 2)x))2 xα−2(1 − x)β−2

− (α + β − 2)xα−1(1 − x)β−1 − (α − (α + β)x) ((α − 1)−
(α + β − 2)x)xα−2(1 − x)β−2 + (α + β)xα−1(1 − x)β−1

The sign of G(cid:48)(cid:48)(x) is the same as that of P (x) = − ((α + β) − 4) (x(1 − x)) + (−1 +
2x) ((α − 1) − (α + β − 2)x).

By simplifying, we get P (x) = −(α + β)x2 + 2αx − (α − 1). This polynomial is always
α+β − 1) − α + 1 =

α+β − α + 1 = α2( 2

α+β ) = − α2

α+β + 2 α2

negative because its maximum is P ( α
α2
α+β − α + 1 = α+β−αβ

α+β

.

Since G(cid:48)(cid:48)(x) < 0, ∀x ∈ [0, 1] and G(cid:48)(0) = 0, then G(cid:48)(x) < 0, ∀x ∈ [0, 1]. Similarly,
G(cid:48)(x) < 0, ∀x ∈ [0, 1] and G(0) = 0, implies G(cid:48)(x) < 0, ∀x ∈ [0, 1], which in turn implies
that F satisﬁes Assumption 1.

A.3. Continuous distribution leading to a utility with two global maximizers

Consider a distribution which cumulative distribution function F is piece-wise linear on
[0, v] at least. We consider that it changes slope at a1v < v, and that it is constant on
[a2v, v], as in Figure 8. We denote by b1 = F (a1v) and b2 = F (a2v). For simplicity we
assume that F is constant on [a2v, a3v] it is linear and does not change slope on [a3v, 1]
with a3 > 1. We make the following assumptions

(cid:40)

a2v > v/2,
a2v ≤ v+a1v

2 − a2v−a1v
b2−b1

b1
2 .

(5)

Then

Figure 8: Example of F

Fast Rate Learning in Stochastic First Price Bidding

• On [0, a1v] Uv(x) = b1

a1v x, and the optimum on this interval is v/2. The optimal value

on this interval is Uv(v/2) = b1
a1v

v2
4 on this interval.

(cid:16) b2−b1

(cid:17)

• On [a1v, a2v], Uv(x) =

a2v−a1v (x − a1v) + b1

v(x) =
b2−b1
b1
a2v−a1v (v − 2x + a1v) − b1 and U (cid:48)
2 . The optimizer
2 − a2v−a1v
on this interval is hence a2v, if v+a1v
b1
2 > a2v. Under this condition, the
b2−b1
optimal value is Uv(a2v) = b2(v − a1v) on this interval. This can also be extended to
the whole interval [a1v, v], since U is decreasing after a2v.

v(x) = 0 ⇐⇒ x = v+a1v

(v − x), and on this interval, U (cid:48)

2 − a2v−a1v
b2−b1

Setting

b1
a1

v
4

= b2

(6)

leads to the utility having two global maximizers, v/2 and a2v.

To summarize, the utility’s argmax is {v/2, a2v} if the set of Equations 5 holds.
We can for example choose :

29
32
This choice of parameters satisﬁes Condition 5 and Condition 6. Figure 9 shows the

v = 1/2; a2 =

b1; b1 = 0.5

128
29

; a1 =

; b2 =

15
16

corresponding utility on [0, v].

Figure 9: Associated Utility with two maximizers

Appendix B. Lower Bound

Theorem 2 Let C denote the class of cumulative distribution functions on [0, 1]. Any
strategy, whether it assumes knowledge of F or not, must satisfy

lim inf
T →∞

maxv∈[0,1],F ∈C Rv,F
T
√
T

≥

1
64

,

Achddou Capp´e Garivier

Proof

√

3 , 2v

We exhibit a choice of F , and two alternative Bernoulli value distributions Ber(v)
and Ber(v(cid:48)) that are diﬃcult to distinguish but whose diﬀerence is large enough so that
mistaking one for the other necessarily leads to a regret of the order of
T when the
cumulative distribution function is F .

3 ) = 2A + 3 ∆T

3 , 1(cid:9) such that F ( v

Let v < 1 and consider a discrete distribution with support (cid:8) v

3 ) =
A and F ( 2v
v , where ∆T and A are positive constants, that we will ﬁx later
on. A maximizer of the utility can only be a point of the support, since Uv,F decreases in
the intervals where F is constant. It can not be 1, because v < 1. We have Uv,F ( v
3 ) = 2vA
3
and Uv,F ( 2v
3 + ∆T , while Uv,F (1) ≤ 0. Consequently, when the value is v, the
optimum is achieved by bidding 2v
3 yields a regret of at least
∆T . Now let us consider the alternative situation in which the value is v(cid:48) = v − δT , with
δT > 0. We get Uv(cid:48),F ( v
v ). When
∆T < δT (2A + 3∆T
3 and the regret incurred by bidding more than 2v
3
is at least δT (A + 3∆T
2−3δT /v , we ensure that the regret incurred
by bidding on the wrong side of 2v
3 is larger than ∆T , whether the value is v or v(cid:48). Further,
by setting δT = (cid:112)v(1 − v)/T , we force the error ∆T to be of the order of 1/

v ) − ∆T . By setting ∆T = AδT

3 and bidding less than 2v

3 + ∆T − δT (2A + 3∆T

v ), the optimal bid is v

3 − δT A and Uv(cid:48),F ( 2v

3 ) = 2vA

3 ) = 2Av

3 ) = 2Av

T .

√

We also set A = 1

T > 16 > (11/3)2, 4

4 , and v = 1/2. We can prove that ∀T > 16, 2A + 3 ∆T
v < 1 ; Indeed, if
1√
= 6∆T < 1
< 2 − 6√
T
2 =
3 < 2
2− 6√
T
T

which implies 2
3

T − 6 hence

√

4
√

T

3

1 − 2A.

We denote by Pv,F (·) the probability of an event under the ﬁrst conﬁguration (respec-
tively Ev,F (·) the expectation of a random variable under the ﬁrst conﬁguration), and by
Pv(cid:48),F (·) the probability of an event under the second conﬁguration (respectively Ev−δ−T,F (·)
the expectation of a random variable under the ﬁrst conﬁguration). We denote by It the
information collected up to time t + 1 : (Mt, V (cid:48)
v(cid:48)) denotes
the law of It in the ﬁrst (respectively second) conﬁguration.
We consider the Kullback Leibler divergence between PIt

v,F (respectively PIt

t , . . . M1, V (cid:48)

1). PIt

v(cid:48),F . We prove that it

v,F and PIt

is equal to

KL(PIt

v , PIt

v(cid:48),F ) = kl(v, v(cid:48))E[Nt],

(7)

where kl(·, ·) denotes the Kullback Leibler divergence between two Bernoulli distributions.
Indeed, thanks to the chain rule for conditional KL,

KL(PIt

v,F , PIt

v(cid:48),F ) = KL(PIt

v,F , PIt

v(cid:48),F ) + KL(P(Mt,V (cid:48)

v,F

t )|It

, P(Mt,V (cid:48)
v(cid:48),F

t )|It

),

and

KL(P(Mt,V (cid:48)
v,F

t )|It

, P(Mt,V (cid:48)
v(cid:48),F

t )|It

) = E[E[KL(νIt ⊗ DF , ν(cid:48)
= E[kl(v, v(cid:48))1(Bt > Mt)].

It ⊗ DF )|It]]

where νIt(respectively ν(cid:48)
It
spectively the second), and DF the law of Mt.

) denotes the law of V (cid:48)

t knowing It in the ﬁrst conﬁguration (re-

By induction, we obtain

KL(PIt

v,F , PIt

v(cid:48),F ) = kl(v, v(cid:48))Ev,F [Nt].

Fast Rate Learning in Stochastic First Price Bidding

We stress that in either of the former conﬁgurations (under (v, F ) or (v(cid:48), F )), playing

on the wrong side of 2

3 v yields a regret larger than ∆T . Using this, we get that ∀T > 16,

(Rv,F

T + Rv−δ,F
(cid:18)
(cid:18)

T

)

T
(cid:88)

∆T Pv,F

Bt <

(cid:18)

(cid:18)

∆T Pv,F

Bt <

(cid:19)

(cid:18)

v

+ ∆T Pv(cid:48),F

Bt >

(cid:19)(cid:19)

2
3

v

(cid:19)

(cid:18)

v

+ ∆T

1 − Pv(cid:48),F (Bt >

(cid:19)(cid:19)

v)

2
3

2
3

2
3

max(Rv,F

T , Rv(cid:48),F

T

) ≥

≥

≥

≥

≥

≥

≥

1
2

1
2

1
2

1
2

1
2

1
2

1
2

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=2

(cid:17)

(cid:33)

(cid:16)

∆T

1 − T V (PIt

v,F , PIt

v(cid:48),F )

(cid:32)

∆T

1 −

(cid:32)

∆T

1 −

(cid:32)

∆T

1 −

(cid:114) 1
2
(cid:114) 1
2
(cid:114) 1
2

KL(PIt

v,F , PIt

v(cid:48),F )

(cid:33)

Ev,F [Nt]kl(v, v(cid:48))

(cid:33)

T kl(v, v(cid:48))

where we used Pinsker’s inequality in the ﬁfth inequality and where T V (·, ·) denotes the
(cid:82) 1
total variation. Yet, since kl(v, v(cid:48)) = (v(cid:48)−v)2
0 g(cid:48)(cid:48)(v(cid:48) + s(v(cid:48) + s(v − v(cid:48)))2(1 − s)ds, where
g(x) = kl(x, v(cid:48)) thanks to Taylor’s inequality,

2

kl(v, v(cid:48)) ≤

(v(cid:48) − v)2
2

≤ (v(cid:48) − v)2

(cid:90) 1

g(cid:48)(cid:48)(u)ds

0

2 max
u∈[v,v(cid:48)]
1
minu∈[v,v(cid:48)] u(1 − u)

≤

(v(cid:48) − v)2
v(cid:48)(1 − v(cid:48))

,

since v = 1
2 .

Therefore,

Achddou Capp´e Garivier

max(Rv,F

T , Rv(cid:48),F

T

) ≥

≥

≥

≥

(cid:32)

∆T

1 −

(cid:32)

∆T

1 −

(cid:114) 1
2
(cid:115) 1
8

T
(cid:88)

t=1
T
(cid:88)

t=1

(cid:32)

×

AδT
2 − 3/2δT

T

1
2

1
2

1
2

1

(cid:32)

√

T

T

√

16 − 12/

(cid:33)

T kl(v, v(cid:48))

(cid:33)

1
)(1/2 + 1
√

2

)

T

T

1 −

2

(1/2 − 1
√
(cid:115) 1
8
(cid:115) 1
8

1 −

(cid:33)

(cid:33)

1
)(1/2 + 1
√

2

)

T

T

1
)(1/2 + 1
√

2

)

T

T

2

2

(1/2 − 1
√

(1/2 − 1
√

Finally

T , Rv(cid:48),F
max(Rv,F
√
T

T

)

≥

1
16

(cid:32)

1 −

(cid:33)

(cid:114) 1
2

≥

1
64

lim inf
T →∞

Appendix C. Preliminary Results

C.1. Concentration inequalities used for the upper bounds
C.1.1. On the value Vt

Lemma 16 The following concentration inequality on the values holds

(cid:18)

( ˆVt − v)2 ≥

T
(cid:88)

t=2

P

γ log(t − 1)
2Nt

(cid:19)

≤

T
(cid:88)

t=1

√

2e

γ(log(t))t−γ.

Proof We have, for all ηt−1,

T
(cid:88)

t=2

P

(cid:18)

( ˆV (Nt) − v)2 ≥

(cid:19)

ηt−1
2Nt

≤

≤

T
(cid:88)

t=2
T
(cid:88)

t=1

(cid:16)

P

∃m : 1 ≤ m ≤ t, 2m( ˆV (m) − v)2 ≥ ηt−1

(cid:17)

2e(cid:112)ηt−1 log(t − 1) exp(−ηt−1) := l1(T )

where the second inequality comes from Lemma 11 in (Capp´e et al., 2013), and from the
fact that Vt is a positive random variable bounded by 1, so 1/2− sub-Gaussian.

Therefore, if ηt := γ log t,

l1(T ) =

√

2e

T
(cid:88)

t=2

γ(log(t − 1))(t − 1)−γ

which tends to a ﬁnite limit as soon as γ > 1.

Fast Rate Learning in Stochastic First Price Bidding

C.1.2. On the cumulative distribution function of Mt

Lemma 17 The following concentration inequality holds on the empirical cumulative dis-
tribution ˆFt.

T
(cid:88)

(cid:18)

P

(cid:107) ˆFt − F (cid:107)∞ ≥

γ log(t − 1)
2(t − 1)

(cid:19)

≤ 2

t−γ.

T
(cid:88)

t=1

t=2

Proof It holds

T
(cid:88)

t=2

P

(cid:18)

( max
b∈[0,1]

|Ft(b) − F (b)|)2 ≥

(cid:19)

γ log(t − 1)
2(t − 1)

(cid:18)

(cid:107) ˆFt − F (cid:107)2

∞ ≥

P

(cid:19)

γ log(t − 1)
2(t − 1)

2e− 2γ log(t

2t

2t−γ,

≤

≤

≤

T
(cid:88)

t=2
T −1
(cid:88)

t=1
T
(cid:88)

t=1

according to the Dvoretzky–Kiefer–Wolfowitz inequality (see Massart (1990)).
Note that this also yields

T
(cid:88)

t=2

P

(cid:18)

(cid:107) ˆFt − F (cid:107)∞ ≥

γ log(t − 1)
2Nt

(cid:19)

γ log(t − 1)
2(t − 1)

(cid:19)

T
(cid:88)

(cid:18)

P

≤

(cid:107) ˆFt − F (cid:107)∞ ≥

t=2
T
(cid:88)

≤ 2

t=1

t−γ.

C.1.3. Local concentration inequality

This lemma is key for the proof of the upper bound of the regret of UCBid1+. It quantiﬁes
the variation of ˆFt on a small interval.

Lemma 12 For any a, b ∈ [0, 1], if F is continuous and increasing, then

| ˆFt(x) − F (x) − ( ˆFt(a) − F (a))|

sup
a≤x≤b

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

≤

2(F (b) − F (a)) log

(cid:18)

√

√
e

t
2(F (b)−F (a))η

(cid:19)

t

log(

t
2(F (b)−F (a)η2 )

6t

+

,

(8)

with probability 1 − η

Achddou Capp´e Garivier

Remark : it follows from the lemma that the the maximal gap between ˆFt(x) − F (x)

and ˆFt( a+b

2 ) − F ( a+b

2 ) can easily be bounded by :

| ˆFt(x) − F (x) − ( ˆFt(

sup
a≤x≤b

a + b
2

) − F (

a + b
2

))|

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

≤ 2

2(F (b) − F (a)) log

(cid:18)

√

t

√

e

t
2η(F (b)−F (a))

(cid:19)

+ 2

log(

t
2(F (b)−F (a)η2 )

6t

with probability 1 − η.

Proof :
Let X1, . . . , Xn

iid∼ dF . Let m > 2 For every 1 ≤ i ≤ m, let xi be such that

i
m
By Bernstein’s inequality, since t(cid:0) ˆFt(xi) − ˆFt(a)(cid:1) ∼ B(n, F (xi) − F (a)) has a variance
bounded by t(cid:0)F (b) − F (a)), there is an event A of probability at least 1 − me−z on which

(cid:0)F (b) − F (a)(cid:1) .

F (xi) = F (a) +

max
0≤i≤m

(cid:12) ˆFt(xi) − ˆFt(a) − (F (xi) − F (a))(cid:12)
(cid:12)

(cid:12) ≤

(cid:115)

2(cid:0)F (b) − F (a)(cid:1)z
t

+

z
3t

:= δ,

by a union bound. Besides, for i = 0, ˆFt(xi) − ˆFt(a) − (F (xi) − F (a)) = 0.

On this event, for every xi−1 ≤ x ≤ xi:

ˆFt(x) − ˆFt(a) − (F (x) − F (a)) ≤ ˆFt(xi) − ˆFt(a) − (F (xi) − F (a)) + F (xi) − F (x) ≤ δ +
ˆFt(x) − ˆFt(a) − (F (x) − F (a)) ≥ ˆFt(xi−1) − ˆFt(a) − (F (xi−1) − F (a)) + F (xi−1) − F (x)

1
m

,

≥ −δ −

1
m

.

and hence

sup
a≤t≤b

Now, take

(cid:12) ˆFt(x) − ˆFt(a) − (F (x) − F (a))(cid:12)
(cid:12)

(cid:12) ≤

(cid:115)

2(cid:0)F (b) − F (a)(cid:1)z
t

+

z
3t

+

1
m

.

(cid:115)

(cid:108)

m =

t
2(cid:0)F (b) − F (a)(cid:1)

(cid:109)

Fast Rate Learning in Stochastic First Price Bidding

and z = log(m/η): one gets that with probability at least 1 − η,

(cid:12)
(cid:12) ˆFt(x) − ˆFt(a) − (F (x) − F (a))(cid:12)
(cid:12)

sup
a≤t≤b

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

≤

≤

2(cid:0)F (b) − F (a)(cid:1) log

(cid:32) (cid:113)

t
2(F (b)−F (a))

η

(cid:33)

(cid:32) (cid:113)

log

t

2(cid:0)F (b) − F (a)(cid:1) log

(cid:18)

√

t

√

e

t
2(F (b)−F (a))η

+

(cid:19)

+

t
2(F (b)−F (a))

(cid:33)

η

3t

(cid:16)

log

t
2(F (b)−F (a))η2

6t

2(cid:0)F (b) − F (a)(cid:1)
t

(cid:115)

+

(cid:17)

.

C.2. General bound on the instantaneous regret

In the following, we will repeatedly use the following general bound on the instantaneous
regret conditioned on the past and on a current victory.

Lemma 18 Let A be an Ft−1-measurable event. Let St denote (Vt − b∗)1(Mt < b∗) − (Vt −
Bt)1(Mt < Bt). The following inequality holds:

E [St1(Bt > b∗)1(A)|Ft−1 ∨ σ(1(Bt > Mt))] ≤

U (b∗) − U (Bt)
F (b∗)

1(Mt ≤ Bt)1(A).

Proof When Bt > b∗, the instantaneous regret can be decomposed as follows

St1(Bt > b∗) = (Bt − v)1(Mt ≤ b∗)1(Bt > b∗) + (Bt − b∗)1 {(Mt ≤ b∗ ≤ Bt)} .

(9)

Note that in particular, there is no instantaneous regret when Mt > Bt. Therefore

E [St1(Bt > b∗)1(A)|Ft−1 ∨ 1(Bt > Mt)]

≤

≤

(Bt − b∗)F (b∗) + (Bt − v)(F (Bt) − F (b∗))
F (Bt)

U (b∗) − U (Bt)
F (b∗)

1(Mt ≤ Bt)1(A),

1(Mt ≤ Bt)1(Bt > b∗)1(A)

since U (b∗) − U (Bt) = (v − b∗)F (b∗) − (v − Bt)F (Bt), which also equals (Bt − b∗)F (b∗) +
(Bt − v)(F (Bt) − F (b∗)).

C.3. Other lemmas

Lemma 19 The expectations E
always be bounded as follows

(cid:104)(cid:80)T

t=2

1
Nt

(cid:105)
1{Mt ≤ Bt}

and E

(cid:104)(cid:80)T

t=2

(cid:113) 1
Nt

(cid:105)
1{Mt ≤ Bt}

can




E



E

(cid:104)(cid:80)T
(cid:104)(cid:80)T

t=2

t=2

1
Nt
(cid:113) 1
Nt

1{Mt ≤ Bt}

(cid:105)

1{Mt ≤ Bt}

≤ 1 + log T,
(cid:105)

√

≤ 1 +

T .

Achddou Capp´e Garivier

Proof Since winning an auction increments the number of observations Nt by 1,

T
(cid:88)

t=2

(cid:104)(cid:114) 1
Nt

E

Similarly, we get

T
(cid:88)

t=2

E

(cid:104) 1
Nt

(cid:105)
1(Mt ≤ Bt)

≤

T
(cid:88)

T −1
(cid:88)

(cid:114) 1
n

1{Nt = n, Nt+1 = n + 1}

1{Nt = n, Nt = n + 1}

t=2
T −1
(cid:88)

n=1
T −1
(cid:88)

n=1

≤

≤

≤ 1 +

n=1

T
(cid:88)

t=2

(cid:114) 1
n
(cid:114) 1
n

T −1
(cid:88)

(cid:90) n

n−1

n=2
√
T .

≤ 1 +

(cid:114) 1
u

du

(cid:105)
1(Mt ≤ Bt)

≤

T
(cid:88)

T −1
(cid:88)

1
n

1{Nt = n, Nt+1 = n + 1}

n=1

t=2
T −1
(cid:88)

n=1
T −1
(cid:88)

n=1

≤

≤

≤ 1 +

1{Nt = n, Nt = n + 1}

T
(cid:88)

t=2

1
n

1
n

T −1
(cid:88)

(cid:90) n

n=2

n−1

1
u

du

≤ 1 + log T.

Lemma 20 If g1 and g2 are two functions such that (cid:107)g1 − g2(cid:107)∞ ≤ δ, then

g1(b∗

1) − g1(b∗

2) ≤ 2δ

where b∗

1 = max(arg maxb∈[0,1] g1(b)) and b∗

2 = max(arg maxb∈[0,1] g2(b)).

Proof Indeed,

0 ≤ g1(b∗

1) − g1(b∗

1) − g2(b∗

2) + g2(b∗

2) − g1(b∗
2)

2) ≤ g1(b∗
≤ 2δ.

Fast Rate Learning in Stochastic First Price Bidding

Lemma 21 For any a > 0, t ≥ 2a log(a) implies t ≥ a log t.

Proof

a log t ≥ a

(cid:18) t
2a
≥ t/2 + a log(a),

+ log(2a)

(cid:19)

where the ﬁrst inequality follows from the fact that log(x/y) ≤ x/y for any positive x and
y. Hence when t > 2a log(a), t ≥ t/2 + a log t ≥ a log t.

Appendix D. Known F

D.1. Upper Bounds of the Regret of UCBid1

We prove the somewhat more precise form of Theorem 8.

Theorem 8 UCBid1 incurs a regret bounded as follows

RT ≤

1
F (b∗)

(cid:112)γ log T (

√

T + 1) + O(1).

Proof We denote by U U CBid1
decomposed as follows.

t

the function b (cid:55)→ ( ˆVt + (cid:15)t − b)F (b). The regret can be

RT ≤ 1 +

T
(cid:88)

t=2

(cid:16)

P

| ˆVt − v| ≥ (cid:15)t

(cid:17)

+

T
(cid:88)

t=2

(cid:104)
St1

(cid:110)
| ˆVt − v| ≤ (cid:15)t

E

(cid:111)(cid:105)

,

Lemma 16 yields the following bound on the probability of over-estimating ˆVt:

T
(cid:88)

t=2

P(| ˆVt − v| ≥ (cid:15)t) ≤

√

2e

γ(log t)t−γ.

t
(cid:88)

t=1

Since F (x) ≤ 1, ∀x ∈ [0, 1], and (cid:107)U U CBid1

− U (cid:107)∞ = (cid:107)( ˆVt − v + (cid:15)t)F (x)(cid:107)∞ ≤ | ˆVt − v + (cid:15)t|,
we can bound the diﬀerence between the utility function and its (upper conﬁdence) estimate
with high probability:

t

T
(cid:88)

t=2

P((cid:107)U U CBid1
t

− U (cid:107)∞ ≥ 2(cid:15)t) ≤

T
(cid:88)

t=1

2e

√

γ(log t)t−γ.

When (cid:107)U U CBid1

t

− U (cid:107)∞ ≤ 2(cid:15)t, then

|U (b∗) − U (Bt)| ≤ 4(cid:15)t,

Achddou Capp´e Garivier

thanks to Lemma 20. Additionally, using Lemma 1, if ˆVt + (cid:15)t − v ≥ 0 , then Bt ≥ b∗
Therefore,

T
(cid:88)

t=2

1
F (b∗)

(cid:104)

E

St1 {Mt ≤ Bt} 1 {b∗ ≤ Bt} 1

(cid:110)
| ˆVt − v| ≤ (cid:15)t

(cid:111)(cid:105)

(cid:20) U (b∗) − U (Bt)
F (b∗)

E

(cid:20) U (b∗) − U (Bt)
F (b∗)

E

1 {b∗ ≤ Bt} 1 {Mt ≤ Bt} 1

(cid:110)
| ˆVt − v| ≤ (cid:15)t

(cid:111)(cid:21)

(cid:21)
1 {b∗ ≤ Bt} 1 {Mt ≤ Bt} 1 {U (b∗) − U (Bt) ≤ 4(cid:15)t}

1
F (b∗)

E [4(cid:15)t1 {Mt ≤ Bt} 1 {(U (b∗) − U (Bt) ≤ 4(cid:15)t}]

T
(cid:88)

t=2
T
(cid:88)

t=2
T
(cid:88)

t=2
T
(cid:88)

≤

≤

≤

≤

≤

t=2
1
F (b∗)

1
F (b∗)

(cid:114)
2

γ log T
Nt

(cid:112)2γ log T (1 +

√

T ),

where the second inequality comes from Lemma 18 (in fact
and the last inequality comes from Lemma 19.

Using Lemma 16 yields

(cid:110)
| ˆVt − v| ≤ (cid:15)t

(cid:111)

is Ft−1-measurable)

T
(cid:88)

t=2

P(| ˆVt − v| ≥ (cid:15)t) ≤

√

2e

γ(log t)t−γ.

T
(cid:88)

t=1

Combining this with the above decomposition of the regret yields

RT ≤ 1 +

T
(cid:88)

t=1

√

2e

γ(log t)t−γ +

1
F (b∗)

(cid:112)2 log T (1 +

√

T ),

When γ > 1, (cid:80)T

t=1 2e

√

γ(log t)t−γ tends to a constant, and

RT ≤

1
F (b∗)

(cid:112)2γ log T (1 +

√

T ) + O(1),

which concludes the proof.

Theorem 9 If F satisﬁes Assumption 1 and 2, then

RT ≤

2γλC2
f
F (b∗)cf

log2(T ) + O(log T ),

when γ > 1.

Fast Rate Learning in Stochastic First Price Bidding

Proof

Thanks to Lemma 1, if ˆVt + (cid:15)t − v ≥ 0 , then Bt ≥ b∗. Additionally,

thanks to Lemma 4. In particular, if ˆVt + (cid:15)t − v < 2(cid:15)t,

Bt − b∗ ≤ ( ˆVt + (cid:15)t − v),

Bt − b∗ ≤ 2(cid:15)t.

The regret can therefore be decomposed as follows :

RT ≤ 1 +

T
(cid:88)

t=2

P( ˆVt + (cid:15)t − v ≤ 0) +

T
(cid:88)

t=2

P( ˆVt − (cid:15)t − v ≥ 0)

(cid:34) T

(cid:88)

(cid:35)
St1(Bt ∈ [b∗, b∗ + min(2(cid:15)t, ∆)]

+

+ E

t=2

T
(cid:88)

t=2

E [St1(Bt ∈ [b∗ + min(2(cid:15)t, ∆), b∗ + ∆])]

(10)

Let us bound the third term of this inequality. Thanks to Lemma 18 ,

E [St1(Bt ∈ [b∗, b∗ + (cid:15)t])|Ft−1 ∨ σ(1 {Mt ≤ Bt})]

≤

U (b∗) − U (Bt)
F (b∗)

× 1 {Mt ≤ Bt} 1 {b∗ ≤ Bt ≤ b∗ + 2(cid:15)t} ,

(11)

because (Bt ∈ [b∗, b∗ + (cid:15)t]) is Ft−1- measurable. This is why

T
(cid:88)

t=2

E [E [St1(Bt ∈ [b∗, b∗ + min(2(cid:15)t, ∆])|Ft−1 ∨ σ({1 {Mt ≤ Bt})}]]

≤

≤

≤

T
(cid:88)

t=2
T
(cid:88)

t=2
T
(cid:88)

t=2
(cid:34)

≤ E

(cid:20) U (b∗) − U (Bt)
F (b∗)

E

(cid:21)
× 1 {Mt ≤ Bt} 1 {b∗ ≤ Bt ≤ b∗ + min(2(cid:15)t, ∆}

(cid:20) W (q∗) − W (Qt)
F (b∗)

E

× 1 {Mt ≤ Bt} 1 {q∗ ≤ Qt ≤ b∗ + 2Cf (cid:15)t}

(cid:21)

E

(cid:20) λ(q∗ − Qt)2
cf F (b∗)

(cid:21)
× 1 {Mt ≤ Bt} 1 {q∗ ≤ Qt ≤ b∗ + 2Cf (cid:15)t}

λ(2Cf )2
cf F (b∗)

(cid:19)

T
(cid:88)

t=2

(cid:18) γ log T
2Nt

(cid:35)

1 {Mt ≤ Bt}

≤

2λγ ¯Cf
cf F (b∗)

log T (log T + 1),

where the third inequality comes from Lemma 7 and the last one follows from Lemma 19.
Thanks to Lemma 16, the sum of the ﬁrst term and the second term of Equation (10)
tγ which

t=2 P( ˆVt − (cid:15)t − v ≥ 0) ≤ (cid:80)T

t=2 P( ˆVt − v < (cid:15)t) + (cid:80)T

can be bounded by (cid:80)T
is bounded by a constant when γ > 1.

γ log t

t=1 e

√

Achddou Capp´e Garivier

The last term of Equation (10) can be bounded as follows:

T
(cid:88)

t=2

E [St1(Bt ∈ [b∗ + min(2(cid:15)t, ∆), b∗ + ∆])] ≤

≤

≤

≤

≤

T
(cid:88)

t=2
T
(cid:88)

t=2
T
(cid:88)

t=2
T −1
(cid:88)

n=1
T −1
(cid:88)

n=1

P [(∆ > 4(cid:15)t, Mt ≤ Bt, Bt > b∗]

(cid:20)
∆2 > 4

P

γ log T
2Nt

, Mt ≤ Bt, Bt > b∗

(cid:21)

T −1
(cid:88)

(cid:20)

P

∆2 > 2

(cid:21)

γ log T
2Nt

1 [Nt = n, Nt+1 = n + 1]

n=1
(cid:20)

1

n < 4

γ log T
2∆2

(cid:26)

1

n < 4

γ log T
2∆2

1 {Nt = n, Nt+1 = n + 1}

(cid:21) T

(cid:88)

t=2

(cid:27)

where the ﬁrst inequality comes from the fact that when Bt > b∗, a positive instantaneous
regret can only occur if Mt ≤ Bt. By summing all components of the regret,

≤ 4

γ log T
2∆2

RT ≤ 1 + 4

γ log T
2∆2 +

2γλC2
f
F (b∗)cf

(log2(T ) + log T ).

RT ≤

2γλC2
f
F (b∗)cf

log2(T ) + O(log T )

In conclusion,

when γ > 1.

D.2. Lower bound of the regret of optimistic strategies

Lemma 10 Consider all environments where Vt follows a Bernoulli distribution with ex-
pectation v and F satisﬁes Assumption 1 and is such that φ(cid:48) ≤ λ, and there exists cf and
Cf such that 0 < cf < f (b) < Cf , ∀b ∈ [0, 1]. If a strategy is such that, for all such envi-
ronments, Rv,F
T ≤ O(T a), for all a > 0, and there exists γ > 0 such that P(Bt < b∗) < t−γ,
then this strategy must satisfy:

lim inf
T →∞

Rv,F
T
log T

≥ c2

f λ2

(cid:18) v(1 − v)(v − b∗

v,F )

32

(cid:19)

.

Note that this proof is an adaptation of the proof of the parametric lower bound of (Achddou
et al., 2021).

Fast Rate Learning in Stochastic First Price Bidding

Lemma 22 If RT ≤ O(T a), ∀a > 0, and F admits a density which is lower bounded by a
positive constant and upper bounded. Then,
(cid:20) Nt
t
Proof The fraction of won auctions is E (cid:2) Nt
t
F admits a density f , upper bounded by a constant Cf ,

s=1 F (Bs], by the tower rule. Since

(cid:3) = E[ 1
t

= F (b∗).

lim
t→∞

(cid:80)t

E

(cid:21)

E[(F (Bt) − F (b∗))2]] ≤ C2

f E[(Bt − b∗)2].

The consistency assumption implies (cid:80)T
t=1 E[(Bt −b∗)2] ≤ O(T a), ∀a > 0, because of Lemma
6. In particular limt→∞ E[(Bt − b∗)2] = 0. Combining the two previous arguments yields
limt→∞ E[(F (Bt) − F (b∗))2] = 0. Then, because L2-convergence implies L1-convergence,
limt→∞ E[F (Bt)] = F (b∗).
Together with the equality E (cid:2) Nt
(cid:3) = E[ 1
t
t
result proves suﬃces to prove the lemma.

s=1 F (Bs)], and with the Cesaro theorem, this

(cid:80)t

We set a time step t ∈ [1, T ]. We consider two alternative conﬁgurations with identical
distributions for Mt but that diﬀer by the distribution of Vt. The value Vt is distributed
according to a Bernoulli distribution of expectation v in the ﬁrst conﬁguration, respectively
v(cid:48)
t = v +

F (b∗)t , in the second conﬁguration.

(cid:113) v(1−v)

t

Notation. We let Pv(·) denote the probability of an event under the ﬁrst conﬁguration (re-
spectively Ev(·) the expectation of a random variable under the ﬁrst conﬁguration), whereas
Pv(cid:48)
(·)
(·) denotes the probability of an event under the second conﬁguration (respectively Ev(cid:48)
the expectation of a random variable under the ﬁrst conﬁguration). The information col-
lected up to time t + 1 is denoted It : (Mt, V (cid:48)
) is
the law of It in the ﬁrst (respectively second) conﬁguration.

v (respectively PIt
v(cid:48)
t

1). Finally, PIt

t , . . . M1, V (cid:48)

t

The Kullback Leibler divergence between PIt

can be proved to satisfy

v and PIt
v(cid:48)
t

KL(PIt

v , PIt
v(cid:48)
t

) = kl(v, v(cid:48)

t)E[Nt],

exactly like in Equation 7.

Using Lemma 22, ∀(cid:15) > 0, ∃t1((cid:15)), ∀t ≥ t1((cid:15)),

KL(PIt

v , PIt
v(cid:48)
t

) ≤ kl(v, v(cid:48)

t)(1 + (cid:15))F (b∗).

Using the data processing inequality (see for example Garivier et al. (2019)), we get
v,F + b∗
b∗
v(cid:48)
t,F
2

v + b∗
v(cid:48)
t,F

v , PIt
v(cid:48)
t

KL(PIt

) ≥ kl

Bt >

Bt >

, Pv(cid:48)

(cid:33)(cid:33)

Pv

(cid:33)

(cid:32)

(cid:32)

(cid:32)

2

t

(cid:32)

(cid:32)

≥ 2

Pv

Bt >

(cid:32)

(cid:32)

≥ 2

Pv

Bt >

(cid:33)

(cid:33)

v,F + b∗
b∗
v(cid:48)
t,F
2

v,F + b∗
b∗
v(cid:48)
t,F
2

(cid:32)

− Pv(cid:48)

t

Bt >

(cid:32)

+ Pv(cid:48)

t

Bt ≤

v,F + b∗
b∗
v(cid:48)
t,F
2

v,F + b∗
b∗
v(cid:48)
t,F
2

(cid:33)(cid:33)2

(cid:33)

(cid:33)2

− 1

,

Achddou Capp´e Garivier

where the second inequality comes from Pinsker inequality. Consequently, we get

(cid:32)

Pv

Bt >

b∗
v,F + b∗
v(cid:48)
t,F
2

(cid:33)

(cid:32)

+ Pv(cid:48)

t

Bt ≤

(cid:33)

b∗
v,F + b∗
v(cid:48)
t,F
2

≥ 1 −

(cid:114) 1
2

KL(PIt

v , PIt
v(cid:48)
t

).

Speciﬁcally, ∀t > t0((cid:15)),
b∗
v,F + b∗
v(cid:48)
t,F
2

Bt >

Pv

(cid:32)

(cid:33)

(cid:32)

+ Pv(cid:48)

t

Bt ≤

(cid:33)

b∗
v,F + b∗
v(cid:48)
t,F
2

≥ 1 −

(cid:114) 1
2

kl(v, v(cid:48)

t)(1 + (cid:15))F (b∗

v,F )t.

Using the fact that Ev[(Bt − b∗

v,F )2] ≥

(cid:18)

b∗
v,F −

v,F +b∗
b∗
v(cid:48)
t,F
2

(cid:19)2

(cid:18)

Pv

Bt >

(cid:19)

v,F +b∗
b∗
v(cid:48)
t,F
2

yields

Ev[(Bt − b∗

v,F )2] ≥

(cid:32) b∗

v,F − b∗
v(cid:48)
t,F
2

(cid:33)2

(cid:32)

Pv

Bt >

(cid:33)

b∗
v,F + b∗
v(cid:48)
t,F
2

(cid:18)

≥

λ

v − v(cid:48)
t
2

(cid:19)2

≥ λ2 v(1 − v)
4F (b∗
v,F )t

(cid:32)

Pv

Bt >

(cid:32)

1 −

(cid:114) 1
2

(cid:33)

v,F + b∗
b∗
v(cid:48)
t,F
2

(1 + (cid:15))kl(v, v(cid:48)

t)F (b∗

v,F )t − 1/tγ

(cid:33)

,

where the second inequality comes from the fact that v = φF (b∗
and that φ(cid:48)
outputs a bid that does not underestimate b∗
t,F with high probability: Pv(cid:48)
v(cid:48)

t = φF (b∗
t,F ))
v(cid:48)
F ≤ λ and the the second inequality stems from the assumption that the algorithm
t,F ) < 1
(Bt < b∗
tγ .
v(cid:48)
(cid:19)
≤ 1+(cid:15)
2F (b∗

We use the fact that ∀(cid:15) > 0, ∃t2(v, (cid:15)), ∀t ≥ t2(v, (cid:15)), kl

v,F ) (resp. v(cid:48)

v, v +

(cid:114)

(cid:18)

t

v(1−v)
F (b∗
v,F )t

v,F )t
(cid:82) 1
0 g(cid:48)(cid:48)(v(cid:48) + s(v(cid:48) + s(v − v(cid:48)))2(1 − s)ds,

which is proved by observing that kl(v, v(cid:48)) = (v(cid:48)−v)2
where g(x) = kl(x, v(cid:48)); and that thanks to Taylor’s inequality,

2

kl(v, v(cid:48)) ≤

(v(cid:48) − v)2
2

≤ (v(cid:48) − v)2

(cid:90) 1

g(cid:48)(cid:48)(u)ds

0

2 max
u∈[v,v(cid:48)]
1
minu∈[v,v(cid:48)] u(1 − u)

and that ∀(cid:15) > 0, ∃t2(v, (cid:15)), such that minu∈[v,v(cid:48)] u(1 − u) < 1+(cid:15)

v(1−v) . Putting all the pieces

together yields
∀t ≥ max(t1((cid:15)), t2(v, (cid:15))),

Ev[(Bt − b∗

v,F )2] ≥

v(1 − v)
4F (b∗
v,F )t

(cid:32)

1 −

(cid:114) 1
4

(cid:33)

(1 + (cid:15))2 − 1/tγ

.

Let t0(v, (cid:15)) = max(t1((cid:15)), t2(v, (cid:15))). We obtain

T
(cid:88)

t=1

Ev[(Bt − b∗

v,F )2] ≥

T
(cid:88)

t=t0(v,(cid:15))

λ2 v(1 − v)
4F (b∗
v,F )t

(cid:18)

1 −

1
2

(1 + (cid:15)) − 1/tγ

(cid:19)

.

Fast Rate Learning in Stochastic First Price Bidding

Recall that, according to Lemma 6,

RT (v) =

T
(cid:88)

t=1

E (cid:2)U (b∗

v,F ) − U (Bt)(cid:3) ≥

U (b∗
v,F )
4

T
(cid:88)

t=1

Ev[(Qt−q∗)2] ≥

v,F )

f U (b∗
c2
4

T
(cid:88)

t=1

Ev[(Bt−b∗

v,F )2].

Hence, ∀(cid:15) > 0,

RT (v) ≥ λ2

v,F )

f U (b∗
c2
4

(cid:18) v(1 − v)
4

(cid:18)

1 −

1
2

(cid:19)(cid:19)

(1 + (cid:15))

log

T
t0(v, (cid:15))

− O(1).

And ∀(cid:15) > 0,

lim inf
T →∞

RT (v)
log T

≥

f λ2U (b∗
c2
4

v,F )

(cid:32)

v(1 − v)
4F (b∗
v,F )

(cid:18)

1 −

1
2

(cid:19)(cid:33)

(1 + (cid:15))

.

Since this holds for all (cid:15),

lim inf
T →∞

RT (v)
log T

≥ λ2c2
f

(cid:18) v(1 − v)(v − b∗

v,F )

32

(cid:19)

.

Appendix E. Unknown F

E.1. Upper Bound of the Regret of O-UCBid1

Theorem 23 O-UCBid1 incurs a regret bounded by

RT ≤

√
2
4
F (b∗)

(cid:112)γ log T (

√

T + 1) + O(1).

We ﬁrst observe that the algorithm overbids (Bt > b∗) when F and v belong to their

conﬁdence regions Ft = { ˜F , (cid:107)F − ˆFt(cid:107) ≤ (cid:15)t} and Vt = [v − (cid:15)t, v + (cid:15)t].

Lemma 24 The bid submitted by O-UCBid1 is an upper bound of b∗ when (cid:107) ˆUt−U (cid:107)∞ ≤ 2(cid:15)t.

(cid:110)

(cid:107) ˆUt − U (cid:107)∞ ≤ 2(cid:15)t

(cid:111)

implies b∗ ≤ Bt.

Proof Let us pick b ∈ arg max ˆUt.

ˆUt(b) − ˆUt(b∗) = ˆUt(b) − U (b∗) + U (b∗) − ˆUt(b∗) ≤ 4(cid:15)t.

We deduce that ˆUt(b∗) ≥ ˆUt(b) − 4(cid:15)t ≥ max ˆUt − 4(cid:15)t.
Hence, b∗ ∈

(cid:110)
b ∈ [0, 1], ˆUt(b) ≥ max ˆUt − 2(cid:15)t

(cid:111)

. By deﬁnition of Bt, this yields Bt ≥ b∗.

Achddou Capp´e Garivier

Next we observe that if F and v lie in their conﬁdence regions Ft and Vt, then (cid:107) ˆUt −

U (cid:107)∞ ≤ 2(cid:15)t. (Recall that ˆUt(b) = ( ˆVt − b) ˆFt(b).) Indeed, we have

ˆUt(b) − U (b) = ( ˆVt − b) ˆFt(b) − (v − b)F (b)

= ( ˆVt − v)F (b) + ˆVt( ˆFt(b) − F (b)) + b(F (b) − ˆFb)
= ( ˆVt − v)F (b) + ( ˆVt − b)( ˆFt(b) − F (b))

which yields

| ˆUt(b) − U (b)| ≤ | ˆVt − v| + (cid:107)F (b) − ˆFt(b)(cid:107)∞.

(12)

We then decompose the regret into

E(RT ) =

T
(cid:88)

t=1

E(U (b∗) − U (Bt))

≤ 1 +

T
(cid:88)

t=

P(F /∈ Ft or v /∈ Vt) +

(cid:16)

E

T
(cid:88)

t=2

St1(Bt > b∗)1((cid:107) ˆUt − U (cid:107)∞ ≤ 2(cid:15)t, F ∈ Ft, v ∈ Vt

(cid:17)

.

(13)

The second term of the second hand side of Equation 13 is easily bounded thanks to the
concentration inequalities in Lemmas 16 and 17. In fact, combining these latter lemmas
yields the following bound.

Lemma 25

T
(cid:88)

t=2

P(F /∈ Ft or v /∈ Vt) ≤ 2

√

2e

γ(log t)t−γ

T
(cid:88)

t=1

We apply Lemma 18 to bound the third term of the second hand side of Equation 13 as

follows:

(cid:105)

(cid:104)
E

]St1(Bt > b∗)1((cid:107) ˆUt − U (cid:107)∞ ≤ 2(cid:15)t, F ∈ Ft, v ∈ Vt)
1
F (b∗)

(cid:104)
E

(cid:105)
U (b∗) − U (Bt)) × 1(Mt ≤ Bt)1((cid:107)U − ˆUt(cid:107)∞ ≤ 2(cid:15)t, F ∈ Ft, v ∈ Vt)1(Bt > b∗)

,

≤

(14)

because 1(Bt > b∗)1((cid:107) ˆUt −U (cid:107)∞ ≤ 2(cid:15)t, F ∈ Ft, v ∈ Vt) is Ft−1-measurable. We then bound
the deviation (U (b∗) − U (Bt))1(Mt ≤ Bt) by 8(cid:15)t by using Lemma 20.

Lemma 26 When applying the O-UCBid1 strategy, if (cid:107)U − ˆUt(cid:107)∞ ≤ 2(cid:15)t, then

|U (Bt) − U (b∗)| ≤ 8(cid:15)t.

Proof Assume (cid:107)U − ˆUt(cid:107)∞ ≤ 2(cid:15)t. Note that ˆUt(Bt)− ˆUt(b∗) = ˆUt(Bt)− ˆUt(ˆb)+ ˆUt(ˆb)− ˆUt(b∗),
where ˆb = max arg maxb∈[0,1]( ˆVt − b) ˆFt(b).

By design , we have ˆUt(Bt) − ˆUt(ˆb) = −2(cid:15)t. Thanks to Lemma 20, and because (cid:107)U −

ˆUt(cid:107)∞ ≤ 2(cid:15)t we know that 0 ≤ ˆUt(ˆb) − ˆUt(b∗) ≤ 4(cid:15)t. This yields | ˆUt(Bt) − ˆUt(b∗)| ≤ 4(cid:15)t.

Fast Rate Learning in Stochastic First Price Bidding

Finally

|U (Bt) − U (b∗)| ≤ 8(cid:15)t.

Then, by summing, we get

(cid:105)
(cid:104)
]St1(Bt > b∗)1((cid:107) ˆUt − U (cid:107)∞ ≤ 2(cid:15)t, F ∈ Ft, v ∈ Vt)

E

T
(cid:88)

t=2

1
F (b∗)

(cid:16)

E

U (b∗) − U (Bt)) × 1(Mt ≤ Bt)1(Bt > b∗)1((cid:107)U − ˆUt(cid:107)∞ ≤ 2(cid:15)t, F ∈ Ft, v ∈ Vt)

(cid:17)

1
F (b∗)

(cid:104)

(cid:105)
8(cid:15)t × 1(Mt ≤ Bt)1((cid:107)U − ˆUt(cid:107)∞ ≤ 2(cid:15)t)1(Bt > b∗)

E

T
(cid:88)

t=2
T
(cid:88)

t=2
T
(cid:88)

≤

≤

≤

≤

t=2
1
F (b∗)

1
F (b∗)

(cid:104)

E

(cid:114) log T
8
2Nt
√

4(cid:112)2 log T (

T + 1),

1(Mt ≤ Bt)

(cid:105)

where the last inequality comes from Lemma 19. Using Equation 13 and Lemma 25 yields

RT ≤

1
F (b∗)

4(cid:112)2 log T (

√

T + 1) +

T
(cid:88)

t=2

2e

√

γ(log t)t−γ.

Consequently, when γ > 1,

RT ≤

1
F (b∗)

4(cid:112)2 log T (

√

T + 1) + O(1).

E.2. General Upper Bound of the Regret of UCBid1+

We prove a slightly diﬀerent version of Theorem 2 than that of the main paper.

Theorem 2 UCBid1+ incurs a regret bounded by

RT ≤ 12

≤ 12

(cid:114) γα
F (b∗)
1
√
U (b∗)

√

(cid:112)log T

T + O(log T )

vγ(cid:112)log T

√

T + O(log T ),

where α := v

v−b∗ , provided that γ > 2.

Proof We denote by E the event {∀t0 < t < T, | ˆVt − v| ≤ (cid:15)t, (cid:107)F − ˆFt(cid:107)∞ ≤
where t0 := min(3, 1 + 8 γ(α+1)2

).

(cid:16)

(cid:17)

4 γ(α+1)2
α(F (b∗))2

α(F (b∗))2 log

(cid:113) γ log(t−1)

2(t−1) },

Using Lemmas 16 and 17, this event happens with high probability, when γ > 2.

Achddou Capp´e Garivier

Lemma 27 The probability of the complementary of E is bounded as follows

P (cid:0)E C(cid:1) ≤ 4e(γ − 1)(log T )(T )1−γ.

provided that γ > 2.

Proof

We have

(cid:18)

∃t ∈ [t0, T ], ( ˆV (Nt) − v)2 ≥

P

γ log(t − 1)
2Nt

(cid:19)

≤ P

(cid:18)

∃t ∈ [2, T ], ( ˆV (Nt) − v)2 ≥

(cid:19)

γ log(t − 1)
2Nt
(cid:19)

γ log(t − 1)
2Nt

,

T
(cid:88)

t=2
T
(cid:88)

t=1
(cid:90) T

≤

≤

≤

(cid:18)

( ˆV (Nt) − v)2 ≥

P

2e log(t)t−γ

2e log(t)u−γdu

u=1

≤ 2e(γ − 1) log(T )(T )1−γ,

thanks to Lemma 16. Similarly,


∃t ∈ [t0, T ], (cid:107)F − ˆF (cid:107)∞ ≥

P

(cid:115)

γ log(t − 1)
2Nt



 ≤

T
(cid:88)

t=t0


 (cid:107)F − ˆF (cid:107)∞ ≥

P

(cid:115)

γ log(t − 1)
2Nt





≤ 2

T
(cid:88)

t=t0

t−γ

(cid:90) T

≤

u=2

2u−γdu

≤ 2(γ − 1)(T )1−γ

thanks to Lemma 17.

When E occurs, it is possible to prove that F (Bt) is lower-bounded by a positive constant

as soon as t is large enough.

(cid:16)

3, 1 + 8 γ(α+1)2

α(F (b∗))2 log

(cid:16)

4 γ(α+1)2
α(F (b∗))2

(cid:17)(cid:17)

, F (Bt)

Lemma 28 On E, provided that t > t0 := min
is lower bounded by

F (Bt) >

where α = v

v−b∗ .
Proof b∗ = α−1

α v. Since we are on E,

F (b∗)
2α

,

b∗ ≤

α − 1
α

( ˆVt + (cid:15)t).

Fast Rate Learning in Stochastic First Price Bidding

Hence

Since Bt > 0,

And

By deﬁnition of Bt,

which implies

Now,

ˆVt + (cid:15)t ≤ α( ˆVt + (cid:15)t − b∗).

ˆVt + (cid:15)t − Bt ≤ α( ˆVt + (cid:15)t − b∗).

ˆVt + (cid:15)t − Bt
ˆVt + (cid:15)t − b∗

≤ α.

( ˆVt + (cid:15)t − Bt) ˆFt(Bt) ≥ ( ˆVt + (cid:15)t − b∗) ˆFt(b∗)

ˆFt(Bt) ≥

ˆVt + (cid:15)t − b∗
ˆVt + (cid:15)t − Bt

ˆFt(b∗) ≥

1
α

ˆFt(b∗)

F (Bt) ≥ ˆFt(Bt) −

(cid:115)

γ log(t − 1)
2(t − 1)

(cid:115)

ˆFt(b∗) −

γ log(t − 1)
2(t − 1)
(cid:19) (cid:115)

F (b∗) −

+ 1

(cid:18) 1
α

γ log(t − 1)
2(t − 1)

,

≥

≥

1
α

1
α

because we assume that we are on E. Note that if t > t0, then

thanks to Lemma 21, and

4γ(α + 1)2

F (b∗)2 <

(t − 1)
log(t − 1)

,

(cid:19) (cid:115)

(cid:18) 1
α

+ 1

γ log(t − 1)
2(t − 1)

<

1
2α

F (b∗),

so that

which concludes the proof.

F (Bt) ≥

F (b∗)
2α

,

Lemma 29 ∀t > t0,

(cid:18)

P

Nt <

1
4α

F (b∗)(t − t0), E

(cid:19)

(cid:32)

≤ exp

−

2(( 1

2α F (b∗))2
4

(cid:33)

(t − t0)

.

Achddou Capp´e Garivier

Indeed if t ≥ t0, then Nt is larger than the sum N (cid:48)

Proof
Bernoulli distribution with average 1
intersected with E can be bounded as follows.

2α F (b∗) , hence the probability that Nt < 1

t of t − t0 samples from a
4α F (b∗)(t−t0)

(cid:18)

P

Nt <

1
4α

F (b∗)(t − t0), E

(cid:19)

≤ P

≤ P

(cid:18) 1
2α
(cid:32)

≤ exp

−

(cid:32)

≤ exp

−

(cid:18)

N (cid:48)

t < +

(cid:19)

F (b∗)(t − t0)

1
4α

F (b∗)(t − t0) − (N (cid:48)

2(( 1

2α F (b∗))2
4

2(( 1

2α F (b∗))2
4

(t − t0)

(cid:33)

(t − t0)

,

(cid:19)

F (b∗)(t − t0)

1
4α

t − t0) >
(cid:33)

where we used Hoeﬀding’s inequality for the third inequality.

Finally, we can prove that the expected instantaneous regret conditioned on Bt

is

bounded by a multiple of (cid:15)t.

Lemma 30

U (Bt) − U (b∗) ≤ 6(cid:15)t

Proof

Thanks to Equation 12, we have (cid:107) ˆUt − U (cid:107)∞ ≤ 2(cid:15)t. Very similarly we have

(cid:107)U U CBid1+

t

− ˆU (cid:107)∞ = max
b∈[0,1]

|(cid:15)t ˆFt(b)| ≤ (cid:15)t,

where U U CBid1+ : b (cid:55)→ ( ˆVt + (cid:15)t − b) ˆFt(b). Hence,

By Lemma 20, this yields

(cid:107)U U CBid1+

t

− U (cid:107)∞ ≤ 3(cid:15)t.

U (Bt) − U (b∗) ≤ 6(cid:15)t

Proof of the Theorem We use the following decomposition

RT ≤ T × P(E c) +

T
(cid:88)

t=1

E[St1{E}]

≤ T × P(E c) + t0 +

T
(cid:88)

t=t0

E[St1{E}]

Fast Rate Learning in Stochastic First Price Bidding

Thanks to Lemma 28, and when t > t0, F (Bt) ≥ 1
1
4α F (b∗)(t − t0), ∀t > t0 with high probability.

Thanks to Lemma 29,

2α F (b∗). Using this, we get Nt >

(cid:32)

E[St1{E}] ≤ exp

−

(cid:32)

≤ exp

−

2(( 1

2α F (b∗))2
4

2(( 1

2α F (b∗))2
4

By summing,

(cid:33)

(t − t0)

+ E

(cid:20)
St1{Nt ≥

1
4α

(cid:21)
F (b∗)(t − t0)}

(cid:33)

(t − t0)

+ E

(cid:115)

(cid:34)
6

4αγ log T
F (b∗)(t − t0)

1{Nt ≥

1
4α

(cid:35)
F (b∗)(t − t0)}

;

T
(cid:88)

t=t0

E[St1{E}] ≤

(cid:32)

exp

−

2(( 1

2α F (b∗))2
4

T
(cid:88)

t=t0

(cid:33)

(t − t0)

+

(cid:115)

T
(cid:88)

t=t0

6

4αγ log T
F (b∗)(t − t0)

≤

≤

1
2( 1

1 − exp(−

4
1
2α F (b∗)

+ 6

2α F (b∗))2
4

(cid:115)

4αγ
F (b∗)

(cid:112)log T

√

T

(cid:115)

4αγ
F (b∗)

+ 6

)

(cid:112)log T (

√

T ),

where the last inequality comes from 1 − exp(−u) ≥ 2/u, for any positive u. Using the
decomposition of the regret yields

RT ≤ t0 + T P(E C) +

≤ 4 + 8

≤ 4 +

γ(α + 1)2
α(F (b∗))2 log
8α
F (b∗)

+ 8

(cid:112)log T

√

T

4
1
2α F (b∗)
(cid:18)

(cid:115)

+ 6

4α
F (b∗)
(cid:19)

4

γ(α + 1)2
α(F (b∗))2

+ 4e(γ − 1) log T (T )2−γ +

γ(α + 1)2
α(F (b∗))2 log

(cid:18)
4

γ(α + 1)2
α(F (b∗))2

(cid:19)

+ 4e(γ − 1) log T + 12

(cid:114) αγ

F (b∗)

8α
F (b∗)

+ 12

(cid:114) αγ

(cid:112)log T

√

T

F (b∗)
(cid:112)log T (

√

T ),

which concludes the proof.

E.3. Proof of an Intermediary Regret Rate under Assumptions 1 and 2

In this section, we prove an easier version of Theorem 13. We will use lemmas of the
previous subsection for this version as well as for the more complex version. In particular
we have already proven that E ∩ {Nt ≥ 1
4α F (b∗)t}, occurs with high probability. Under
Assumptions 1 and 2 and on this event, we prove the following result.

Lemma 31 Under Assumptions 1 and 2 and if t > max(t0, t1),
t and |v − ˆVt| ≤ (cid:15)+
t ,

• (cid:107)F − ˆFt(cid:107)∞ ≤ (cid:15)+

• |U (b∗) − U (Bt)| ≤ 6(cid:15)+
t

Achddou Capp´e Garivier

• |b∗ − Bt| ≤ ∆,

• |b∗ − Bt| ≤ 1/

√

(cid:113)

6(cid:15)+
t .

cU

• |U (b∗) − U (Bt)| ≤ CU (b∗ − Bt)2

on E ∩ {Nt ≥ 1

4α F (b∗)t}, where

(cid:16)

3, 1 + 8 γ(α+1)2

α(F (b∗))2 log

(cid:16)

4 γ(α+1)2
α(F (b∗))2

(cid:17)(cid:17)

F (b∗) log T,






t0 = min
√

Cu∆1/4 γα
t1 = 2
(cid:113) 2αγ log t
(cid:15)+
t =
F (b∗)t ,
1
4 U (b∗),
cU = cf
CU = Cf
λ.
cf

Proof On the event E ∩ {Nt ≥ 1
(cid:15)+
t =
and 29.

(cid:113) 2αγ log t
F (b∗)t

from Lemmas, 16,17 29 and |U (b∗) − U (Bt)| ≤ 6(cid:15)t ≤ 6(cid:15)+
t

4α F (b∗)t}, (cid:107)F − ˆFt(cid:107)∞ ≤ (cid:15)+

t and |v − ˆVt| ≤ (cid:15)+

t where
from Lemmas 30

Under Assumptions 1 and 2, we prove that after t1, we have |Bt − b∗| ≤ ∆ on E ∩ {Nt ≥
1
4α F (b∗)t}, so that we will be able to use the boundedness of the density after this time
step.

When F satisﬁes assumption 1, U is unimodal, as shown in the proof of Lemma 3, and

so if

then

U (b∗) − U (b) ≤ min(U (b∗) − U (b∗ − ∆), U (b∗) − U (b∗ + ∆)),

b ∈ [b∗ − ∆, b∗ + ∆].

It follows that if

6(cid:15)+

t ≤ min(U (b∗) − U (b∗ − ∆), U (b∗) − U (b∗ + ∆))

and therefore 6(cid:15)+

t ≤ Cu∆ where Cu := λCf /cf (see Lemma 7), then

|b∗ − Bt| ≤ ∆
√

cu∆1/4 γα

F (b∗) log T := t1, we have |Bt − b∗| ≤ ∆

on E ∩ {Nt ≥ 1
on E ∩ {Nt ≥ 1

4α F (b∗)t}. Then, for all t > 2
4α F (b∗)t}.

Under Assumption 1, for any q ∈ [0, 1], Wv,F (q∗

v,F ) − Wv,F (q) ≥ 1

4 (q∗

We have U = W ◦F , so that if t > t1, then Bt ∈ [b∗−∆, b∗+∆] and U (b∗)−U (Bt) ≥ cf
Bt)2U (b∗) := cU (b∗ − Bt)2. In this case, we can also prove that |b∗ − Bt| ≤ 1/
cU
under E ∩ {Nt ≥ 1

√

4α F (b∗)t}.

v,F − q)2Wv,F (q∗
v,F ).
1
4 (b∗−
(cid:113)
6(cid:15)+
t ,

Proposition 32 Under Assumptions 1 and 2 and if t > max(t0, t1), δt < ∆ , |Bt −b∗| ≤ δt,
and (cid:15)+
Then

t ≤ M δt,

|Bt − b∗|2 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

6
cU

Cf δt log

(cid:16) M e2t

√

2t

2cf η2

t

(cid:17)

+

2 log( M t

√
2t
2cf η2 )

cU t

+

2
cU

(2Cf + 1)δt

(cid:115)

2αγ log T
F (b∗)t

,

Fast Rate Learning in Stochastic First Price Bidding

with probability 1 − η on E ∩ {Nt ≥ 1

4α F (b∗)t}.

Proof It is clear from Lemma 12 that

| ˆFt(b)−F (b)−( ˆFt(b∗)−F (b∗))| ≤ 2

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

2Cf δt log

(cid:19)

√
t√
e
2cf δtη

(cid:18)

t

log(

+2

t
2cf δtη2 )
6t

:= βt,

sup
b∗−δt≤b≤b∗+δt

with probability 1−η. We can also decompose U (b)−U U CBid1+
into

t

(b)−(U U CBid1+
t

(b∗)−U (b∗))

U (b) − U U CBid1+
t
= (v − b)F (b) − ( ˆVt + (cid:15)t − b) ˆFt(b) −

(b) − (U U CBid1+
t

(b∗) − U (b∗))

(cid:16)

(v − b∗)F (b∗) − ( ˆVt + (cid:15)t − b∗) ˆF ∗

(cid:17)

t(b)

= (v − b)F (b) − (v − b) ˆFt(b) −

(cid:16)

(cid:16)

= (v − b∗)

F (b) − ˆFt(b) −
+ (b∗ − b)( ˆF (b) − ˆFt(b))

(cid:16)

(v − b∗)F (b∗) − (v − b∗) ˆF ∗
(cid:17)(cid:17)

F (b∗) − ˆF ∗

t(b)

− ( ˆVt + (cid:15)t − v)

− ( ˆVt + (cid:15)t − v)
(cid:17)

(cid:16) ˆFt(b) − ˆFt(b∗)

(cid:17)

t(b)

(cid:17)
(cid:16) ˆFt(b) − ˆFt(b∗)

which in turn proves that

|U (b) − U U CBid1+
t

(b) − (U U CBid1+
t

(b∗) − U (b∗))| ≤ βt + 2(cid:15)t| ˆFt(b) − ˆFt(b∗)| + δt| ˆFt(b) − ˆFt(b)|

≤ βt + 2(cid:15)+
≤ βt + 2(cid:15)+
≤ 3βt + (2Cf + 1)δt(cid:15)+
t

t (Cf δt + βt) + δt(cid:15)+
t
t βt + (2Cf + 1)δt(cid:15)+
t
:= γt,

for all b in [b∗ − δt, b∗ + δt].

Now, we know that U (b∗) − U (b) is lower bounded by cU (b∗ − b)2, on this interval

(b) − U (b) + U U CBid1+

and (cid:107)U U CBid1+
t
t
the shifted version of U deﬁned by G(b) = U (b) + U U CBid1+
and G(b∗) − G(b) is lower bounded by cU (b∗ − b)2
then cU (Bt − b∗)2 ≤ G(b∗) − G(Bt) ≤ 2γt (see Lemma 20).

t

(b∗) − U (b∗)(cid:107)∞ ≤ γt on [b∗ − δt, b∗ + δt]. We call G
(b∗) − U (b∗). Its argmax is b∗

Then , by deﬁnition of γt and βt:

(Bt − b∗)2 ≤

≤

≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

6
cU

6
cU

6
cU

Cf δt log

(cid:16) e2t

2cf δtη2

(cid:17)

t

(cid:16)

Cf δt log

(cid:17)

M e2t
2cf (cid:15)+

t η2

Cf δt log

t
(cid:16) M e2t
2cf η2

√

(cid:17)

t

t

+

2 log(

t
2cf δtη2 )
cU t

+

2
cU

+

(2Cf + 1)δt(cid:15)+
t

t η2 )

+

2 log( M t
2cf (cid:15)+
cU t
√
t
2cf η2 )

2 log( M t

cU t

+

2
cU

(2Cf + 1)δt(cid:15)+
t

+

(2Cf + 1)δt

2
cU
(cid:113) F (b∗)t

(cid:115)

2αγ log T
F (b∗)t

.

√

t since α, γ ≥ 1.

where the last inequality stems from that fact that 1/(cid:15)+

t =

2αγ log t ≤

Achddou Capp´e Garivier

Theorem 33 Under Assumptions 1 and 2,

RT ≤ O(T 3/8 log T ).

Proof

From Lemma 31, we have that |b∗ − Bt| ≤ 1/

√

(cid:113)

cU
(cid:113)

6(cid:15)+

t , on E ∩ {Nt ≥ 1

4α F (b∗)t}}.

√
cU√
6

Therefore, we can apply Proposition 32 with δt = 1√
cU

6(cid:15)+

t with M =

, and η = 1

t .

We use the general fact that log(Atα) ≤ 2α log t as soon as tα > A, for all A, a > 0, to

derive the following two inequalities :

∀t ≥

(cid:16) M e2
2cf

(cid:17) 1
4 ,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

6
cU

Cf δt log

√

(cid:16) M e2t
2cf η2

t

(cid:17)

t

≤

√
6

8(cid:112)Cf

(cid:114)

5
c
4
U

δt log t
t

=

24(72αγ)
5
4

U F (b∗)
c

(cid:115)

1

8 (cid:112)Cf

log2 t

1
8

5
4

t

.

∀t ≥ ( M
2cf

1
4 ,

)

√
2 log( M t2t
2cf

t

)

cU t

≤

16
cU

log t
t

.

We also have, for all t,

2
cU

(2Cf + 1)δt

(cid:115)

2γα log T
F (b∗)t

≤

=

(cid:115)

2
cU

(2Cf + 1)

(cid:114)

log t
t

δt

2γα
F (b∗)
(cid:115)

2(72αγ)

3
2

U F (b∗)
c

1
4

1
4

(2Cf + 1)

2γα
F (b∗)

(log t)
1
4

t

(cid:114)

1
4

log t
t

Therefore |Bt − b∗|2 ≤

(cid:32)

c
(cid:16) M e2
ability 1 − 1
2cf
U (b∗) − U (Bt) ≤ CU (b∗ − Bt)2

t , for t ≥ max(

24(72αγ)
5
1
4
U F (b∗)
8
(cid:17) 1

√

1
8

Cf

+ 16
cU

+ 2(72αγ)
3
2
U F (b∗)

c

1
4

1
4

(2Cf + 1)

(cid:114) 2γα
F (b∗)

1
8

(cid:33)

log t
5
8

t

with prob-

4 , ( M
2cf

1

)

4 ) := t2 on E ∩ {Nt ≥ 1

4α F (b∗)t}. On this event,

Fast Rate Learning in Stochastic First Price Bidding

We use the following decomposition

RT ≤ T × P(E c) +

T
(cid:88)

t=1

E[St1{E}]

≤ T × P(E c) + max(t0, t1, t2) +

T
(cid:88)

E[St1{E}]

≤ T × P(E c) + max(t0, t1, t2) +

t=max(t0,t1,t2)

T
(cid:88)

t=max(t0,t1,t2)

P(E ∩ {Nt <

1
4α

F (b∗)t})

T
(cid:88)

+

t=max(t0,t1,t2)

E[St1{E ∩ {Nt ≥

1
4α

F (b∗)t}}]

≤ T × P(E c) + max(t0, t1, t2) +

T
(cid:88)

t=max(t0,t1,t2)

P(E ∩ {Nt <

1
4α

F (b∗)t})

T
(cid:88)

+

t=max(t0,t1,t2)

CU E[(b∗ − Bt)2]

≤ T × P(E c) + max(t0, t1, t2) +

T
(cid:88)

t=max(t0,t1,t2)

P(E ∩ {Nt <

1
4α

F (b∗)t})

T
(cid:88)

+

t=max(t0,t1,t2)

C0

log t
5
8

t

+

T
(cid:88)

t=max(t0,t1,t2)

1
t

≤ T × P(E c) + max(t0, t1, t2) +

T
(cid:88)

t=max(t0,t1,t2)

P(E ∩ {Nt <

1
4α

F (b∗)t})

+ C0

8
3

3
8 log T + log T

T

≤ log T + 4e(γ − 1) log T (T )2−γ + max(t0, t1, t2) +

8α
F (b∗)

+

8
3

C0T

3
8 log T.

t0 = min
√

t1 = 2

where

t2 = max(

(cid:16)

3, 1 + 8 γ(α+1)2

α(F (b∗))2 log

(cid:16)

4 γ(α+1)2
α(F (b∗))2

(cid:17)(cid:17)

Cu∆1/4 γα
(cid:17) 1

F (b∗) log T,
cU
4 , (
√

√

(cid:16) √
2cf

2cf

cU e2
√
6
√

(cid:32)

1
8

24(72αγ)
5
4
U F (b∗)
c

1
8

Cf

+ 16
cU

+ 2(72αγ)
3
2
U F (b∗)
c

1
4

1
4 ) = (

)

6

cU e2
√
6

1
4 ,

)

√

2cf
1
4

(2Cf + 1)

(cid:33)

(cid:113) 2γα
F (b∗)

CU .






C0 =

Therefore

RT ≤

8
3

C0T

3
8 log T + o(T

3
8 log T ).

Achddou Capp´e Garivier

E.4. Proof of Theorem 13

Theorem 33 is proved by applying Proposition 32 once. By iterating the argument, we can
actually achieve a regret of the order of T a, for any a > 1
3 . The proof involves an induction
argument. The following lemma is the main element of the proof of the induction.

Lemma 34 Assume that t and F satisfy the assumptions of Proposition 32. Assume that
such that δ(k)
|Bt − b∗| is bounded by δ(k)
t = min(1, C(k) log(t)t−uk ) with probability 1 −
η(k), and uk < 2/3, C(k) ≥ 1 . Then |Bt − b∗| is bounded by δ(k+1)
=
min(1, C(k+1) log(t)t− 1
where C(k+1) = C (cid:0)C(k)(cid:1) 1

4 (1+uk)) with probability 1 − η(k) − 1
2Cf

4 and where C = max

such that δ(k+1)

Kt ,

√

1,

(cid:18)

(cid:19)

12

(2Cf + 1)

.

t

t

t

(cid:113) 2γα
F (b∗

+ 16
cU

+ 2
cU

cU

Proof

prove that

|Bt −b∗|2 ≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

6
cU

We use Proposition 32, and the fact that (cid:15)+

t ≤ (cid:112)2αγ/F (b∗) log t

t−uk ≤ (cid:112)2αγ/F (b∗)δ(k)

t

to

Cf δ(k)
t

log

(cid:16) M e2t

√

2tK2t2

(cid:17)

2cf

t

√
2 log( M K2t2t
2cf

2t

)

cU t

+

+

2
cU

(2Cf +1)δ(k)

t

(cid:115)

2αγ log t
F (b∗)t

,

with probability (1 − η(k))(1 − 1

Kt ) and with M = (cid:112)2αγ/F (b∗)

We use the general fact that log(Atα) ≤ 2α log t as soon as tα > A, for all A, a > 0, to

derive the following two inequalities :
(cid:17) 1
4 ,

∀t ≥

(cid:16) M e2K2
2cf

(cid:118)
(cid:117)
(cid:117)
(cid:116)

6
cU

Cf δ(k)
t

log

(cid:16) M e2t

√

2tK2t2

(cid:17)

2cf

t

≤

6(cid:112)8Cf
cU

(cid:115)

δ(k)
t

log t
t

:= C1

(cid:115)

δ(k)
t

log t
t

:= C1β1,t.

∀t ≥ ( M K2
2cf

1
4 ,

)

√
2 log( M K2t2t
2cf

2t

)

cU t

≤

16
cU

log t
t

:= C2

log t
t

:= C2β2,t.

We also have, for all t,

(cid:115)

2
cU

(2Cf + 1)δ(k)

t

2αγ log T
F (b∗)t

≤

2
cU

(2Cf + 1)

(cid:115)

2γα
F (b∗ δ(k)

t

(cid:114)

log t
t

:= C3δ(k)

t

(cid:114)

log t
t

:= C3β3,t

We can derive the following bounds

• β3,t ≤ β1,t since δ(k)

t ≤ 1.

• β2,t ≤ β1,t since δ(k)

t = min(1, C(k) log(t)t−uk ) ≥ log t
t

.

Fast Rate Learning in Stochastic First Price Bidding

Hence

|Bt − b∗|2 ≤ (C1 + C2 + C3)β1,t = (C1 + C2 + C3)

(cid:115)

δ(k)
t

log t
t

,

with probability 1 − η(k) 1

Kt . This yields

|Bt − b∗| ≤ (cid:112)(C1 + C2 + C3)

(cid:32)

δ(k)
t

(cid:33) 1
4

log t
t

≤ (cid:112)(C1 + C2 + C3)

(cid:32)

min(1, C(k) log2(t)t−uk )
t

(cid:33) 1
4

≤ (cid:112)(C1 + C2 + C3)(C(k))1/4t− 1
t− 1

4 (1+uk) log t,

C(k)(cid:17)1/4

≤ C

(cid:16)

4 (1+uk) log t

Proposition 35 Assume that t and F satisfy the assumptions of Proposition 32. If t >
1
4

2αγ/F (b∗)e2K2

2αγ/F (b∗)K2

√

√

, then on E ∩ {Nt ≥ 1

t3 = max

(cid:18)
(

1
4 , (

(cid:19)

)

)

4α F (b∗)t},

2cf

2cf

|Bt − b∗| ≤ C(0)C

3 + 1

3×4K + 1

4K+1 ,

1

3 log(t)t− 1
√

12

2Cf

cU

+ 16
cU

+ 2
cU

(2Cf + 1)

(cid:19)

(cid:113) 2γα
F (b∗)

, and C(0) =

with probability 1 − 1

(cid:18)

max

1,

(cid:113) 1
cU

(cid:16) 72γα
F (b∗)

t where C = max
(cid:17) 1
4

(cid:19)

(cid:18)

1,

Proof The proposition follows from using an induction argument based on Lemma 34. We
can initiate an induction argument with δ(0)

such that

t

δ(0)
t = min(1, C(0) log(t)t−uk ),

writing u0 = 1
and C(k) as deﬁned as in Lemma 34 satisfy uk+1 = 1

4 and C(0) = max(1,

(cid:113) 1
cU

(cid:16) 72αγ
F (b∗)

(cid:17)1/4

), thanks to Lemma 31. The fact that uk

4 (1 + uk) which yields

uK =

(cid:19)K

(cid:18) 1
4

u0 +

(cid:19)K

K
(cid:88)

i=1

1
4i =

(cid:18) 1
4

u0 + 4

1/4 − (1/4)K+1
3

and C(k+1) = C × (C(k))

1
4 which yields

C(K) =

(cid:16)

C(0)(cid:17) 1

4K C

(cid:80)K

i=1

1
4i ≤ C

1
3 ,

Achddou Capp´e Garivier

suﬃces to complete the induction.

We recall Theorem 13.

Theorem 13 Under Assumptions 1 and 2,

for any (cid:15) > 0 as long as γ > 2.

RT ≤ O(T 1/3+(cid:15)),

We choose K such that 1

3 + 2

3×4K + 2

4K+1 < 1

3 + (cid:15). (We can choose K = (cid:6) log4

for example). Then, thanks to proposition 35, for all t > t3, on E ∩ {Nt ≥ 1

(cid:1) (cid:7)) + 1

1
(cid:15)

(cid:0) 3
14
4α F (b∗)t},

|Bt − b∗| ≤ C(0)C

1

3 log(t)t− 1

3 + 1

3×4K + 1

4K+1 ,

with probability 1 − 1
Theorem 33.

t . We can therefore do the same decomposition as in the proof of

Fast Rate Learning in Stochastic First Price Bidding

RT ≤ T × P(E c) + max(t0, t1, t3) +

T
(cid:88)

t=max(t0,t1,t3)

P(E ∩ {Nt <

1
4α

F (b∗)t})

T
(cid:88)

+

t=max(t0,t1,t3)

E[St1{E ∩ {Nt ≥

1
4α

F (b∗)t}}]

≤ T × P(E c) + max(t0, t1, t3) +

T
(cid:88)

t=max(t0,t1,t3)

P(E ∩ {Nt <

1
4α

F (b∗)t})

T
(cid:88)

+

t=max(t0,t1,t3)

CU E[(b∗ − Bt)2]

≤ T × P(E c) + max(t0, t1, t3) +

T
(cid:88)

t=max(t0,t1,t3)

P(E ∩ {Nt <

1
4α

F (b∗)t})

1

3 (log t)t− 2

3 + 2

3×4K + 2

4K+1

T
(cid:88)

+

C(0)CU C

t=max(t0,t1,t3)

T
(cid:88)

+

t=max(t0,t1,t3)

1
t

≤ T × P(E c) + max(t0, t1, t3) +

T
(cid:88)

1
3

+ C(0)CU C

1
3×4K + 2
4K+1
≤ log T + 4e(γ − 1) log T (T )2−γ + max(t0, t1, t3)

3×4K + 2

3 + 2

T

1

t=max(t0,t1,t3)
3 + 2

1

P(E ∩ {Nt <

1
4α

F (b∗)t})

4K+1 log T + log T

+

8α
F (b∗)

+ 3C(0)CU C

1
3 T

1

3 +(cid:15).

(cid:16)

3, 1 + 8 γ(α+1)2

α(F (b∗))2 log

(cid:16)

4 γ(α+1)2
α(F (b∗))2

(cid:17)(cid:17)






t0 = min
√
√

t1 = 2

t3 = (

cu∆1/4 γα
2αγ/F (b∗)e2K2

F (b∗) log T,
1
4 ,
)
(cid:16) 72γα
F (b∗)

(cid:113) 1
cU
√

2cf
(cid:18)

1,

12

2Cf

cU

C(0) = max
(cid:18)

C = max

1,

where

Hence

(cid:19)

(cid:17) 1
4

.

+ 16
cU

+ 2
cU

(2Cf + 1)

(cid:19)

(cid:113) 2γα
F (b∗)

RT ≤ O(T 1/3+(cid:15)).

Achddou Capp´e Garivier

Appendix F. Further ﬁgures

We present in Figure 10 the histogram of the normalized data used to simulate the real-world
experiment.

Figure 10: Bidding Data histogram

0.00.20.40.60.81.0010002000300040005000