Separation of Alpha-Stable Random Vectors
Mathieu Fontaine, Roland Badeau, Antoine Liutkus

To cite this version:

Mathieu Fontaine, Roland Badeau, Antoine Liutkus. Separation of Alpha-Stable Random Vectors.
Signal Processing, 2020, pp.107465. ￿10.1016/j.sigpro.2020.107465￿. ￿hal-02433213￿

HAL Id: hal-02433213

https://inria.hal.science/hal-02433213

Submitted on 9 Jan 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Separation of Alpha-Stable Random Vectors(cid:73)

Mathieu Fontaine

Université de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France

Roland Badeau

LTCI, Télécom Paris, Institut Polytechnique de Paris, Paris, France

Antoine Liutkus

Inria, LIRMM, University of Montpellier, France

Abstract

Source separation aims at decomposing a vector into additive components. This
is often done by ﬁrst estimating source parameters before feeding them into a
ﬁltering method, often based on ratios of covariances. The whole pipeline is
traditionally rooted in some probabilistic framework providing both the likeli-
hood for parameter estimation and the separation method. While Gaussians
are ubiquitous for this purpose, many studies showed the beneﬁt of heavy-tailed
models for estimation. However, there is no counterpart ﬁltering method to date
exploiting such formalism, so that related studies revert to covariance-based ﬁl-
tering after estimation is ﬁnished.

Here, we introduce a new multivariate separation technique, that fully ex-
ploits the ﬂexibility of α-stable heavy-tailed distributions. We show how a spa-
tial representation can be exploited, which decomposes the observation as an
inﬁnite sum of contributions originating from all directions. Two methods for
separation are derived. The ﬁrst one is non-linear and similar to a beamforming
technique, while the second one is linear, but minimizes a covariation criterion,
which is the counterpart of the covariance for α-stable vectors. We evaluate the
proposed techniques in a large number of challenging and adverse situations on
synthetic experiments, demonstrating their performance for the extraction of
signals from strong interferences.

Keywords: alpha-stable distribution, separation theory, additive models,
measure theory, optimization

(cid:73)This work was partly supported by the research programmes KAMoulox (ANR-15-CE38-
0003-01) and EDiSon3D (ANR-13-CORD-0008-01) funded by ANR, the French State agency
for research.

Email addresses: fontaine.mathieu2@gmail.com (Mathieu Fontaine),

roland.badeau@telecom-paris.fr (Roland Badeau), antoine.liutkus@inria.fr
(Antoine Liutkus)

Preprint submitted to Signal Processing

November 19, 2019

2

1. Introduction

Source separation is the task that consists in decomposing a signal into
additive components.
It is a very active research area in signal processing,
notably because of its numerous applications. In audio for instance, it is the
natural paradigm for denoising (Godsill and Rayner [17], Godsill et al. [16],
Fontaine et al. [14]) or for the demixing of music and speech recordings (Raﬁi
et al. [33], Vincent et al. [39]). It also ﬁnds applications in image processing and
biological signal processing (Damon et al. [10], Cavalcant et al. [8]), to name
just a few (Comon and Jutten [9]).

Although there was some successful recent research on end-to-end methods
that directly produce source estimates when fed with a mixture (Wang et al.
[42], Venkataramani et al. [38]), the most common separation processing pipeline
consists in two steps done sequentially. First, the mixture is fed into some es-
timation system. This results in a set of parameters that are used in a second
step to design a source-speciﬁc ﬁlter, which is applied to the mixture to pro-
duce estimates. Formally, this ﬁltering operation boils down to computations
performed on many mixture vectors in an independent manner. For instance,
the observed (multivariate) mixture samples may directly be assumed indepen-
dent as routinely done in biological signal processing. In other cases like audio
where temporal dependencies cannot be neglected, it is common to apply some
Time-Frequency (TF) analysis ﬁrst and then to assume independence in this
transformed domain (Benaroya et al. [4], Duong et al. [11], Liutkus et al. [22]).
Separating an observed vector into additive components requires further as-
sumptions that are usually encoded into a probabilistic model permitting infer-
ence. More precisely, the required feature of such a model is to allow derivation
of the posterior distribution of one source given the observation of the mixture
and the knowledge of the model parameters. An ubiquitous example of such a
model is the Gaussian case, for which each source is described through a covari-
ance matrix, and separation is easily performed in an analytical form. It turns
out that such a model subsumes the popular, yet degenerate case where each
source lies in a linear subspace, corresponding to its direction of arrival (Duong
et al. [11]). In any case, this whole linear estimation theory enjoys a rich history
whose roots can be traced back to the work of N. Wiener in the 1940s (Wiener
[43]). From a broader perspective, we see that the core challenge faced by most
source separation methods is strongly related to additive probabilistic mod-
els (Duvenaud et al. [12], Febrero-Bande and González-Manteiga [13], Wood
et al. [44], Marra and Wood [25]). The particular twist in this respect is that
the models chosen for separation should provide a way to recover the additive
sources.

Although covariance modeling for source separation has enjoyed a strong
popularity due to the simplicity and eﬀectiveness of the separation procedure,
experience shows that it also suﬀers from some weaknesses. First and foremost,

3

Gaussian processes realizations may not explore more than a few standard de-
viations, which means that Bayesian inference in these models is intrinsically
very sensitive to initialization, since the probability mass is almost everywhere
negligible. A common workaround is to further constrain covariance models
through shallow (Ozerov et al. [31]) or deep (Nugraha et al. [30]) parametric con-
straints, but another complementary route is to simply opt for heavy-tail mod-
els, for which much more robust inference is possible. For instance, multivariate
Laplace ﬁlters (Wang et al. [41]) were successfully used for robust detection and
result from Bayesian inference in a state-space model where some variables are
Laplace distributed. In the same vein, a Student’s t ﬁlter (Roth et al. [34]) was
also proposed for a tracking scenario and exploits the heavy-tailed nature of
the Student’s t distribution. Likewise, this distribution was also already con-
sidered for robust estimation of source separation parameters (Kitamura et al.
[20], Yoshii et al. [45]).

Although the Laplace and Student’s t distributions mentioned above are
characterized as featuring heavy tails and are thus suitable for robust estimation
of sources parameters, their density is not stable under convolution, which means
that the distribution of sums of such random variables does not belong to the
same family. As a consequence, they do not straightforwardly lead to convenient
ﬁlters that may be used for the separation stage.
In this context, a natural
solution is to take the target signal as deterministic, and only pick a heavy-
tailed distribution for the noise term. However, such an approach breaks down
when uncertainty is to be considered for the target, that becomes stochastic,
or when more than two components are mixed, which is for instance common
in source separation. Consequently, the strategy employed, e.g.
in (Kitamura
et al. [20], Yoshii et al. [45]) is to use robust models for estimation only, and
then revert to a covariance separation approach. The only principled separation
method we are aware of, that is based on heavy tailed modeling, is the α-Wiener
ﬁlter presented in (Liutkus and Badeau [21]) and further developed in (Fontaine
et al. [14]). It is based on α-stable distributions but is however restricted to the
scalar case.

In this paper, we build on the scalar α-Wiener ﬁlter (Liutkus and Badeau
[21]) to extend it to the multivariate case and hence propose for the ﬁrst time a
ﬁlter based on multivariate α-stable distributions. Doing so, we enable the use
of such heavy-tail models not only for parameter estimation, but also for sep-
aration. The α-stable distributions and processes (Samoradnitsky and Taqqu
[35]) are deﬁned as the only class of distributions that are stable under convolu-
tion, and thus under addition of independent realizations. They hence naturally
appear in the generalized version of the central limit theorem, which is appli-
cable even when the random variables under consideration do not have ﬁnite
moments. This is useful for modeling very volatile noise or signals, or to allow it-
erative parameter estimation strategies even with bad initialization. They were
ﬁrst put forward by Mandelbrot to model ﬁnancial time series (Mandelbrot [24])
and have found widespread applications ever since: impulse noise modeling in
landline connections (Stuck and Kleiner [36]), modeling of background speckle
patterns in SAR images (Achim et al. [1]), and audio noise modeling (Bassiou

4

et al. [2]), to name a few.

The dependencies between the entries of α-stable random vectors are not
encoded in covariance matrices as in the Gaussian case. Instead, they are de-
scribed through a unique measure deﬁned on the hypersphere (Hardin J. [18]),
which can be understood as providing the strength of each direction of arrival.
This makes the expressive power of the model much larger than in the Gaussian
case, that is limited to ellipsoidal proﬁles. Throughout this paper, we call this
object the spatial density 1. That object regularly attracts some attention. It
has for instance been considered for independent component analysis (Kidmose
[19], Wang et al. [40]), for exchange rates estimation in ﬁnancial data (Nolan
et al. [29]), and more recently for audio source localization (Fontaine et al.
[15]). As a mathematical object deﬁned on the hypersphere, some authors also
proposed alternative equivalent representations, notably through spherical har-
monics (Pivato and Seco [32]).

In this paper, we show how to design digital ﬁlters speciﬁcally for α-stable
random vectors. For this purpose, we go further than both (Kidmose [19]),
that focused on linear determined mixtures, and (Liutkus and Badeau [21]),
which is limited to the scalar case. First, we present some theoretical results in
Section 2, that develop a spatial spectrum representation for α-stable random
vectors. Based on this representation, we propose two diﬀerent ﬁltering meth-
ods. In Section 3, multivariate observations are decomposed into their spatial
spectrum, whose components are then ﬁltered individually for reconstruction.
An alternative approach is presented in Section 4, where the sources are esti-
mated directly as a combination of linear ﬁlters, but with a design involving the
spatial density. Both methods are evaluated in Section 5 and compared to their
Gaussian counterpart.

Notation

Throughout the paper, scalars are denoted with a normal, light font, e.g.
N. Vectors are indicated with bold lowercase letters, e.g.
CK×K. We

(0, 2) or K
CK and matrices with bold uppercase letters, e.g. P

α
x
furthermore use the following notation:

∈
∈

∈

∈

K : denotes either C or R.
KK : set of K-valued vectors of dimension K.

•

•

• S

K =
sphere.

(θ1 . . . θK)

(cid:110)

KK,

∈

K
k=1 |

θk

(cid:80)

2 = 1
|

: K

1 dimensional hyper-

−

(cid:111)

• B

S

K

: set of Borelian sets on

K.

S

(cid:0)

(cid:1)

1In the literature, the spatial density is rather called spectral measure (Samoradnitsky and
Taqqu [35]). We have deliberately chosen to avoid the term ”spectral” here for two reasons.
First, we believe that it may bring some confusion for a signal processing audience. Second,
we think that calling it spatial better highlights the fact that it encodes dependencies between
covariates.

5

•

•

θ: vector from the hypersphere θ
entries θk.

∈ S

K, also called a direction, with

Θ: A partition

Θ1, . . . , ΘP
{

}

of

S

K, for which:

– All cells Θp have the same area ∆Θ.

– θp

∈

Θp denotes an element of cell Θp.

: real part of a complex number.

• (cid:60)

.(cid:63): Hermitian transposition (resp. conjugation) of a complex vector (resp.
complex number).

•

• (cid:104)

: inner product on KK:
., .
(cid:105)

(cid:105)
yjk: kth component of a vector yj.

w, x

(cid:104)

= w(cid:63)x.

•

•

•

•

ek: kth vector from the canonical basis.

.(cid:104).(cid:105): signed power function (Samoradnitsky and Taqqu [35], Nikias and
Shao [28]):

z

z
∀
(cid:44): “equal by deﬁnition to”.

K, z(cid:104)α(cid:105) = z

∈

|

α−1
|

: assignment of a value to a variable in an algorithm.

• ←

2. The multivariate α-stable probabilistic model

2.1. Isotropic symmetric α-stable random variables

Figure 1: SαS1

c density probability functions in the real case with σ = 1

−20−1001020x10−1410−1110−810−510−2p(x)α=1.0α=1.4α=1.8α=22.2 α-stable random vectors

6

An isotropic (circular) symmetric α-stable scalar random variable x

can be deﬁned by its characteristic function (chf.) ϕx : u
as follows:

K

∈

(cid:55)→

c (σx)

SαS1
(u(cid:63)x)))

∼
E (exp (i
(cid:60)

u

K, ϕx (u) = exp (

u

α σα
x )
|

∀

− |

∈
where σx is called a scale factor. The real number α
(0, 2] is called the char-
acteristic exponent and deﬁnes the heaviness of the tails in a stable distribution:
the smaller α, the heavier the tails. Examples of probability density functions
(pdf.) for such scalar random variables are represented in the real case in Fig. 1.
It is remarkable that such a pdf. is not available in closed-form in general, but
only in some particular cases, like the Cauchy (α = 1) and Gaussian (α = 2)
distributions.

∈

2.2. α-stable random vectors

In this paper, we limit our attention to symmetric α

i.e. α-stable random vectors x such that x and
deﬁned as in (Samoradnitsky and Taqqu [35]):

−

stable random vectors,
x have the same distribution,

−

Deﬁnition 1. Let x be a random vector in KK associated to its characteristic
)). A symmetric isotropic α-stable
u, x
function ϕx : u
(cid:105)
∈
distribution with α

(0, 2) is fully described by the unique representation:

E (exp (i

KK

(cid:60) (cid:104)

(cid:55)→

∈

u

K, ϕx (u) = exp

K

∀

∈

(cid:90)θ∈SK |(cid:104)
where Γx is a symmetric measure on the sphere
Henceforth, we note x
distribution with spatial density Γx.3

(cid:19)
K called the spatial density2.
c (Γx) whenever x follows a symmetric α-stable

SαSK

−

∼

(cid:18)

S

u, θ

α Γx (dθ)
(cid:105)|

,

(1)

SαSK

−

stable vector x

c (Γx) belongs to a larger
Remark 2. A symmetric α
the elliptically multivariate contoured (EMC) distribution (Cambanis
class:
In many cases, the sum of EMC vectors x, y, respectively asso-
et al. [6]).
ciated to the so-called scatter matrices Rx and Ry, is still an EMC vector.
However, the parameter Rx+y called scatter matrix of the mix is usually not
equal to Rx + Ry. In the α
stable case, Deﬁnition 1 ensures that the spatial
density of the mix Γx+y is equal to Γx + Γy.

∼

−

We highlight the fact that the Gaussian (α = 2) case is omitted in Deﬁni-

tion 1, because the representation (1) is not unique in that case.

2See Footnote 1 on page 4.
3Γx is a symmetric measure on SK in the sense that for any continuous function f
θ∈SK f (θ) Γx (zdθ) =

deﬁned on SK and for any z ∈ K such that |z| = 1, we have (cid:82)
(cid:82)
θ∈SK f (θ) Γx (dθ).

2.3 Spatial spectrum and spatial representation

7

2.3. Spatial spectrum and spatial representation

In this section, we show how the spatial density Γx, featured in Deﬁnition 1,
of a symmetric α -stable distribution can be understood through a so-called
spatial representation, which is central to the ﬁltering methods we propose later
in Sections 3 and 4.

Let

be an independently scattered α-stable random measure on

S
control measure Γx (Samoradnitsky and Taqqu [35]). This means that:

X

K, with

1. for any Borelian set A
(A)

is distributed as

⊂ B
∼
(A) is independent from
K

X
subsets A and B

2.

X

(cid:0)
X
.

S

⊂ B

S

SαSc (Γx (A)),
(cid:1)

(B) whenever A

K

, the scalar random variable

(A)

K

∈

X

B =

∩

∅

for any two Borelian

We call

the spatial spectrum of the distribution SαSK

(cid:0)

(cid:1)

c (Γx).

X

Remark 3. In (Ma and Nikias [23]), the α
spectrum terminology is used in a
diﬀerent way as the spatial spectrum. It describes the so-called covariation (see
Section 4.1 for further details) between the input single-channel signal and the
output single-channel signal.We have the following ﬁrst result:

−

Theorem 4. Let x
following spatial representation:

SαSK

∼

c (Γx), with spatial density Γx. Then x admits the

(cid:90)θ∈SK
= means “equal in distribution” and

where d
measure Γx.

d
=

x

θ

(dθ) ,

X

(2)

is the spatial spectrum with control

X

The proof of this result is given in Appendix 1. The representation theorem
means that an SαSK
random vector is distributed as the sum of inﬁnitely
c
K on the sphere. Γx (dθ)
many contributions, coming from all directions θ
may thus be interpreted as the scale factor of the contributions pointing in
direction θ. Very interestingly, the spatial representation in Theorem 4 provides
a straightforward way to generate samples from SαSK
c (Γx) random vectors:
, and then use (2) to construct the SαSK
ﬁrst, generate the spatial spectrum
c
random vector x. The method is summarized in Algorithm 1.

∈ S

X

The integration over the real or complex sphere, appearing in (2), is replaced
by ﬁnite sums. This is done by simply constructing a regular partition Θ of the
K, and substituting the integration by the corresponding sum carried
sphere
over the Θp’s. Let f be a function deﬁned on the hypersphere and M be a
measure on the sphere. For P large enough, the approximation goes as:

S

θf (θ) M (dθ)

(cid:90)SK

≈

p
(cid:88)

θpf (θp) M (Θp) ,

(3)

where M (Θp) may further be approximated as m (θp) ∆Θ (where ∆Θ is the
Lebesgue measure of Θp), whenever M is dominated by the Lebesgue measure
and thus equal to M (dθ) = m (θ) dθ for some measurable function m.

2.4 Mixtures of α-stable random vectors

8

Algorithm 1 Sampling of SαSK
representation.
1. Input

c (Γx) random vectors through their spatial

Number N of desired realizations

•

•

•

Partition Θ =

Θ1, . . . , ΘP
{
Characteristic exponent α

K as described in Section 1

of

S
(0, 2)

}

∈

Spatial density Γx
2. Spatial spectrum generation

•

n, p,

np

SαSc (Γx (Θp)) where Γx (Θp) is the restriction of Γx to the

X

∼

∀
set Θp up to renormalization.
n, xn

3. Synthesis

p θp

∀

←

np

X

(cid:80)

Given our model for α-stable vectors, we now discuss the distribution of

mixtures of such random vectors.

2.4. Mixtures of α-stable random vectors

c (Γ1) (in green) and y2 ∼ SαSK
8 + π
8 + π

Figure 2: Spatial densities in the real case with K = 2. On the left, density plots of y1 ∼
SαSK
c (Γ2) (in red), where the maxima of Γ1 and Γ2 are
reached for (cid:8) 5π
(cid:9). These plots show the inﬂuence of the
8 + π
6 , 5π
spatial density on the dependence patterns between covariates. On the right, a density plot
for the mixture x = y1 + y2 shows the additive property of spatial densities.

(cid:9) and (cid:8) 5π

8 , 5π

2

3

In the ﬁltering and signal processing literature, it is common to assume that
KK that we want
KK is the sum of J components yj ∈
c (Γj) as described above,

the observed vector x
∈
to recover. Here, we take each component yj ∼
with its own spatial density Γj:

SαSK

J
j=1 yj

x =
j, yj ∼
(cid:80)
∀

(cid:40)

SαSK

c (Γj) .

(4)

Because x is the sum of α-stable vectors, then x itself follows an α-stable dis-
tribution, with the following spatial density:

9

x
∼
Γx =

(cid:40)

SαSK

c (Γx)
j Γj.

d
=

(cid:44)
(cid:82)

θ∈SK θ

j (dθ), where

Y
j also deﬁnes a spatial spectrum associated to x.

By invoking the spatial representation Theorem 4 on each latent vector yj, we
(cid:80)
get:
j, yj
j denotes the spatial spectrum of yj.
∀
Moreover,
In-
formally, it simply means that x is also the sum of inﬁnitely many contribu-
K on the sphere, each one of them
(dθ), coming from all directions θ
tions
being in turn the sum of the contributions
j (dθ) for all components that come
from this particular direction. An illustration of the relationships between the
spatial densities Γx, Γj is given in Fig. 2.

∈ S
Y

j Y

(cid:80)

X

X

Y

The next two sections make diﬀerent uses of this spatial representation to
devise ﬁlters, which aim at estimating the latent vectors yj given x, provided
the spatial densities Γj, which are parameters, are known. This allows us to
dissociate the actual ﬁltering problem from the question of estimating signal
parameters. This strategy is for instance classical in the literature focusing on
Wiener-based ﬁlter design (Wiener [43], Duong et al. [11]).

3. Spatial spectrum ﬁlter (SSF)

Our objective is to estimate each latent vector yj, such that

yj = x. The
strategy we discuss here proceeds in two steps. Firstly, we estimate the spatial
spectrum
of the mixture, as described in Section 3.1. Secondly, this estimate
is used to reconstruct the desired latent vectors yj, as detailed in Section 3.2.

(cid:80)

X

3.1. Spatial spectrum estimation

We assume that the observation x

c (Γx) and its spatial density Γx
∼
are known. The ﬁrst step in the ﬁlter we discuss here is to estimate the spatial
(dθ)
. For this purpose, we choose the a posteriori expectation
spectrum
X
(dθ) given x, deﬁned in the following sense: for any continuous function ψ
of
K, satisfying
on

α Γx (dθ) < +

, we have:

X

(cid:98)

SαSK

X
S

ψ (θ)
|

θ∈SK |
(cid:82)

∞

E

(cid:20)(cid:90)θ∈SK
Note that it is such that
case of an SαSK

ψ (θ)

X

(dθ)

|

x

=

(cid:21)

(cid:90)θ∈SK

ψ (θ)

X

(dθ) .

(5)

θ∈SK θ

(cid:98)
(dθ) = x. It turns out that in the particular

c (Γx) observation x,
(cid:82)

(dθ) has the following form:

X

X

(cid:98)

Proposition 5. Under the previous assumptions,

(cid:98)

(dθ) can be rewritten as:

X

with:

X

(cid:98)

(dθ) = g

X

(x, θ) Γx (dθ)

a.s.
(cid:98)

θ

∀

∈ S

K, g

X

(x, θ) =

N (x, θ)
D (x)

,

(6)

(7)

3.1 Spatial spectrum estimation

where

and

N (x, θ) = αi

(cid:90)u∈KK (cid:104)

θ, u

(cid:104)α−1(cid:105)ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105))du
(cid:105)

D (x) =

ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105))du.

(cid:90)u∈KK
(x, θ) is a continuous map on

If α > 1, the density g

X

measure

X

(dθ) is dominated by Γx (dθ).

K, and hence the

S

10

(8)

(9)

Proposition 5 is proved in Appendix 2.
(cid:98)
Although g
X

(x, θ) has the closed-form expression (7), its computation is
not straightforward because it requires two integrations over KK. Let Ix (u) (cid:44)
ln (ϕx (u)) be the Levy exponent of x (Unser and Tafti [37]). From (1), it is
α Γx (dθ). We have the following result, with
(cid:105)|

u, θ

−
given by: Ix (u) =
θ∈SK |(cid:104)
proofs given in Appendix 2:
(cid:82)

Proposition 6. Let β = 1 if K = R, or β =2 if K = C. Then (8) is equivalent
to

N (x, θ) =

(cid:90)θ(cid:48)∈SK

where:

(cid:104)α−1(cid:105)

θ, θ(cid:48)
(cid:104)

(cid:105)
Ix (θ(cid:48))

θ(cid:48), x
(cid:104)
βK+α
α

η

(cid:105)

2
θ(cid:48), x
(cid:105)|
|(cid:104)
2
Ix (θ(cid:48))
α (cid:33)

(cid:32)

dθ(cid:48),

+∞

η (ρ) =

(
−

2n+βK+α
α

1) nfΓ
(cid:16)
22n+1n! (n + 1)!

ρn.

(cid:17)

n=0
(cid:88)

with fΓ the Gamma function. In the same way, (9) is equivalent to

(10)

(11)

D (x) =

SK θN (x, θ) Γx (dθ)
x
(cid:107)
Proposition 6 is proved in Appendix 2.
Note that in (12), any norm could be picked for the computation and yield
1 turns out to be a good compromise between

(12)

(cid:13)
(cid:82)
(cid:13)

(cid:107)1

(cid:13)
(cid:13)

1

.

the same result. The (cid:96)1
computational cost and numerical stability.

norm

.
(cid:107)
(cid:107)

−

Equations (10) and (12) in Proposition 6 provide an estimator of g
X

is computationally tractable via integrations on the compact set
to (7) that requires integration over KK.

S

, which
K, as opposed

The quantity η (ρ) is a power series with an inﬁnite radius of convergence
when α > 1. It is a smooth function of ρ and independent of the data and the
model parameters. It can hence be computed beforehand.

The main computational burden for this method lies in the computation
of the alternating power series η in (11). It converges slowly and goes through
extreme values, requiring a fairly large numerical precision in practice. However,
there are some cases where a closed-form expression is available, for instance

3.1 Spatial spectrum estimation

11

when α = 2, K = 2, β = 2, where η (ρ) = 1
16
this result, we decided to approximate η in all cases as:

ρ2

−

20ρ + 64

e−ρ/4. Inspired by

(cid:1)

e−dρ

(13)

(cid:0)
aρ2 + bρ + c

η (ρ)

≈

η (ρ) =

(cid:0)

(cid:1)

(cid:98)

∈

where a, b, c, d
R are model parameters. Using such a parameterized version
allows us to avoid the on-demand time-consuming evaluations of η. We esti-
2
mated the parameters that minimize the mean-square-error (MSE)
2.
ˆη
(cid:107)
We highlight that both the choice of this parametric model (13) and the choice
of the MSE criterion are driven by ad-hoc considerations, indeed, there is no
theoretical result we are aware of that would justify the convergence of the power
series to an exponential function. However, the plots for η and ˆη for β = 1 are
displayed in Fig. 3(a), and the error of ﬁt for η for β = 2 as a function of α is
displayed in Fig. 3(b) and quality is very good. Results were similar in the real
and complex cases. η was computed for 50 regularly spaced values α
(1, 2),
and for ρ
(0, 10), because ρ only has positive values in (10). For getting a
suitable convergence, η was calculated up to order 105.

η
(cid:107)

−

∈

∈

(a) η and ˆη for α = 1.3 and β = 1 .

(b) MSE as a function of α.

Figure 3: Performance of a parametric ﬁt ˆη for η in (11) .

0246810ρ0.00.51.01.5valuesηR,α=1.3bηR,α=1.31.21.41.61.82.0α−150−100MSE(dB)3.2 Signal reconstruction

12

3.2. Signal reconstruction

Once the spatial spectrum

(dθ) of the observation is estimated, or equiv-
alently g
(x, θ) is computed through (7), (10) and (12), our next step is to
X
construct an estimate for the components yj of interest. We pick the a posteri-
ori expectation:

, which is given as follows:

X

x

yj

(cid:44) E

Theorem 7. Let x be the sum of SαSK
c (Γj),
c
each with known spatial density Γj. Then the a posteriori expectation of each
component yj given x is:

random vectors yj ∼

SαSK

(cid:98)

(cid:3)

yj |
(cid:2)

(cid:44) E

yj

yj |
(cid:2)
was deﬁned in (7).
(cid:98)

where g
X

x

=

(cid:3)

(cid:90)θ∈SK

θg

X

(x, θ) Γj (dθ) ,

(14)

The proof of this theorem is also in Appendix 2. A summary of this ﬁltering

technique is given in Algorithm 2:

Algorithm 2 α
trum estimation.
1. Input

−

SSF: multivariate α-stable ﬁltering through a spatial spec-

•

•

•

•

•

Observation x of size K

Regular partition Θ =

Characteristic exponent α

Θ1, . . . , ΘP
{
(1, 2)

∈

of

K

S

}

spatial densities Γj
2. Spatial spectrum estimation

•

Using ˆη in (13), compute

Compute D (x) in (12)

p, N (x, θp) in (10)

∀

Compute g
X
3. Reconstruction:

•

(x, θp) = N (x,θp)
D(x)

yj =

p θpg

X

(x, θp) Γj (Θp)

(cid:80)

(cid:98)

4. Covariation-minimizing ﬁlter (CMF)

Despite a relatively simple expression of g
X

, the estimation technique in Sec-
tion 3 is computationally demanding, due to several numerical integrations. In
this section, the spatial representation in Theorem 4 will be exploited diﬀerently,
leading to a faster ﬁltering method.

4.1. Covariation between stable variables

Many signal processing studies exploit second-order statistics for the design
of digital ﬁlters (Cardoso [7], Duong et al. [11], Moussaoui et al. [27]). This
convenient strategy ﬁnds a straightforward probabilistic interpretation through

4.2 Covariation minimization ﬁltering (CMF) technique

13

Gaussian processes (α = 2), that are characterized by their covariance functions.
However, this strategy breaks down for α-stable processes with α < 2, because
the moments of order p
α are inﬁnite. For this reason, the covariation was
introduced (Samoradnitsky and Taqqu [35], Nikias and Shao [28, p. 87]) as a
substitute of the covariance, with many similar properties.

≥

Deﬁnition 8. The covariation between two random variables (x1, x2), jointly
distributed as: (x1, x2) (cid:44) x

c (Γx) for α > 1, is deﬁned as:

SαS2

∼

[x1, x2]α

(cid:44)

(cid:90)z=(z1,z2)∈S 2

1 z(cid:104)α−1(cid:105)
z∗

2

Γx (dz) .

Moreover, the covariation norm (Samoradnitsky and Taqqu [35, p 95])

of u

∼

SαS1

c is:

α = ([u, u]α)1/α .

u
(cid:107)

(cid:107)

Remark 9. The covariation is always anti-linear in its left argument. It is also
linear in the right argument if and only if all terms of the linear combina-
tion on the right-hand side are mutually independent: if (x, x1, x2) are jointly
SαS3
c with x1 and x2 independent, then [x, x1 + x2]α = [x, x1]α + [x, x2]α.
In addition,
SαS1

if x1 and x2 are independent then [x1, x2]α = 0, and if x

∼

c (σx), then

x
(cid:107)

(cid:107)α = σx.

4.2. Covariation minimization ﬁltering (CMF) technique

Our objective in this section is to build a ﬁlter to extract the component yj

yjk =

from the mixture x. Motivated by (Masry [26]), we seek ﬁltering vectors wjk
KK such that
wjk, x
(cid:104)
Additionally, we enforce
the mixture: x =
(cid:98)
j
problem with linear equality constraints:
(cid:80)

∈
α
α.
j wjk = ek to guarantee perfect reconstruction of
yj. For each k, this results in the following optimization

minimizes the covariation norm

yjk
(cid:107)

yjk

(cid:80)

−

(cid:98)

(cid:107)

(cid:105)

(cid:98)
minimize

subject to

yjk

j (cid:107)

(cid:80)

α
α w.r.t. wjk

− (cid:104)

wjk, x
j wjk = ek.

(cid:105)(cid:107)

(15)

(cid:80)
The constraints and covariation norm have convenient properties. Firstly, the
constraints are linear. Secondly, the criterion is a diﬀerentiable function whose
derivative is continuous, and it is convex. Thirdly, the covariation norm is
coercive. By invoking the Karush, Khun and Tucker theorem (Boyd and Van-
denberghe [5]), this optimization problem thus has a unique solution.

We apply the spatial representation in Theorem 4 to get x

(dθ)
j (dθ), with integrations done over SK. The Lagrangian of the

X

θ

d
=

(cid:82)

and yj

d
=

θ

Y

(cid:82)

4.2 Covariation minimization ﬁltering (CMF) technique

14

problem (15) is

L

{

(cid:16)

wjk

}j , λk

=

(cid:17)

j
(cid:88)

yjk
(cid:107)

− (cid:104)

wjk, x

α
α

(cid:105)(cid:107)

+ α

λ∗

k 

(cid:60) 

wjk

ek

−





(16)

j
(cid:88)


where the factor α in the second line is introduced to simplify the following
calculations. Now, thanks to properties of the covariation given in Remark 9,
the development of

α
α for all j, k yields:

yjk







yjk
(cid:107)

−

(cid:13)
(cid:13) α
α

(cid:98)

(cid:10)wjk, yj(cid:48)

(cid:11)

(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

α

α

(cid:88)

yjk −

(cid:13)
(cid:13)yjk − (cid:98)yjk
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

j(cid:48)

(cid:90)

=

=

=

=

(cid:90)

(cid:90)

(cid:13)
(cid:13)
(cid:0)θk − (cid:10)wjk, θ(cid:11)(cid:1) Yj (dθ)
(cid:13)
(cid:13)

α

α

(cid:90)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:10)wjk, θ(cid:11) Yj(cid:48) (dθ)
(cid:13)
(cid:13)

α

α

(cid:88)

+

j(cid:48)(cid:54)=j
(cid:90)

(cid:88)

(cid:12)
(cid:12)θk − (cid:10)wjk, θ(cid:11)(cid:12)
α Γj (dθ) +
(cid:12)

(cid:10)wjk, θ(cid:11)(cid:12)
(cid:12)
α Γj(cid:48) (dθ)
(cid:12)
(cid:12)

j(cid:48)(cid:54)=j
(cid:0)(cid:12)
(cid:10)wjk, θ(cid:11)(cid:12)
α − (cid:12)
(cid:12)θk − (cid:10)wjk, θ(cid:11)(cid:12)
α(cid:1) Γj (dθ) +
(cid:12)
(cid:12)
(cid:12)

(cid:90)

(cid:10)wjk, θ(cid:11)(cid:12)
(cid:12)
α Γx(dθ).
(cid:12)
(cid:12)

(17)

By substituting (17) in (16), and by zeroing the gradient of

get for all j:

w.r.t. wjk, we

L

λk =

(cid:90)

−

θ

(cid:90)

(cid:16)

k − (cid:104)θ, wjk(cid:105))(cid:104)α−1(cid:105) + (cid:104)θ, wjk(cid:105) (cid:104)α−1(cid:105)(cid:17)
(θ∗

Γj (dθ)

θ (cid:104)θ, wjk(cid:105) (cid:104)α−1(cid:105)Γx (dθ) .

By noting that z(cid:104)α−1(cid:105) = z

|z|2−α , (18) can be written as:

where the K

×

K matrix Rjk and the vector rjk

∈

λk =

−

P jkwjk + rjk,

KK are deﬁned as:

(cid:90) (cid:18)

P jk =

θθ(cid:63)
|θk − (cid:104)wjk, θ(cid:105)|2−α −

θθ(cid:63)
|(cid:104)wjk, θ(cid:105)|2−α

(cid:19)

Γj (dθ)

+

(cid:90)

(cid:90)

θθ(cid:63)

|(cid:104)wjk, θ(cid:105)|2−α Γx (dθ) .

θθ(cid:63)
k

|θk − (cid:104)wjk, θ(cid:105)|2−α Γj (dθ) .

rjk =

Therefore wjk is a ﬁxed point of the following equation:

wjk = P −1

jk (rjk

λk) .

−

(18)

(19)

(20)

(21)

(22)

4.3 The Gaussian case (α = 2)

A sum over j in (22) leads to:

−1

λk =

P −1

jk 







j
(cid:88)

15

(23)

P −1

jk rjk

ek



−



.













j
(cid:88)

Putting together the above results, we propose to design the ﬁlters wjk based
on a ﬁxed-point approach where P jk in (20), wjk in (22) and λk in (23) are
updated in turn. This is summarized in Algorithm 3. The integrals are replaced
by a discrete sum as in (3).

Algorithm 3 α
imization

1. Input

CMF: multivariate α-stable ﬁltering through covariation min-

−

j, k, wjk = 1
j, k, the entries of P j,k are independently drawn from the standard

J ek

•

•

•

•

• ∀

• ∀

• ∀

• ∀

•

• ∀

Observation x of size K

Regular partition Θ =

Characteristic exponent α

Spatial densities Γj

Θ1, . . . , ΘP
{
(1, 2)

∈

of

K

S

}

2. Initialization

normal distribution

k, λk = 0

3. Updates of P jk, wj,k and λk

j, k, update P jk as in (20)
(cid:16)(cid:80)

(cid:17)−1 (cid:16)(cid:16)(cid:80)

∀k, λk ← λk +

j P −1
j, k, update wjk as in (22) and (21)

j wjk

jk

(cid:17)

(cid:17)

− ek

4. Reconstruction:

j, k,

∀

yjk =

wjk, x
(cid:104)

(cid:105)

(cid:98)

4.3. The Gaussian case (α = 2)

Although the derivations done above focus on the case α

(1, 2), it is
interesting to note the limiting behaviour of the proposed ﬁltering method when
α

2. We get:

∈

→

j, k, P jk = P =

∀

(cid:90)

θθ(cid:63)Γx (dθ) ,

(24)

(25)

2,

→

(26)

which is the covariance matrix of the mixture x.
representation in (1), we have:

Indeed, exploiting the chf.

16

u

∀

∈

K

K, ϕx (u) = exp

u, θ

2 Γx (dθ)
(cid:105)|

(cid:19)

,

(cid:18)
= exp (

|(cid:104)
−
(cid:90)
u(cid:63)P u) ,

−

which is the chf. of a Gaussian vector of covariance matrix P . It is straight-
P j, where P j is the covariance matrix of the jth
forward to show that P =
component, given as in (24) but by integrating against Γj:
(cid:80)

P j =

θθ(cid:63)Γj (dθ) .

(cid:90)

Then, rjk in (21) becomes the kth column of P j. Consequently, when α

the estimates

yj for the components become:

(cid:98)

yj = P j

−1

x,



P j(cid:48)



(cid:98)



j(cid:48)
(cid:88)
which is exactly the classical multichannel Wiener ﬁlter (MWF), but with pa-
rameters computed by exploiting the spatial densities Γj. This is the linear ﬁlter-
ing method which minimizes the MSE between the sources and their estimates
when second-order moments are available. As expected, the CMF technique
presented in Section 4.2 is a generalization of the MWF to α-stable distribu-
tions.



Finally, we will propose a last estimator, which is another generalization
of the MWF to α-stable distributions. Because second-order moments are not
deﬁned for α < 2, matrices P j cannot be deﬁned through P j = E
as in
the case α = 2. Nevertheless, it is remarkable that the expression (25) remains
computable whatever α
(1, 2), making it an interesting method to estimate the
parameters of what becomes an ad-hoc ﬁlter (26), which we call MWF through
an abuse of notation, since it is only equivalent to MWF when α = 2.

yjy(cid:63)
j

∈

(cid:3)

(cid:2)

5. Evaluation

In this section, we assess the performance of the three ﬁltering methods pre-
sented in Sections 3, 4.2 and 4.3. In this regard, we stress that this paper only
addresses the design of ﬁlters associated to α-stable processes with known pa-
rameters Γj. Hence, the estimation of those parameters is kept out of the scope
of the present study. The reader can however ﬁnd spatial measure estimation
techniques in (Nolan et al. [29]) and (Pivato and Seco [32]). The rationale for
disentangling ﬁltering and parameter estimation is to provide a grounded basis
for the last ﬁltering step of whole processing pipelines involving α-stable pro-
cesses, as is routinely done for Gaussian processes with the MWF (Duong et al.
[11], Liutkus et al. [22]).

5.1 Experimental setup

17

Consequently, this evaluation focuses on the performance of the proposed
ﬁlters on synthetic data only. The procedure is always the same: ﬁrstly, we gen-
erate the realizations yj according to multivariate symmetric α-stable distribu-
tions with known spatial densities Γj as in Algorithm 1, then, these realizations
are summed to produce the observations x to be ﬁltered, and the performance
scores are computed by comparing the true yj with their estimates. That said,
the set of parameters considered spans a wide range of conﬁgurations of various
diﬃculties, allowing us to assess the strengths and weaknesses of the proposed
methods.

5.1. Experimental setup
SSF, as presented
Evaluated methods. We investigate the performance of α
CMF with 50 iterations, as presented in Section 4.2.
in Section 3, and of α
We compare them with the MWF method described in Section 4.3, i.e. with
parameters P j computed as in (25) using the true Γj.

−

−

Metrics. Because α > 1, the relative root mean-square error does not exist
(except for α = 2). Thus, we only consider the relative mean-absolute error
(MAE) deﬁned as:

where expectations are approximated by the empirical mean.

(cid:98)

MAE (y,

y) =

,

(cid:1)

(27)

j E

yj

yj −
yj

(cid:80)

j E
(cid:0)(cid:12)
(cid:12)

(cid:80)

(cid:0)(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:98)
(cid:12)
(cid:1)
(cid:12)

The Von-Mises Fisher distribution. As mentioned above, we evaluate all meth-
ods in the case of known spatial densities Γj. As a running example, we will
take the Γj as mixtures of Von-Mises Fisher (VMF) distributions, written
µ,κ,
which are deﬁned as:

V

V

∝

dθ,

exp

∈ S

µ,κ (dθ)

κµ(cid:62)θ
K is the mean direction and κ > 0 is a concentration parameter,
where µ
which is higher when the mass concentrates close to µ. Since the spatial densities
are symmetric, we sample them on the hyper-hemisphere only. Although any
K could be made, we picked the VMF
other choice of a distribution over
because it is the maximum entropy distribution of a random variable on the
sphere with known location and spread parameters.

(28)

S

(cid:0)

(cid:1)

5.2. Performance versus the spatial distance of components

In this ﬁrst round of experiments, we focus on the spatial resolution of each
algorithm, because ﬁltering out components that are spatially close has many
applications, e.g. in audio processing. For this purpose, we study the ﬁltering
performance of mixtures of J = 2 real-valued sources of dimension K = 2, so
that a direction θ

K can be understood as a point on the unit circle.

We take each component yj as originating mostly from one direction µj,
with both components sharing the same concentration parameter κ = 15, so

∈ S

5.3 Performance versus the number of components

18

Figure 4: Two Spatial Von-Mises densities Γ1, Γ2 on the semicircle with respective mean
directions µ1 = π
2 and concentration κ = 15. The red area indicates the overlap
between the spatial densities.

3 , µ2 = π

µj ,κ. Depending on the choice of the directions µj, this allows us to
that Γj =
V
[1.2, 2], and
create some overlap between the Γj’s. We set a 0.2 step-size for α
the mean directions µj for the sources are separated by
degrees,
with µ1 randomly positioned on the semi-circle. An example is given in Fig. 4.
CMF were run with partitions Θ composed of
P = 180 regions. For each conﬁguration of the µj, the performance of all
methods was evaluated on the ﬁltering of N = 2000 independent realizations.
100 diﬀerent such experiments were conducted to report the scores.

5, 15, . . . , 85
{

Algorithms α

SSF and α

−

−

∈

}

The corresponding MAE (27) values for α = 1.6 are displayed in Fig. 5. As
SSF globally outperforms the other methods for
CMF. While these

can be seen on this ﬁgure, α
−
all the angular distances between the sources, followed by α
−
two methods behave similarly, they both outperform MWF.

In Fig. 6, we show the evolution of these scores as a function of α, for a ﬁxed
deviation of 25 degrees between the sources. As can be seen, smaller values for α
lead to a degradation of the performance, due to the extremely heavy tails of
the distributions. However, we notice that all proposed methods remain quite
robust. Hence, even MWF, the proposed approach that exploits the spatial
densities Γj to build the MWF ﬁlters as discussed in Section 4.3, is remarkably
In practice, decreasing P often causes instability for MWF, while
eﬀective.
increasing P does not signiﬁcantly change scores.

5.3. Performance versus the number of components

In this second set of experiments, we evaluate our proposed ﬁltering methods
in the complex case with K = 2. Our objective here is to assess the performance
with a varying number J of components to separate. Hence, for J
,
2, . . . , 8
}
[1.2, 2], and N = 2000 independent realizations, we run 100 independents
α

∈ {

∈

0.00.51.01.52.02.53.0angle(θ)0.000.050.100.150.200.250.300.350.40PV2µ,κ(θ)Γ1(θ)Γ2(θ)5.3 Performance versus the number of components

19

Figure 5: MAE box plot (lower is better) of MWF, α−CMF and α−SSF methods for several
angular deviations between spatial densities and α = 1.6. The box plots shows the minimum
and maximum for whiskers, and 75th/25th percentiles for boxes.

Figure 6: MAE performance (lower is better) as a function of α, for a distance of 25 degrees
between the sources.

5.3 Performance versus the number of components

20

Figure 7: MAE averaged over all sources for α ∈ [1.2, 2]. The solid lines display the median,
and the light areas the standard deviation of each method.

experiments, where the spatial densities are VMF distributions with random
parameters κj and µj.

Regarding the computation of our integrals as in (3) for this complex case,

they were performed with P = 400 values for the partition Θ.

The results shown in Fig. 7 are in line with those for J = 2 reported above,
SSF method globally outperforms the three other meth-

and suggest that the α
ods for all conﬁgurations.

−

MWF α
0.02
0.02
0.02
0.02

CMF α
−
0.18
0.20
0.45
0.65

SSF

−
1.02
1.11
1.12
1.16

J = 2
J = 3
J = 5
J = 8

Table 1: Elapsed time (in sec., lower is better) for each ﬁltering method.

Now, we show that this increase of performance comes at the price of an
Indeed, the computational complexity of the

4

K 5N 3 + JP 3K 2N

, where P (cid:48) is the number

increased computational cost.
(cid:48)

α

SSF method is

P 4

P

−

O

(cid:18)

−

CMF it is

(cid:16)
(cid:17)
IJK 4P 4N

of samples for the discretization of integrals in (10) wrt. the Lebesgue measure,
while for α
, where I denotes the number of iterations
K
performed in Algorithm 3. The number P and P (cid:48) of cells used to sample
(cid:1)
inherently depends on K, and increase exponentially according to the curse
of dimensionality (see Bellman [3]). This explains why evaluations are only
performed when K = 2, and suggests an important research direction to scale

O

S

(cid:0)

(cid:19)

5.4 Filtering spatially scattered sources

21

the proposed method to higher spatial dimensions. In Table 1, we report the
average computing time for the diﬀerent methods to process N = 2000 samples,
as observed with our Python implementation running on a regular small laptop
computer with an i7-4810MQ CPU and 32 GB of RAM. We observe that the
MWF method is the fastest, followed by α-CMF and by α-SSF. Analyzing the
methods further, we observe that a large part of the computing time for α
SSF
is spent on computing g
(x, θ), so that separating additional components J
X
doesn’t yield a signiﬁcant increase in computational load, as for MWF.

−

5.4. Filtering spatially scattered sources

In the preceding sections, we estimated components whose spatial density Γj
was a simple VMF distribution, hence corresponding to only one direction of
arrival µj, with some variations brought in by the concentration parameter κj.
We now investigate more diverse spatial densities, where each (real) source is
characterized by several directions of arrival. This is done by taking each Γj as
a mixture of C VMF distributions:

j, Γj (dθ) =

∀

c
(cid:88)

wjc

µjc,κjc (dθ) ,
V

µjc,κjc is deﬁned in (28) and wjc
V
j,

[0, 1] are weight parameters, such
c wjc = 1. Examples of realizations for such models are depicted in

∈

where
that
Fig. 2 for K = 2.

∀

(cid:80)

We considered 5 regularly spaced α

[1.2, 2] and the separation of N = 2000
∈
i.i.d. samples from J = 4 components, whose spatial densities Γj are mixtures
of C = 2, 3, 4 VMF distributions, with parameters wjc, µjc and κjc drawn
randomly anew for each of the 100 experiments. The circle is uniformly divided
into P = 360 arcs.

Figure 8: MAE boxplot for α = 1.4.

22

Results for this experiment are depicted in Fig. 8, giving the MAE as a
function of the number C of directions of arrivals for each component. We
see that α
SSF slightly outperforms the other proposed methods and that the
performance is overall not so sensitive to the number of components. We believe
that the slight gain brought in by α-SSF can be explained by the fact that it
is a non-linear ﬁlter and may hence better handle more sophisticated spatial
models.

−

6. Conclusion & Future works

In this paper, we showed how the multivariate symmetric α-stable (SαS)

distribution can be used in ﬁltering applications.

An SαS distribution features remarkably heavy tails, that permit the mod-
eling of signals with very large dynamics. As we showed, an SαS vector is
characterized by a spatial density, that indicates the amount of energy origi-
nating from every direction in space. It hence naturally relaxes the common
assumption of deterministic directions of arrival made for multivariate obser-
vations. One key asset of this model is then to straightforwardly extend to
mixtures of such vectors, owing to the stability property of their distributions.
Equipped with such a powerful multivariate probabilistic model, we pro-
posed several ﬁlters able to recover SαS vectors from the observation of their
sum. The ﬁrst one relies on a spatial spectrum decomposition and may be un-
derstood as the combination of a nonlinear beamformer followed by a scalar
ﬁlter. The second one enforces linear ﬁltering and minimizes the covariation of
the diﬀerence between estimates and target. As we show, these ﬁlters generalize
the classical multivariate Wiener ﬁlter to heavy-tailed signals.

Throughout the paper, we considered the separation of real and complex
SαS vectors. A very straightforward application of these developments is the
ﬁltering of multivariate time series, via their short-time Fourier transforms. In-
deed, this would simply mean generalizing the recently proposed α-harmonizable
processes (Liutkus and Badeau [21]) to the multivariate case.

A natural route for future work is the combination of such ﬁlters with ef-
fective parameter estimation techniques similar to those presented in (Fontaine
et al. [15]). All together, they would form a principled processing pipeline for
heavy-tailed and impulsive multivariate signals. Furthermore, ﬁnding a way
to proceed to the required integrations over the hypersphere without a brute-
force partitioning as done here would allow the use of the proposed method in
high-dimensional settings.

AppendixA. Proofs

1

−

Proof of spatial representation theorem
If K = C, by substituting in the theorem 6.3.4.

Taqqu [35, p. 284]), the terms:

in (Samoradnitsky and

K, x = θ, M (dx) =

E =
X
j = k, zj = uk, T = [1 . . . K] , tj = k, ftj (x) = xtj = θk

(dθ) , m (dx) = Γx (dθ) , d = K,

S

23

we get that the chf. of

θ∈SK θ

X

(dθ) is:

∀u ∈ C

K , E

(cid:20)

exp

(cid:18)

i(cid:60)

(cid:90)

(cid:82)
(cid:28)
u,

(cid:29)(cid:19)(cid:21)

θ X (dθ)

(cid:90)

(cid:18)

−

= exp

SK

SK

|(cid:104)u, θ(cid:105)|α Γx (dθ)

(cid:19)

.

(A.1)

The right side of (A.1) is exactly the chf. of x. This identiﬁcation achieves the
proof of Theorem 4.

If K = R, (2) can be demonstrated by applying the theorem 3.5.6. in (Samorad-

nitsky and Taqqu [35, p 131]).

Proof of spatial spectrum estimation

2
Proof of Proposition 5

−

Let

v
∀

∈

K, c (v) (cid:44) ϕ(cid:82)

θ∈SK ψ(θ)X (dθ)|x (v). Note that if the ﬁrst derivative

exists (in the sense of the Wirtinger derivatives), we have:

E

(cid:20)(cid:90)θ∈SK

ψ (θ)

(dθ)

|

X

x

=

(cid:21)

dc
dv(cid:63) (v = 0) .

i
−

(A.2)

In order to ﬁnd a suitable form of c, we start by calculating the joint chf.
θ∈SK ψ (θ)

(dθ) and x,

KK:

K,

u

of

(cid:82)

v
∀

ϕ((cid:82)

∈

X

∀
θ∈SK ψ(θ)X (dθ),x) (v, u) (cid:44) E
= E

θ∈SK ψ(θ) X (dθ)+(cid:104)u,x(cid:105))

∈
ei(cid:60)(v∗ (cid:82)
(cid:104)
ei(cid:60)((cid:82)
(cid:104)
= ϕ(cid:82)
θ∈SK (v∗ψ(θ)+(cid:104)u,θ(cid:105)) X (dθ) (1)
= e− (cid:82)
θ∈SK |v∗ψ(θ)+(cid:104)u,θ(cid:105)|α Γx(dθ),

(cid:105)
θ∈SK (v∗ψ(θ)+(cid:104)u,θ(cid:105)) X (dθ))

(cid:105)

.

(A.3)

where the last equality holds because

Thus, c has the following form:

θ∈SK (v∗ψ (θ) +
SαSK
c
θ∈SK |
(cid:82)
v
∀

K,

(cid:0)(cid:82)

∈

v∗ψ(θ) +

u, θ
(cid:104)

)
(cid:105)
u, θ
(cid:104)

X
(cid:105)|

(dθ)
α Γx (dθ)

.

(cid:1)

∼

c(v) =

(cid:82)

=

u∈KK ϕ(cid:82)

θ∈SK ψ(θ)X (dθ),x (v, u) e−i(cid:60)((cid:104)u,x(cid:105))du
u∈KK ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105))du
u∈KK e− (cid:82)
θ∈SK |v∗ψ(θ)+(cid:104)u,θ(cid:105)|α Γx(dθ)e−i(cid:60)((cid:104)u,x(cid:105))du
(cid:82)
u∈KK ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105))du

(cid:82)
The existence of dc

dv(cid:63) (v = 0) is because

(cid:82)

ν

(cid:55)→

(cid:90)u∈KK

ϕ(cid:82)

θ∈SK ψ(θ)X (dθ),x (v, u) e−i(cid:60)((cid:104)u,x(cid:105))du

is the Fourier transform of a characteristic function whose ﬁrst derivative exists
because α > 1 (if a random vector admits a 1st order moment, then its charac-
teristic function is continuously diﬀerentiable at zero). Consequently, by using

the diﬀerentiation under the integral sign theorem (the domination is induced
by the fact that a chf. is bounded by 1) and by combining (A.2) and (A.3) we
obtain:

24

αi (cid:82)

u∈KK ((cid:82)

θ∈SK ψ(θ)(cid:104)θ,u(cid:105)(cid:104)α−1(cid:105)Γx(dθ))ϕx(u)e−i(cid:60)((cid:104)u,x(cid:105))du

E

θ∈SK ψ (θ)

(dθ)

x

|

X

=

=

=

θ∈SK ψ (θ)

αi (cid:82)

(cid:82)

(cid:20)

(cid:2)(cid:82)
(cid:82)
u∈KK ϕx(u)e−i(cid:60)((cid:104)u,x(cid:105))du
u∈KK (cid:104)θ,u(cid:105)(cid:104)α−1(cid:105)ϕx(u) e−i(cid:60)((cid:104)u,x(cid:105))du
u∈KK ϕx(u) e−i(cid:60)((cid:104)u,x(cid:105))du

(cid:3)

(cid:82)

θ∈SK ψ (θ) g

X

(x, θ) Γx (dθ) .

(A.4)

Γx (dθ)

(cid:21)

where we have used (7). Equation (6) is obtained by identifying (A.4) with (5)
for any continuous function ψ (θ).

(cid:82)

Proof of Proposition 5 about continuity

∀

∈

u

(cid:55)→

θ, u
(cid:104)

KK, θ

h (θ, u) =

(cid:104)α−1(cid:105) ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105)) is a continuous
(cid:105)
function. Moreover, the probability density function (pdf.) of an SαSc (non-
degenerated) distribution is inﬁnitely continuous. As a result, the character-
KK(which is the Fourier transform of a pdf.)
istic function ϕx (u) for all u
∈
In
decreases faster than any power of
α−1
K,
particular, u
(cid:107)
f (u). By applying the
(x, θ) is a

θ, u
(cid:104)
theorem of continuity under the integral sign, we conclude that g
(cid:12)
(cid:12)
(cid:12)
(cid:12)
X
continuous function of θ.
(cid:12)
(cid:12)

u
(cid:107)
(cid:107)
ϕx (u)
|
|
KK,
h (θ, u)

u
(cid:107)
α−1 and
(cid:107)

is any norm on KK).

is integrable. Besides,

f (u) =
u

(where

∈ S

≤ (cid:107)

| ≤

(cid:104)α−1(cid:105)

.
(cid:107)

(cid:55)→

u

∈

θ

∀

∀

(cid:107)

(cid:105)

|

Proof of Proposition 6

We consider the numerator N (x, θ) of g

of variables u (cid:44) rθ(cid:48)
du = rβK−1drdθ(cid:48). Then we get:

KK where r

∈

∈

(x, θ) in (8) and apply the change
K, which is such that

X

R+and θ(cid:48)

∈ S

(cid:90)

N (x, θ) = i

(cid:10)θ, θ(cid:48)(cid:11) (cid:104)α−1(cid:105)

θ(cid:48)∈SK

(cid:32)(cid:90)

r∈R+

(cid:16)

αrα−1e−rαIx(θ(cid:48))(cid:17) (cid:16)

rβK−1e−ir(cid:60)((cid:104)θ(cid:48),x(cid:105))(cid:17)

dr

(cid:33)

dθ(cid:48).

Applying an integration by parts to (cid:82)
yields:

r∈R+

(cid:16)

αrα−1e−rαIx(θ(cid:48))(cid:17) (cid:16)

rβK−1e−ir(cid:60)((cid:104)θ(cid:48),x(cid:105))(cid:17)

dr

N (x, θ) = A (θ, x) + (βK

1) iB (θ, x)

−

where

A (θ, x) =

B (θ, x) =

(cid:90)

(cid:90)

θ(cid:48)∈SK

(cid:104)θ, θ(cid:48)(cid:105) (cid:104)α−1(cid:105)(cid:60) ((cid:104)θ(cid:48), x(cid:105))
Ix (θ(cid:48))
(cid:32)(cid:90)

r∈R+

θ(cid:48)∈SK

(cid:104)θ, θ(cid:48)(cid:105) (cid:104)α−1(cid:105)
Ix (θ(cid:48))
(cid:32)(cid:90)

r∈R+

rβK−1e−rαIx(θ(cid:48))e−ir(cid:60)((cid:104)θ(cid:48),x(cid:105))dr

rβK−2e−rαIx(θ(cid:48))e−ir(cid:60)((cid:104)θ(cid:48),x(cid:105))dr

25

(cid:33)

dθ(cid:48)

(cid:33)

dθ(cid:48).

By developing the complex exponential as a power series and by applying
the identity
to both integrals A (θ, x) and
b fΓ
B (θ, x), we get:

b a− n+1

xne−axb

dx = 1

+∞
0

n+1
b

(cid:82)

(cid:0)

(cid:1)

A (θ, x) = 1
α

B (θ, x) = 1
α

(cid:90)

(cid:90)

θ(cid:48) ∈SK

θ(cid:48) ∈SK

+∞
(cid:88)

(−i)n fΓ

(cid:16) n+βK
α

(cid:17)

n!

n=0

+∞
(cid:88)

n=0

(−i)n fΓ

(cid:16) n+βK−1
α

(cid:17)

n!

(cid:104)θ,θ(cid:48) (cid:105)<α−1> 1

2n+1 Bn+1((cid:104)θ(cid:48),x(cid:105),(cid:104)x,θ(cid:48)(cid:105))

dθ(cid:48)

(A.5)

Ix(θ(cid:48) )

n+βK+α
α

(cid:104)θ,θ(cid:48)(cid:105)(cid:104)α−1(cid:105) 1

2n Bn((cid:104)θ(cid:48),x(cid:105),(cid:104)x,θ(cid:48) (cid:105))
n+βK−1+α
α

Ix(θ(cid:48))

dθ(cid:48).

(A.6)

unvn−k. Finally, we remark from (8)

where

n (u, v) = (u + v) n =

n
k=0

B

n
k

(cid:19)

z
∀

(cid:80)
∈ S

(cid:18)
1
K, which shows that in (A.5), all odd values
that N (zx, θ) = zN (x, θ) ,
of n, and all values of k diﬀerent from n
2 +1, can be discarded (the corresponding
terms vanish when integrated). In the same way, in (A.6), all even values of
n, and all values of k diﬀerent from n+1
2 , can be discarded, which ﬁnally leads
to (10) and (11). Equation (11) deﬁnes a power series with an inﬁnite radius of
convergence when α > 1, which is smooth and independent the mixing model.

The estimate of x

→

D (x) is obtained by noting that:

x =

(cid:90)θ∈SK

θg

X

(x, θ) Γx (dθ) =

(cid:90)θ∈SK

θ

N (x, θ)
D (x)

Γx (dθ) .

Proof of Theorem 7

We ﬁrst determine the joint chf. of yj and x:

v, u

∀

∈

KK,

ϕ(yj ,x) (v, u) (cid:44) E
= E

ei(cid:60)(
v,yj
(cid:104)
(cid:104)
ei(cid:60)(
v+u,yj
(cid:104)

+(cid:104)u,x(cid:105))
(cid:105)

+(cid:80)
(cid:105)

(cid:105)
j(cid:48)(cid:54)=j(cid:104)

u,yj(cid:48)

)
(cid:105)

(cid:104)

= ϕyj (v + u)

ϕyj(cid:48) (u)

(cid:105)

j(cid:48)(cid:54)=j
(cid:89)
θ∈SK |(cid:104)v+u,θ(cid:105)|αΓj (dθ)−(cid:80)

= e− (cid:82)

j(cid:48) (cid:54)=j

(cid:82)

θ∈SK |(cid:104)u,θ(cid:105)|αΓj(cid:48) (dθ),

which permits us to deduce the chf. of yj given x,

v

∀

∈

KK :

ϕyj |x (v) =

(cid:82)
u∈KK e

− (cid:82)

θ∈SK |(cid:104)v+u,θ(cid:105)|α Γj (dθ)−(cid:80)
(cid:82)

u∈KK ϕx(u)e−i(cid:60)((cid:104)u,x(cid:105))du

j(cid:48) (cid:54)=j

(cid:82)

θ∈SK |(cid:104)u,θ(cid:105)|αΓ

j(cid:48) (dθ)

e−i(cid:60)((cid:104)u,x(cid:105))du

26

.

The proof of the existence of
Thus, we get:

∇

v(cid:63) ϕyj |x is exactly the same as in Proposition 5.

(cid:2)yj | x(cid:3) = −i∇v(cid:63) ϕyj |x (v = 0)

E

(cid:90)

(cid:90)

=

=

θ∈SK

θ∈SK

(cid:34) αi (cid:82)

θ

u∈KK (cid:104)u, θ(cid:105) (cid:104)α−1(cid:105)ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105))du
u∈KK ϕx (u) e−i(cid:60)((cid:104)u,x(cid:105))du

(cid:82)

(cid:35)

Γj (dθ)

θ g

X

(x, θ) Γj (dθ) ,

which completes the proof.

—————–

[1] Achim, A., Kuruoglu, E. E., Zerubia, J., 2006. SAR image ﬁltering based

on the heavy-tailed Rayleigh model 15 (9), 2686–2693.

[2] Bassiou, N., Kotropoulos, C., Pitas, I., 2014. Greek folk music denoising
under a symmetric α-stable noise assumption. In: 10th International Con-
ference on Heterogeneous Networking for Quality, Reliability, Security and
Robustness (QShine). IEEE, pp. 18–23.

[3] Bellman, R., 2013. Dynamic programming. Courier Corporation.

[4] Benaroya, L., Bimbot, F., Gribonval, R., 2006. Audio source separation
with a single sensor. IEEE Transactions on Audio, Speech, and Language
Processing 14 (1), 191–199.

[5] Boyd, S., Vandenberghe, L., 2004. Convex optimization. Cambridge uni-

versity press.

[6] Cambanis, S., Huang, S., Simons, G., 1981. On the theory of elliptically
contoured distributions. Journal of Multivariate Analysis 11 (3), 368–385.

[7] Cardoso, J.-F., 1998. Blind signal separation: statistical principles 86 (10),

2009–2025.

[8] Cavalcant, Y., Oberlin, T., Dobigeon, N., Févotte, C., Stute, S., Taube, C.,
2019. Unmixing dynamic pet images: combining spatial heterogeneity and
non-gaussian noise. In: ICASSP 2019-2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 1373–
1377.

[9] Comon, P., Jutten, C., 2010. Handbook of Blind Source Separation: Inde-

pendent component analysis and applications. Academic press.

27

[10] Damon, C., Liutkus, A., Gramfort, A., Essid, S., 2013. Non-negative matrix
factorization for single-channel eeg artifact rejection. In: 2013 IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing. IEEE, pp.
1177–1181.

[11] Duong, N. Q. K., Vincent, E., Gribonval, R., Sept. 2010. Under-determined
reverberant audio source separation using a full-rank spatial covariance
model 18 (7), 1830 –1840.

[12] Duvenaud, D., Nickisch, H., Rasmussen, C., 2011. Additive gaussian pro-
cesses. In: Advances in neural information processing systems. pp. 226–234.

[13] Febrero-Bande, M., González-Manteiga, W., 2013. Generalized additive

models for functional data. Test 22 (2), 278–292.

[14] Fontaine, M., Liutkus, A., Girin, L., Badeau, R., 2017. Explaining the pa-
rameterized Wiener ﬁlter with alpha-stable processes. In: Proc. of Work-
shop on Applications of Signal Processing to Audio and Acoustics (WAS-
PAA). IEEE.

[15] Fontaine, M., Vanwynsberghe, C., Liutkus, A., Badeau, R., Aug. 2017.
Scalable source localization with multichannel alpha-stable distributions.
In: Proc. of 25th European Signal Processing Conference (EUSIPCO). pp.
11–15.

[16] Godsill, S., Rayner, P., Cappé, O., 2002. Digital audio restoration. Appli-
cations of digital signal processing to audio and acoustics, 133–194.

[17] Godsill, S. J., Rayner, P. J. W., 1995. A Bayesian approach to the restora-

tion of degraded audio signals 3 (4), 267–278.

[18] Hardin J., C. D., 1982. On the spectral representation of symmetric stable

processes. Journal of Multivariate Analysis 12 (3), 385–401.

[19] Kidmose, P., 2001. Blind separation of heavy tail signals. Ph.D. thesis, In-
stitute of Mathematical Modelling, Technical University of Denmark, Kon-
gens Lyngby, Denmark.

[20] Kitamura, K., Bando, Y., Itoyama, K., Yoshii, K., 2016. Student’s t
multichannel nonnegative matrix factorization for blind source separation.
In: 2016 IEEE International Workshop on Acoustic Signal Enhancement
(IWAENC). IEEE, pp. 1–5.

[21] Liutkus, A., Badeau, R., 2015. Generalized Wiener ﬁltering with fractional
power spectrograms. In: Proc. of International Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, pp. 266–270.

[22] Liutkus, A., Badeau, R., Richard, G., 2011. Gaussian processes for under-

determined source separation 59 (7), 3155–3167.

28

[23] Ma, X., Nikias, C., 1995. Parameter estimation and blind channel iden-
tiﬁcation in impulsive signal environments. IEEE Transactions on signal
processing 43 (12), 2884–2897.

[24] Mandelbrot, B., May 1960. The Pareto-Lévy law and the distribution of

income. International Economic Review 1 (2), 79–106.

[25] Marra, G., Wood, S., 2011. Practical variable selection for generalized addi-
tive models. Computational Statistics & Data Analysis 55 (7), 2372–2387.

[26] Masry, E., 2000. Alpha-stable signals and adaptive ﬁltering 48 (11), 3011–

3016.

[27] Moussaoui, S., Hauksdottir, H., Schmidt, F., Jutten, C., Chanussot, J.,
Brie, D., Douté, S., Benediktsson, J. A., 2008. On the decomposition of
Mars hyperspectral data by ICA and Bayesian positive source separation.
Neurocomputing 71 (10-12), 2194–2208.

[28] Nikias, C. L., Shao, M., 1995. Signal processing with alpha-stable distribu-
tions and applications. Adaptive and learning systems for signal processing,
communications, and control. Wiley.

[29] Nolan, J. P., Panorska, A. K., McCulloch, J. H., 2001. Estimation of stable
spectral measures. Mathematical and Computer Modelling 34 (9), 1113–
1122.

[30] Nugraha, A., Liutkus, A., Vincent, E., 2018. Deep neural network
based multichannel audio source separation. In: Audio Source Separation.
Springer, pp. 157–185.

[31] Ozerov, A., Févotte, C., Vincent, E., 2018. An introduction to multichannel
nmf for audio source separation. In: Audio Source Separation. Springer, pp.
73–94.

[32] Pivato, M., Seco, L., 2003. Estimating the spectral measure of a multivari-
ate stable distribution via spherical harmonic analysis. Journal of Multi-
variate Analysis 87 (2), 219–240.

[33] Raﬁi, Z., Liutkus, A., Stoter, F.-R., Mimilakis, S., FitzGerald, D., Pardo,
B., 2018. An overview of lead and accompaniment separation in mu-
sic. IEEE/ACM Transactions on Audio, Speech and Language Processing
(TASLP) 26 (8), 1307–1335.

[34] Roth, M., Özkan, E., Gustafsson, F., 2013. A Student’s t ﬁlter for heavy
tailed process and measurement noise. In: Proc. of International Conference
on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 5770–
5774.

[35] Samoradnitsky, G., Taqqu, M., 1994. Stable non-Gaussian random pro-

cesses: stochastic models with inﬁnite variance. Vol. 1. CRC Press.

29

[36] Stuck, B. W., Kleiner, B., 1974. A statistical analysis of telephone noise.

Bell Labs Technical Journal 53 (7), 1263–1320.

[37] Unser, M., Tafti, P., 2014. An introduction to sparse stochastic processes.

Cambridge University Press.

[38] Venkataramani, S., Casebeer, J., Smaragdis, P., 2018. End-to-end source
separation with adaptive front-ends. In: 2018 52nd Asilomar Conference
on Signals, Systems, and Computers. IEEE, pp. 684–688.

[39] Vincent, E., Virtanen, T., Gannot, S., 2018. Audio source separation and

speech enhancement. John Wiley & Sons.

[40] Wang, B., Kuruoglu, E., Zhang, J., 2009. Ica by maximizing non-stability.
In: International Conference on Independent Component Analysis and Sig-
nal Separation. Springer, pp. 179–186.

[41] Wang, D., Zhang, C., Zhao, X., 2008. Multivariate Laplace ﬁlter: a heavy-
tailed model for target tracking. In: Proc. of 19th International Conference
on Pattern Recognition (ICPR). IEEE, pp. 1–4.

[42] Wang, Z.-Q., Roux, J. L., Wang, D., Hershey, J., 2018. End-to-end speech
separation with unfolded iterative phase reconstruction. arXiv preprint
arXiv:1804.10204.

[43] Wiener, N., 1949. Extrapolation, interpolation, and smoothing of station-

ary time series. Vol. 7. MIT press Cambridge, MA.

[44] Wood, S., Goude, Y., Shaw, S., 2015. Generalized additive models for large
data sets. Journal of the Royal Statistical Society: Series C (Applied Statis-
tics) 64 (1), 139–155.

[45] Yoshii, K., Itoyama, K., Goto, M., 2016. Student’s t nonnegative matrix fac-
torization and positive semideﬁnite tensor factorization for single-channel
audio source separation. In: 2016 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE, pp. 51–55.

