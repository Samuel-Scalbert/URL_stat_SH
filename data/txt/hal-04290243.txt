Probing neural language models for understanding of
words of estimative probability
Damien Sileo, Marie-Francine Moens

To cite this version:

Damien Sileo, Marie-Francine Moens. Probing neural language models for understanding of words
of estimative probability. Proceedings of the 12th Joint Conference on Lexical and Computational
Semantics (*SEM 2023), Jul 2023, Toronto, France. pp.469-476, ￿10.18653/v1/2023.starsem-1.41￿.
￿hal-04290243￿

HAL Id: hal-04290243

https://hal.science/hal-04290243

Submitted on 16 Nov 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

469
Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 469–476
July 13-14, 2023 ©2023 Association for Computational Linguistics

ProbingneurallanguagemodelsforunderstandingofwordsofestimativeprobabilityDamienSileo1andMarie-FrancineMoens21Univ.Lille,Inria,CNRS,CentraleLille,UMR9189-CRIStAL,F-59000Lille,France2DepartmentofComputerScience,KULeuven,Belgiumdamien.sileo@inria.frAbstractWordsofEstimativeProbability(WEP)arephrasesusedtoexpresstheplausibilityofastatement.Examplesincludetermslikeproba-bly,maybe,likely,doubt,unlikely,andimpossi-ble.Surveyshaveshownthathumanevaluatorstendtoagreewhenassigningnumericalprob-abilitylevelstotheseWEPs.Forinstance,thetermhighlylikelyequatestoamedianproba-bilityof0.90±0.08accordingtoasurveybyFagen-Ulmschneider(2015).Inthisstudy,ourfocusistogaugethecompetencyofneurallan-guageprocessingmodelsinaccuratelycaptur-ingtheconsensualprobabilitylevelassociatedwitheachWEP.OurfirstapproachisutilizingtheUNLIdataset(Chenetal.,2020),whichlinkspremisesandhypotheseswiththeirper-ceivedjointprobabilityp.Fromthis,wecraftpromptsintheform:"[PREMISE].[WEP],[HYPOTHESIS]."ThisallowsustoevaluatewhetherlanguagemodelscanpredictiftheconsensualprobabilitylevelofaWEPalignscloselywithp.Inoursecondapproach,wedevelopadatasetbasedonWEP-focusedprob-abilisticreasoningtoassessiflanguagemod-elscanlogicallyprocessWEPcompositions.Forexample,giventheprompt"[EVENTA]islikely.[EVENTB]isimpossible.",awell-functioninglanguagemodelshouldnotcon-cludethat[EVENTA&B]islikely.Throughourstudy,weobservethatbothtaskspresentchallengestoout-of-the-boxEnglishlanguagemodels.However,wealsodemonstratethatfine-tuningthesemodelscanleadtosignificantandtransferableimprovements.1IntroductionExpressionofuncertaintyisanimportantpartofcommunication.Formalstatisticsaretherigorouswaytoquantifyuncertaintybutdonotfitallcom-municationstyles.Wordsofestimativeprobability(WEP)suchasmaybeandbelieveareadverbsorverbsthatareinformalalternatives.Kent(1964)notedtheimportanceofclarifyingWEPmeaningforintelligenceanalysisintheCentralIntelligenceAgency,andprovidedguidelinesformappingWEPtonumericalprobabilities.SeveralstudiesthenmeasuredthehumanperceptionsofprobabilitywordsanddiscoveredsomeagreementwithKent(1964)’sguidelines.Inthiswork,weusethescalederivedfromasurvey(Fagen-Ulmschneider,2015),whichisthelargestandmostrecentWEPpercep-tionsurveyavailable.123participantswereaskedtolabelWEPwithnumericalprobabilities.WeusethemedianoftheparticipantanswerstoassignaconsensualvaluetoeachWEP.Associatedprob-abilitiesforthe19WEPweuseareavailableinAppendixA,table2.Here,weassesswhetherneurallanguagemod-elslearntheconsensualprobabilityjudgmentofWEPfromlanguagemodelingpretraining.Wedevelopdatasetsandamethodologytoprobeneu-rallanguagemodelunderstandingofWEP.Thefirstdatasetleveragespreviouslyannotatedproba-bilityscoresbetweenapremiseandahypothesis,inordertomeasurealanguagemodel’sabilitytocapturetheagreementbetweennumericalproba-bilitiesandWEP-expressedprobabilities.Thesec-onddatasetisbasedoncompositionsoffactswithWEP-expressedprobabilities,andmeasuresverbalprobabilisticreasoninginlanguagemodels.Ourcontributionsareasfollows:(i)twodatasetsandmethodstomeasureunderstandingofWEP;and(ii)evaluationoftheabilityofneurallanguagemodels(GPT2,RoBERTa-trainedonMNLI)totackleWEP-relatedproblems,showingthatoff-the-shelfmodelsareverylittleinfluencedbythem,eventhoughfine-tuningonourconstructeddatasetsquicklyleadstohighaccuracies.Thecodeandgenerateddatasetsarepubliclyavailable11/hf.co/.../probability_words_nli470

2RelatedworkOurworkprobesaparticularaspectoflanguageunderstanding.Wedonotanalyzetheinsideofthemodels(Rogersetal.,2020).Wefocusonthemodels’abilitytoperformcontrolledtasks(Naiketal.,2018;Richardsonetal.,2020)involvingWEP.WEPwerestudiedinthecontextofintel-ligenceanalysisandlinguistics,ourworkisthefirsttolookatthemthroughnaturallanguagepro-cessing(NLP)models.OurstudyalsopertainstoNLPanalysesoflogicalreasoningandprobabilityproblems,andtouncertaintyinnaturallanguageinferencetasks.LinguisticsstudyofWEPKent(1964)’ssemi-nalworkwasthefirsttolinkWEPandnumericalprobabilityestimates,withintelligenceanalysismotivations(DhamiandMandel,2021)andapre-scriptivistapproach.Thisinspiredfurtherquantifi-cationsofhumanperceptionsofWEP,inthecon-textofmedicalreports(O’Brien,1989;Ott,2021)andweatherreports(Lenhardtetal.,2020).Fagen-Ulmschneider(2015)proposedthelargestsurveyuptodatewith123participantsaboutgeneral-domainWEPperception.LogicalandprobabilisticreasoningAnotherstrandofworkprobesNLPtextencoderscapa-bilities,notablyreasoningabilities.Westonetal.(2015)probedunderstandingofspecificproblemslikenegation,spatialandtemporalreasoningwiththebAbIdataset.Richardsonetal.(2020)probeunderstandingoffirst-orderlogicreasoning,SileoandLernould(2023)probeepistemiclogicreason-ing.Ourworkisthefirsttoaddressprobabilisticlogic,alongsideDriesetal.(2017);Susteretal.(2021)whoconstructadatasetofnaturallanguageprobabilityproblems,e.g.,"Abaghas4whiteand8bluemarbles.Youpulloutonemarbleanditisblue.Youpulloutanothermarble,whatistheprob-abilityofitbeingwhite?".TheyalsorelyontheProbLogsolver(DeRaedtetal.,2007),butfocusonnumericprobabilityproblems.Bycontrast,ourworktargetsWEP,andtextualprobabilisticlogicalreasoning.Naturallanguageinference,uncertainty,modal-ity,evidentialityUncertaintywasalsostudiedinthecontextofnaturallanguageinferencetasks.Zhouetal.(2022)studythedisagreementacrossannotatorswhenlabelingentailmentrelationships.Zhangetal.(2017)annotategradedentailmentwith5probabilitylevels,andtheUNLIdataset(Chenetal.,2020)gofurtherbyannotatingnumericalprobabilities.Ourworkalsopertainstothestudyofmodality(Palmer,1992;Sauríetal.,2006)andmoreparticularlyevidentiality(Suetal.,2010),butwherepreviousworkfocusedonWEP.3ProbingWEPunderstanding3.1VerbalizationanddistractorgenerationOurgoalistomeasuretheunderstandingofWEP.OnerequirementofWEPunderstandingiscaptur-ingtheconsensualprobabilitylevel.Totestthat,weusecontexts(PREMISE)pairedwithaconclu-sions(HYPOTHESIS).Thelikelihoodofaconclu-sion,p,dependsontheassociatedcontext.OneexamplefromUNLI(Chenetal.,2020),whichannotatesthat,is(Amaninawhiteshirttakingapicture,Amantakesapicture,1.0).Weconvertatriplet(PREMISE,HYPOTHESIS,p)tothefollowingverbalization:PREMISE.Tp(HYPOTHESIS).(1)whereTpisatexttemplateassignedtotheprob-abilityp.Toselectatemplate,wefindtheWEPwhoseassociatedmedianprobability(seetable2)istheclosesttop.WethenusehandcraftedtemplatestoconstructamodalsentencefromtheselectedWEPandthehypothesis,e.g.,"Itiscertainthatamantakesapicture".Table3inappendixBdisplaysthetemplatesthatweassociatewitheachWEP.Wealsogenerateaninvalidverbalizationbyran-domlyselectinganincorrectWEP(aWEPwhoseconsensualprobabilitydiffersfrompbyatleast40%)2,e.g.,Itisunlikelythatamantakesapicture.Wehypothesizethatlanguagemodelsandentail-mentrecognitionmodelsshouldgiveahigherscore(respectivelylikelihoodandentailmentprobability)tothecorrectvalidverbalizationthantotheinvalidverbalizationofp.3.2WEP-UNLI:probability/WEPmatchingTheUNLIdatasetannotates(PREMISE,HYPOTH-ESIS)pairsfromtheSNLIdataset(Bowmanetal.,2015)withjointprobabilityscoresp,totaling55ktrainingexamples,3k/3kvalidation/testex-amples.WeusetheseexamplestogenerateWEP-understandingdatasetwithverbalizationvaliditypredictionasshownintheprevioussubsection.2Thisthresholdensuressufficientdistance,whilealsoen-suringthateachWEPhasatleastonepossibledistractor.471

% Round 1 templateSampled round 1 (premise)p1::factA.There is a very good chance that Bernhard is a swan.p2::factB.It is almost certain that Greg is gray.p3::factC.There is a better than even chance that Sandra left the apple.% Round 2 templateSampled round 2 (premise, continued)p4::factX:-op1(fact1, fact2).Chances are slight that if Bernhard is a swan, or Sandra left the apple, then sheep are afraid of mice.p5::factY:-op2(fact3, fact4).It is improbable that if Greg is gray, and Bernhard is a swan, then Lily is a rhino.p6::factZ:-op3(fact5, fact6).There is a very good chance that if Greg is gray, and Sandra left the apple, then Sumit is thirsty.% Round 3 templateSampled hypothesishypothesis:-op4(fact7, fact8).Either Bernhard is a swan or sheep are afraid of mice.query(hypothesis).ProbLogReasonerp=0.7235Tp’(hyp.): It is likely that either Bernhard is a swan or sheep are afraid of mice.1Tp’(hyp.): It is unlikely that either Bernhard is a swan or sheep are afraid of mice.0Reasoningtemplatepremisehyp.pSample bAbI facts A,B,C,X,Y,ZSample facts 1...8 from facts A,B,C,X,Y,Z in previous roundsSample op 1…4 from {and, or, xor}Sample chances p1…p6distractorCompute hypothesis likelihood, relevant WEPfactAGenerated label y:p verbalization validityTp1Probability verbalizationpremiseTp’(hyp.)yp’ ≈ phypothesis likelihoodGenerated input: premise, Tp’(hyp.)Figure1:WEP-reasoningtaskconstructions,with2hops.Wesamplerandomlyconcretefactsfactiandprobabilitiespithenbuildmodalsentenceswithverbalizationtemplates.Werandomlysamplelogicaloperatorstocomposethemodalsentencesfromthepreviousroundstoconstructapremise,thenahypothesis,andweuseaprobabilisticsoftlogicsolvertocomputethehypothesisprobability.Wethencorrectlyandincorrectlyverbalizethisprobability.Thisprocessgeneratesdataforthetaskofprobabilityverbalizationvalidity.1hopreasoningskipsthesecondround:fact7andfact8aresampledfrom{factA,factB,factC}3.3WEP-Reasoning:WEPcompositionsHere,ourgoalistoassessmodels’abilitytorea-sonovercombinationsofprobabilisticstatements.Weconstructsynthetic(PREMISE,HYPOTHESIS,p)examplesfromrandomfactoidsextractedfromthebAbIdataset(Westonetal.,2015).Figure1illustratestheconstructionofWEP-reasoningex-amples:Werandomlysampleinitialfactsandassoci-atedprobabilitylevels,andweverbalizethemwiththepreviouslymentionedtemplatesfromTable3(Round1).Wefurthercomposethemwithran-domlysampledlogicaloperators(and,or,xor).Wethengenerateahypothesiswithlogicalcombina-tionsofthepreviousround.Finally,wefeedtheconstructedpremiseandhypothesistoaprobabilis-ticsoftreasoningengineinordertoderivethelike-lihoodofthehypothesisgiventhepremise.WerelyontheProbLog(DeRaedtetal.,2007)reasonerwhichimplementsDantsin(1992)semantics.Toevaluatedifferentcomplexitiesofreasoning,weproposetwovariants:2-hopreasoning,wherefactsinRound2combinefactsfromRound1,andthefinalhypothesiscombinesfactsfromRound2.and1-hopreasoningwherefactsfromthehypoth-esiscombineRound1facts(Round2isskipped).SincewewanttosamplemorethantwofactsandwecannotaprioriusetextfromtheUNLIdataset,becauseUNLIonlyprovidesentailmentlikelihoodforspecificpairs.Combiningseveralsentencescouldcauseunaccountedinterference.Therefore,wesamplesubject/verb/objectfactoidsfromthebAbI(Westonetal.,2015)datasetsinstead,whichisbuiltwithhandwrittenarbitraryfactoidssuchasJohnwenttothekitchen.Tosamplemultiplefactoids,wepreventanyoverlapofconcepts(verb,subject,object)betweenanypairoffactstomakethefactsindependentofoneanother.Wesampleprobabilitylevelsfromthelistofme-diansofallWEPtopreventsamplingthelevelsthattoodistantfromaknownWEP.WhenweassignaWEPtoaprobabilitylevel,weassumethatthecor-rectsemanticsistheconsensualone,buthumansdiffersslightlyfromthisconsensus.Still,whenaddingrandomperturbationsof20%tosampledp1...6,thehypothesisprobabilityisperturbedbylessthan40%for98%ofexamples.Wegenerate5kexamplesusingthetemplatedepictedinFigure1,anduse10%/10%ofthedataforthevalidation/testsplits.AppendixCshowsthedistributionofcorrectWEPforeachdataset.4ExperimentsWeconductverbalizationvalidityprediction(bi-naryclassificationtaskofWEPcorrectnessdetec-tionbetweentwocandidates)undertwosettings.472

WEP-Reasoning(1hop)WEP-Reasoning(2hops)WEP-UNLIChance50.050.050.0Humanbaseline97.0±1.093.5±1.589.5±2.5GPT2likelihoodzero-shot50.1±0.050.0±0.045.6±0.0RoBERTalikelihoodzero-shot63.4±0.063.2±0.053.2±0.0RoBERTa-MNLIzero-shot49.2±5.441.7±4.254.6±3.7RoBERTa+WEP-Reasoning(1hop)fine-tuning97.8±0.481.6±1.361.2±0.4RoBERTa+WEP-Reasoning(2hops)fine-tuning85.0±1.691.1±0.162.3±1.7RoBERTa+WEP-UNLIfine-tuning62.4±0.464.3±0.184.4±0.5Table1:Testaccuracypercentageofdifferentmodelsoverthe3WEP-understandingtasks.Thelastthreerowsdisplaytheaccuracywhenfine-tuningoneachtask,andtransferabilityofthefine-tunedmodeloutsidethediagonal.4.1Zero-shotmodelsWeuseoff-the-shelflanguagemodelstoassignlikelihoodscorestoacontextanditsconclusion.Weevaluatetherateatwhichvalidverbalizationisscoredhigherthaninvalidverbalization.Werefinethescoresbyalsoconsideringtheaveragelikelihoodpertoken(Brownetal.,2020;SchickandSchütze,2021)andcalibratedscores(Brownetal.,2020;Zhaoetal.,2021)wherewedividethescoreofaPREMISE.Tp(HYPOTHESIS).bythescoreofTp(HYPOTHESIS).Weevaluatethenor-malized,length-normalized,andcalibratedlike-lihoodonthevalidationsetsofeachdatasetandselectthemostaccuratemethodforeachdatasetandmodel.Wealsoconsiderapretrainednaturallanguageinferencemodel,whichistrainedtopredictentail-mentscoresbetweenacontextandaconclusion.GPT2WeusethepretrainedGPT2basever-sionwith127Mparameters(Radfordetal.,2019),whichisacausallanguagemodeltrainedtoesti-matetextlikelihood.Weconcatenatethepremiseandhypothesisandcomputetheirlikelihoodasaplausibilityscore.RoBERTaWealsousethepretrainedRoBERTabasemodelwith123Mparameters(Liuetal.,2019)toscorethemaskedlanguagemodelinglikelihoodofthepremise/hypothesispair.RoBERTa-MNLIWefine-tuneRoBERTaontheMNLIentailmentdetectiondataset(Williamsetal.,2018)withstandardhyperparameters(seethefol-lowingsubsection).HumanbaselineToestablishhumanbaselineperformanceontheconstructeddataset,wehadtwoNLPresearchersannotate100examplesran-domlysampledfromthetestsetofeachdataset,withamultiple-choicequestionansweringsetting.Overallinter-annotatoragreementisrelativelyhigh,withaFleiss’sκof0.70/0.68/0.71forWEPRea-soning1hop,2hopsandWEP-UNLIrespectively.4.2Fine-tuningandtransferacrossprobesWefine-tuneRoBERTa-basemodelsonourdatasets,usingstandard(Mosbachetal.,2021)hy-perparameters3(3epochs,sequencelengthof256,learningrateof2.10−5batchsizeof16.Weuselength-normalizationwithGPT2likelihoodandcal-ibrationwithRoBERTalikelihoodastheyworkedbestonthevalidationsets.).Weuseamultiple-choice-questionansweringsetup(wepredictlogitscoresforthevalidandinvalidverbalization,com-binetheirscorewithasoftmax,thenoptimizethelikelihoodofthevalidverbalization).Thesamefor-matisappliedtoalltasks,sowecanalsostudythetransferofcapacitiesacquiredduringfine-tuningofeachprobe,forinstance,betweenprobabilitymatchingandcompositionalreasoning.4.3ResultsanddiscussionTable1showstheresultsofourexperiments.Theverylowaccuracyofcausalandmaskedlanguagemodels(firsttworows)demonstrateshowchalleng-ingtheWEP-understandingtasksare.RoBERTafine-tunedonMNLIdatasetperformsbetterthanchanceforWEP-UNLI.MNLIcontains814instancesofprobablyintheMNLIdataset,butwefoundlittletonoevidenceofWEPcomposi-tionsamongthem,whichcanexplaintheresults.Finally,fine-tuningonthedatasetofaparticularprobeleadstohightestaccuracyontheassoci-atedtestset.Moresurprisingly,fine-tuningononedatasetalsocausessubstantialaccuracygainonotherprobes.Thissuggeststhatourdatasetscan3Deviationfromthesehyperparametersdidnotyieldsig-nificantimprovementonthevalidationsets.473

beincorporatedintextencodertraininginordertoimproveWEPhandling.5ConclusionWeinvestigatedWEPunderstandinginneurallan-guagemodelswithnewdatasetsandexperiments,showingthatWEPprocessingischallengingbuthelpedbysupervisionwhichleadstotransferableimprovement.FutureworkcouldextractWEPprobabilityscalesfromtheUNLIdatasetasanalternativetohumanperceptionsurveys,butourworksuggeststhatthisrequireslanguagemodelingprogress.6AcknowledgementsThisworkispartoftheCALCULUSproject,whichisfundedbytheERCAdvancedGrantH2020-ERC-2017ADG7885064.ReferencesSamuelR.Bowman,GaborAngeli,ChristopherPotts,andChristopherD.Manning.2015.Alargeanno-tatedcorpusforlearningnaturallanguageinference.InProceedingsofthe2015ConferenceonEmpiri-calMethodsinNaturalLanguageProcessing,pages632–642,Lisbon,Portugal.AssociationforCompu-tationalLinguistics.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems,33:1877–1901.TongfeiChen,ZhengpingJiang,AdamPoliak,KeisukeSakaguchi,andBenjaminVanDurme.2020.Un-certainnaturallanguageinference.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages8772–8779,On-line.AssociationforComputationalLinguistics.EugeneDantsin.1992.Probabilisticlogicprogramsandtheirsemantics.InLogicProgramming,pages152–164,Berlin,Heidelberg.SpringerBerlinHeidelberg.LucDeRaedt,AngelikaKimmig,andHannuToivonen.2007.Problog:Aprobabilisticprologanditsappli-cationinlinkdiscovery.InIJCAI,volume7,pages2462–2467.Hyderabad.MandeepKDhamiandDavidRMandel.2021.Wordsornumbers?communicatingprobabilityinintelli-genceanalysis.AmericanPsychologist,76(3):549.4https://calculus-project.eu/AntonDries,AngelikaKimmig,JesseDavis,VaishakBelle,andLucdeRaedt.2017.Solvingprobabilityproblemsinnaturallanguage.InProceedingsoftheTwenty-SixthInternationalJointConferenceonArtificialIntelligence,IJCAI-17,pages3981–3987.WadeFagen-Ulmschneider.2015.Perceptionofproba-bilitywords.ShermanKent.1964.Wordsofestimativeprobability.Studiesinintelligence,8(4):49–65.EmilyDLenhardt,RachaelNCross,MakenzieJKro-cak,JosephTRipberger,SeanRErnst,CarolLSilva,andHankCJenkins-Smith.2020.Howlikelyisthatchanceofthunderstorms?astudyofhownationalweatherserviceforecastofficesusewordsofestima-tiveprobabilityandwhattheymeantothepublic.JournalofOperationalMeteorology,8(5).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:Arobustlyoptimizedbertpretrainingap-proach.arXivpreprintarXiv:1907.11692.MariusMosbach,MaksymAndriushchenko,andDiet-richKlakow.2021.Onthestabilityoffine-tuningbert:Misconceptions,explanations,andstrongbase-lines.InInternationalConferenceonLearningRep-resentations.AakankshaNaik,AbhilashaRavichander,NormanSadeh,CarolynRose,andGrahamNeubig.2018.Stresstestevaluationfornaturallanguageinference.InProceedingsofthe27thInternationalConferenceonComputationalLinguistics,pages2340–2353,SantaFe,NewMexico,USA.AssociationforCom-putationalLinguistics.BJO’Brien.1989.Wordsornumbers?theevaluationofprobabilityexpressionsingeneralpractice.TheJournaloftheRoyalCollegeofGeneralPractitioners,39320:98–100.DouglasEOtt.2021.Wordsrepresentingnumericprob-abilitiesinmedicalwritingareambiguousandmis-interpreted.JSLS:JournaloftheSocietyofLaparo-scopic&RoboticSurgeons,25(3).F.R.Palmer.1992.Wordsandworlds;onthelinguisticanalysisofmodality.(europeanuniversitystudies,seriesxiv,vol.191):Richardmatthews,frankfurtammain/bern/newyork/paris,peterlang,1991.310pp.sfr76.00(pb.).Lingua,88(1):87–90.AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog,1(8):9.KyleRichardson,HaiHu,LawrenceMoss,andAshishSabharwal.2020.Probingnaturallanguageinferencemodelsthroughsemanticfragments.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages8713–8721.474

AnnaRogers,OlgaKovaleva,andAnnaRumshisky.2020.AprimerinBERTology:WhatweknowabouthowBERTworks.TransactionsoftheAssociationforComputationalLinguistics,8:842–866.RoserSaurí,MarcVerhagen,andJamesPustejovsky.2006.Annotatingandrecognizingeventmodalityintext.InProceedingsoftheNineteenthInternationalFloridaArtificialIntelligenceResearchSocietyCon-ference,MelbourneBeach,Florida,USA,May11-13,2006,pages333–339.AAAIPress.TimoSchickandHinrichSchütze.2021.Exploitingcloze-questionsforfew-shottextclassificationandnaturallanguageinference.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAsso-ciationforComputationalLinguistics:MainVolume,pages255–269,Online.AssociationforComputa-tionalLinguistics.DamienSileoandAntoineLernould.2023.Mindgames:Targetingtheoryofmindinlargelanguagemodelswithdynamicepistemicmodallogic.arXivpreprintarXiv:2305.03353.AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,AdamFisch,AdamRBrown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,etal.2022.Beyondtheimitationgame:Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.arXivpreprintarXiv:2206.04615.QiSu,Chu-RenHuang,andKai-yunChen.2010.Evi-dentialityfortexttrustworthinessdetection.InPro-ceedingsofthe2010WorkshoponNLPandLin-guistics:FindingtheCommonGround,pages10–17,Uppsala,Sweden.AssociationforComputationalLinguistics.SimonSuster,PieterFivez,PietroTotis,AngelikaKim-mig,JesseDavis,LucdeRaedt,andWalterDaele-mans.2021.Mappingprobabilitywordproblemstoexecutablerepresentations.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3627–3640,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.JasonWeston,AntoineBordes,SumitChopra,Alexan-derMRush,BartVanMerriënboer,ArmandJoulin,andTomasMikolov.2015.Towardsai-completequestionanswering:Asetofprerequisitetoytasks.arXivpreprintarXiv:1502.05698.AdinaWilliams,NikitaNangia,andSamuelBowman.2018.Abroad-coveragechallengecorpusforsen-tenceunderstandingthroughinference.InProceed-ingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLin-guistics:HumanLanguageTechnologies,Volume1(LongPapers),pages1112–1122.AssociationforComputationalLinguistics.ShengZhang,RachelRudinger,KevinDuh,andBen-jaminVanDurme.2017.Ordinalcommon-senseinference.TransactionsoftheAssociationforCom-putationalLinguistics,5:379–395.ZihaoZhao,EricWallace,ShiFeng,DanKlein,andSameerSingh.2021.Calibratebeforeuse:Improv-ingfew-shotperformanceoflanguagemodels.InProceedingsofthe38thInternationalConferenceonMachineLearning,volume139ofProceedingsofMachineLearningResearch,pages12697–12706.PMLR.XiangZhou,YixinNie,andMohitBansal.2022.Dis-tributednli:Learningtopredicthumanopiniondis-tributionsforlanguagereasoning.InFindingsoftheAssociationforComputationalLinguistics:ACL2022.AssociationforComputationalLinguistics.475

AAssociatedprobabilitiesWEPMedianprobabilityjudgmentcertain100†almostcertain95.0±10.9highlylikely90.0±8.4verygoodchance80.0±10.8webelieve75.0±15.0likely70.0±11.3probably70.0±12.9probable70.0±14.7betterthaneven60.0±9.1abouteven50.0±4.9probablynot25.0±14.4wedoubt20.0±16.9unlikely20.0±15.0littlechance10.0±12.2chancesareslight10.0±10.9improbable10.0±17.5highlyunlikely5.0±17.3almostnochance2.0±17.0impossible0†Table2:Medianprobabilitypercentageassociatedtowordsofestimativeprobabilityaccordingto(Fagen-Ulmschneider,2015).Firstandlastwords(†)aretakenfrom(Kent,1964).BWEPverbalizationtemplateWEPVerbalizationtemplateaboutevenchancesareabouteventhat[FACT]almostcertainitisalmostcertainthat[FACT]almostnochancethereisalmostnochancethat[FACT]betterthaneventhereisabetterthanevenchancethat[FACT]certainitiscertainthat[FACT]chancesareslightchancesareslightthat[FACT]highlylikelyitishighlylikelythat[FACT]highlyunlikelyitishighlyunlikelythat[FACT]impossibleitisimpossiblethat[FACT]improbableitisimprobablethat[FACT]likelyitislikelythat[FACT]littlechancethereislittlechancethat[FACT]probableitisprobablethat[FACT]probablyitisprobablythecasethat[FACT]probablynotitisprobablynotthecasethat[FACT]unlikelyitisunlikelythat[FACT]verygoodchancethereisaverygoodchancethat[FACT]webelievewebelievethat[FACT]wedoubtwedoubtthat[FACT]Table3:TemplatesusedtoconvertafactandaWEPexpresseduncertaintyintoamodalsentence.476

CWEPfrequenciesonthegenerateddatasetsWEP-reasoning(1hop)WEP-Reasoning(2hops)WEP-USNLIWEPfrequencyWEPfrequencyWEPfrequencyabouteven11.1impossible13.2impossible25.6probablynot9.7abouteven10.8betterthaneven10.7betterthaneven7.7probablynot9.0certain7.2webelieve7.1highlyunlikely8.2abouteven6.9highlylikely6.4almostnochance8.0almostcertain6.7certain6.0betterthaneven6.6highlylikely6.0highlyunlikely5.9webelieve4.3verygoodchance5.9almostnochance5.8highlylikely4.0almostnochance5.0impossible5.3verygoodchance4.0webelieve4.1almostcertain5.1wedoubt4.0highlyunlikely4.1verygoodchance4.7improbable3.9probablynot3.4chancesareslight3.6chancesareslight3.9likely2.5littlechance3.5unlikely3.6probable2.4probable3.2littlechance3.5probably2.4unlikely3.1almostcertain2.9unlikely1.5likely3.1certain2.7littlechance1.5probably3.0likely2.5chancesareslight1.5wedoubt2.9probable2.4improbable1.4improbable2.9probably2.2wedoubt1.4Table4:ValidationsetfrequencyofWEPinthecorrectanswerofeachdataset(percentages).