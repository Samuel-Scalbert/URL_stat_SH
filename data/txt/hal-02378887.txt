Model-Based Reinforcement Learning Exploiting
State-Action Equivalence
Mahsa Asadi, Mohammad Sadegh Talebi, Hippolyte Bourel, Odalric-Ambrym

Maillard

To cite this version:

Mahsa Asadi, Mohammad Sadegh Talebi, Hippolyte Bourel, Odalric-Ambrym Maillard. Model-Based
Reinforcement Learning Exploiting State-Action Equivalence. ACML 2019, Proceedings of Machine
Learning Research, Nov 2019, Nagoya, Japan. pp.204 - 219. ￿hal-02378887￿

HAL Id: hal-02378887

https://hal.science/hal-02378887

Submitted on 29 Nov 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Proceedings of Machine Learning Research 101:204–219, 2019

ACML 2019

Model-Based Reinforcement Learning Exploiting
State-Action Equivalence

Mahsa Asadi
Mohammad Sadegh Talebi
Hippolyte Bourel
Odalric-Ambrym Maillard
Inria Lille – Nord Europe

Editors: Wee Sun Lee and Taiji Suzuki

mahsa.asadi@inria.fr
sadegh.talebi@inria.fr
hippolyte.bourel@ens-rennes.fr
odalric.maillard@inria.fr

Abstract
Leveraging an equivalence property in the state-space of a Markov Decision Process (MDP)
has been investigated in several studies. This paper studies equivalence structure in the
reinforcement learning (RL) setup, where transition distributions are no longer assumed
to be known. We present a notion of similarity between transition probabilities of various
state-action pairs of an MDP, which naturally deﬁnes an equivalence structure in the
state-action space. We present equivalence-aware conﬁdence sets for the case where the
learner knows the underlying structure in advance. These sets are provably smaller than
their corresponding equivalence-oblivious counterparts. In the more challenging case of
an unknown equivalence structure, we present an algorithm called ApproxEquivalence
that seeks to ﬁnd an (approximate) equivalence structure, and deﬁne conﬁdence sets using
the approximate equivalence. To illustrate the eﬃcacy of the presented conﬁdence sets,
we present C-UCRL, as a natural modiﬁcation of UCRL2 for RL in undiscounted MDPs. In
the case of a known equivalence structure, we show that C-UCRL improves over UCRL2 in
terms of regret by a factor of pSA/C, in any communicating MDP with S states, A
actions, and C classes, which corresponds to a massive improvement when C (cid:28) SA. To
the best of our knowledge, this is the ﬁrst work providing regret bounds for RL when
an equivalence structure in the MDP is eﬃciently exploited. In the case of an unknown
equivalence structure, we show through numerical experiments that C-UCRL combined with
ApproxEquivalence outperforms UCRL2 in ergodic MDPs.
Keywords: Reinforcement Learning, Regret, Conﬁdence Bound, Equivalence.

1. Introduction

This paper studies the Reinforcement Learning (RL) problem, where an agent interacts with
an unknown environment in a single stream of observations, with the aim of maximizing the
cumulative reward gathered over the course of experience. The environment is modeled as a
Markov Decision Process (MDP), with ﬁnite state and action spaces, as considered in most
literature; we refer to (Puterman, 2014; Sutton and Barto, 1998) for background materials
on MDPs, and to Section 2. In order to act optimally or nearly so, the agent needs to
learn the parameters of the MDP using the observations from the environment. The agent
thus faces a fundamental trade-oﬀ between exploitation vs. exploration: Namely, whether to
gather more experimental data about the consequences of the actions (exploration) or acting

c(cid:13) 2019 M. Asadi, M.S. Talebi, H. Bourel & O.-A. Maillard.

Model-Based RL Exploiting Equivalences

consistently with past observations to maximize the rewards (exploitation); see (Sutton and
Barto, 1998). Over the past two decades, a plethora of studies have addressed the above
RL problem in the undiscounted setting, where the goal is to minimize the regret (e.g.,
Bartlett and Tewari (2009); Jaksch et al. (2010); Gheshlaghi Azar et al. (2017)), or in the
discounted setting (as in, e.g., Strehl and Littman (2008)) with the goal of bounding the
sample complexity of exploration as deﬁned in (Kakade, 2003). In most practical situations,
the state-space of the underlying MDP is too large, but often endowed with some structure.
Directly applying the state-of-the-art RL algorithms, for instance from the above works, and
ignoring the structure would lead to a prohibitive regret or sample complexity.

In this paper, we consider RL problems where the state-action space of the underlying
MDP exhibits some equivalence structure. This is quite typical in many MDPs in various
application domains. For instance, in a grid-world MDP when taking action ‘up’ from state
s or ‘right’ from state s0 when both are away from any wall may result in similar transitions
(typically, move towards the target state with some probability, and stay still or transit to
other neighbors with the remaining probability); see, e.g., Figure 1 in Section 2. We are
interested in exploiting such a structure in order to speed up the learning process. Leveraging
an equivalence structure is popular in the MDP literature; see (Ravindran and Barto, 2004;
Li et al., 2006; Abel et al., 2016). However, most notions are unfortunately not well adapted
to the RL setup, that is when the underlying MDP is unknown, as opposed to the known
MDP setup. In particular, amongst those considering such structures, to our knowledge,
none has provided performance guarantees in terms of regret or sample complexity. Our goal
is to ﬁnd a near-optimal policy, with controlled regret or sample complexity. To this end, we
follow a model-based approach, which is popular in the RL literature, and aim at providing
a generic model-based approach capable of exploiting this structure, to speed up learning.
We do so by aggregating the information of state-action pairs in the same equivalence class
when estimating the transition probabilities or reward function of the MDP.

Contributions. We make the following contributions. (i) We ﬁrst introduce a notion of
similarity between state-action pairs, which naturally yields a partition of the state-action
space S × A, and induces an equivalence structure in the MDP (see Deﬁnition 1–2). To
our knowledge, while other notions of equivalence have been introduced, our proposed
deﬁnition appears to be the ﬁrst, in a discrete RL setup, explicitly using proﬁle (ordering)
of distributions. (ii) We present conﬁdence sets that incorporate equivalence structure of
transition probabilities and reward function into their deﬁnition, when the learner has access
to such information. These conﬁdence sets are smaller than those obtained by ignoring
equivalence structures. (iii) In the case of an unknown equivalence structure, we present
ApproxEquivalence, which uses conﬁdence bounds of various state-action pairs as a proxy to
estimate an empirical equivalence structure of the MDP. (iv) Finally, in order to demonstrate
the application of the above equivalence-aware conﬁdence sets, we present C-UCRL, which is
a natural modiﬁcation of the UCRL2 algorithm (Jaksch et al., 2010) employing the presented
conﬁdence sets. As shown in Theorem 13, when the learner knows the equivalence structure,
C-UCRL achieves a regret which is smaller than that of UCRL2 by a factor of pSA/C, where
C is the number of classes. This corresponds to a massive improvement when C (cid:28) SA. We
also verify, through numerical experiments, the superiority of C-UCRL over UCRL2 in the case
of an unknown equivalence structure.

205

Asadi et al.

Related Work. There is a rich literature on state-abstraction (or state-aggregation) in
MDPs; we refer to (Li et al., 2006) on earlier methods, and to (Abel et al., 2016) for a good
survey of recent approaches. (Ravindran and Barto, 2004) introduces aggregation based
on homo-morphisms of the model, but with no algorithm nor regret analysis. (Dean et al.,
1997; Givan et al., 2003) consider a partition of state-space of MDPs based on the notion
of stochastic bi-simulation, which is a generalization of the notion of bi-simulation from
the theory of concurrent processes to stochastic processes. This path is further followed in
(Ferns et al., 2004, 2011), where bi-simulation metrics for capturing similarities are presented.
Bi-simulation metrics can be thought of as quantitative analogues of the equivalence relations,
and suggest to resort to optimal transport, which is intimately linked with our notions of
similarity and equivalence (see Deﬁnition 1). However, these powerful metrics have only been
studied in the context of a known MDP, and not the RL setup. The approach in (Anand
et al., 2015) is similar to our work in that it considers state-action equivalence. Unlike the
present paper, however, it does not consider orderings, transition estimation errors, or regret
analysis. Another relevant work to our approach is (Ortner, 2013) on aggregation of states
(but not of pairs, and with no ordering) based on concentration inequalities, a path that
we follow. We also mention the works (Brunskill and Li, 2013; Mandel et al., 2016), where
clustering of the state-space is studied. As other relevant works, we refer to (Leﬄer et al.,
2007), where relocatable action model is introduced, and to (Diuk et al., 2009) that studies
RL in the simpler setting of factored MDPs. We also mention interesting works revolving
around complementary RL questions including the one on selection amongst diﬀerent state
representations in (Ortner et al., 2014) and on state-aliasing in (Hallak et al., 2013).

As part of this paper is devoted to presenting an equivalence structure aware variant
of UCRL2, we provide here a brief review of the literature related to undiscounted RL.
Undiscounted RL dates back at least to (Burnetas and Katehakis, 1997), and is thoroughly
investigated later on in (Jaksch et al., 2010). The latter work presents UCRL2, which is
inspired by multi-armed bandit algorithms. Several studies continued this line, including
(Bartlett and Tewari, 2009; Maillard et al., 2014; Gheshlaghi Azar et al., 2017; Dann et al.,
2017; Talebi and Maillard, 2018; Fruit et al., 2018), to name a few. Most of these works
present UCRL2-style algorithms, and try to reduce the regret dependency on the number of
states, as in, e.g., (Gheshlaghi Azar et al., 2017; Dann et al., 2017) (restricted to the episodic
RL with a ﬁxed and known horizon). Although the concept of equivalence is well-studied
in MDPs, no work seems to have investigated the possibility of deﬁning an aggregation
that both is based on state-action pairs (instead of states only) for RL problems, and uses
optimal transportation maps combined with statistical tests. Especially, the use of proﬁle
maps seems novel and we show it is also eﬀective.

2. Model and Equivalence Classes

2.1. The RL Problem

In this section, we describe the RL problem, which we study in this paper. Let M =
(S, A, p, ν) be an undiscounted MDP1, where S denotes the discrete state-space with cardi-
nality S, and A denotes the discrete action-space with cardinality A. Here, p represents the

1. Our results can be extended to the discounted case as well.

206

Model-Based RL Exploiting Equivalences

transition kernel such that p(s0|s, a) denotes the probability of transiting to state s0, starting
from state s and executing action a. Finally, ν is a reward distribution function on [0, 1],
whose mean is denoted by µ.

The game proceeds as follows. The learner starts in some state s1 ∈ S at time t = 1. At
each time step t ∈ N, the learner chooses one action a ∈ A in its current state st based on its
past decisions and observations. When executing action at in state st, the learner receives a
random reward rt := rt(st, at) drawn independently from distribution ν(st, at), and whose
mean is µ(st, at). The state then transits to a next state st+1 ∼ p(·|st, at), and a new decision
step begins. We refer to (Sutton and Barto, 1998; Puterman, 2014) for background material
on MDPs and RL. The goal of the learner is to maximize the cumulative reward gathered
in the course of interaction with the environment. As p and ν are unknown, the learner
has to learn them by trying diﬀerent actions and recording the realized rewards and state
transitions. The performance of the learner can be assessed through the notion of regret2
with respect to an optimal oracle, being aware of p and ν. More formally, as in (Jaksch
et al., 2010), under a learning algorithm A, we deﬁne the T -step regret as

R(A, T ) := T g? −

T
X

t=1

rt(st, at) ,

where g? denotes the average reward (or gain3) attained by an optimal policy, and where at
is chosen by A as a function of ((st0, at0)t0<t, st). Alternatively, the objective of the learner is
to minimize the regret, which calls for balancing between exploration and exploitation. In
the present work, we are interested in exploiting equivalence structure in the state-action
space in order to speed up exploration, which, in turn, reduces the regret.

2.2. Similarity and Equivalence Classes

We now present a precise deﬁnition of the equivalence structure considered in this paper.
We ﬁrst introduce a notion of similarity between state-action pairs of the MDP:

Deﬁnition 1 (Similar state-action pairs) The pair (s0, a0) is said to be ε-similar to the
pair (s, a), for ε = (εp, εµ) ∈ R2

+, if

kp(σs,a(·)|s, a) − p(σs0,a0(·)|s0, a0)k1 (cid:54) εp

and

|µ(s, a) − µ(s0, a0)| (cid:54) εµ ,

where σs,a : {1, . . . , S} → S indexes a permutation of states such that p(σs,a(1)|s, a) (cid:62)
p(σs,a(2)|s, a) (cid:62) . . . (cid:62) p(σs,a(S)|s, a). We refer to σs,a as a proﬁle mapping (or for short,
proﬁle) for (s, a), and denote by σ = (σs,a)s,a the set of proﬁle mappings of all pairs.

The notion of similarity introduced above naturally yields a partition of the state-action

space S × A, as detailed in the following deﬁnition:

Deﬁnition 2 (Equivalence classes) (0, 0)-similarity is an equivalence relation and in-
duces a canonical partition of S × A. We refer to such a canonical partition as equivalence
classes or equivalence structure, denote it by C, and let C := |C|.

2. We note that in the discounted setting, the quality of a learning algorithm is usually assessed through

the notion of sample complexity as deﬁned in (Kakade, 2003).
3. See, e.g., (Puterman, 2014) for background material on MDPs.

207

Asadi et al.

In order to help understand Deﬁnitions 1 and 2, we present in Figure 1 an MDP with 13
states, where the state-action pairs (6,Up) and (8,Right) are equivalent up to a permutation:
Let the permutation σ be such that σ(2) = 9, σ(6) = 8, and σ(i) = i for all i 6= 2, 6. Now
p(σ(x)|6, Up) = p(x|8, Right) for all x ∈ S, and thus, the pairs (8,Right) and (6,Up) belong
to the same class.

Figure 1: A grid-world MDP showing similar
transitions from state-action pairs
(6,Up) and (8,Right).

Remark 3 Crucially, the equivalence relation is not only stated about states, but about
state-action pairs. For instance, pairs (6,Up) and (8,Right) in this example are in the same
class although corresponding to playing diﬀerent actions in diﬀerent states.

Remark 4 The proﬁle mapping σs,a in Deﬁnition 1 may not be unique in general, especially
if distributions have sparse supprts. For ease of presentation, in the sequel we assume that
the restriction of σs,a to the support of p(·|s, a) is uniquely deﬁned. We also remark that
Deﬁnition 1 can be easily generalized by replacing the k · k1 norm with other contrasts, such
as the KL divergence, squared distance, etc.

In many environments considered in RL with large state and action spaces, the number
C of equivalent classes of state-action pairs using Deﬁnitions 1–2 stays small even for large
SA, thanks to the proﬁle mappings. This is the case in typical grid-world MDPs as well as
in RiverSwim shown in Figure 2. For example, in Ergodic RiverSwim with L states, we have
C = 6. We also refer to Appendix F for additional illustrations of grid-world MDPs. This
remarkable feature suggests that leveraging this structure may yield signiﬁcant speed-up in
terms of learning guarantees if well-exploited.

3. Equivalence-Aware Conﬁdence Sets

We are now ready to present an approach that deﬁnes conﬁdence sets for p and µ taking
into account the equivalence structure in the MDP. The use of conﬁdence bounds in a
model-based approach is related to strategies implementing the optimism in the face of
uncertainty principle, as in stochastic bandit problems (Lai and Robbins, 1985; Auer et al.,
2002). Such an approach relies on maintaining a set of plausible MDPs (models) that are
consistent with the observations gathered, and where the set contains the true MDP with
high probability. Exploiting equivalence structure of the MDP, one could obtain a more
precise estimation of mean reward µ and transition kernel p of the MDP by aggregating
observations from various state-action pairs in the same class. This, in turn, yields smaller
(hence, better) sets of models.

208

Model-Based RL Exploiting Equivalences

0.4

s1

0.6

s2

0.1

0.6

0.05

0.9

0.9
(r = 0.05)

0.6

s3

0.1

0.35

0.05

0.9

0.1

0.35

0.05

0.9

0.95
(r = 1)

sL

0.6

sL−1

0.1

0.35

0.05

0.9

0.1

0.35

0.05

1

Figure 2: The L-state Ergodic RiverSwim MDP

Notations. We introduce some necessary notations. Under a given algorithm, for a pair
(s, a), we denote by Nt(s, a) the total number of observations of (s, a) up to time t. Let us
deﬁne bµt(s, a) as the empirical mean reward built using Nt(s, a) i.i.d. samples from ν(s, a),
and bpt(·|s, a) as the empirical distribution built using Nt(s, a) i.i.d. observations from p(·|s, a).
For a set c ⊆ S × A, we denote by nt(c) the total number of observations of pairs in c
up to time t, that is nt(c) := P
(s,a)∈c Nt(s, a). For c ⊆ S × A, we further denote by bµt(c)
and bpt(·|c) the empirical mean reward and transition probability built using nt(c) samples,
respectively; we provide precise deﬁnitions of bµt(c) and bpt(·|c) later on in this section.

For a given conﬁdence parameter δ and time t, we write Mt,δ to denote the set of

plausible MDPs at time t, which may be generically expressed as

Mt,δ = {(S, A, p0, ν0) : p0 ∈ CBt,δ and µ0 ∈ CB0

t,δ} ,

(1)

where CBt,δ (resp. CB0
t,δ) denotes the conﬁdence set for p (resp. µ) centered at bp (resp. bµ), and
where µ0 is the mean of ν0. Note that both CBt,δ and CB0
t,δ depend on Nt(s, a), (s, a) ∈ S × A.
For ease of presentation, in the sequel we consider the following conﬁdence sets4 used in
several model-based RL algorithms, e.g., (Jaksch et al., 2010; Dann et al., 2017):

CBt,δ := (cid:8)p0 : kbpt(·|s, a) − p0(·|s, a)k1 (cid:54) βNt(s,a)
(cid:0) δ
CB0
SA

t,δ := (cid:8)µ0 : |bµt(s, a) − µ0(·|s, a)| (cid:54) β0

Nt(s,a)

(cid:1), ∀s, a(cid:9) ,

(cid:0) δ
SA
(cid:1), ∀s, a(cid:9) ,

βn(δ) =

s

2(1 + 1

n ) log (cid:0)√

n + 1 2S −2

δ

(cid:1)

n

s

and β0

n(δ) =

√

(1 + 1

n ) log(
2n

(2)

where

n + 1/δ)

, ∀n.

(3)

These conﬁdence sets were derived by combining Hoeﬀding’s (Hoeﬀding, 1963) and Weiss-
man’s (Weissman et al., 2003) concentration inequalities with the Laplace method (Peña
et al., 2008; Abbasi-Yadkori et al., 2011), which enables to handle the random stopping times
Nt(s, a) in a sharp way; we refer to (Maillard, 2019) for further discussion. In particular,
this ensures that the true transition function p and mean reward function µ are contained
in the conﬁdence sets with probability at least 1 − 2δ, uniformly over all time t.

Remark 5 As the bounds for µ and p are similar, to simplify the presentation, from now
on we assume the mean reward function µ is known5.

We now provide modiﬁcations to CBt,δ in order to exploit the equivalence structure C,
when the learner knows C in advance. The case of an unknown C is addressed in Section 4.

4. The approach presented in this section can be extended to other concentration inequalities, as well.
5. This is a common assumption in the RL literature; see, e.g., (Bartlett and Tewari, 2009).

209

Asadi et al.

3.1. Case 1: Known Classes and Proﬁles

Assume that an oracle provides the learner with a perfect knowledge of the equivalence
classes C as well as proﬁles σ = (σs,a)s,a. In this ideal situation, the knowledge of C and σ
allows to straightforwardly aggregate observations from all state-action pairs in the same
class to build more accurate estimates of p and µ. Formally, for a class c ⊂ C, we deﬁne

bpσ
t (x|c) =

1
nt(c)

X

(s,a)∈c

Nt(s, a)bpt

(cid:0)σs,a(x)|s, a(cid:1) ,

∀x ∈ S,

(4)

where we recall that nt(c) = P
aggregate empirical distribution bpσ
conﬁdence set (2) by modifying the L1 bound there as follows:

(s,a)∈c Nt(s, a). The superscript σ in (4) signiﬁes that the
t , we modify the

t depends on σ. Having deﬁned bpσ

kbpσ

t (σ−1(·)|c) − p0(·|c)k1 (cid:54) βnt(c)

(cid:0) δ
C

(cid:1) ,

∀c ∈ C,

(5)

and further deﬁne: CBt,δ(C, σ) := (cid:8)p0 : (5) holds(cid:9), where (C, σ) stresses that C and σ are
provided as input. Then, for all time t and class c ∈ C, by construction, the true transition
p belongs to CBt,δ(C, σ), with probability greater than 1 − δ.

Remark 6 It is crucial to remark that the above conﬁdence set does not use elements of C
as “meta states” (i.e., replacing the states with classes), as considered for instance in the
literature on state-aggregation. Rather, the classes are only used to group observations from
diﬀerent sources and build more reﬁned estimates for each pair: The plausible MDPs are
built using the same state-space S and action-space A, unlike in, e.g., (Ortner, 2013).

3.2. Case 2: Known Classes, Unknown Proﬁles

Now we consider a more realistic setting when the oracle provides C to the learner, but σ is
unknown. In this more challenging situation, we need to estimate proﬁles as well. Given
time t, we ﬁnd an empirical proﬁle mapping (or for short, empirical proﬁle) σs,a,t satisfying

bpt(σs,a,t(1)|s, a) (cid:62)

bpt(σs,a,t(2)|s, a) (cid:62) . . . (cid:62)

bpt(σs,a,t(S)|s, a) ,

and deﬁne σt = (σs,a,t)s,a. We then build the modiﬁed empirical estimate in a similar fashion
to (4): For any c ∈ C,

bpσt
t (x|c) =

1
nt(c)

X

(s,a)∈c

Nt(s, a)bpt(σs,a,t(x)|s, a) ,

∀x ∈ S.

Then, we may modify the L1 inequality in (2) as follows:
t (·)|c) − p0(·|c)k1 (cid:54) 1
nt(c)

t (σ−1

kbpσt

X

(s,a)∈c

Nt(s, a)βNt(s,a)

(cid:0) δ
C

(cid:1) ,

∀c ∈ C,

(6)

which further yields the following modiﬁed conﬁdence set that uses only C as input:
CBt,δ(C) := (cid:8)p0 : (6) holds(cid:9). The above construction is justiﬁed by the following non-
expansive property of the ordering operator, as it ensures that Weissman’s concentration
inequality also applies to the ordered empirical distribution:

210

Model-Based RL Exploiting Equivalences

Lemma 7 (Non-expansive ordering) Let p and q be two discrete distributions, deﬁned
on the same alphabet S, with respective proﬁle mappings σp and σq. Then,

kp(σp(·)) − q(σq(·))k1 (cid:54) kp − qk1 .

The proof of Lemma 7 is provided in Appendix B. An immediate corollary follows.

Corollary 8 The conﬁdence set CBt,δ(C) contains the true transition function p with prob-
ability at least 1 − δ, uniformly over all time t.

4. Unknown Classes: The ApproxEquivalence Algorithm

In this section, we turn to the most challenging situation when both C and σ are unknown to
the learner. To this end, we ﬁrst introduce an algorithm, which we call ApproxEquivalence,
that ﬁnds an approximate equivalence structure in the MDP by grouping transition proba-
bilities based on statistical tests. ApproxEquivalence is inspired by (Khaleghi et al., 2016)
that provides a method for clustering time series. Interestingly enough, ApproxEquivalence
does not require the knowledge of the number of classes in advance.

We ﬁrst introduce some deﬁnitions. Given u, v ⊆ S × A, we deﬁne the distance between
u and v as d(u, v) := kpσu(·|u) − pσv (·|v)k1. ApproxEquivalence relies on ﬁnding subsets
of S × A that are statistically close in terms of the distance function d(·, ·). As d(·, ·) is
unknown, ApproxEquivalence relies on a lower conﬁdence bound on it: For u, v ⊆ S × A,
we deﬁne the lower-conﬁdence distance function between u and v as

bd(u, v) := bdt,δ(u, v) := (cid:13)

(cid:13)bpσu,t

t

(·|v)(cid:13)

(cid:13)1 − εu,t − εv,t ,

(·|u) − bpσv,t
P

t

‘∈u Nt(‘)βNt(‘)

(cid:0) δ
SA

(cid:1). We stress that,

where for u ∈ S × A and t ∈ N, we deﬁne εu,t := 1
unlike d(·, ·), bd(·, ·) is not a distance function.

nt(u)

Deﬁnition 9 (PAC Neighbor) For a given equivalence structure C, and given c ∈ C, we
say that c0 ∈ C is a PAC Neighbor of c if it satisﬁes: (i) bd(c, c0) (cid:54) 0; (ii) bd({j}, {j0}) (cid:54) 0,
for all j ∈ c and j0 ∈ c0; and (iii) bd({j}, c ∪ c0) (cid:54) 0, for all j ∈ c ∪ c0. We further deﬁne
N (c) := (cid:8)c0 ∈ C \ {c} : (i)–(iii) hold(cid:9) as the set of all PAC Neighbors of c.

Deﬁnition 10 (PAC Nearest Neighbor) For a given equivalence structure C and c ∈ C,
we deﬁne the PAC Nearest Neighbor of c (when it exists) as:

bd(c, u) .

Near(c, C) ∈ argmin
u∈N (c)
ApproxEquivalence proceeds as follows. At time t, it receives as input a parameter
α > 1 that controls the level of aggregation, as well as Nt(s, a) for all pairs (s, a). Starting
from the trivial partition of {1, . . . , SA} into C0 := (cid:8){1}, . . . , {SA}(cid:9), the algorithm builds
a coarser partition by iteratively merging elements of C0 that are statistically close. More
precisely, the algorithm sorts elements of C0 in a non-increasing order of nt(c), c ∈ C0 so
as to promote pairs with the tightest conﬁdence intervals. Then, starting from c with
the largest nt(c), it ﬁnds the PAC Nearest Neighbor c0 of c, that is c0 = Near(c, C0). If
(cid:54) α, where L(c) = |c|, the algorithm merges c and c0, thus leading to a novel
1
α

(cid:54) nt(c)/L(c)
nt(c0)/L(c0)

211

Asadi et al.

partition C1, which contains the new cluster c ∪ c0, and removes c and c0. The algorithm
continues this procedure with the next set in C0, until exhaustion, thus ﬁnishing the creation
of the novel partition C1 of {1, . . . , SA}. ApproxEquivalence continues this process, by
ordering the elements of C1 in a non-increasing order, and carrying out similar steps as
before, yielding the new partition C2. ApproxEquivalence continues the same procedure
until iteration k when Ck+1 = Ck (convergence). The pseudo-code of ApproxEquivalence is
shown in Algorithm 1.

Algorithm 1 ApproxEquivalence
Input: Nt, α

Initialization: C0 ← {{1}, {2}, . . . , {SA}}; n ← Nt; L ← 1SA
changed ← True; k ← 1;
while changed do
Ck+1 ← Ck;
changed ← False;
Index ← argsort(n);
for all i ∈ Index do
if n(i) = 0 then

Break;

end if
if Near(i, Ck−1) 6= ∅ then

j ← Near(i, Ck−1);
(cid:54) n(i)/L(i)
if 1
n(j)/L(j)
α
(·|j) ← 1

(cid:54) α then

(cid:16)

(·|j) + n(i)bpσi,t

t

t

t

n(j)+n(i)

n(j)bpσj,t

bpσj,t
L(i) ← L(j) + L(i); n(i) ← n(j) + n(i);
n(j) ← 0, L(j) ← 0;
Ck+1 ← Ck+1 \ ({i}, {j}) ∪ {i, j};
changed ← True;

(cid:17)

(·|i)

end if

end if
end for
k ← k + 1;

end while

output Ck

(cid:54) nt(c)/L(c)
nt(c0)/L(c0)

The purpose of condition 1
α

(cid:54) α is to ensure the stability of the algorithm.
It prevents merging pairs whose numbers of samples diﬀer a lot. We note that a very similar
condition (with α = 2) is considered in (Ortner, 2013) for state-aggregation. Nonetheless,
we believe such a condition could be relaxed.
Remark 11 Since at each iteration, either two or more subsets are merged, ApproxEquivalence
converges after, at most, SA − 1 steps.

We provide a theoretical guarantee for the correctness of ApproxEquivalence for the

case when α tends to inﬁnity. The result relies on the following separability assumption:

Assumption 1 (Separability) There exists some ∆ > 0 such that

∀c 6= c0 ∈ C, ∀‘ ∈ c, ∀‘0 ∈ c0,

d({‘}, {‘0}) (cid:62) ∆ .

Proposition 12 Under Assumption 1, provided that mins,a Nt(s, a) > f −1(∆), where f :
n 7→ 4βn( δ
SA ), ApproxEquivalence with the choice α → ∞ outputs the correct equivalence
structure C of state-action pairs with probability at least 1 − δ.

212

Model-Based RL Exploiting Equivalences

The proof of Proposition 12 is provided in Appendix C. We note that Assumption 1
bears some similarity to the separability assumption used in (Brunskill and Li, 2013). Note
further although the proposition relies on Assumption 1, we believe one may be able to
derive a similar result under a weaker assumption as well. We leave this for future work.

Now we turn to deﬁning the aggregated conﬁdence sets. Given t, let Ct denote the

equivalence structure output by the algorithm. We may use the following conﬁdence set:

CBt,δ(Ct) :=

n
p0 : kbpσt

t (σ−1

t (·)|c) − p0(·|c)k1 (cid:54)X

Nt(s,a)
nt(c) βNt(s,a)

(cid:0) δ
SA

(cid:1) , ∀c ∈ Ct

o

.

(7)

(s,a)∈c

5. Application: The C-UCRL Algorithm

t = argmaxπ:S→A maxM ∈Mt,δ gM

This section is devoted to presenting some applications of equivalence-aware conﬁdence sets
introduced in Section 3. We present C-UCRL, a natural modiﬁcation of UCRL2 (Jaksch et al.,
2010), which is capable of exploiting the equivalence structure of the MDP. We consider
variants of C-UCRL depending on which information is available to the learner in advance.
First, we brieﬂy recall UCRL2. At a high level, UCRL2 maintains the set Mt,δ of MDPs at
time t,6 which is deﬁned in (1). It then implements the optimistic principle by trying to
compute π+
π denotes the gain of policy π in MDP
M . This is carried out approximately by the Extended Value Iteration (EVI) algorithm
t and MDP fMt such that g eMt
that builds a near-optimal policy π+
π − 1√
.
π+
t
t
Finally, UCRL2 does not recompute π+
t at each time step. Instead, it proceeds in internal
episodes (indexed by k ∈ N), and computes π+
t only at the starting time tk of each episode,
t > tk−1 : ∃s, a, νtk−1:t(s, a) (cid:62) Ntk−1(s, a)+o
n
deﬁned as t1 = 1 and for all k > 1, tk = min
,
where νt1:t2(s, a) denotes the number of observations of pair (s, a) between time t1+1 and t2,
and where for z ∈ N, z+ := max{z, 1}. We provide the pseudo-code of UCRL2 in Appendix A.

(cid:62) maxπ,M ∈Mt,δ gM

π , where gM

5.1. C-UCRL: Known Equivalence Structure

Here we assume that the learner knows C and σ in advance, and provide a variant of UCRL2,
referred to as C-UCRL(C, σ), capable of exploiting the knowledge on C and σ. Given δ, at
time t, C-UCRL(C, σ) uses the following set of models

Mt,δ(C, σ) =

n

(S, A, p0, ν) : p0 ∈ Pw(C) and p0

C ∈ CBt,δ(C, σ)

o

,

where Pw(C) denotes the state-transition functions that are piece-wise constant on C, and
C denotes the function induced by p0 ∈ Pw(C) over C (that is p0(·|s, a) = p0
where p0
C(·|c) for
all (s, a) ∈ c). Moreover, C-UCRL(C, σ) deﬁnes

tk+1 = min

n
t > tk : ∃c ∈ C : X

(s,a)∈c

νtk:t(s, a) (cid:62) ntk (c)+o

.

We note that forcing the condition p0 ∈ Pw(C) may be computationally diﬃcult. To ensure
eﬃcient implementation, we use the same EVI algorithm of UCRL2, where for (s, a) ∈ c,

6. This set is described by the Weismann conﬁdence bounds combined with the Laplace method. The
original UCRL2 algorithm in (Jaksch et al., 2010) uses looser conﬁdence bounds relying on union bounds
instead of the Laplace method.

213

Asadi et al.

we replace bpt(·|s, a) and βNt(s,a)( δ
SA ) respectively with bpσ
C ). The precise
modiﬁed steps of C-UCRL(C, σ) are presented in Appendix A for completeness. An easy
modiﬁcation of the analysis of (Jaksch et al., 2010) yields:

t (·|c) and βnt(c)( δ

Theorem 13 (Regret of C-UCRL(C, σ)) With probability higher than 1 − 3δ, uniformly
over all time horizon T ,

R(C-UCRL(C, σ), T ) (cid:54) 18

CT (cid:0)S + log(2C

q

√

T + 1/δ)(cid:1) + DC log2( 8T

C ) .

The proof of Theorem 13 is provided in Appendix D. This theorem shows that eﬃciently
exploiting the knowledge of C and σ yields an improvement over the regret bound of UCRL2
by a factor of pSA/C, which could be a huge improvement when C (cid:28) SA. This is the case
in, for instance, many grid-world environments thanks to Deﬁnitions 1-2; see Appendix F.

5.2. C-UCRL: Unknown Equivalence Structure

Now we consider the case where C is unknown to the learner. In order to accommodate this
situation, we use ApproxEquivalence in order to estimate the equivalence structure.

We introduce C-UCRL, which proceeds similarly to C-UCRL(C, σ). At each time t,
ApproxEquivalence outputs Ct as an estimate of the true equivalence structure C. Then,
C-UCRL uses the following set of models taking Ct as input:

(cid:26)

Mt,δ(Ct) =

(S, A, p0, ν) : p0 ∈ Pw(Ct) and p0

Ct ∈ CBt,δ(Ct)

.

(cid:27)

Further, it sets the starting step of episode k + 1 as:

tk+1 = min

(cid:26)

t > tk : ∃c ∈ Ctk , X

(s,a)∈c

νtk:t(s, a) (cid:62) ntk (c)+ or ∃s, a, νtk:t(s, a) (cid:62) Ntk (s, a)+

(cid:27)

.

Similarly to C-UCRL(C, σ), we use a modiﬁcation of EVI to implement C-UCRL.

Remark 14 Note that Mt,δ(C) 6= Mt,δ(Ct) as we may have Ct 6= C. Nonetheless, the design
of ApproxEquivalence, which relies on conﬁdence bounds, ensures that Ct is informative
enough, in the sense that Mt,δ(Ct) could be much smaller (hence, better) than a set of
models that one would obtain by ignoring equivalence structure; this is also validated by the
numerical experiments in ergodic environments in the next subsection.

5.3. Numerical Experiments

We conduct numerical experiments to examine the performance of the proposed vari-
ants of C-UCRL, and compare it to that of UCRL2-L7. In our experiments, for running
(cid:1) for u ∈ S × A (in
ApproxEquivalence, as a sub-routine of C-UCRL, we set εu,t = βnt(u)
the deﬁnition of both bd(·, ·) and CBt,δ(Ct)). Although this could lead to a biased estimation
of p, such a bias is controlled thanks to using α > 1 (in our experiments, we set α = 4).

(cid:0) δ

3SA

7. UCRL2-L is a variant of UCRL2, which uses conﬁdence bounds derived by combining Hoeﬀding’s and
Weissman’s inequalities with the Laplace method, as in (2). We stress that UCRL2-L attains a smaller
regret than the original UCRL2 of (Jaksch et al., 2010).

214

Model-Based RL Exploiting Equivalences

(a) 25-state Ergodic RiverSwim

(b) 50-state Ergodic RiverSwim

Figure 3: Regret of various algorithms in Ergodic RiverSwim environments

In the ﬁrst set of experiments, we examine the regret of various algorithms in ergodic
environments. Speciﬁcally, we consider the ergodic RiverSwim MDP, shown in Figure 2,
with 25 and 50 states. In both cases, we have C = 6 classes. In Figure 3, we plot the regret
against time steps under C-UCRL(C, σ), C-UCRL, and UCRL2-L executed in the aforementioned
environments. The results are averaged over 100 independent runs, and the 95% conﬁdence
intervals are shown. All algorithms use δ = 0.05, and for C-UCRL, we use α = 4. As
the curves show, the proposed C-UCRL algorithms signiﬁcantly outperform UCRL2-L, and
C-UCRL(C, σ) attains the smallest regret. In particular, in the 25-state environment and at
the ﬁnal time step, C-UCRL(C, σ) attains a regret smaller than that of UCRL2-L by a factor
of approximately pSA/C = p50/6 ≈ 2.9, thus verifying Theorem 13. Similarly, we may
expect an improvement in regret by a factor of around pSA/C = p100/6 ≈ 4.1 in the
other environment. We however get a better improvement (by a factor of around 8), which
can be attributed to the increase in the regret of UCRL2-L due to a long burn-in phase (i.e.,
the phase before the algorithm starts learning).

We now turn our attention to the quality of approximate equivalence structure produced
by ApproxEquivalence (Algorithm 1), which is run as a sub-routine of C-UCRL. To this aim,
we introduce two performance measures to assess the quality of clustering: The ﬁrst one is
deﬁned as the total number of pairs that are mis-clustered, normalized by the total number
SA of pairs. We refer to this measure as the mis-clustering ratio. More precisely, let Ct
denote the empirical equivalence structure output by ApproxEquivalence at time t. For a
given c ∈ Ct, we consider the restriction of C to c, denoted by C|c. We ﬁnd ‘(c) ∈ C|c that
has the largest cardinality: ‘(c) ∈ argmaxx∈C|c |x|. Now, we deﬁne

mis-clustering ratio at time t :=

1
SA

X

c∈Ct

|c \ ‘(c)| .

Note that the mis-clustering ratio falls in [0, 1] as P
c∈Ct |c| = SA for all t. Our second
performance measure accounts for the error in the aggregated empirical transition probability

215

0.000.250.500.751.001.251.501.752.00Time steps×1050123456Regret×104C-UCRL(classes, profiles)C-UCRLUCRL2-L012345Time steps×1050.00.51.01.52.02.53.03.54.0Regret×105C-UCRL(classes, profiles)C-UCRLUCRL2-LAsadi et al.

(a) Mis-clustering bias

(b) Mis-clustering ratio

Figure 4: Assessment of quality of approximate equivalence structures for Ergodic RiverSwim

with 25 and 50 states

due to mis-clustered pairs. We refer to this measure as mis-clustering bias. Precisely speaking,
for a given pair e ∈ S × A, we denote by ce ∈ Ct the set containing e in Ct. Then, we deﬁne
the mis-clustering bias at time t as

mis-clustering bias at time t := X
c∈Ct

X

e /∈‘(c)

kbpt(·|ce) − bpt(·|ce \ {e})k1 .

In Figure 4, we plot the “mis-clustering ratio” and “mis-clustering bias” for the empirical
equivalence structures produced in the previous experiments. We observes on the ﬁgures
that the errors in terms of the aforementioned performance measures reduce. These errors
do now vanish quickly, thus indicating that the generated empirical equivalence structures do
not agree with the true one. Yet, they help reduce uncertainty in the transition probabilities,
and, in turn, reduce the regret; we refer to Remark 14 for a related discussion.

In the second set of experiments, we consider two communicating environments: 4-
room grid-world (with 49 states) and RiverSwim (with 25 states). These environments
are described in Appendix E. In Figure 5, we plot the regret against time steps under
C-UCRL(C, σ), C-UCRL, and UCRL2-L, and similarly to the previous case, we set δ = 0.05
and α = 4. The results are averaged over 100 independent runs, and the 95% conﬁdence
intervals are shown. In both environments, C-UCRL(C, σ) signiﬁcantly outperforms UCRL2-L.
However, C-UCRL attains a regret, which is slightly worse than that of UCRL2-L. This can
be attributed to the fact that ApproxEquivalence is unable to ﬁnd an accurate enough
equivalence structure in these non-ergodic environments.

6. Conclusion

We introduced a similarity measure of state-action pairs, which induces a notion of equivalence
In the
of proﬁle distributions in the state-action space of a Markov Decision Process.

216

02004006008001000Time (x1000)0.0100.0150.0200.0250.0300.0350.040Mis-clustering bias50 states25 states02004006008001000Time (x1000)0.100.150.200.250.300.350.40Mis-clustering ratio50 states25 statesModel-Based RL Exploiting Equivalences

(a) 25-state RiverSwim

(b) Grid-World

Figure 5: Regret of various algorithms in communicating environments

case of a known equivalence structure, we have presented conﬁdence sets incorporating
such knowledge that are provably tighter than their corresponding counterparts ignoring
equivalence structure. In the case of an unknown equivalence structure, we presented an
algorithm, based on conﬁdence bounds, that seeks to estimate an empirical equivalence
structure for the MDP. In order to illustrate the eﬃcacy of our developments, we further
preseted C-UCRL, which is a natural modiﬁcation of UCRL2 using the presented conﬁdence
sets. We show that when the equivalence structure is known to the learner, C-UCRL attains
a regret smaller than that of UCRL2 by a factor of pSA/C in communicating MDPs, where
C denotes the number of classes. In the case of an unknown equivalence structure, we
show through numerical experiments that in ergodic environments, C-UCRL outperforms
UCRL2 signiﬁcantly. The regret analysis in this case is much more complicated, and we
leave it for future work. We believe that the presented conﬁdence sets can be combined
with model-based algorithms for the discounted setup, which we expect to yield improved
performance in terms of sample complexity both in theory and practice.

Acknowledgements

This work has been supported by CPER Nord-Pas-de-Calais/FEDER DATA Advanced
data science and technologies 2015-2020, the French Ministry of Higher Education and
Research, Inria, and the French Agence Nationale de la Recherche (ANR), under grant
ANR-16-CE40-0002 (project BADASS).

References

Y. Abbasi-Yadkori, D. Pál, and Cs. Szepesvári. Improved algorithms for linear stochastic

bandits. In Proc. of NIPS, pages 2312–2320, 2011.

D. Abel, D. Hershkowitz, and M. L. Littman. Near optimal behavior via approximate state

abstraction. In Proc. of ICML, pages 2915–2923, 2016.

217

012345Time steps×1050.00.20.40.60.81.01.21.4Regret×105C-UCRL(classes, profiles)C-UCRLUCRL2-L012345Time steps×1050.00.51.01.52.02.53.03.5Regret×104C-UCRL(classes, profiles)C-UCRLUCRL2-LAsadi et al.

A. Anand, A. Grover, Mausam, and P. Singla. ASAP-UCT: Abstraction of state-action

pairs in UCT. In Proc. of IJCAI, pages 1509–1515, 2015.

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite time analysis of the multiarmed bandit

problem. Machine Learning, 47(2-3):235–256, 2002.

P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement

learning in weakly communicating MDPs. In Proc. of UAI, pages 35–42, 2009.

E. Brunskill and L. Li. Sample complexity of multi-task reinforcement learning. In Proc. of

UAI, page 122, 2013.

A. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for Markov decision processes.

Mathematics of Operations Research, 22(1):222–255, 1997.

C. Dann, T. Lattimore, and E. Brunskill. Unifying PAC and regret: Uniform PAC bounds

for episodic reinforcement learning. In Proc. of NIPS, pages 5711–5721, 2017.

T. Dean, R. Givan, and S. Leach. Model reduction techniques for computing approximately
optimal solutions for Markov decision processes. In Proc. of UCAI, pages 124–131, 1997.

C. Diuk, L. Li, and B. R. Leﬄer. The adaptive k-meteorologists problem and its application
to structure learning and feature selection in reinforcement learning. In Proc. of ICML,
pages 249–256, 2009.

N. Ferns, P. Panangaden, and D. Precup. Metrics for ﬁnite Markov decision processes. In

Proc. of UAI, pages 162–169, 2004.

N. Ferns, P. Panangaden, and D. Precup. Bisimulation metrics for continuous Markov

decision processes. SIAM Journal on Computing, 40(6):1662–1714, 2011.

R. Fruit, M. Pirotta, A. Lazaric, and R. Ortner. Eﬃcient bias-span-constrained exploration-

exploitation in reinforcement learning. In Proc. of ICML, pages 1573–1581, 2018.

M. Gheshlaghi Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement

learning. In Proc. of ICML, pages 263–272, 2017.

R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in Markov

decision processes. Artiﬁcial Intelligence, 147(1-2):163–223, 2003.

A. Hallak, D. Di-Castro, and S. Mannor. Model selection in Markovian processes. In Proc. of

ACM SIGKDD, pages 374–382, 2013.

W. Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of

the American Statistical Association, 58(301):13–30, 1963.

T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning.

JMLR, 11:1563–1600, 2010.

S. Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of

London London, England, 2003.

218

Model-Based RL Exploiting Equivalences

A. Khaleghi, D. Ryabko, J. Mary, and P. Preux. Consistent algorithms for clustering time

series. JMLR, 17(1):94–125, 2016.

T. L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in

Applied Mathematics, 6(1):4–22, 1985.

B. R. Leﬄer, M. L. Littman, and T. Edmunds. Eﬃcient reinforcement learning with

relocatable action models. In Proc. of AAAI, volume 7, pages 572–577, 2007.

L. Li, T. J. Walsh, and M. L. Littman. Towards a uniﬁed theory of state abstraction for

MDPs. In ISAIM, 2006.

O.-A. Maillard. Mathematics of statistical sequential decision making. Habilitation à Diriger

des Recherches, 2019.

O.-A. Maillard, T. A. Mann, and S. Mannor. How hard is my MDP? “the distribution-norm

to the rescue”. In Proc. of NIPS, pages 1835–1843, 2014.

T. Mandel, Y.-E. Liu, E. Brunskill, and Z. Popovic. Eﬃcient Bayesian clustering for

reinforcement learning. In Proc. of IJCAI, pages 1830–1838, 2016.

R. Ortner. Adaptive aggregation for reinforcement learning in average reward Markov

decision processes. Annals of Operations Research, 208(1):321–336, 2013.

R. Ortner, O.-A. Maillard, and D. Ryabko. Selecting near-optimal approximate state

representations in reinforcement learning. In Proc. of ALT, pages 140–154, 2014.

V. H. Peña, T. L. Lai, and Q.-M. Shao. Self-normalized processes: Limit theory and Statistical

Applications. Springer Science & Business Media, 2008.

M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons, 2014.

B. Ravindran and A. G. Barto. Approximate homomorphisms: A framework for non-exact

minimization in Markov decision processes. In Proc. of KBCS, 2004.

A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for Markov
decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press

Cambridge, 1998.

M. S. Talebi and O.-A. Maillard. Variance-aware regret bounds for undiscounted reinforce-

ment learning in MDPs. In Proc. of ALT, pages 770–805, 2018.

T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. Inequalities for
the L1 deviation of the empirical distribution. Hewlett-Packard Labs, Technical Report,
2003.

219

