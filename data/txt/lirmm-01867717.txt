Eﬀicient Scheduling of Scientific Workflows using Hot
Metadata in a Multisite Cloud
Ji Liu, Luis Pineda, Esther Pacitti, Alexandru Costan, Patrick Valduriez,

Gabriel Antoniu, Marta Mattoso

To cite this version:

Ji Liu, Luis Pineda, Esther Pacitti, Alexandru Costan, Patrick Valduriez, et al.. Eﬀicient Scheduling
of Scientific Workflows using Hot Metadata in a Multisite Cloud. IEEE Transactions on Knowledge
and Data Engineering, 2019, 31 (10), pp.1940-1953. ￿10.1109/TKDE.2018.2867857￿. ￿lirmm-01867717￿

HAL Id: lirmm-01867717

https://hal-lirmm.ccsd.cnrs.fr/lirmm-01867717

Submitted on 4 Sep 2018

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Eﬃcient Scheduling of Scientiﬁc Workﬂows using Hot Metadata
in a Multisite Cloud

Ji Liu∗, Luis Pineda†, Esther Pacitti‡,
Alexandru Costan§, Patrick Valduriez¶, Gabriel Antoniu(cid:107), and Marta Mattoso∗∗

Abstract

Large-scale, data-intensive scientiﬁc applications
are often expressed as scientiﬁc workﬂows (SWfs).
In this paper, we consider the problem of eﬃcient
scheduling of a large SWf in a multisite cloud,
i.e. a cloud with geo-distributed cloud data cen-
ters (sites). The reasons for using multiple cloud
sites to run a SWf are that data is already dis-
tributed, the necessary resources exceed the limits
at a single site, or the monetary cost is lower. In a
multisite cloud, metadata management has a criti-
cal impact on the eﬃciency of SWf scheduling as it
provides a global view of data location and enables
task tracking during execution. Thus, it should be
readily available to the system at any given time.
While it has been shown that eﬃcient metadata
handling plays a key role in performance, little re-
search has targeted this issue in multisite cloud.
In this paper, we propose to identify and exploit
hot metadata (frequently accessed metadata) for
eﬃcient SWf scheduling in a multisite cloud, us-
ing a distributed approach. We implemented our
approach within a scientiﬁc workﬂow management
system, which shows that our approach reduces the
execution time of highly parallel jobs up to 64% and
that of the whole SWfs up to 55%.
Keywords: Metadata management, hot meta-

∗jiliuwork@gmail.com, Inria, LIRMM and University of

Montpellier, France

†luis.pineda-morales@inria.fr, Inria, CNRS, IRISA/INSA

and University of Rennes, France

‡esther.pacitti@lirmm.fr, LIRMM, Inria and University

of Montpellier, France

§alexandru.costan@irisa.fr, IRISA/INSA Rennes, France
¶patrick.valduriez@inria.fr, Inria, LIRMM and Univer-

sity of Montpellier, France

(cid:107)gabriel.antoniu@inria.fr, Inria, CNRS, IRISA/INSA and

University of Rennes, France

∗∗marta@cos.ufrj.br, COPPE/UFRJ, Brazil

data, multisite cloud, scientiﬁc workﬂows, geo-
distributed applications

1

Introduction

Many scientiﬁc applications today process large
amounts of data, in the order of Petabytes. As the
size of the data increases, so do the requirements
for computing resources. Such data-intensive ap-
plications can be expressed as Scientiﬁc Workﬂows
(SWfs). A SWf is an assembly of scientiﬁc data
processing jobs, such as loading input data, data
processing, data analysis, and aggregating output
data [26]. The application is modeled as a graph, in
which vertices represent processing jobs, and edges
their data dependencies. Since the input data of
a job may be distributed or partitioned, a job can
be further decomposed in a number of executable
tasks. Such structuring of a SWf provides a clear
view of the application ﬂow and facilitates the exe-
cution of the application in a geo-distributed en-
vironment. Currently, many Scientiﬁc Workﬂow
Management Systems (SWfMSs) are available, e.g.
Pegasus [17] and Swift [43]. Some of them, e.g. Ch-
iron [27], support distributed, multisite execution.
The cloud stands out as a convenient infrastruc-
ture for handling SWfs, as it oﬀers the possibility to
lease resources at a very large scale and relatively
low cost. In this paper, we consider the execution
of a large SWf in a multisite cloud, i.e. a cloud with
geo-distributed cloud data centers (sites). How-
ever,
let us point out that using multiple cloud
sites is not necessarily better than a single cloud
site. However, there are important cases where us-
ing a multisite cloud is a good option. This option
is now well supported by all popular public clouds,
e.g. Microsoft Azure, Amazon EC2, and Google
Cloud, which provide the capability of using multi-

1

ple sites with a single cloud account, thus avoiding
the burden of using multiple accounts.

There are three main reasons for using multiple
cloud sites: already distributed data, resource lim-
its at a single cloud site and monetary cost. First,
the data to be processed by the SWf may already be
distributed at diﬀerent sites, because it is sourced
from diﬀerent experiments, sensing devices or labo-
ratories (e.g. the well-known ALICE LHC Collab-
oration spans over 37 countries [1]). In this case,
it may be hard to move the data to a single site,
e.g. because the data is too large or not allowed
to leave the hosting site. Second, the resource re-
quirements to execute the SWf may well exceed the
limits imposed by the cloud provider at a single
site. For instance, Microsoft Azure imposes a max-
imum number of virtual CPUs (20 by default) in
the VMs per site for both standard and premium
accounts.
In addition, there are other limits on
storage, network bandwidth and almost all the re-
sources provided per site. There are similar limita-
tions for other cloud providers, e.g. Amazon EC2
and Google Cloud. Third, it may be less expensive
to use VMs at multiple sites [28]. Cloud providers,
e.g. Amazon, Microsoft Azure and Google Cloud,
typically have diﬀerent VM prices at diﬀerent sites.
Furthermore, they charge for inter-site data trans-
fer. Thus, using appropriate scheduling algorithms,
e.g. DIM [27] and ActGreedy [28], we can use VMs
at diﬀerent sites and reduce data transfer so as to
reduce the overall monetary cost of SWf execution.
Metadata has a critical impact on the eﬃciency
of a SWfMS as it provides a global view of data lo-
cation and enables task tracking during execution.
Thus, it should be readily available to the system
at any given time. While it has been shown that
eﬃcient metadata handling plays a key role in per-
formance [41, 11], little research has targeted this
issue in multisite cloud. Some SWf metadata even
needs to be persisted as provenance data to allow
traceability and reproducibility of the SWf’s jobs.
In particular, some metadata is more frequently ac-
cessed than others ( e.g. the status of tasks in ex-
ecution in a multisite SWf is queried more often
than a job’s creation date). We denote such meta-
data by hot metadata and argue that it should be
dynamically identiﬁed and handled in a speciﬁc,
more quickly accessible way than the rest of the
metadata.

In a multisite infrastructure, inter-site network

latency is much higher than intra-site latency. This
consideration must stay at the core of the design
of a multisite metadata management system. As
we discuss in Section 4, several design principles
must be taken into account. In most data process-
ing systems (should they be distributed), metadata
is typically stored, managed and queried at some
centralized server (or set of servers) located at a
speciﬁc site [17, 20, 38]. However, in a multisite
setting, with high-latency inter-site networks and
large amounts of concurrent metadata operations,
a centralized strategy for hot metadata manage-
ment is far from the best solution.

reducing execution time.

Furthermore, in a multisite cloud, the execution
of the tasks of each job, which may involve dis-
tributed data, should be eﬃciently scheduled to a
corresponding site. The multisite scheduling pro-
cess should use scheduling algorithms to decide at
which site to execute the tasks to achieve a given
objective, e.g.
In the
scheduling process, the metadata should also be
provisioned to the scheduler to make smart deci-
sions. A centralized strategy for hot metadata man-
agement is easy to implement and works well with
a few concurrent queries. However, this strategy
will be ineﬃcient if the number of queries is high
or the bandwidth is low. Furthermore, the input
data of a SWf may be distributed at diﬀerent sites
so the tasks of one job may need to be executed at
diﬀerent sites. In this case, a centralized strategy
will incur additional communication.

In this paper, we take a diﬀerent strategy based
on the distribution of hot metadata in order to in-
crease the locality of access by the diﬀerent com-
puting nodes. Inspired by [35], we adapt two dis-
tributed metadata management strategies, i.e. Dis-
tributed Hash Table (DHT) and replication strat-
egy (Rep), for hot metadata management. DHT
stores the hot metadata at the site corresponding
to its hash value and Rep stores the hot metadata
at the sites where it is generated and the site cor-
responding to its hash value. We propose a local
storage based hot metadata management strategy,
i.e. LOC, which stores the hot metadata at the
site where it is generated. We take the centralized
strategy as a baseline to show the advantage of the
distributed strategies.

A major contribution of this paper is to demon-
strate the eﬀectiveness of using hot metadata for
SWf execution in a multisite cloud. In the context

2

of thousands of SWf tasks executed across multiple
sites, separating hot metadata improves SWf exe-
cution time, yet at no additional cost. In addition,
during SWf execution, some hot metadata may be-
come cold or vice-versa. We show that metadata
monitoring and dynamic identiﬁcation of hot or
cold metadata can address this dynamic variation
of the “temperature” of metadata.

This paper is a major extension of [36], with more
insights on hot metadata management strategies
for SWf execution in a multisite cloud.
In par-
ticular, we implemented and validated new tech-
niques for identifying hot metadata dynamically
and provisioning of hot metadata for scheduling al-
gorithms. The paper makes the following contribu-
tions:

• Based on the notion of hot metadata, we intro-
duce a distributed architecture in a multisite
cloud with dynamic hot metadata identiﬁca-
tion. We adapt two strategies for hot meta-
data management and propose a local storage
based strategy to optimize the access to hot
metadata and ensure availability (see Section
5).

• We develop a prototype by implementing our
proposed architecture and strategies within a
state-of-the-art multisite SWfMS, namely Ch-
iron [32], using a relational DBMS (RDBMS)
to manage hot metadata (Section 6).

• We combine the hot metadata management
strategies and three scheduling algorithms, i.e.
OLB, MCT and DIM, by enabling hot meta-
data management provisioning for multisite
task scheduling (see Section 6).

• We demonstrate that dynamic hot metadata
identiﬁcation and eﬃcient management of hot
metadata reduces the execution time of SWfs
by enabling timely data provisioning and
avoiding unnecessary cold metadata handling
(see Section 7). We also show the advan-
tages of decentralized hot metadata manage-
ment strategies with diﬀerent scheduling algo-
rithms (Section 7).

This paper is organized as follows. Section 2 dis-
cusses related work. Section 3 presents our SWf
model and discusses the challenges for hot meta-
data management. Section 4 introduces our design

principles. Section 5 describes the SWfMS architec-
ture with dynamic hot metadata identiﬁcation and
our hot metadata management strategies. Section
6 gives the implementation of our proposed strate-
gies. Section 7 presents our experimental results.
Finally, Section 8 concludes.

2 Comparison with Related

Work

Most SWfMSs take advantage of the optimizations
provided by the data management layer. For in-
stance, data management techniques try to strate-
gically place the data before or during the execution
of a SWf.
In multisite environments, these tech-
niques favor starting the SWf only after gathering
all the needed data in a shared-disk ﬁle system at
one data center, which is time consuming. Less
attention has been given to metadata, often con-
sidered a second class citizen in the SWf life cycle.
Furthermore, although scheduling has been studied
for SWf execution in a multisite cloud, little atten-
tion is paid to the combination of hot metadata
management strategies and diﬀerent scheduling al-
gorithms.

SWf execution in a multisite cloud. The
execution of SWfs in a multisite cloud is diﬀerent
from other execution environments. First, com-
pared with traditional business workﬂows, e.g. pur-
chase order processing, which are rather simple and
manipulate small amounts of data, SWfs are data-
intensive and can be very complex (in terms of
number and combinations of jobs), thus making
execution time a major issue [26]. Furthermore,
they need to ensure result reproducibility, which
requires registering and managing provenance data
regarding data sets, intermediate data and job ex-
ecutions. Note that the recent big data analytics
workﬂows share many similarities with SWfs. Sec-
ond, the multisite cloud environment is diﬀerent
from the geographically distributed environment,
which only needs to deal with multiple servers with-
out considering the execution within each server. A
multisite cloud environment is composed of multi-
ple cloud sites, each of which contains distributed
nodes (servers) connected by a fast local network,
e.g. inﬁniband. Thus, both intra-site and inter-site
execution must be considered, as well as the inter-

3

action between them. Furthermore, compared with
data processing systems for cluster environments,
e.g. Spark [45] or Hadoop [2], multisite SWf ex-
ecution must handle data in multiple clusters and
use existing programs at certain cloud sites, with
limited network connections among diﬀerent sites.
Centralized strategies. Metadata is usu-
ally handled by means of centralized registries im-
plemented on top of relational databases, which
only hold static information about data locations.
SWfMSs like Pegasus [17], Swift [43] or Chiron [32]
leverage such schemes, typically involving a single
In case of
server that processes all the requests.
increased client concurrency or high I/O pressure,
however, the single metadata server can quickly be-
come a performance bottleneck. Also, the work-
loads involving many small ﬁles, which translate
into heavy metadata accesses, are penalized by the
overheads from transactions and locking [40, 39].
A lightweight alternative to databases is to in-
dex the metadata, although most indexing tech-
niques [42, 44] are designed for data rather than
metadata. Even the dedicated index-based meta-
data schemes [24] use a centralized index and are
not adequate for large-scale SWfs, nor can they
scale to multisite deployments.

Distributed strategies. As a ﬁrst step towards
complete geographical distribution, some SWfMSs
rely on distributed ﬁle systems that partition the
metadata and store it at each node (e.g. [19], [29]),
in a shared-nothing architecture. Hashing is the
most common technique for uniform partitioning:
it consists of assigning metadata to nodes based
on a hash of a ﬁle identiﬁer. Giraﬀa [7] uses
full pathnames as key in the underlying HBase [3]
store. Lustre [9] hashes the tail of the ﬁlename
and the ID of the parent directory. Similar hashing
schemes are used by [14, 31, 13] with a low mem-
ory footprint, granting access to data in almost con-
stant time. FusionFS [48] implements a distributed
metadata management based on DHTs as well. Ch-
iron itself has a version with distributed control us-
ing an in-memory distributed DBMS. All these sys-
tems are well suited for single-cluster deployments
or SWfs that run on supercomputers. However,
they are unable to meet the practical requirements
of SWfs executed on clouds. Similarly to us, Calv-
inFS [41] uses hash-partitioned key-value metadata
across geo-distributed sites to handle small ﬁles, yet
it does not account for SWf semantics.

Hybrid strategies. Zhao et al. [47] proposed
using both a distributed hash table (FusionFS [48])
and a centralized database (SPADE [46]) to man-
age the metadata. Similarly to us, their metadata
model includes both ﬁle operations and provenance
information. However, they do not make the dis-
tinction between hot and cold metadata, and they
mainly target single site clusters.

Dynamic hot metadata identiﬁcation. The
“temperature” of metadata may vary during exe-
cution and trying to predict hot metadata before
execution can be error prone. Thus, the solution
is to dynamically identify hot metadata at run-
time. Some frameworks assess the data “temper-
ature” oﬄine, i.e.
they perform a later analysis
on a frequency-of-access log to avoid overhead dur-
ing the operation [25]. However, this approach is
only useful when there are subsequent runs. More
interestingly, online approaches maintain a rank
on the frequency of access to the data alongside
the execution, for example in adaptive replacement
cache [30]. Then, we can dynamically identify the
hot metadata based on the rank and frequency.
The dynamic identiﬁcation of hot and cold meta-
data is diﬀerent from temporal locality [22, 37],
which places a recently used object into memory or
in a same node while assuming that this object will
be accessed in the near future. The temporal local-
ity is generally used to handle intermediate data.
Our dynamic identiﬁcation is focused on metadata
and is based on the access frequency generated from
a relatively long observation of the metadata. In
addition, our diﬀerent hot metadata management
strategies handle the hot metadata at an inter-site
level instead of multiple nodes (intra-site level).

Metadata in the cloud. Most of the previ-
ous work on metadata management comes from the
HPC world, with solutions relying on low-latency
networks for message passing and tiered cluster
deployments that separate compute and storage
nodes. On the other hand, cloud computing is dif-
ferent from HPC: high latency networks connect
the sites, a much lower degree of (per-object) con-
currency, a more specialized storage interface pro-
vided to applications, which are explicitly aware
of the anticipated workloads and access patterns.
An important diﬀerence with past work is our fo-
cus on a whole SWf application and its interaction
with the cloud infrastructure. Because their target
use-cases and interface semantics diﬀer, parallel ﬁle

4

systems cannot be used out-of-the-box in the cloud
and are often considered to be mutually inappro-
priate.
Instead, we borrow ideas from the HPC
community, leveraging the SWf semantics and the
cloud services publicly available.

Multisite scheduling. There are many algo-
rithms to schedule tasks in a distributed system. In
this paper, we use three scheduling algorithms, i.e.
OLB (Opportunistic Load Balancing), MCT (Min-
imum Completion Time) and DIM (Data-Intensive
Multisite task scheduling) [27]. OLB and MCT are
basic algorithms, which are widely used in diverse
SWfMSs. DIM is the best in our context since it
is designed with a centralized metadata manage-
ment strategy, i.e. all the metadata is stored and
managed at a single site, for SWf distributed execu-
tion. Although we propose decentralized hot meta-
data management strategies, the cold metadata is
always stored in a centralized database, which ﬁts
well with the assumption of DIM. OLB randomly
selects an available or an earliest available site for
a task while MCT schedules a task to the site that
can ﬁnish the execution ﬁrst with the consideration
of the time to transfer intermediate data. DIM ﬁrst
schedules the tasks to the site where the input data
is located. Then, it manages the workload at each
site in order to achieve load balancing and reduce
the overall execution. In order to reduce the time
to transfer intermediate data, both MCT and DIM
are data location aware, i.e. they schedule many
tasks to where the input data is. Since it may
take much time to transfer intermediate data be-
tween sites, MCT generally has better performance
than OLB. In addition, since it may also take much
time to transfer metadata, DIM generally outper-
forms both MCT and OLB. In this paper, we also
evaluate the performance of diﬀerent hot metadata
management strategies in combination with diﬀer-
ent scheduling algorithms.

3 Hot Metadata Based Ap-

proach

Metadata management signiﬁcantly impacts the
performance of computing systems that must deal
with thousands or millions of individual ﬁles, as
with SWfs. In this section, we introduce our SWf
model, and discuss centralized metadata manage-

Figure 1: SWf diagram depicting the relation be-
tween jobs (J), tasks (T) and data chunks (D).

ment and multisite cloud scalability. Then, we de-
ﬁne hot metadata and the challenges for hot meta-
data management.

3.1 SWf Model

A SWf is modeled as a graph, in which vertices
represent data processing jobs and edges represent
dependencies between them (Figure 1). A job (J)
is a description of a piece of work that forms a log-
ical step within a SWf representation. Since SWf
jobs may process multiple data chunks, one job can
actually correspond to several executable tasks for
diﬀerent parts of input data during execution. A
task (T) is the representation of a job within a one-
time execution of this job, which processes a data
chunk (D) [26], i.e. there is a task for each unit of
data to be processed.

3.2 Centralized Metadata Manage-

ment

SWfMSs handle more than ﬁle-speciﬁc meta-
data. Running the SWf itself generates a signif-
icant amount of execution-speciﬁc metadata, e.g.
scheduling metadata (i.e. which task is executed
where) and data-to-task mappings. Most of to-
day’s SWfMSs handle metadata in a centralized
way. File-speciﬁc metadata is stored in a central-
ized server, either own-managed or through an un-
derlying ﬁle system, while execution-speciﬁc meta-
data is normally kept in the execution’s master en-
tity.

Controlling and combining all these sorts of
metadata translate into a critical workload as sci-
entiﬁc datasets get larger. For instance, the Cy-
berShake SWf [15] runs more than 800,000 tasks,
handling an equal number of individual data pieces,
processing and aggregating over 80,000 input ﬁles

5

(which translates into 200 TB of data read), and
requiring all of these ﬁles to be tracked and anno-
tated with metadata [23, 15]. Tasks’ runtime is in
the order of milliseconds, e.g., in a Montage SWf of
0.5 degree (see Section 7.1), out of 80 tasks, 36 tasks
execute under 500 milliseconds. With many tasks,
the load of parallel metadata operations becomes
very heavy, and handling it in a centralized fashion
represents a serious performance bottleneck.

3.3 Multisite Cloud Scalability

scientiﬁc data is so huge and
Often enough,
widespread that it cannot be processed or stored
at a single cloud site. On the one hand, the data
size or the computing requirements might exceed
the capacity of the site or the limits imposed by
the cloud provider. On the other hand, data might
be widely distributed, and due to their size, it is
more eﬃcient to process them closer to where they
reside than to bring them together. For instance,
the US Earthquake Hazard Program monitors more
than 7,000 sensors systems across the country re-
porting to the minute [10]. In either case, multisite
clouds are progressively being used for executing
large-scale SWfs.

Managing metadata in a centralized way for such
scenarios is not appropriate. On top of the con-
gestion generated by concurrent metadata opera-
tions, remote inter-site operations cause severe de-
lays in SWf execution. To address this issue, some
approaches propose the use of decentralized meta-
data servers [41]. In our previous work [35], we also
implemented a decentralized management architec-
ture that proved to handle metadata up to twice as
fast as a centralized solution.

In this paper we make one step further. Our fo-
cus is on the metadata access frequency, in particu-
lar on identifying fractions of metadata that do not
require multiple updates. The goal is to enable a
more eﬃcient decentralized metadata management,
reducing the number of inter-site metadata opera-
tions by favoring the operations on frequently ac-
cessed metadata, which we call hot metadata.

3.4 Hot Metadata

The term hot data refers to data that needs to be
frequently accessed [25]. Hot data is usually critical
for the application and must be placed in a fast and

easy-to-query storage [21]. We apply this concept
to the context of SWf management and deﬁne hot
metadata as the metadata that is frequently ac-
cessed during the execution of a SWf. Conversely,
less frequently accessed metadata will be denoted
cold metadata. We distinguish two types of hot
metadata: task metadata and ﬁle metadata.

Task metadata is the metadata for the execu-
tion of tasks, which is composed of the command,
parameters, start time, end time, status and exe-
cution site. Hot job metadata enables SWfMSs to
search and generate executable tasks. During exe-
cution, the status and the execution site of tasks are
queried many times by each site to search for new
tasks to execute and determine if a job is ﬁnished.
In addition, the status of a task may be updated
several times. As a result, it is important to get
this metadata quickly.

is relative to the size,

File metadata, which we consider as “hot” for
the SWf execution,
loca-
tion and possible replicas of a given piece of data.
Knowledge of ﬁle hot metadata allows the SWfMS
to place the data close to the corresponding task,
or vice-versa. This is especially relevant in multi-
site settings: timely availability of the ﬁle metadata
enables moving data before it is needed, hence re-
ducing the impact of low-speed inter-site networks.
In general, other metadata such as ﬁle ownership is
not critical for the execution and thus regarded as
cold metadata.

3.5 Challenges for Hot Metadata

Management

There are several implications in order to eﬀectively
apply the concept of hot metadata to real systems.
At this stage of our research, we apply simple, yet
eﬃcient solutions to these challenges.

How to decide which metadata is hot? We

have empirically chosen the aforementioned
task and ﬁle metadata as hot, since they have
statistically proven to be more frequently
accessed by the SWfMS we use. A sample
execution of a 1-degree Montage SWf (see Fig-
ure 2) as described in Section 7.1, running 820
jobs and 57K metadata operations reveals that
in a centralized execution, 33% of them are ﬁle
metadata operations (storeFile, getFile) and
32% are task metadata operations (loadTask,

6

not time-bounded, is the number of metadata
operations performed.
In our experimental
evaluation (see Section 7) we present results
in terms of both execution time and number
of tasks performing such operations.

The next section describes how the concept of
hot metadata translates into architectural design
choices for eﬃcient multisite SWf processing.

4 Design Principles

The foundation of our architecture is based on three
key design choices: two-layer multisite SWf man-
agement, adaptive placement for hot metadata, and
eventual consistency for high-latency communica-
tion.

Two-Layer Multisite SWf Management.
We propose to use a two-layer multisite system (see
details in Section 5): (1) The lower intra-site layer
operates as current single-site SWfMS: a site com-
posed of several computing nodes and a common
ﬁle system, one of such nodes acts as master and
coordinates communication and task execution. (2)
An additional higher inter-site layer coordinates
the interactions at the site-level through a mas-
ter/slave architecture (one site being the master
site). The master node in each site is in charge of
synchronization and data transfers.

Adaptive Placement for Hot Metadata.
Job dependencies in a SWf form common structures
(e.g. pipeline, data distribution and data aggrega-
tion) [12]. SWfMSs usually take into account these
dependencies to schedule the job execution in a con-
venient way to minimize data movements (e.g. job
co-location). Accordingly, diﬀerent SWfs will yield
diﬀerent scheduling patterns. In order to take ad-
vantage of these scheduling optimizations, we must
adapt the SWf’s metadata storage scheme. How-
ever, maintaining an updated version of all meta-
data across a multisite environment consumes a
signiﬁcant amount of communication time, incur-
ring also monetary costs. To reduce this impact,
we will evaluate diﬀerent storage strategies for hot
metadata during the SWf’s execution, while keep-
ing cold metadata stored locally and synchronizing
such cold metadata only during the execution of
the job.

Eventual Consistency for High-Latency
Communication. While cloud sites are normally

Figure 2: Relative frequency of metadata opera-
tions in Montage.

storeTask), whereas in a distributed run, up
to 67% are ﬁle operations, and task operations
represent 11%. The rest correspond mostly to
monitoring and node/site related operations.

However, a particular SWf might actually use
other metadata more often. Since SWfs are
typically deﬁned in structured formats (e.g.
XML ﬁles), another way to account for user-
deﬁned hot metadata would be to add a prop-
erty tag to each job deﬁnition where the user
could specify which metadata to consider as
hot. However, scientiﬁc applications evolve
during execution. At a given point, some data
might no longer be as relevant as it was ini-
tially; in other words, hot data becomes cold,
or vice-versa. In the case of hot to cold data,
SWfMSs might remove them from the fast-
access storage or even delete them. Conversely,
data that becomes relevant can be promoted
In addition, a user may not
to fast storage.
have enough information about the “temper-
ature” of the metadata and wrongly identify
cold metadata as hot metadata. Thus, dy-
namic hot metadata identiﬁcation is imple-
mented in our system when using user’s tags
(see details in Section 5).

How to assess that the choice of hot metadata is right?

Evaluating the eﬃciency of choosing hot meta-
data is not trivial. Metadata is much smaller
than the application’s data and handling it
over networks with ﬂuctuating throughput
may produce inconsistent results in terms
of execution time. Nevertheless, an indi-
cator of the improvement brought by an
adequate choice of hot metadata, which is

7

metadata with dynamic hot metadata identiﬁca-
tion across multiple sites. Third, we adapt two
metadata management strategies to hot metadata
management and propose a local storage based hot
metadata management strategy.

Two-level Multisite Architecture. Follow-
ing our design principles, the basis for our SWfMS
is a 2-level multisite architecture, as shown in Fig-
ure 3.

1. At the inter-site level, all communication and
synchronization is handled through a set of
master nodes (M), one per site. One site acts
as a global coordinator (master site) and is in
charge of scheduling jobs/tasks to each site.
Every master node holds a metadata store,
which is part of the global metadata storage
and is directly accessible to all other master
nodes.

2. At the intra-site level, our system preserves the
typical master/slave scheme widely-used today
on single-site SWfMS: the master node coor-
dinates a group of slave nodes which execute
the SWf tasks. All nodes within a site are con-
nected to a shared ﬁle system to access data
resources. Metadata updates are propagated
to other sites through the master node, which
classiﬁes hot and cold metadata as explained
below.

Dynamic Hot Metadata Identiﬁcation.
During execution, when there is no user’s tag, we
take the task and ﬁle metadata as hot metadata, as
explained in Section 3.4. However, when the user
puts the tags of hot metadata in the SWf deﬁni-
tion, the user’s tags are taken into consideration at
the beginning of execution. The access frequency
of all the metadata in each multi-task job, i.e. the
job that consists of more than a single task, is mon-
itored and ranked. The metadata that meets one
of two following requirements is identiﬁed as the
hot metadata. The ﬁrst requirement is that its
access frequency is bigger than 1 per task, which
means that the metadata becomes more frequently
accessed when there are more tasks. The second
requirement is that if there is no metadata meeting
the ﬁrst requirement, the metadata of the highest
frequency in the rank is identiﬁed as hot metadata.
If the hot metadata (identiﬁed by the user) meets

Figure 3: Multisite SWf execution architecture
with decentralized metadata. Dotted lines repre-
sent inter-site interactions.

interconnected by high-speed networks, the latency
is ultimately bounded by the physical distance be-
tween sites and communication time might reach
the order of seconds [4]. Under these circum-
stances, it is unreasonable to aim for a system with
a fully consistent state in all of its components at a
given moment without strongly compromising the
performance of the application. SWf semantics al-
lows us the ﬂexibility to opt for an eventually con-
sistent system: a task processes one or several spe-
ciﬁc pieces of data; such task will begin its exe-
cution only when all the pieces it needs are avail-
able in the metadata storage; however, the rest of
tasks continue executing independently. Thus, with
a reasonable delay due to the higher latency prop-
agation, the system is guaranteed to be eventually
consistent (see [34] for details on eventual consis-
tency).

5 Architecture

In our previous work [35], we explored diﬀerent
strategies for SWf-driven multisite metadata man-
agement, focusing on ﬁle metadata. Our study
found that a hybrid strategy combining decentral-
ized metadata and replication better suits the needs
It also
of large-scale multisite SWf execution.
showed that the right strategy to apply depends on
the SWf structure. In this section, we elaborate on
top of such observations. First, we present an ar-
chitecture for multisite cloud SWf processing which
features decentralized metadata management. Sec-
ond, we enrich this architecture with a component
speciﬁcally dedicated to the management of hot

8

at the same site.

Hashed without replication (DHT) Hot

metadata is queried and updated following
the principle of a distributed hash table
(DHT). The site location of a metadata entry
is determined by a simple hash function
applied to its key attribute, ﬁle-name in
case of ﬁle metadata, and task-id for task
metadata. We assume that the impact of
inter-site updates will be compensated by the
linear complexity of read operations.

Hashed with local replication (REP) We

combine the two previous strategies by keep-
ing both a local record of the hot metadata
and a hashed copy. Intuitively, this would re-
duce the number of inter-site reading requests.
We expect this hybrid strategy to highlight
the trade-oﬀs between metadata locality and
DHT linear operations.

6 DMM-Chiron SWfMS

In order to validate our architecture in a multisite
cloud, we developed a SWfMS prototype, called
Decentralized-Metadata Multisite Chiron (DMM-
Chiron), which provides support for decentralized
metadata management, with the dynamic identiﬁ-
cation of hot metadata.
In this section, we ﬁrst
present the baseline system, i.e. multisite Chi-
ron. Then, we describe DMM-Chiron, including its
metadata model, the implementation of dynamic
hot metadata identiﬁcation and hot metadata man-
agement strategies, and provisioning hot metadata
for multisite scheduling.

6.1 Baseline: Multisite Chiron

This work builds on Multisite Chiron [27], a
SWfMS speciﬁcally designed for multisite clouds.
Its layered architecture is presented in Figure 5; it
is composed of nine modules. Multisite Chiron ex-
ploits a textual UI to interact with users. The SWf
is analyzed by the Job Manager to identify exe-
cutable jobs, i.e. unexecuted jobs, for which the
input data is ready. The same module generates
the executable tasks at the beginning of job execu-
tion at the master site.

Figure 4: The hot metadata ﬁltering component.

one of the two requirements, it is kept as hot. Oth-
erwise, it is identiﬁed as cold data. The cold data
(identiﬁed by the user) that meets one of the two
requirements is identiﬁed as hot.

Separate Management of Hot and Cold
Metadata. Following our characterization of hot
metadata in Section 3.4, we incorporate an inter-
mediate component that ﬁlters out cold metadata
operations. This ensures that: hot metadata oper-
ations are managed with high priority over the net-
work, and cold metadata updates are propagated
only during periods of low network congestion.

The ﬁlter is located in the master node of each
site (see Figure 4). It separates hot and cold meta-
data,
favoring the propagation of hot metadata
and thus alleviates congestion during metadata-
intensive periods. The storage location of the hot
metadata is then selected based on some metadata
management strategies, as developed below.

Decentralized Hot Metadata Management
Strategies. We consider two diﬀerent alternative
strategies for decentralized metadata management
(previously explored in [35]).
In this paper, we
study their application to hot metadata. We also
propose a local storage based hot metadata man-
agement strategy, i.e. LOC. These three strategies
all include a metadata server in each of the sites
where execution nodes are deployed. They diﬀer in
the way hot metadata is stored and replicated, as
explained below.

Local without replication (LOC) Every new
hot metadata entry is stored at the site where
it has been created. For read operations,
metadata is queried at each site and the site
that stores the data will give the response.
If no reply is received within a time thresh-
old, the request is resent. This strategy will
typically beneﬁt pipeline-like SWf structures,
where consecutive tasks are usually co-located

9

jobs. Each element in the model, i.e. job, task, site,

Figure 5: Layered architecture of Multisite Chiron
[27].

Scheduling is done in two phases: the Multisite
Task Scheduler at the master site schedules each
task to a site, following one of three scheduling al-
gorithms, i.e. OLB, MCT and DIM. The users of
Multisite Chiron can choose the multisite schedul-
ing algorithm. While the Single Site Task Scheduler
applies the default dynamic FAF (First job First)
approach used by Chiron [32] to schedule tasks to
computing nodes. Some jobs (query jobs) can be
expressed as queries, which exploit the DBMS to
be executed.
In order to synchronize execution,
the query jobs are executed at a master site. It is
worth to clarify that optimizations to the schedul-
ing algorithms are out of the scope of this paper.

Then, the Task Executor at each computing node
runs the tasks. Along the execution, metadata is
handled by the Metadata Manager at the master
site. Since the metadata structure is well deﬁned,
we use a relational database, namely PostgreSQL,
to store it. All data (input, intermediate and out-
put) is stored in a Shared File System at each site.
The ﬁle transfer between two diﬀerent sites is per-
formed by the Multisite File Transfer module. The
Multisite Message Communication module of the
master node at each site is in charge of synchro-
nization through a master/slave architecture while
the Multisite File Transfer module exploits a peer-
to-peer approach for data transfers.

6.2 Metadata Model

Figure 6 (see details in [27]) shows the metadata
model for SWf execution in a multisite cloud. A
SWf is related to multiple jobs. Each job is related
to several relations, tasks and ﬁles and each task is
related to a site, a VM, a relation and multiple ﬁles.
A relation is a dataset to be processed by tasks or

10

Figure 6: Multisite Metadata Model [27].

ﬁle and VM, corresponds to a table in the relational
database. We implement this model at each site in
order to enable the execution at each site. The
data stored in the task, ﬁle and job tables (for the
experiment of dynamic hot metadata identiﬁcation
in Section 7.4) is diﬀerent at each site according to
diﬀerent hot metadata management strategies used
during SWf execution. The data stored in other
tables is synchronized by a mediator implemented
at each site at the beginning and the end of the
execution of the SWf and each job. A mediator is
a software module that exploits encoded knowledge
about certain subsets of data at each site to create
information at the site for SWf execution [34].

6.3 Dynamic Hot Metadata

Since the temperature of hot or cold metadata
varies, we look into the two situations: promoting
cold to hot metadata and downgrading hot to cold
metadata.

Promoting Cold to Hot Metadata. User-

deﬁned hot metadata as discussed so far
would not allow metadata to be dynamically
promoted, since a SWf deﬁnition ﬁle is rather
static. However, we build on this idea and
integrate an online ranking:
given a SWf
deﬁned through an XML ﬁle (or any other
deﬁnition language), a list of metadata at-
tributes is passed to the SWfMS in the same
ﬁle; then, the SWfMS monitors the access
frequency of each of such attributes and peri-
odically produces a ranking to verify whether
an attribute meets one of the requirements
explained in Section 5, and thus promotes it
to hot metadata. The maximum number of
attributes allowed as hot metadata could be
also dynamically calculated according to the
aggregated size of the metadata stores.

Downgrading Hot to Cold Metadata. Less

frequently accessed metadata could also be
identiﬁed using the same approach as above.
Upon identiﬁcation, degrading some metadata
to cold also incurs that metadata previously
considered hot is removed from fast-access
storage.

The process of dynamic identiﬁcation of hot
metadata occurs at the end of the execution of
each multi-task job in the master node of the mas-
ter site. The identiﬁcation results are synchronized
and broadcasted to all the sites for the next execu-
tion. To avoid interfering with the SWf scheduling
and execution processes, these scenarios are imple-
mented transparently within the metadata storage
system.

6.4 Combining Multisite and Hot

Metadata Management

To implement and evaluate our strategies to de-
centralized metadata management, we further ex-
tended Multisite Chiron by adding multisite meta-
data protocols. We mainly modiﬁed two modules
as described in the next sections: the Job Manager
and the Metadata Manager.

From Single- to Multisite Job Manager.
The Job Manager handles the process that veri-
ﬁes if the execution of a job is ﬁnished, in order to
launch the next jobs. Originally, this veriﬁcation

was done on the metadata stored at the master site.
In DMM-Chiron we implement an optimization to
each of the hot metadata management strategies
(Section 5):
for LOC, the local DMM-Chiron in-
stance veriﬁes only the tasks scheduled at that site
and the master site conﬁrms that the execution of
a job is ﬁnished when all the sites ﬁnish their cor-
responding tasks. For DHT and REP, the mas-
ter DMM-Chiron instance of the master site checks
each task of the job.

RDBMS. Metadata obeys to a data model [33],
which is equivalent to a data structure. Thus, it is
convenient to store the metadata in a structured
database. A data item represents a set of val-
ues corresponding to diﬀerent attributes as deﬁned
in the metadata model. An RDBMS is very eﬃ-
cient to query structured data by comparing the
value of diﬀerent attributes of each data item in a
structured database. We choose the popular Post-
greSQL RDBMS to manage hot metadata.

Protocols for Multisite Hot Metadata. The
following protocols illustrate our system’s metadata
operations. We recall that metadata operations are
triggered by the slave nodes at each site, which are
the actual executors of the SWf tasks.

Metadata Write As shown in Figure 7a, a new
metadata record is passed on from the slave to
the master node at each site (1). Upon recep-
tion, the master ﬁlters the record as either hot
or cold (2). The hot metadata is assigned by
the master node to the metadata storage pool
at the corresponding site(s) according to one
metadata strategy. Created cold metadata is
kept locally and propagated asynchronously to
the master site during the execution of the job.

Metadata Read Each master node has access to
the entire pool of metadata stores so that it
can get hot metadata from any site. Figure
7b shows the process. When a read opera-
tion is requested by a slave (1), a master node
sends a request to each metadata store (2) and
it processes the response that comes ﬁrst (3),
provided such response is not an empty set.
This mechanism ensures that the master node
gets the required metadata in the shortest
time. During execution, DMM-Chiron gath-
ers all the task metadata stored at each site
to verify whether the execution of a job is ﬁn-
ished.

11

(a) Write

(b) Read

Figure 7: Metadata Protocols.

6.5 Hot Metadata Provisioning for

Multisite Scheduling

In order to eﬃciently schedule the tasks at diﬀer-
ent sites, the scheduling process of MCT and DIM
needs to get the information about where the in-
put data of each task is located. This information
is available in the hot metadata, i.e. ﬁle metadata.
Thus, the hot metadata is provisioned to the mul-
tisite task scheduler. Since the job manager gen-
erates tasks for job execution at the master site,
the ﬁle metadata is also available at the master
site. Thus, we directly load the ﬁle metadata at
the master site for multisite scheduling.
In addi-
tion, we load the ﬁle metadata in the memory once
and then use the data for the whole scheduling pro-
cess in order to reduce scheduling as explained in
[27].

7 Experimental Evaluation

In this section, we evaluate and compare our dis-
tributed hot metadata management strategies with
the (state-of-the-art) centralized metadata man-
agement strategy, using our implementation of
DMM-Chiron in a multisite cloud (using Azure).
First, we present the experimental setup with two
real-life SWf use cases. Second, we compare the
performance of the diﬀerent hot metadata manage-
ment strategies, i.e. centralized, LOC, DHT and
REP), against diﬀerent SWf structures. Third, we
study the impact of two diﬀerent scheduling algo-
rithms, i.e. MCT and DIM, on the performance
of these strategies. Finally, we evaluate the perfor-
mance of dynamic hot metadata identiﬁcation.

12

Figure 8: Montage execution time for diﬀerent
strategies and degrees. Avg.
intermediate data
shown in parenthesis.

7.1 Experimental Setup

DMM-Chiron was deployed on the Microsoft Azure
cloudusing a total of 27 nodes of A4 standard
VMs (8 cores, 14 GB memory). The VMs were
evenly distributed among three sites: West Eu-
rope (WEU, Netherlands), North Europe (NEU,
Ireland) and Central US (CUS, Iowa). Control mes-
sages between master nodes are delivered through
the Azure Bus.

We consider two popular real-life SWfs, with

which we have a long experience:

a

is

by

created

Montage

the
toolkit
NASA/IPAC Infrared Science Archive and used to
generate custom mosaics of the sky from a set of
images [16]. Additional input for the SWf includes
the desired region of the sky, as well as the size of
the mosaic in terms of square degrees. We model
the Montage SWf using the proposal of Juve et
al. [23]. Montage contains 13 jobs, in which 4 jobs,
i.e. mProjectPP (mPro), Prepare (Prep), mDiﬀFit
(mDif) and mBackground (mBac), correspond to
multiple tasks.

BuzzFlow is a big data SWf that searches for
trends and measures correlations by analyzing the
data in scientiﬁc publications [18].
It analyzes
data collected from bibliography databases such as
DBLP or PubMed. Buzz is composed of 13 jobs
and 4 jobs, i.e. FileSplit, Buzz, BuzzHistory and
HistogramCreator, correspond to multiple tasks.

 0 10 20 30 40 500.5 (1.4GB)1 (4.9GB)2 (16.6GB)Execution time (min)Degree of the Montage SWfCentralizedLocalDHTReplicated7.2 Comparison of Hot Metadata

Management Strategies

Our hypothesis is that no single decentralized strat-
egy can well ﬁt all SWf structures. For instance, a
highly parallel task would exhibit diﬀerent meta-
data access patterns than a concurrent data gath-
ering task. Thus, the improvements brought to one
type of SWf by either of the strategies might turn
to be detrimental for another. To evaluate this hy-
pothesis, we ran several combinations of our strate-
gies with the featured SWfs and OLB while taking
ﬁle and task metadata as hot metadata. The re-
sults are shown in Figures 8 - 9 and 11 - 12.

Figure 8 shows the average execution time for
the Montage SWf generating 0.5-, 1-, and 2-degree
mosaics of the sky, using in all the cases a 5.5 GB
image database distributed across the three sites.
With a larger degree, a larger volume of intermedi-
ate data is handled and a mosaic of higher resolu-
tion is produced. Table 1 summarizes the volumes
of intermediate data generated per execution.

In the chart (Figure 8), we note in the ﬁrst place
a clear time gain of up to 28% by using LOC in-
stead of a centralized strategy, for all degrees. This
result was expected since the hot metadata is now
managed in parallel by three instances instead of
one, and only the cold metadata is forwarded to
the master site for scheduling purposes.

We observe that for mosaics of Degree 1 and
under, the use of distributed hashed storage also
outperforms the centralized version. However, we
note a performance degradation in DHT, starting
at 1-degree and getting more evident at 2-degree.
We attribute this to the fact that there is a larger
number of long-distance hot metadata operations
compared to the centralized strategy. With hashed
strategies, 1 out of 3 operations are carried out
on average between CUS and NEU. In the central-
ized strategy, NEU only performs operations in the
WEU site, thus such long latency operations are
reduced. We also associate this performance drop
with the size of intermediate data being handled by
the system: while we try to minimize inter-site data
transfers, with larger volumes of data such trans-
fers aﬀect the execution time up to a certain degree
and independently of the metadata management
strategy. We conclude that while the DHT strategy
might seem eﬃcient due to linear read and write op-
erations, it is not well suited for geo-distributed ex-

13

Figure 9: Buzz SWf execution time. Left Y-axis
scale corresponds to 60 MB execution, right Y-axis
to 1.2 GB.

ecutions, which favor locality and penalize remote
operations.

In a similar experiment, we validated our strate-
gies using the Buzz SWf, which is rather data in-
tensive, with two DBLP database dumps of 60 MB
and 1.2 GB. The results are shown in Figure 9.
Note that the left and right Y-axes diﬀer by one
order of magnitude. We observe again that DMM-
Chiron brings a general improvement in the execu-
tion time with respect to the centralized strategy:
10% for LOC in the 60 MB dataset and 6% for 1.2
GB, while for DHT and REP, the time improve-
ment was of less than 5%.

In order to better understand the performance
improvements brought by DMM-Chiron, and also
to identify the reason of the low runtime gain for
the Buzz SWf, we evaluated Montage and Buzz in a
per-job granularity. Although the time gains per-
ceived in the experiments might not seem signiﬁ-
cant at ﬁrst glance, two important aspects must be
taken into consideration:

Optimization at no cost Our proposed solu-
tions are implemented using exactly the same
number of resources as their counterpart cen-
tralized strategies: the decentralized metadata
stores are deployed within the master nodes
of each site and the control messages are sent
through the same existing channels. This
means that such gains come at no additional
cost for the user.

Actual monetary savings Our longest experi-
ment (Buzz 1.2 GB) runs in the order of hun-

 0 20 40 60 80 10060 MB1.2 GB 0 200 400 600 800 1000Completion time (min)DBLP input sizeCentralizedLocalDHTReplicated0.5-degree

CEN 1.4 (0.5, 0.5, 0.4)
LOC 1.3 (0.7, 0.2, 0.4)
DHT 1.5 (0.6, 0.6, 0.4)
REP 1.4 (0.5, 0.5, 0.4)

1-degree
4.9 (1.7, 1.5, 1.7)
4.8 (2.1, 1.0, 1.7)
4.9 (1.9, 1.3, 1.8)
4.9 (1.5, 1.9, 1.5)

2-degree
17.1 (6.0, 6.4, 4.7)
16.2 (8.4, 4.6, 3.2)
16.6 (5.4, 4.9, 6.2)
16.8 (6.6, 3.8, 6.4)

Table 1: Intermediate data in GB for diﬀerent degrees of Montage executions. Per-site breakdown is
expressed as: Aggregated (size WEU, size NEU, size CUS).

(a) Buzz

(b) Montage

Figure 10: SWf per-job breakdown. Very small jobs are enhanced for visibility.

Figure 11: Execution time of multi-task jobs on the
Buzz SWf with 60 MB input data.

Figure 12: Execution time of multi-task jobs on
the Montage SWf of 0.5 degree. Avg. intermediate
data shown in parenthesis.

dreds of minutes. With today’s scientiﬁc ex-
periments running at this scale and beyond, a
gain of 10% actually implies savings of hours
of cloud computing resources.

In DMM-Chiron, the various tasks of multi-task
jobs are evenly distributed to the available sites and
thus can be executed in parallel.
It is precisely
with these kind of jobs that DMM-Chiron yields
its best performance. Figure 10 shows a break-
down of Buzz and Montage SWfs with the propor-
tional size of each of their jobs from two diﬀerent
perspectives:
tasks count and average execution
time. Our goal is to characterize the most relevant

jobs in each SWf by number of tasks and conﬁrm
their relevance by looking at their relative execu-
tion time. In Buzz, we notice that both metrics are
highly dominated by three jobs: Buzz (676 tasks),
BuzzHistory (2134) and HistogramCreator (2134),
while the others are so small that they are barely
noticeable. FileSplit comes fourth in terms of ex-
ecution time and it is indeed the only remaining
multi-task job (3 tasks). Likewise, we identify for
Montage the only four multi-task jobs: mProject
(45 tasks), prepare (45), mDiﬀ (107) and mBack-
ground (45).

In Figures 11 and 12 we look into the execution

14

 0 5 10 15 20 25 30FileSplitBuzzBuzzHistoryHistogramCreatorExecution time (min)CentralizedLocalDHTReplicated 0 20 40 60 80 100mProprepmDifmBacExecution time (sec)CentralizedLocalDHTReplicatedtime of the multi-task jobs of Buzz and Montage,
respectively. In Figure 11, we observe that except
for one case, namely the Buzz job with REP, the de-
centralized strategies outperform considerably the
baseline (up to 20% for LOC, 16% for DHT and
14% for REP). In the case of FileSplit, we argue
that the execution time is too short and the num-
ber of tasks too small to reveal a clear improvement.
However, the other three jobs conﬁrm that DMM-
Chiron performs better for highly parallel jobs. It is
important to note that these gains are much larger
than those of the overall execution time (Figure 9)
since there are still a number of workloads executed
sequentially, which have not been optimized by the
current release of DMM-Chiron.

Figure 12 shows the execution of each multi-task
job for the Montage SWf of 0.5 degree. The ﬁgure
reveals that, on average, hot metadata distribution
substantially improves centralized management in
most cases (up to 40% for LOC, 53% for DHT
and 64% for REP). However, we notice some unex-
pected peaks and drops when using DHT. There is
a possibility that most tasks of some jobs are sched-
uled to the site corresponding to the hash code of
the metadata. In this case, the DHT strategy has
a good performance. Otherwise, its performance is
poor.
In addition, after a number of executions,
such cases can also be due to common network la-
tency variations of the cloud environment added to
the fact that the job execution time is rather short
(in the order of seconds).

7.3 Impact
rithms

of Scheduling Algo-

There is no single decentralized strategy that can ﬁt
all scheduling algorithms and some scheduling al-
gorithms may be optimized for a certain metadata
management strategy. Thus, the improvements
brought to one scheduling algorithm by either of
the strategies might turn to be not so good or even
detrimental for another. To better understand this
relationship between hot metadata management
strategy and scheduling algorithm, we ran several
combinations of our strategies with Montage and
two other scheduling algorithms, i.e. MCT and
DIM. We use the same conﬁguration except for the
scheduling algorithm as explained in Section 7.2.

Figure 13 shows that LOC is always better (up
to 28%) than the centralized strategy in terms of

15

Figure 13: The execution time of the Montage
SWf with MCT/DIM for diﬀerent strategies and
intermediate data shown in paren-
degrees. Avg.
thesis.

overall SWf execution time with MCT and DIM.
Since DIM is optimized for the centralized strategy,
it is reasonable that the gain brought by LOC with
DIM is less obvious compared with MCT. LOC only
stores the hot metadata at the site where it is pro-
duced. The hot metadata is read many times by the
same site when using MCT or DIM. Thus, LOC al-
ways reduces the time to read hot metadata and
always outperforms the centralized strategy. In ad-
dition, ﬁgure 14 shows that LOC is generally better
(up to 31% for MCT and 34% for DIM) than the
centralized strategy while DHT and REP may be
worse than the centralized strategy in terms of ex-
ecution time of multi-task jobs.

However, the centralized strategy may outper-
form DHT and REP when using MCT and DIM.
DMM-Chiron gathers the data to the master site
(WEU) for query jobs. The input data of the jobs,
which process the output of these query jobs, is cen-
tralized at the master site. In addition, MCT and
DIM are data location aware and schedule most of
the tasks where the input data is.
In this case,
the centralized strategy stores the hot metadata at
the master site, which is read many times by the
same site during execution, while DHT and REP
distribute the hot metadata to other sites. As a re-
sult, the centralized strategy outperforms DHT and
REP. However, in this case, similar to the central-
ized strategy, LOC also places the hot metadata
at the site where the metadata is produced, i.e.

 0 5 10 15 20 25 300.5 (MCT)1 (MCT)0.5 (DIM)1 (DIM)Execution time (min)Degree of the Montage SWfCentralizedLocalDHTReplicatedFigure 14: Execution time of multi-task jobs on the
Montage SWf of 0.5 degree with MCT/DIM. The
left four groups, i.e.
left mPro, prep, mDif and
mBac, represent the execution with MCT while the
right four groups represent the execution with DIM.

the master site. As a result, LOC can always out-
perform the centralized strategy in terms of overall
SWf execution time.

Figure 15 shows the diﬀerent performance of the
scheduling algorithms. The performance of MCT
is better than that of OLB and MCT when us-
ing the centralized or LOC strategies. The perfor-
mance of MCT is the worst when using DHT. We
attribute the reason to the wrong estimation of the
time to transfer metadata. When using REP, the
performance of DIM becomes slightly poor since
DIM is based on a centralized metadata manage-
ment strategy and it is diﬃcult to estimate the time
to transfer hot metadata with the distributed and
replicated hot metadata. The combination of LOC
and DIM reduces the overall SWf execution time
up to 38%, which is much better than the best re-
sult (28%) presented in [36], of the execution time
of Montage compared with the combination of the
centralized strategy and OLB. In addition, the ad-
vantage of the combination of LOC and DIM is up
to 55% (by simulation based on the real execution
of Buzz) in terms of the execution time of Buzz
with 60 MB input data (not shown in Figure 15).

7.4 Performance of Dynamic Hot

Metadata Identiﬁcation

We now evaluate the performance of dynamic hot
metadata identiﬁcation, by comparison with the
static approach. We assume that the job metadata,
e.g. job start time, job end time, is wrongly tagged

Figure 15: Execution time of the Montage SWf (0.5
degree) with diﬀerent scheduling algorithms. Avg.
intermediate data shown in parenthesis.

by users as hot metadata. But in fact, the job
metadata is cold while the ﬁle and task metadata is
hot during execution. Thus, by using dynamic hot
metadata identiﬁcation, the job metadata is turned
to cold metadata and the ﬁle and task metadata is
identiﬁed as hot metadata. However, the static ap-
proach always takes job metadata as hot. In the
experiment, we use LOC to manage the hot meta-
data.

Figure 16 shows that the advantage of dynamic
hot metadata identiﬁcation is up to 26% compared
to the static identiﬁcation in terms of the total SWf
execution. The advantage is up to 40% in terms of
In addition,
multi-task job, e.g. mBackground.
the combination of dynamic hot metadata identiﬁ-
cation and DIM is up to 37% better (in terms of
total SWf execution) and 55% (in terms of the ex-
ecution time of mBackground) compared with the
combination of static identiﬁcation and OLB.

8 Conclusion

Eﬃcient metadata handling is critical for the per-
formance of large-scale SWf execution in a multi-
site cloud. In this paper, we proposed to dynami-
cally identify and exploit the hot metadata for ef-
ﬁcient SWf scheduling in a multisite cloud, using
a distributed approach. Our solution includes a
distributed architecture with dynamic hot meta-
data identiﬁcation and three hot metadata man-
agement strategies:
two strategies adapted from
related work (DHT and REP) and a new strat-
egy (LOC) that stores the hot metadata at the site

16

 0 20 40 60 80 100mProprepmDifmBacmProprepmDifmBacExecution time (sec)CentralizedLocalDHTReplicated 0 5 10 15 20CentralizedLocalDHTReplicatedExecution time (min)OLBMCTDIMwhere it is generated.

We implemented our approach within a multisite
SWfMS, using an RDBMS to manage the meta-
data. We also adapted three scheduling algorithms,
i.e. OLB, MCT and DIM, to perform multisite
scheduling by provisioning hot metadata. Our ap-
proach provides eﬃcient access to hot metadata,
hiding inter-site network latencies and remaining
non-intrusive and easy to deploy.

We validated our approach by deploying the
three metadata management strategies and run-
ning real-life SWfs with diﬀerent scheduling algo-
rithms in a multisite cloud (using Azure). Our ex-
perimental results show an improvement of up to
55% for the whole SWf’s execution time and 64%
for speciﬁc highly-parallel jobs, compared to state-
of-the-art centralized and OLB solutions, at no ad-
ditional cost. In addition, the results show that dy-
namic hot metadata identiﬁcation is up to 40% bet-
ter (for highly-parallel jobs) compared with static
hot metadata identiﬁcation. Furthermore, our ex-
periments show that, although no single decentral-
ized strategy can well ﬁt all SWf structures, our
proposed hot metadata strategy, i.e. LOC, always
outperforms other strategies in terms of overall SWf
execution time.

For genericity, we validated our solution in a mul-
tisite cloud, which is the handiest way for scientists
to execute large scale experiments.
In this case,
our solution is useful to reduce the monetary cost
of using the multisite cloud while maximizing the
use of available resources. However, when there is
no budget for such cloud-based execution, our ap-
proach could still be used in large-scale experimen-
tal testbeds that federate multiple sites with free
access to researchers and scientists, e.g. Grid5000
[8], FutureGrid [6] and PlanetLab [5]. All these
platforms share the same vision: they are freely
accessible to scientists, support high quality, repro-
ducible experiments and allow a complete access to
the nodes’ hardware in exclusive mode - from one
node to the whole infrastructure (i.e. Hardware-as-
a Service). In these contexts, the initial assump-
tions of our solution (e.g. high latency inter-site,
low latency intra-site, etc.) still hold and our pro-
posed solution could well reduce the execution time
of SWfs by more than 50%. This translates into
better resource usage, making the underlying plat-
forms (more) energy sustainable.

We end this conclusion by discussing the oppor-

17

Figure 16: Execution time of the mBackground
job and the whole Montage SWf (0.5 degree) with
static/dynamic hot metadata identiﬁcation. SWf
represents the execution time of the whole SWf (left
Y-axis) and mBack corresponds to the execution of
mBackground Job (right Y-axis).

tunity of hot metadata management for multisite
SWfs. Note that we did not argue that handling
metadata as a whole (i.e. without distinction be-
tween hot and cold) is ineﬃcient. As discussed
in Section 2, many of today’s ﬁle systems do not
make any distinction when processing metadata,
and some have very good performance. However,
these systems are general purpose and deployed at
a single site. In this paper, we showed that in the
context of multisite SWfs, where thousands of tasks
are executed across sites, separating hot metadata
reduces execution time, yet at no additional cost.

Our focus in this paper was on handling meta-
data in a distributed way in order to improve
job/task execution time when processing a large
number of data pieces. While our techniques show
an improvement with respect to centralized man-
agement, we also notice that when the scale of the
SWf and the size of data become larger, there is
a degradation in the performance of DMM-Chiron
(see Figure 8) due to the increase of intermediate
data transfers. To mitigate this degradation, in-
memory caching techniques can be used in order to
reduce the time to read or write data in a multisite
cloud environment.

Finally, we have only considered the case of a ho-
mogeneous multisite environment, where each site
has the same amount of VMs of the same type.
While this conﬁguration is often utilized, in several
cases multisite clouds are heterogeneous (diﬀerent
number of resources, diﬀerent VMs sizes). A next

 7 8 9 10 11 12 13 14OLBMCTDIM 10 20 30 40 50 60Execution time (minute)Static SWfDynamic SWfStatic mBackDynamic mBackstep in this path would be to account for these vari-
ations in order to balance the hot metadata load
according to the site’s computing capacity.

Acknowledgment

This work has been partially funded by the
MSR - Inria Joint Centre (Z-CloudFlow project),
the ANR OverFlow project, and the EU H2020
programme (HPC4e project), MCTI/RNP-Brazil,
CNPq, FAPERJ, and Inria (MUSIC project).
It
has been performed (for the members of Inria and
LIRMM) in the context of the Computational Bi-
ology Institute (http://ibc-montpellier.fr). The ex-
periments were carried out using the Azure infras-
tructure provided by Microsoft in the context of the
Z-CloudFlow project.

References

[1] Alice Collaboration.

http://aliceinfo.

cern.ch/general/index.html.

[2] Apache Hadoop.

http://hadoop.apache.

org/.

[3] Apache HBase. http://hbase.apache.org.

[4] Azure Speed Test. http://www.azurespeed.

com/.

[5] Chameleon. https://www.chameleoncloud.

org.

[6] FutureGrid. http://futuregrid.org.

[7] Giraﬀa.

https://code.google.com/a/

apache-extras.org/p/giraffa/.

[8] Grid5000. https://www.grid5000.fr.

[9] Lustre - OpenSFS. http://lustre.org/.

[10] USGS ANSS - Advanced National Seis-
mic System. http://earthquake.usgs.gov/
monitoring/anss/.

[11] S. R. Alam, H. N. El-Harake, K. Howard,
N. Stringfellow, and F. Verzelloni. Parallel I/O
and the metadata wall. In Workshop on Paral-
lel Data Storage (PDSW), pages 13–18, 2011.

[12] S. Bharathi, A. Chervenak, E. Deelman,
G. Mehta, M. Su, and K. Vahi. Characteri-
zation of scientiﬁc workﬂows. In Workshop on
WFs in Support of Large-Scale Science, pages
1–10, 2008.

[13] S. A. Brandt, E. L. Miller, D. D. E. Long,
and L. Xue. Eﬃcient metadata management
in large distributed storage systems. In IEEE
NASA Goddard Conf. on Mass Storage Sys-
tems and Technologies, pages 290–298, 2003.

[14] P. F. Corbett and D. G. Feitelson. The vesta
parallel ﬁle system. ACM Trans. on Computer
Systems (TOCS), 14(3):225–264, 1996.

[15] E. Deelman, S. Callaghan, E. Field, H. Fran-
coeur, R. Graves, N. Gupta, V. Gupta,
T. H. Jordan, C. Kesselman, P. Maechling,
J. Mehringer, G. Mehta, D. Okaya, K. Vahi,
and L. Zhao. Managing large-scale workﬂow
execution from resource provisioning to prove-
nance tracking: The cybershake example. In
IEEE Int. Conf. on e-Science and Grid Com-
puting, pages 14–14, 2006.

[16] E. Deelman, G. Singh, M. Livny, B. Berriman,
and J. Good. The cost of doing science on the
cloud: The montage example.
In Int. Conf.
for High Performance Computing, Network-
ing, Storage and Analysis (SC), pages 1–12,
2008.

[17] E. Deelman, G. Singh, M.-H. Su, J. Blythe,
Y. Gil, C. Kesselman, G. Mehta, K. Vahi,
G. B. Berriman, J. Good, A. Laity, J. C. Ja-
cob, and D. S. Katz. Pegasus: A framework
for mapping complex scientiﬁc workﬂows onto
distributed systems. Scientiﬁc Programming,
13(3):219–237, 2005.

[18] J. Dias, E. Ogasawara, D. De Oliveira,
F. Porto, P. Valduriez, and M. Mattoso. Alge-
braic dataﬂows for big data analysis. In IEEE
Int. Conf. on Big Data, pages 150–155, 2013.

[19] A. Gehani, M. Kim, and T. Malik. Eﬃcient
querying of distributed provenance stores. In
ACM Int. Symp. on High Performance Dis-
tributed Computing (HPDC), pages 613–621,
2010.

18

[20] S. Ghemawat, H. Gobioﬀ, and S. Leung. The
Google File System. SIGOPS Operating Sys-
tems Review, 37(5):29–43, 2003.

[21] J. Hsieh, T. Kuo, and L. Chang. Eﬃcient iden-
tiﬁcation of hot data for ﬂash memory stor-
age systems. ACM Trans. on Storage (TOS),
2(1):22–40, 2006.

[30] N. Megiddo and D. S. Modha. ARC: A self-
tuning, low overhead replacement cache.
In
USENIX Conf. on File and Storage Technolo-
gies (FAST), 2003.

[31] Ethan L. Miller and Randy H. Katz. RAMA:
An easy-to-use, high-performance parallel ﬁle
system. Parallel Computing, 23(4):419–446.

[22] S. Jin and A. Bestavros. Greedydual* web
caching algorithm: exploiting the two sources
of temporal locality in web request streams.
Computer Communications, 24(2):174–183,
2001.

[32] E. Ogasawara, J. Dias, F. Porto, P. Valduriez,
and M. Mattoso. An algebraic approach
for data-centric scientiﬁc workﬂows.
Pro-
ceedings of the VLDB Endowment (PVLDB),
4(12):1328–1339, 2011.

[23] G.

Juve, A. Chervenak, E. Deelman,
S. Bharathi, G. Mehta,
and K. Vahi.
Characterizing and proﬁling scientiﬁc work-
ﬂows. Future Generation Computer Systems
(FGCS), 29(3):682 – 692, 2013.

[24] A. W. Leung, M. Shao, T. Bisson, S. Pasupa-
thy, and E. L. Miller. Spyglass: Fast, scalable
metadata search for large-scale storage sys-
tems. In USENIX Conf. on File and Storage
Technologies (FAST), pages 153–166, 2009.

[25] J. J Levandoski, P. Larson, and R. Stoica.
Identifying hot and cold data in main-memory
databases. In Int. Conf. on Data Engineering
(ICDE), pages 26–37, 2013.

[26] J. Liu, E. Pacitti, P. Valduriez, and M. Mat-
toso. A survey of data-intensive scientiﬁc
workﬂow management. Journal of Grid Com-
puting, 13(4):457–493, 2015.

[27] J. Liu, E. Pacitti, P. Valduriez, and M. Mat-
toso.
Scientiﬁc workﬂow scheduling with
provenance data in a multisite cloud. Trans-
actions on Large-Scale Data- and Knowledge-
Centered Systems (TLDKS), 33:80–112, 2016.

[28] J. Liu, E. Pacitti, P. Valduriez, D. De Oliveira,
and M. Mattoso. Multi-objective scheduling
of scientiﬁc workﬂows in multisite clouds. Fu-
ture Generation Computer Systems (FGCS),
63:76–95, 2016.

[29] T. Malik, L. Nistor, and A. Gehani. Tracking
and sketching distributed data provenance. In
Int. Conf. on e-Science, pages 190–197, 2010.

[33] E. S. Ogasawara, J. Dias, V. Silva, F. S. Chiri-
gati, D. de Oliveira, F. Porto, P. Valduriez,
and M. Mattoso. Chiron: a parallel engine
for algebraic scientiﬁc workﬂows. Concurrency
and Computation: Practice and Experience,
25(16):2327–2341, 2013.

[34] M. T. ¨Ozsu and P. Valduriez. Principles of
Distributed Database Systems, Third Edition.
Springer, 2011.

[35] L. Pineda-Morales, A. Costan, and G. Anto-
niu. Towards multi-site metadata management
for geographically distributed cloud workﬂows.
In IEEE Int. Conf. on Cluster Computing,
pages 294–303, 2015.

[36] L. Pineda-Morales,

J. Liu, A. Costan,
E. Pacitti, G. Antoniu, P. Valduriez, and
M. Mattoso. Managing hot metadata for sci-
entiﬁc workﬂows on multisite clouds. In IEEE
Int. Conf. on Big Data, pages 390–397, 2016.

[37] D. Saha, A. Samanta, and S. R. Sarangi. The-
oretical framework for eliminating redundancy
in workﬂows. In IEEE Int. Conf. on Services
Computing, pages 41–48, 2009.

[38] F. Schmuck and R. Haskin. GPFS: A shared-
disk ﬁle system for large computing clusters.
In USENIX Conf. on File and Storage Tech-
nologies (FAST), pages 231–244, 2002.

[39] M. Stonebraker and U. Cetintemel. ”one size
ﬁts all”: an idea whose time has come and
gone.
In Int. Conf. on Data Engineering
(ICDE), pages 2–11, 2005.

19

[40] M. Stonebraker, S. Madden, D. J. Abadi, and
N. Hachem. The end of an architectural era:
Time for a complete rewrite. In Int. Conf. on
Very Large Data Bases (VLDB), pages 1150–
1160.

[41] A. Thomson and D. J Abadi. CalvinFS: con-
sistent wan replication and scalable metadata
management for distributed ﬁle systems.
In
USENIX Conf. on File and Storage Technolo-
gies (FAST), pages 1–14, 2015.

[42] J. Wang, S. Wu, H. Gao, J. Li, and B. C.
Indexing multi-dimensional data in a
Ooi.
cloud system. In ACM SIGMOD Int. Conf. on
Management of Data (SIGMOD), pages 591–
602, 2010.

[43] J. M. Wozniak, T. G. Armstrong, M. Wilde,
D. S. Katz, E. L. Lusk, and I. T. Foster.
Swift/t: scalable data ﬂow programming for
many-task applications.
In ACM SIGPLAN
Symp. on Principles and Practice of Parallel
Programming, pages 309–310, 2013.

[44] S. Wu, D. Jiang, B. C. Ooi, and K. Wu. Eﬃ-
cient b-tree based indexing for cloud data pro-

cessing. Proceedings of the VLDB Endowment
(PVLDB), 3(1-2):1207–1218, 2010.

[45] M. Zaharia, M. Chowdhury, M. J. Franklin,
S. Shenker, and I. Stoica. Spark: Cluster com-
puting with working sets. In USENIX Work-
shop on Hot Topics in Cloud Computing (Hot-
Cloud), pages 10–10, 2010.

[46] M. J. Zaki. Spade: An eﬃcient algorithm for
mining frequent sequences. Machine Learning,
42(1):31–60.

[47] D. Zhao, C. Shou, T. Maliky, and I. Raicu.
Distributed data provenance for large-scale
data-intensive computing. In IEEE Int. Conf.
on Cluster Computing (CLUSTER), pages 1–
8, 2013.

[48] D. Zhao, Z. Zhang, X. Zhou, T. Li, K. Wang,
D. Kimpe, P. Carns, R. Ross, and I. Raicu.
Fusionfs: Toward supporting data-intensive
scientiﬁc applications on extreme-scale high-
performance computing systems. In IEEE Int.
Conf. on Big Data, pages 61–70, 2014.

20

