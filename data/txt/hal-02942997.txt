Differential Privacy at Risk: Bridging Randomness and
Privacy Budget
Ashish Dandekar, Debabrota Basu, Stéphane Bressan

To cite this version:

Ashish Dandekar, Debabrota Basu, Stéphane Bressan. Differential Privacy at Risk: Bridging Random-
ness and Privacy Budget. Proceedings on Privacy Enhancing Technologies, 2021, 2021 (1), pp.64-84.
￿10.2478/popets-2021-0005￿. ￿hal-02942997￿

HAL Id: hal-02942997

https://inria.hal.science/hal-02942997

Submitted on 18 Sep 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Proceedings on Privacy Enhancing Technologies ..; .. (..):1–21

Ashish Dandekar*, Debabrota Basu*, and Stéphane Bressan
Diﬀerential Privacy at Risk: Bridging
Randomness and Privacy Budget

Abstract: The calibration of noise for a privacy-
preserving mechanism depends on the sensitivity of the
query and the prescribed privacy level. A data steward
must make the non-trivial choice of a privacy level that
balances the requirements of users and the monetary
constraints of the business entity.
Firstly, we analyse roles of the sources of randomness,
namely the explicit randomness induced by the noise
distribution and the implicit randomness induced by
the data-generation distribution, that are involved in
the design of a privacy-preserving mechanism. The ﬁner
analysis enables us to provide stronger privacy guaran-
tees with quantiﬁable risks. Thus, we propose privacy
at risk that is a probabilistic calibration of privacy-
preserving mechanisms. We provide a composition the-
orem that leverages privacy at risk. We instantiate the
probabilistic calibration for the Laplace mechanism by
providing analytical results.
Secondly, we propose a cost model that bridges the gap
between the privacy level and the compensation budget
estimated by a GDPR compliant business entity. The
convexity of the proposed cost model leads to a unique
ﬁne-tuning of privacy level that minimises the compen-
sation budget. We show its eﬀectiveness by illustrat-
ing a realistic scenario that avoids overestimation of the
compensation budget by using privacy at risk for the
Laplace mechanism. We quantitatively show that com-
position using the cost optimal privacy at risk provides
stronger privacy guarantee than the classical advanced
composition. Although the illustration is speciﬁc to the
chosen cost model, it naturally extends to any convex
cost model. We also provide realistic illustrations of how
a data steward uses privacy at risk to balance the trade-
oﬀ between utility and privacy.

Keywords: Diﬀerential privacy, cost model, Laplace
mechanism

DOI Editor to enter DOI
Received ..; revised ..; accepted ...

1 Introduction

Dwork et al. [12] quantify the privacy level ε in ε-
diﬀerential privacy (or ε-DP) as an upper bound on the
worst-case privacy loss incurred by a privacy-preserving
mechanism. Generally, a privacy-preserving mechanism
perturbs the results by adding the calibrated amount of
random noise to them. The calibration of noise depends
on the sensitivity of the query and the speciﬁed pri-
vacy level. In a real-world setting, a data steward must
specify a privacy level that balances the requirements
of the users and monetary constraints of the business
entity. For example, Garﬁnkel et al. [14] report on is-
sues encountered when deploying diﬀerential privacy as
the privacy deﬁnition by the US census bureau. They
highlight the lack of analytical methods to choose the
privacy level. They also report empirical studies that
show the loss in utility due to the application of privacy-
preserving mechanisms.

We address the dilemma of a data steward in two
ways. Firstly, we propose a probabilistic quantiﬁcation
of privacy levels. Probabilistic quantiﬁcation of privacy
levels provides a data steward with a way to take quan-
tiﬁed risks under the desired utility of the data. We refer
to the probabilistic quantiﬁcation as privacy at risk. We
also derive a composition theorem that leverages privacy
at risk. Secondly, we propose a cost model that links the
privacy level to a monetary budget. This cost model
helps the data steward to choose the privacy level con-
strained on the estimated budget and vice versa. Con-
vexity of the proposed cost model ensures the existence
of a unique privacy at risk that would minimise the bud-
get. We show that the composition with an optimal pri-
vacy at risk provides stronger privacy guarantees than
the traditional advanced composition [12]. In the end,
we illustrate a realistic scenario that exempliﬁes how the

*Corresponding Author: Ashish Dandekar: DI ENS,
ENS, CNRS, PSL University & Inria, Paris, France, E-mail:
adandekar@ens.fr

*Corresponding Author: Debabrota Basu: Dept. of
Computer Sci. and Engg., Chalmers University of Technology,
Göteborg, Sweden, E-mail: basud@chalmers.se
Stéphane Bressan: National University of Singapore, Singa-
pore, E-mail: steph@nus.edu.sg

data steward can avoid overestimation of the budget by
using the proposed cost model by using privacy at risk.
The probabilistic quantiﬁcation of privacy levels de-
pends on two sources of randomness: the explicit ran-
domness induced by the noise distribution and the im-
plicit randomness induced by the data-generation distri-
bution. Often, these two sources are coupled with each
other. We require analytical forms of both sources of
randomness as well as an analytical representation of
the query to derive a privacy guarantee. Computing the
probabilistic quantiﬁcation of diﬀerent sources of ran-
domness is generally a challenging task. Although we
ﬁnd multiple probabilistic privacy deﬁnitions in the lit-
erature [16, 27] 1, we miss an analytical quantiﬁcation
bridging the randomness and privacy level of a privacy-
preserving mechanism. We propose a probabilistic quan-
tiﬁcation, namely privacy at risk, that further leads to
analytical relation between privacy and randomness. We
derive a composition theorem with privacy at risk for
mechanisms with the same as well as varying privacy
levels. It is an extension of the advanced composition
theorem [12] that deals with a sequential and adaptive
use of privacy-preserving mechanisms. We also prove
that privacy at risk satisﬁes convexity over privacy levels
and a weak relaxation of the post-processing property.
To the best of our knowledge, we are the ﬁrst to ana-
lytically derive the proposed probabilistic quantiﬁcation
for the widely used Laplace mechanism [10].

The privacy level proposed by the diﬀerential pri-
vacy framework is too abstract a quantity to be inte-
grated in a business setting. We propose a cost model
that maps the privacy level to a monetary budget. The
proposed model is a convex function of the privacy level,
which further leads to a convex cost model for privacy
at risk. Hence, it has a unique probabilistic privacy level
that minimises the cost. We illustrate this using a real-
istic scenario in a GDPR-compliant business entity that
needs an estimation of the compensation budget that it
needs to pay to stakeholders in the unfortunate event
of a personal data breach. The illustration, which uses
the proposed convex cost model, shows that the use of
probabilistic privacy levels avoids overestimation of the
compensation budget without sacriﬁcing utility. The il-
lustration naturally extends to any convex cost model.
In this work, we comparatively evaluate the privacy
guarantees using privacy at risk of the Laplace mecha-
nism. We quantitatively compare the composition under

1 A widely-used (ε, δ)-diﬀerential privacy is not a probabilistic
relaxation of diﬀerential privacy [29].

Diﬀerential Privacy at Risk

2

the optimal privacy at risk, which is estimated using the
cost model, with traditional composition mechanisms –
basic and advanced mechanisms [12]. We observe that
it gives stronger privacy guarantees than the ones ob-
tained by the advanced composition without sacriﬁcing
on the utility of the mechanism.

In conclusion, beneﬁts of the probabilistic quantiﬁ-
cation i.e., of the privacy at risk are twofold. It not
only quantiﬁes the privacy level for a given privacy-
preserving mechanism but also facilitates decision-
making in problems that focus on the privacy-utility
trade-oﬀ and the compensation budget minimisation.

2 Background

We consider a universe of datasets D. We explicitly men-
tion when we consider that the datasets are sampled
from a data-generation distribution G with support D.
Two datasets of equal cardinality x and y are said to be
neighbouring datasets if they diﬀer in one data point. A
pair of neighbouring datasets is denoted by x ∼ y. In
this work, we focus on a speciﬁc class of queries called
numeric queries. A numeric query f is a function that
maps a dataset into a real-valued vector, i.e. f : D → Rk.
For instance, a sum query returns the sum of the values
in a dataset.

In order to achieve a privacy guarantee, researchers
use a privacy-preserving mechanism, or mechanism in
short, which is a randomised algorithm that adds noise
to the query from a given family of distributions.
Thus, a privacy-preserving mechanism of a given fam-
ily, M(f, Θ), for the query f and the set of parame-
ters Θ of the given noise distribution, is a function i.e.
M(f, Θ) : D → R. In the case of numerical queries, R is
Rk. We denote a privacy-preserving mechanism as M,
when the query and the parameters are clear from the
context.

Deﬁnition 1 (Diﬀerential Privacy [12]). A privacy-
preserving mechanism M, equipped with a query f and
with parameters Θ, is (ε, δ)-diﬀerentially private if for
all Z ⊆ Range(M) and x, y ∈ D such that x ∼ y:

P(M(f, Θ)(x) ∈ Z) ≤ eε × P(M(f, Θ)(y) ∈ Z) + δ.

An (ε, 0)-diﬀerentially private mechanism is also simply
said to be ε-diﬀerentially private. Often, ε-diﬀerential
privacy is referred to as pure diﬀerential privacy whereas
(ε, δ)-diﬀerential privacy is referred as approximate dif-
ferential privacy.

A privacy-preserving mechanism provides perfect pri-
vacy if it yields indistinguishable outputs for all neigh-
bouring input datasets. The privacy level ε quantiﬁes
the privacy guarantee provided by ε-diﬀerential privacy.
For a given query, the smaller the value of the ε, the
qualitatively higher the privacy. A randomised algo-
rithm that is ε-diﬀerentially private is also ε0-diﬀerential
private for any ε0 > ε.

In order to satisfy ε-diﬀerential privacy, the param-
eters of a privacy-preserving mechanism requires a cal-
culated calibration. The amount of noise required to
achieve a speciﬁed privacy level depends on the query.
If the output of the query does not change drastically
for two neighbouring datasets, then a small amount of
noise is required to achieve a given privacy level. The
measure of such ﬂuctuations is called the sensitivity of
the query. The parameters of a privacy-preserving mech-
anism are calibrated using the sensitivity of the query
that quantiﬁes the smoothness of a numeric query.

Deﬁnition 2 (Sensitivity). The sensitivity of a query
f : D → Rk is deﬁned as

∆f (cid:44) max
x,y∈D
x∼y

kf (x) − f (y)k1.

The Laplace mechanism is a privacy-preserving mecha-
nism that adds scaled noise sampled from a calibrated
Laplace distribution to the numeric query.

Deﬁnition 3 ([35]). The Laplace distribution with
mean zero and scale b > 0 is a probability distribution
with probability density function

Lap(b) (cid:44) 1
2b

(cid:18)

exp

−

(cid:19)

,

|x|
b

where x ∈ R. We write Lap(b) to denote a random vari-
able X ∼ Lap(b)

Deﬁnition 4 (Laplace Mechanism [10]). Given any
function f : D → Rk and any x ∈ D, the Laplace
Mechanism is deﬁned as

∆f
L
ε

(x) (cid:44) M

(cid:18)

f,

(cid:19)

∆f
ε

(x) = f (x) + (L1, ..., Lk),

where Li is drawn from Lap
component of f (x).

(cid:17)

(cid:16) ∆f
ε

and added to the ith

Theorem 1 ([10]). The Laplace mechanism, L
ε0-diﬀerentially private.

∆f
ε0 , is

Diﬀerential Privacy at Risk

3

3 Privacy at Risk: A Probabilistic
Quantiﬁcation of Randomness

The parameters of a privacy-preserving mechanism are
calibrated using the privacy level and the sensitivity of
the query. A data steward needs to choose an appro-
priate privacy level for practical implementation. Lee
et al. [25] show that the choice of an actual privacy
level by a data steward in regard to her business re-
quirements is a non-trivial task. Recall that the privacy
level in the deﬁnition of diﬀerential privacy corresponds
to the worst case privacy loss. Business users are how-
ever used to taking and managing risks, if the risks can
be quantiﬁed. For instance, Jorion [21] deﬁnes Value at
Risk that is used by risk analysts to quantify the loss in
investments for a given portfolio and an acceptable con-
ﬁdence bound. Motivated by the formulation of Value
at Risk, we propose to use the use of probabilistic pri-
vacy level. It provides us with a ﬁner tuning of an ε0-
diﬀerentially private privacy-preserving mechanism for
a speciﬁed risk γ.

Deﬁnition 5 (Privacy at Risk). For a given data gen-
erating distribution G, a privacy-preserving mecha-
nism M, equipped with a query f and with parame-
ters Θ, satisﬁes ε-diﬀerential privacy with a privacy at
risk 0 ≤ γ ≤ 1 if, for all Z ⊆ Range(M) and x, y sam-
pled from G such that x ∼ y:

P

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ln

P(M(f, Θ)(x) ∈ Z)
P(M(f, Θ)(y) ∈ Z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

> ε

≤ γ,

(1)

where the outer probability is calculated with respect to
the probability space Range(M ◦ G) obtained by apply-
ing the privacy-preserving mechanism M on the data-
generation distribution G.

If a privacy-preserving mechanism is ε0-diﬀerentially
private for a given query f and parameters Θ, for
any privacy level ε ≥ ε0, the privacy at risk is 0. We
are interested in quantifying the risk γ with which an
ε0-diﬀerentially private privacy-preserving mechanism
also satisﬁes a stronger ε-diﬀerential privacy, i.e., with
ε < ε0.

Unifying Probabilistic and Random DP
Interestingly, Equation (1) uniﬁes the notions of proba-
bilistic diﬀerential privacy and random diﬀerential pri-
vacy by accounting for both sources of randomness in
a privacy-preserving mechanism. Machanavajjhala et

al. [27] deﬁne probabilistic diﬀerential privacy that in-
corporates the explicit randomness of the noise distribu-
tion of the privacy-preserving mechanism, whereas Hall
et al. [16] deﬁne random diﬀerential privacy that incor-
porates the implicit randomness of the data-generation
distribution. In probabilistic diﬀerential privacy, the
outer probability is computed over the sample space of
Range(M) and all datasets are equally probable.

Connection with Approximate DP
Despite a resemblance with probabilistic relaxations of
diﬀerential privacy [13, 16, 27] due to the added param-
eter δ, (ε, δ)-diﬀerential privacy (Deﬁnition 1) is a non-
probabilistic variant [29] of regular ε-diﬀerential privacy.
Indeed, unlike the auxiliary parameters in probabilis-
tic relaxations, such as γ in privacy at risk (ref. Def-
inition 5), the parameter δ of approximate diﬀerential
privacy is an absolute slack that is independent of the
sources of randomness. For a speciﬁed choice of ε and
δ, one can analytically compute a matching value of δ
for a new value of ε2. Therefore, as other probabilistic
relaxations, privacy at risk cannot be directly related
to approximate diﬀerential privacy. An alternative is to
ﬁnd out a privacy at risk level γ for a given privacy level
(ε, δ) while the original noise satisﬁes (ε0, δ).

Theorem 2. If a privacy preserving mechanism satis-
ﬁes (ε, γ) privacy at risk, it also satisﬁes (ε, γ) approxi-
mate diﬀerential privacy.

We obtain this reduction as the probability measure
induced by the privacy preserving mechanism and
data generating distribution on any output set Z ⊆
Range(M) is additive. 3 The proof of the theorem is
in Appendix A.

3.1 Composition theorem

The application of ε-diﬀerential privacy to many real-
world problem suﬀers from the degradation of privacy
guarantee, i.e., privacy level, over the composition. The
basic composition theorem [12] dictates that the pri-
vacy guarantee degrades linearly in the number of eval-
uations of the mechanism. The advanced composition
theorem [12] provides a ﬁner analysis of the privacy loss

2 For any 0 < ε0 ≤ ε, any (ε, δ)-diﬀerentially private mechanism
also satisﬁes (ε0, (eε − eε0
3 The converse is not true as explained before.

+ δ))-diﬀerential privacy.

Diﬀerential Privacy at Risk

4

over multiple evaluations with a square root dependence
on the number of evaluations. In this section, we provide
the composition theorem for privacy at risk.

Deﬁnition 6 (Privacy loss random variable). For
a
privacy-preserving mechanism M : D → R, any two
neighbouring datasets x, y ∈ D and an output r ∈ R, the
value of the privacy loss random variable C is deﬁned
as:

C(r) (cid:44) ln

P(M(x) = r)
P(M(y) = r)

.

Lemma 1. If a privacy-preserving mechanism M sat-
isﬁes ε0-diﬀerential privacy, then

P[|C| ≤ ε0] = 1.

Theorem 3. For all ε0, ε, γ, δ > 0, the class of ε0-
diﬀerentially private mechanisms, which satisfy (ε, γ)-
privacy at risk under a uniform data-generation distri-
bution, are (ε0, δ)-diﬀerential privacy under n-fold com-
position where

r

2n ln

ε0 = ε0

1
δ

+ nµ,

where µ = 1

2 [γε2 + (1 − γ)ε2
0].

Proof. Let, M1...n : D → R1 × R2 × ... × Rn denote the
n-fold composition of privacy-preserving mechanisms
i=1. Each ε0-diﬀerentially private Mi
{Mi : D → Ri}n
also satisﬁes (ε, γ)-privacy at risk for some ε ≤ ε0 and
appropriately computed γ. Consider any two neighbour-
ing datasets x, y ∈ D. Let,

B =

(

(cid:12)
(cid:12)
(cid:12)
(r1, ..., rn)
(cid:12)
(cid:12)

n
^

i=1

P(Mi(x) = ri)
P(Mi(y) = ri)

)

> eε

Using the technique in [12, Theorem 3.20], it suﬃces to
show that P(M1...n(x) ∈ B) ≤ δ.

Consider

ln

= ln

P(M1...n(x) = (r1, ..., rn))
P(M1...n(y) = (r1, ..., rn))
n
Y

P(Mi(x) = ri)
P(Mi(y) = ri)

i=1

=

n
X

i=1

ln

P(Mi(x) = ri)
P(Mi(y) = ri)

(cid:44)

n
X

i=1

Ci

(2)

where Ci in the last line denotes the privacy loss random
variable related to Mi.

Consider an ε-diﬀerentially private mechanism Mε
and ε0-diﬀerentially private mechanism Mε0
. Let Mε0
satisfy (ε, γ)-privacy at risk for ε ≤ ε0 and appropriately

computed γ. Each Mi can be simulated as the mech-
anism Mε with probability γ and the mechanism Mε0
otherwise. Therefore, the privacy loss random variable
for each mechanism Mi can be written as

Ci = γCi

ε + (1 − γ)Ci
ε0

where Ci
denotes the privacy loss random variable as-
ε
sociated with the mechanism Mε and Ci
denotes the
ε0
privacy loss random variable associated with the mech-
anism Mε0
. Using [5, Remark 3.4], we can bound the
mean of every privacy loss random variable as:

µ (cid:44) E[Ci] ≤

1
2

[γε2 + (1 − γ)ε2
0].

We have a collection of n independent privacy random
variables Ci’s such that P (cid:2)|Ci| ≤ ε0(cid:3) = 1. Using Hoeﬀd-
ing’s bound [18] on the sample mean for any β > 0,

"

P

1
n

X

i

Ci ≥ E[Ci] + β

#

(cid:18)

≤ exp

−

(cid:19)
.

nβ2
2ε2
0

Rearranging the inequality by renaming the upper
bound on the probability as δ, we get:

"

X

P

i

Ci ≥ nµ + ε0

r

2n ln

#

1
δ

≤ δ.

Theorem 3 is an analogue, in the privacy at risk setting,
of the advanced composition of diﬀerential privacy [12,
Theorem 3.20] under a constraint of independent evalu-
ations. Note that if one takes γ = 0, then we obtain the
exact same formula as in [12, Theorem 3.20]. It provides
a sanity check for the consistency of composition using
privacy at risk.

Corollary 1 (Heterogeneous Composition). For
all
εl, ε, γl, δ > 0 and l ∈ {1, . . . , n}, the composition of
{εl}n
l=1-diﬀerentially private mechanisms, which satisfy
(ε, γl)-privacy at risk under a uniform data-generation
distribution, also satisﬁes (ε0, δ)-diﬀerential privacy
where

ε0 =

v
u
u
t2

  n
X

!

ε2
l

l=1
l=1 γl) + Pn

ln

1
δ

+ µ,

where µ = 1

2 [ε2(Pn

l=1(1 − γl)ε2
l ].

Diﬀerential Privacy at Risk

5

A detailed discussion and analysis of proving such het-
erogeneous composition theorems is available in [22,
Section 3.3].

In fact, if we consider both sources of randomness,
the expected value of the loss function must be com-
puted by using the law of total expectation.

E[C] = Ex,y∼G[E[C]|x, y]
Therefore, the exact computation of privacy guaran-
tees after the composition requires access to the data-
generation distribution. We assume a uniform data-
generation distribution while proving Theorem 3. We
can obtain better and ﬁner privacy guarantees account-
ing for data-generation distribution, which we keep as a
future work.

3.2 Convexity and Post-Processing

We show that privacy at risk satisﬁes the convexity
property and does not satisfy the post-processing prop-
erty.

Lemma 2 (Convexity). For a given ε0-diﬀerentially
private privacy-preserving mechanism, privacy at risk
satisﬁes the convexity property.

Proof. Let M be a mechanism that
satisﬁes ε0-
diﬀerential privacy. By the deﬁnition of the privacy at
risk, it also satisﬁes (ε1, γ1)-privacy at risk as well as
(ε2, γ2)-privacy at risk for some ε1, ε2 ≤ ε0 and appro-
priately computed values of γ1 and γ2. Let M1 and
M2 denote the hypothetical mechanisms that satisfy
(ε1, γ1)-privacy at risk and (ε2, γ2)-privacy at risk re-
spectively. We can write privacy loss random variables
as follows:

C1 ≤ γ1ε1 + (1 − γ1)ε0
C2 ≤ γ2ε2 + (1 − γ2)ε0
where C1 and C2 denote privacy loss random variables
for M1 and M2.

Let us consider a privacy-preserving mechanism M
that uses M1 with a probability p and M2 with a prob-
ability (1−p) for some p ∈ [0, 1]. By using the techniques
in the proof of Theorem 3, the privacy loss random vari-
able C for M can be written as:

Proof. The proof follows from the same argument as
that of Theorem 3 of bounding the loss random variable
at step l using γlCl
and then applying the
concentration inequality.

ε + (1 − γl)Cl
εl

where

C = pC1 + (1 − p)C2
≤ γ0ε0 + (1 − γ0)ε0

ε0 =

pγ1ε1 + (1 − p)γ2ε2
pγ1 + (1 − p)γ2

γ0 = (1 − pγ1 − (1 − p)γ2)

Thus, M satisﬁes (ε0, γ0)-privacy at risk. This proves
that privacy at risk satisﬁes convexity [23, Axiom 2.1.2].

Meiser [29] proved that a relaxation of diﬀerential pri-
vacy that provides probabilistic bounds on the privacy
loss random variable does not satisfy post-processing
property of diﬀerential privacy. Privacy at risk is indeed
such a probabilistic relaxation.

Corollary 2 (Post-processing). Privacy at risk does
not satisfy the post-processing property for every pos-
sible mapping of the output.

Though privacy at risk is not preserved after post-
processing, it yields a weaker guarantee in terms of ap-
proximate diﬀerential privacy after post-processing. The
proof involves reduction of privacy at risk to approxi-
mate diﬀerential privacy and preservation of approxi-
mate diﬀerential privacy under post-processing.

Lemma 3 (Weak Post-processing). Let M : D → R ⊆
Rk be a mechanism that satisfy (ε, γ)-privacy at risk and
f : R → R0 be any arbitrary data independent map-
ping. Then, f ◦ M : D → R0 would also satisfy (ε, γ)-
approximate diﬀerential privacy.

Diﬀerential Privacy at Risk

6

at risk. Therefore, we keep privacy at risk for Gaussian
mechanism as the future work.

In this section, we instantiate privacy at risk for the
Laplace mechanism in three cases: two cases involving
two sources of randomness and a third case involving the
coupled eﬀect. These three diﬀerent cases correspond to
three diﬀerent interpretations of the conﬁdence level,
represented by the parameter γ, corresponding to three
interpretations of the support of the outer probability
in Deﬁnition 5. In order to highlight this nuance, we
denote the conﬁdence levels corresponding to the three
cases and their three sources of randomness as γ1, γ2,
and γ3, respectively.

4.1 The Case of Explicit Randomness

In this section, we study the eﬀect of the explicit ran-
domness induced by the noise sampled from Laplace
distribution. We provide a probabilistic quantiﬁcation
for ﬁne tuning for the Laplace mechanism. We ﬁne-tune
the privacy level for a speciﬁed risk under by assuming
that the sensitivity of the query is known a priori.

For a Laplace mechanism L

calibrated with sensi-
tivity ∆f and privacy level ε0, we present the analytical
formula relating privacy level ε and the risk γ1 in The-
orem 4. The proof is available in Appendix B.

∆f
ε0

Proof. Let us ﬁx a pair of neighbouring datasets x and
y, and also an event Z0 ⊆ R0. Let us deﬁne pre-image
of Z0 as Z (cid:44) {r ∈ R : f (r) ∈ Z}. Now, we get

Theorem 4. The risk γ1 ∈ [0, 1] with which a Laplace
∆f
ε0 , for a numeric query f : D → Rk sat-
Mechanism L
isﬁes a privacy level ε ≥ 0 is given by

P(f ◦ M(x) ∈ Z0) = P(M(x) ∈ Z)

eεP(M(y) ∈ Z) + γ

≤
(a)
= eεP(f ◦ M(y) ∈ Z0) + δ

(a) is a direct consequence of Theorem 2.

γ1 =

P(T ≤ ε)
P(T ≤ ε0)

,

(3)

where T is a random variable that follows a distribution
with the following density function.

PT (t) =

21−ktk− 1
√

2 Kk− 1
2πΓ(k)∆f

2

(t)ε0

4 Privacy at Risk for Laplace

where Kn− 1

2

is the Bessel function of second kind.

Mechanism

The Laplace and Gaussian mechanisms are widely used
privacy-preserving mechanisms in the literature. The
Laplace mechanism satisﬁes pure ε-diﬀerential privacy
whereas the Gaussian mechanism satisﬁes approximate
(ε, δ)-diﬀerential privacy. As previously discussed, it is
not straightforward to establish a connection between
the non-probabilistic parameter δ of approximate diﬀer-
ential privacy and the probabilistic bound γ of privacy

Figure 1a shows the plot of the privacy level against
risk for diﬀerent values of k and for a Laplace mecha-
nism L1.0
1.0. As the value of k increases, the amount of
noise added in the output of numeric query increases.
Therefore, for a speciﬁed privacy level, the privacy at
risk level increases with the value of k.

The analytical formula representing γ1 as a func-
tion of ε is bijective. We need to invert it to obtain the
privacy level ε for a privacy at risk γ1. However the an-
alytical closed form for such an inverse function is not

Diﬀerential Privacy at Risk

7

explicit. We use a numerical approach to compute pri-
vacy level for a given privacy at risk from the analytical
formula of Theorem 4.

Result for a Real-valued Query. For the case
k = 1, the analytical derivation is fairly straightfor-
ward. In this case, we obtain an invertible closed-form
of a privacy level for a speciﬁed risk. It is presented in
Equation 4.

For the Laplace mechanism L

calibrated with
sampled sensitivity ∆Sf
and privacy level ε, we evalu-
ate the empirical risk ˆγ2. We present the result in The-
orem 5. The proof is available in Appendix C.

∆Sf
ε

Theorem 5. Analytical bound on the empirical risk,

∆Sf
ˆγ2, for Laplace mechanism L
with privacy level ε
ε
and sampled sensitivity ∆Sf for a query f : D → Rk is

(cid:18)

ε = ln

1
1 − γ1(1 − e−ε0 )

(cid:19)

(4)

ˆγ2 ≥ γ2(1 − 2e−2ρ2n)

(5)

Remarks on ε0. For k = 1, Figure 1b shows the
plot of privacy at risk level ε versus privacy at risk γ1
for the Laplace mechanism L1.0
. As the value of ε0 in-
ε0
creases, the probability of Laplace mechanism generat-
ing higher value of noise reduces. Therefore, for a ﬁxed
privacy level, privacy at risk increases with the value of
ε0. The same observation is made for k > 1.

4.2 The Case of Implicit Randomness

In this section, we study the eﬀect of the implicit ran-
domness induced by the data-generation distribution to
provide a ﬁne tuning for the Laplace mechanism. We
ﬁne-tune the risk for a speciﬁed privacy level without
assuming that the sensitivity of the query.

If one takes into account randomness induced by
the data-generation distribution, all pairs of neighbour-
ing datasets are not equally probable. This leads to es-
timation of sensitivity of a query for a speciﬁed data-
generation distribution. If we have access to an ana-
lytical form of the data-generation distribution and to
the query, we could analytically derive the sensitivity
distribution for the query. In general, we have access
to the datasets, but not the data-generation distribu-
tion that generates them. We, therefore, statistically
estimate sensitivity by constructing an empirical dis-
tribution. We call the sensitivity value obtained for a
speciﬁed risk from the empirical cumulative distribu-
tion of sensitivity the sampled sensitivity (Deﬁnition 7).
However, the value of sampled sensitivity is simply an
estimate of the sensitivity for a speciﬁed risk. In or-
der to capture this additional uncertainty introduced
by the estimation from the empirical sensitivity distri-
bution rather than the true unknown distribution, we
compute a lower bound on the accuracy of this esti-
mation. This lower bound yields a probabilistic lower
bound on the speciﬁed risk. We refer to it as empirical
risk. For a speciﬁed absolute risk γ2, we denote by ˆγ2
corresponding empirical risk.

where n is the number of samples used for estimation of
the sampled sensitivity and ρ is the accuracy parameter.
γ2 denotes the speciﬁed absolute risk.

The error parameter ρ controls the closeness between
the empirical cumulative distribution of the sensitivity
to the true cumulative distribution of the sensitivity.
Lower the value of the error, closer is the empirical cu-
mulative distribution to the true cumulative distribu-
tion. Mathematically,

ρ ≥ sup
∆

|F n

S (∆) − FS(∆)|,

where F n
is the empirical cumulative distribution of
S
sensitivity after n samples and FS is the actual cumu-
lative distribution of sensitivity.

Figure 2 shows the plot of number of samples as a
function of the privacy at risk and the error parameter.
Naturally, we require higher number of samples in order
to have lower error rate. The number of samples reduces
as the privacy at risk increases. The lower risk demands
precision in the estimated sampled sensitivity, which in
turn requires larger number of samples.

If the analytical form of the data-generation distri-
bution is not known a priori, the empirical distribution
of sensitivity can be estimated in two ways. The ﬁrst
way is to ﬁt a known distribution on the available data
and later use it to build an empirical distribution of the
sensitivities. The second way is to sub-sample from a
large dataset in order to build an empirical distribution
of the sensitivities. In both of these ways, the empirical
distribution of sensitivities captures the inherent ran-
domness in the data-generation distribution. The ﬁrst
way suﬀers from the goodness of the ﬁt of the known
distribution to the available data. An ill-ﬁt distribution
does not reﬂect the true data-generation distribution
and hence introduces errors in the sensitivity estima-
tion. Since the second way involves subsampling, it is
immune to this problem. The quality of sensitivity es-
timates obtained by sub-sampling the datasets depend
on the availability of large population.

Diﬀerential Privacy at Risk

8

Fig. 2. Number of samples n for varying pri-
vacy at risk γ2 for diﬀerent error parameter
ρ.

(a)
Fig. 1. Privacy level ε for varying privacy at risk γ1 for Laplace mechanism L1.0
ε0
Figure 1a, we use ε0 = 1.0 and diﬀerent values of k. In Figure 1b, for k = 1 and
diﬀerent values of ε0.

(b)

. In

Let, G denotes the data-generation distribution, ei-
ther known apriori or constructed by subsampling the
available data. We adopt the procedure of [38] to sam-
ple two neighbouring datasets with p data points each.
We sample p − 1 data points from G that are common to
both of these datasets and later two more data points,
independently. From those two points, we allot one data
point to each of the two datasets.

Let, Sf = kf (x) − f (y)k1 denotes the sensitivity
random variable for a given query f , where x and y
are two neighbouring datasets sampled from G. Using
n pairs of neighbouring datasets sampled from G, we
construct the empirical cumulative distribution, Fn, for
the sensitivity random variable.

Deﬁnition 7. For a given query f and for a speciﬁed
risk γ2, sampled sensitivity, ∆Sf , is deﬁned as the value
of sensitivity random variable that is estimated using
its empirical cumulative distribution function, Fn, con-
structed using n pairs of neighbouring datasets sampled
from the data-generation distribution G.

∆Sf

(cid:44) F −1

n (γ2)

If we knew analytical form of the data generation dis-
tribution, we could analytically derive the cumulative
distribution function of the sensitivity, F , and ﬁnd the
sensitivity of the query as ∆f = F −1(1). Therefore, in
order to have the sampled sensitivity close to the sensi-
tivity of the query, we require the empirical cumulative
distributions to be close to the cumulative distribution
of the sensitivity. We use this insight to derive the ana-
lytical bound in the Theorem 5.

4.3 The Case of Explicit and Implicit

Randomness

In this section, we study the combined eﬀect of both
explicit randomness induced by the noise distribution
and implicit randomness in the data-generation distri-
bution respectively. We do not assume the knowledge of
the sensitivity of the query.

We estimate sensitivity using the empirical cumula-
tive distribution of sensitivity. We construct the empiri-
cal distribution over the sensitivities using the sampling
technique presented in the earlier case. Since we use
the sampled sensitivity (Deﬁnition 7) to calibrate the
Laplace mechanism, we estimate the empirical risk ˆγ3.
calibrated with sam-
and privacy level ε0, we present
pled sensitivity ∆Sf
the analytical bound on the empirical sensitivity ˆγ3 in
Theorem 6 with proof in the Appendix D.

For Laplace mechanism L

∆Sf
ε0

Theorem 6. Analytical bound on the empirical risk
ˆγ3 ∈ [0, 1] to achieve a privacy level ε > 0 for Laplace

with sampled sensitivity ∆Sf of a

mechanism L
query f : D → Rk is

∆Sf
ε0

ˆγ3 ≥ γ3(1 − 2e−2ρ2n)

(6)

where n is the number of samples used for estimating
the sensitivity, ρ is the accuracy parameter. γ3 denotes
the speciﬁed absolute risk deﬁned as:

γ3 =

P(T ≤ ε)
P(T ≤ ηε0)

· γ2

Here, η is of the order of the ratio of the true sensitivity
of the query to its sampled sensitivity.

The error parameter ρ controls the closeness between
the empirical cumulative distribution of the sensitivity
to the true cumulative distribution of the sensitivity.
Figure 3 shows the dependence of the error parameter

0.00.20.40.60.81.0Privacy at Risk (γ1)0.00.20.40.60.81.0Privacy level (ε)k=1k=2k=30.00.20.40.60.81.0Privacy at Risk (γ1)0.00.20.40.60.81.0Privacy level (ε)ε0=1.0ε0=1.5ε0=2.0ε0=2.50.00.20.40.60.81.0Privacy at Risk (γ2)102103104105106107108Sample size (n)ρ=0.0001ρ=0.0010ρ=0.0100ρ=0.1000Diﬀerential Privacy at Risk

9

(a)
Fig. 3. Dependence of error and number of samples on the privacy at risk for Laplace mechanism L
hand side, we ﬁx the number of samples to 10000. For the Figure 3b we ﬁx the error parameter to 0.01.

(b)

∆Sf
1.0 . For the ﬁgure on the left

on the number of samples. In Figure 3a, we observe that
for a ﬁxed number of samples and a privacy level, the
privacy at risk decreases with the value of error param-
eter. For a ﬁxed number of samples, smaller values of
the error parameter reduce the probability of similarity
between the empirical cumulative distribution of sensi-
tivity and the true cumulative distribution. Therefore,
we observe the reduction in the risk for a ﬁxed privacy
level. In Figure 3b, we observe that for a ﬁxed value of
error parameter and a ﬁxed level of privacy level, the
risk increases with the number of samples. For a ﬁxed
value of the error parameter, larger values of the sam-
ple size increase the probability of similarity between
the empirical cumulative distribution of sensitivity and
the true cumulative distribution. Therefore, we observe
the increase in the risk for a ﬁxed privacy level.

Eﬀect of the consideration of implicit and explicit
randomness is evident in the analytical expression for
γ3 in Equation 7. Proof is available in Appendix D. The
privacy at risk is composed of two factors whereas the
second term is a privacy at risk that accounts for inher-
ent randomness. The ﬁrst term takes into account the
implicit randomness of the Laplace distribution along
with a coupling coeﬃcient η. We deﬁne η as the ratio
of the true sensitivity of the query to its sampled sen-
sitivity. We provide an approximation to estimate η in
the absence of knowledge of the true sensitivity. It can
be found in Appendix D.

γ3 (cid:44)

P(T ≤ ε)
P(T ≤ ηε0)

· γ2

(7)

5 Minimising Compensation
Budget for Privacy at Risk

Many service providers collect users’ data to enhance
user experience. In order to avoid misuse of this data,
we require a legal framework that not only limits the
use of the collected data but also proposes reparative
measures in case of a data leak. General Data Protection
Regulation (GDPR)4 is such a legal framework.

Section 82 in GDPR states that any person who suf-
fers from material or non-material damage as a result of
a personal data breach has the right to demand compen-
sation from the data processor. Therefore, every GDPR
compliant business entity that either holds or processes
personal data needs to secure a certain budget in the
scenario of the personal data breach. In order to re-
duce the risk of such an unfortunate event, the business
entity may use privacy-preserving mechanisms that pro-
vide provable privacy guarantees while publishing their
results. In order to calculate the compensation budget
for a business entity, we devise a cost model that maps
the privacy guarantees provided by diﬀerential privacy
and privacy at risk to monetary costs. The discussions
demonstrate the usefulness of probabilistic quantiﬁca-
tion of diﬀerential privacy in a business setting.

4 https://gdpr-info.eu/

0.20.40.60.81.0Privacy at Risk (γ3)0.00.20.40.60.81.0Privacy level (ε)ρ=0.010ρ=0.012ρ=0.015ρ=0.0200.20.40.60.81.0Privacy at Risk (γ3)0.00.20.40.60.81.0Privacy level (ε)n=10000n=15000n=20000n=250005.1 Cost Model for Diﬀerential Privacy

Equation 9.

Diﬀerential Privacy at Risk

10

Let E be the compensation budget that a business en-
tity has to pay to every stakeholder in case of a per-
sonal data breach when the data is processed without
any provable privacy guarantees. Let Edp
ε be the com-
pensation budget that a business entity has to pay to
every stakeholder in case of a personal data breach when
the data is processed with privacy guarantees in terms
of ε-diﬀerential privacy.

Privacy level, ε, in ε-diﬀerential privacy is the quan-
tiﬁer of indistinguishability of the outputs of a privacy-
preserving mechanism when two neighbouring datasets
are provided as inputs. When the privacy level is zero,
the privacy-preserving mechanism outputs all results
with equal probability. The indistinguishability reduces
with increase in the privacy level. Thus, privacy level of
zero bears the lowest risk of personal data breach and
the risk increases with the privacy level. Edp
ε needs to
be commensurate to such a risk and, therefore, it needs
to satisfy the following constraints.
1. For all ε ∈ R≥0, Edp
2. Edp
ε
3. As ε → 0, Edp

is a monotonically increasing function of ε.

ε → Emin where Emin is the unavoid-
able cost that business entity might need to pay in
case of personal data breach even after the privacy
measures are employed.

ε ≤ E.

4. As ε → ∞, Edp

ε → E.

There are various functions that satisfy these con-
straints. In absence of any further constraints, we model
Edp

ε as deﬁned in Equation (8).

Edp
ε

(cid:44) Emin + Ee− c
ε .
Edp
ε has two parameters, namely c > 0 and Emin ≥ 0.
c controls the rate of change in the cost as the privacy
level changes and Emin is a privacy level independent
bias. For this study, we use a simpliﬁed model with c = 1
and Emin = 0.

(8)

5.2 Cost Model for Privacy at Risk

Let, Epar
ε0 (ε, γ) be the compensation that a business en-
tity has to pay to every stakeholder in case of a per-
sonal data breach when the data is processed with an
ε0-diﬀerentially private privacy-preserving mechanism
along with a probabilistic quantiﬁcation of privacy level.
Use of such a quantiﬁcation allows us to provide a
stronger privacy guarantee viz. ε < ε0 for a speciﬁed
privacy at risk at most γ. Thus, we calculate Epar
using
ε0

Epar

ε0 (ε, γ) (cid:44) γEdp

ε + (1 − γ)Edp
ε0

(9)

Note that the analysis in this section is speciﬁc to
the cost model in Equation 8. It naturally extends to
any choice of convex cost model.

5.2.1 Existence of Minimum Compensation Budget

We want to ﬁnd the privacy level, say εmin, that yields
the lowest compensation budget. We do that by min-
imising Equation 9 with respect to ε.

Lemma 4. For the choice of cost model in Equation 8,
Epar

ε0 (ε, γ) is a convex function of ε.

By Lemma 4, there exists a unique εmin that minimises
the compensation budget for a speciﬁed parametrisa-
tion, say ε0. Since the risk γ in Equation 9 is itself
a function of privacy level ε, analytical calculation of
εmin is not possible in the most general case. When the
output of the query is a real number, i. e. k = 1, we de-
rive the analytic form (Equation 4) to compute the risk
under the consideration of explicit randomness. In such
a case, εmin is calculated by diﬀerentiating Equation 9
with respect to ε and equating it to zero. It gives us
Equation 10 that we solve using any root ﬁnding tech-
nique such as Newton-Raphson method [37] to compute
εmin.

(cid:18)

− ln

1 −

(cid:19)

1 − eε
ε2

=

1
ε0

1
ε

(10)

5.2.2 Fine-tuning Privacy at Risk

For a ﬁxed budget, say B, re-arrangement of Equation 9
gives us an upper bound on the privacy level ε. We use
the cost model with c = 1 and Emin = 0 to derive the
upper bound. If we have a maximum permissible ex-
pected mean absolute error T , we use Equation 12 to
obtain a lower bound on the privacy at risk level. Equa-
tion 11 illustrates the upper and lower bounds that dic-
tate the permissible range of ε that a data publisher can
promise depending on the budget and the permissible
error constraints.

(cid:20)

(cid:18)

≤ ε ≤

ln

1
T

γE
B − (1 − γ)Edp
ε0

(cid:19)(cid:21)−1

(11)

Thus, the privacy level is constrained by the ef-
fectiveness requirement from below and by the mone-

Diﬀerential Privacy at Risk

11

sure of eﬀectiveness for the Laplace mechanism.

E (cid:2)|L1

ε(x) − f (x)|(cid:3) =

1
ε

(12)

Equation 12 makes use of the fact that the sensitivity of
the count query is one. Suppose that the health centre
requires the expected mean absolute error of at most
two in order to maintain the quality of the published
statistics. In this case, the privacy level has to be at
least 0.5.

In order to compute the budget, the health cen-
tre requires an estimate of E. Moriarty et al. [30] show
that the incremental cost of premiums for the health
insurance with morbid obesity ranges between $5467 to
$5530. With reference to this research, the health cen-
tre takes $5500 as an estimate of E. For the staﬀ size
of 100 and the privacy level 0.5, the health centre uses
Equation 8 in its simpliﬁed setting to compute the total
budget of $74434.40.

Is it possible to reduce this budget without degrad-
ing the eﬀectiveness of the Laplace mechanism? We
show that it is possible by ﬁne-tuning the Laplace mech-
anism. Under the consideration of the explicit random-
ness introduced by the Laplace noise distribution, we
show that ε0-diﬀerentially private Laplace mechanism
also satisﬁes ε-diﬀerential privacy with risk γ, which is
computed using the formula in Theorem 4. Fine-tuning
allows us to get a stronger privacy guarantee, ε < ε0
that requires a smaller budget. In Figure 4, we plot the
budget for various privacy levels. We observe that the
privacy level 0.274, which is same as εmin computed
by solving Equation 10, yields the lowest compensation
budget of $37805.86. Thus, by using privacy at risk, the
health centre is able to save $36628.532 without sacri-
ﬁcing the quality of the published results.

5.4 Cost Model and the Composition of

Laplace Mechanisms

Convexity of the proposed cost function enables us to es-
timate the optimal value of the privacy at risk level. We
use the optimal privacy value to provide tighter bounds
on the composition of Laplace mechanism. In Figure 5,
we compare the privacy guarantees obtained by using
basic composition theorem [12], advanced composition
theorem [12] and the composition theorem for privacy
at risk. We comparatively evaluate them for composi-
tion of Laplace mechanisms with privacy levels 0.1, 0.5
and 1.0. We compute the privacy level after composition
by setting δ to 10−5.

Fig. 4. Variation in the budget for Laplace mechanism L1
ε0
privacy at risk considering explicit randomness in the Laplace
mechanism for the illustration in Section 5.3.

under

tary budget from above. [19] calculate upper and lower
bound on the privacy level in the diﬀerential privacy.
They use a diﬀerent cost model owing to the scenario
of research study that compensates its participants for
their data and releases the results in a diﬀerentially
private manner. Their cost model is diﬀerent than our
GDPR inspired modelling.

5.3 Illustration

Suppose that the health centre in a university that com-
plies to GDPR publishes statistics of its staﬀ health
checkup, such as obesity statistics, twice in a year. In
January 2018, the health centre publishes that 34 out of
99 faculty members suﬀer from obesity. In July 2018, the
health centre publishes that 35 out of 100 faculty mem-
bers suﬀer from obesity. An intruder, perhaps an analyst
working for an insurance company, checks the staﬀ list-
ings in January 2018 and July 2018, which are publicly
available on website of the university. The intruder does
not ﬁnd any change other than the recruitment of John
Doe in April 2018. Thus, with high probability, the in-
truder deduces that John Doe suﬀers from obesity. In
order to avoid such a privacy breach, the health centre
decides to publish the results using the Laplace mecha-
nism. In this case, the Laplace mechanism operates on
the count query.

In order to control the amount of noise, the health
centre needs to appropriately set the privacy level. Sup-
pose that the health centre decides to use the expected
mean absolute error, deﬁned in Equation 12, as the mea-

0.00.10.20.30.40.50.60.7Privacy level (ε)400006000080000100000120000Bpar (in dollars)ε0=0.7ε0=0.6ε0=0.5Diﬀerential Privacy at Risk

12

(a) L1

0.1 satisﬁes (0.08, 0.80)-privacy at risk.

(b) L1

0.5 satisﬁes (0.27, 0.61)-privacy at risk.

(c) L1

1.0 satisﬁes (0.42, 0.54)-privacy at risk.

Fig. 5. Comparing the privacy guarantee obtained by basic composition and advanced composition [12] with the composition obtained
using optimal privacy at risk that minimises the cost of Laplace mechanism L1
ε0

. For the evaluation, we set δ = 10−5.

We observe that the use of optimal privacy at risk
provided signiﬁcantly stronger privacy guarantees as
compared to the conventional composition theorems.
Advanced composition theorem is known to provide
stronger privacy guarantees for mechanism with smaller
εs. As we observe in Figure 5c and Figure 5b, the compo-
sition provides strictly stronger privacy guarantees than
basic composition, in the cases where the advanced com-
position fails.

Comparison with the Moment Accountant

Papernot et al. [33, 34] empirically showed that the
privacy guarantees provided by the advanced compo-
sition theorem are quantitatively worse than the ones
achieved by the state-of-the-art moment accountant [1].
The moment accountant evaluates the privacy guaran-
tee by keeping track of various moments of privacy loss
random variables. The computation of the moments is
performed by using numerical methods on the speciﬁed
dataset. Therefore, despite the quantitative strength of
privacy guarantee provided by the moment accountant,
it is qualitatively weaker, in a sense that it is speciﬁc to
the dataset used for evaluation, in constrast to advanced
composition.

Papernot et al. [33] introduced the PATE frame-
work that uses the Laplace mechanism to provide pri-
vacy guarantees for a machine learning model trained
in an ensemble manner. We comparatively evaluate the
privacy guarantees provided by their moment accoun-
tant on MNIST dataset with the privacy guarantees ob-
tained using privacy at risk. We do so by using privacy
at risk while computing a data dependent bound [33,
Theorem 3]. Under the identical experimental setup,
we use a 0.1-diﬀerentially private Laplace mechanism,

which optimally satisﬁes (0.08, 0.8)-privacy at risk. We
list the calculated privacy guarantees in Table 1. The re-
ported privacy guarantee is the mean privacy guarantee
over 30 experiments.

6 Balancing Utility and Privacy

In this section, we empirically illustrate and discuss the
steps that a data steward needs to take and the issues
that she needs to consider in order to realise a required
privacy at risk level ε for a conﬁdence level γ when seek-
ing to disclose the result of a query.

We consider a query that returns the parameter
of a ridge regression [31] for an input dataset. It is a
basic and widely used statistical analysis tool. We use
the privacy-preserving mechanism presented by Ligett
et al. [26] for ridge regression. It is a Laplace mech-
anism that induces noise in the output parameters of
the ridge regression. The authors provide a theoretical
upper bound on the sensitivity of the ridge regression,
which we refer as sensitivity, in the experiments.

6.1 Dataset and Experimental Setup.

We conduct experiments on a subset of the 2000 US
census dataset provided by Minnesota Population Cen-
ter in its Integrated Public Use Microdata Series [39].
The census dataset consists of 1% sample of the original
census data. It spans over 1.23 million households with
records of 2.8 million people. The value of several at-
tributes is not necessarily available for every household.
We have therefore selected 212, 605 records, correspond-
ing to the household heads, and 6 attributes, namely,

050100150200250300Number of com ositions (n)051015202530Privacy level after com osition (ε′)Advanced com osition for ε0=0.10,δ=10(5Basic Com osition[10]Advanced Com osition[10]Com osition with Privacy at Risk050100150200250300Number of co positions (n)020406080100120140Pri(acy le(el after co position (ε′)Ad(anced co position for ε0=0.50,δ=10−5Basic Co position[10]Ad(anced Co position[10]Co position )ith Pri(acy at Risk050100150200250300Number of compositio s (n)0100200300400500600Privac) level after compositio  (ε′)Adva ced compositio  for ε0=1.00,δ=10−5Basic Compositio [10]Adva ced Compositio [10]Compositio  (ith Privac) at RiskDiﬀerential Privacy at Risk

13

δ

#Queries

10−5
10−5

100
1000

Privacy level for moment accountant(ε)

with diﬀerential privacy [33] with privacy at risk

2.04
8.03

1.81
5.95

Table 1. Comparative analysis of privacy levels computed using three composition theorems when applied to 0.1-diﬀerentially private
Laplace mechanism, which optimally satisﬁes (0.08, 0.8)-privacy at risk. The observations for the moment accountant on MNIST
datasets are taken from [33].

Age, Gender, Race, Marital Status, Education, Income,
whose values are available for the 212, 605 records.

In order to satisfy the constraint in the derivation of
the sensitivity of ridge regression [26], we, without loss
of generality, normalise the dataset in the following way.
We normalise Income attribute such that the values lie
in [0, 1]. We normalise other attributes such that l2 norm
of each data point is unity.

All experiments are run on Linux machine with
12-core 3.60GHz Intel® Core i7™processor with 64GB
memory. Python® 2.7.6 is used as the scripting lan-
guage.

6.2 Result Analysis

We train ridge regression model to predict Income using
other attributes as predictors. We split the dataset into
the training dataset (80%) and testing dataset (20%).
We compute the root mean squared error (RMSE) of
ridge regression, trained on the training data with regu-
larisation parameter set to 0.01, on the testing dataset.
We use it as the metric of utility loss. Smaller the value
of RMSE, smaller the loss in utility. For a given value
of privacy at risk level, we compute 50 runs of an ex-
periment of a diﬀerentially private ridge regression and
report the means over the 50 runs of the experiment.

Let us now provide illustrative experiments under
the three diﬀerent cases. In every scenario, the data
steward is given a privacy at risk level ε and the con-
ﬁdence level γ and wants to disclose the parameters of
a ridge regression model that she trains on the census
dataset. She needs to calibrate the Laplace mechanism
by estimating either its privacy level ε0 (Case 1) or sen-
sitivity (Case 2) or both (Case 3) to achieve the privacy
at risk required the ridge regression query.

The Case of Explicit Randomness (cf. Sec-
tion 4.1). In this scenario, the data steward knows the
sensitivity for the ridge regression. She needs to compute
the privacy level, ε0, to calibrate the Laplace mecha-
nism. She uses Equation 3 that links the desired privacy

at risk level ε, the conﬁdence level γ1 and the privacy
level of noise ε0. Speciﬁcally, for given ε and γ1, she
computes ε0 by solving the equation:

γ1P(T ≤ ε0) − P(T ≤ ε) = 0.

Since the equation does not give an analytical formula
for ε0, the data steward uses a root ﬁnding algorithm
such as Newton-Raphson method [37] to solve the above
equation. For instance, if she needs to achieve a privacy
at risk level ε = 0.4 with conﬁdence level γ1 = 0.6,
she can substitute these values in the above equation
and solve the equation to get the privacy level of noise
ε0 = 0.8.

Figure 6 shows the variation of privacy at risk level
ε and conﬁdence level γ1. It also depicts the variation of
utility loss for diﬀerent privacy at risk levels in Figure 6.
In accordance to the data steward’s problem, if she
needs to achieve a privacy at risk level ε = 0.4 with
conﬁdence level γ1 = 0.6, she obtains the privacy level
of noise to be ε0 = 0.8. Additionally, we observe that
the choice of privacy level 0.8 instead of 0.4 to calibrate
the Laplace mechanism gives lower utility loss for the
data steward. This is the beneﬁt drawn from the risk
taken under the control of privacy at risk.

Thus, she uses privacy level ε0 and the sensitivity

of the function to calibrate Laplace mechanism.

The Case of Implicit Randomness (cf. Sec-
tion 4.2). In this scenario, the data steward does not
know the sensitivity of ridge regression. She assesses
that she can aﬀord to sample at most n times from the
population dataset. She understands the eﬀect of the
uncertainty introduced by the statistical estimation of
the sensitivity. Therefore, she uses the conﬁdence level
for empirical privacy at risk ˆγ2.

Given the value of n, she chooses the value of the
accuracy parameter using Figure 2. For instance, if the
number of samples that she can draw is 104, she chooses
the value of the accuracy parameter ρ = 0.01. Next, she
uses Equation 13 to determine the value of probabilistic
tolerance, α, for the sample size n. For instance, if the
data steward is not allowed to access more than 15, 000

Diﬀerential Privacy at Risk

14

Fig. 6. Utility, measured by RMSE (right y-axis), and privacy at
risk level ε for Laplace mechanism (left y-axis) for varying conﬁ-
dence levels γ1.

Fig. 7. Empirical cumulative distribution of the sensitivities of ridge
regression queries constructed using 15000 samples of neighboring
datasets.

samples, for the accuracy of 0.01 the probabilistic toler-
ance is 0.9.

α = 1 − 2e(−2ρ2n)

(13)

She constructs an empirical cumulative distribution over
the sensitivities as described in Section 4.2. Such an
empirical cumulative distribution is shown in Figure 7.
Using the computed probabilistic tolerance and desired
conﬁdence level ˆγ2, she uses equation in Theorem 5 to
determine γ2. She computes the sampled sensitivity us-
ing the empirical distribution function and the conﬁ-
dence level for privacy ∆Sf
at risk γ2. For instance,
using the empirical cumulative distribution in Figure 7
she calculates the value of the sampled sensitivity to
be approximately 0.001 for γ2 = 0.4 and approximately
0.01 for γ2 = 0.85

Thus, she uses privacy level ε, sets the number of
samples to be n and computes the sampled sensitivity
∆Sf

to calibrate the Laplace mechanism.

The Case of Explicit and Implicit Random-
ness (cf. Section 4.3). In this scenario, the data stew-
ard does not know the sensitivity of ridge regression. She
is not allowed to sample more than n times from a pop-
ulation dataset. For a given conﬁdence level γ2 and the
privacy at risk ε, she calibrates the Laplace mechanism
using illustration for Section 4.3. The privacy level in
this calibration yields utility loss that is more than her
requirement. Therefore, she wants to re-calibrate the
Laplace mechanism in order to reduce utility loss.

For the re-calibration, the data steward uses pri-
vacy level of the pre-calibrated Laplace mechanism, i.e.
ε, as the privacy at risk level and she provides a new
conﬁdence level for empirical privacy at risk ˆγ3. Using

Equation 25 and Equation 23, she calculates:

ˆγ3P(T ≤ ηε0) − αγ2 P(T ≤ ε) = 0

She solves such an equation for ε0 using the root ﬁnd-
ing technique such as Newton-Raphson method [37]. For
instance, if she needs to achieve a privacy at risk level
ε = 0.4 with conﬁdence levels ˆγ3 = 0.9 and γ2 = 0.9, she
can substitute these values and the values of tolerance
parameter and sampled sensitivity, as used in the pre-
vious experiments, in the above equation. Then, solving
the equation leads to the privacy level of noise ε0 = 0.8.
Thus, she re-calibrates the Laplace mechanism with
privacy level ε0, sets the number of samples to be n and
sampled sensitivity ∆Sf

.

7 Related Work

Calibration of mechanisms. Researchers have pro-
posed diﬀerent privacy-preserving mechanisms to make
diﬀerent queries diﬀerentially private. These mecha-
nisms can be broadly classiﬁed into two categories. In
one category, the mechanisms explicitly add calibrated
noise, such as Laplace noise in the work of [11] or Gaus-
sian noise in the work of [12], to the outputs of the query.
In the other category, [2, 6, 17, 41] propose mechanisms
that alter the query function so that the modiﬁed func-
tion satisﬁes diﬀerentially privacy. Privacy-preserving
mechanisms in both of these categories perturb the orig-
inal output of the query and make it diﬃcult for a ma-
licious data analyst to recover the original output of
the query. These mechanisms induce randomness us-

Diﬀerential Privacy at Risk

15

Around the same time of our work, Triastcyn et
al. [40] independently propose Bayesian diﬀerential pri-
vacy that takes into account both of the sources of ran-
domness. Despite this similarity, our works diﬀer in mul-
tiple dimensions. Firstly, they have shown the reduction
of their deﬁnition to a variant of Renyi diﬀerential pri-
vacy. The variant depends on the data-generation distri-
bution. Secondly, they rely on the moment accountant
for the composition of the mechanisms. Lastly, they do
not provide a ﬁner case-by-case analysis of the source of
randomness, which leads to analytical solutions for the
privacy guarantee.

Kifer et al. [24] deﬁne Puﬀerﬁsh privacy framework,
and its variant by Bassily et al. [4], that considers ran-
domness due to data-generation distribution as well as
noise distribution. Despite the generality of their ap-
proach, the framework relies on the domain expert to
deﬁne a set of secrets that they want to protect.

We refer interested readers to [8] for an extensive

review of the diﬀerential privacy and its reﬁnements.

Composition theorem. Recently proposed tech-
nique of the moment accountant [1] has become the
state-of-the-art of composing mechanisms in the area
of privacy-preserving machine learning. Abadi et al.
show that the moment accountant provides much strong
privacy guarantees than the conventional composition
mechanisms. It works by keeping track of various mo-
ments of privacy loss random variable and use the
bounds on them to provide privacy guarantees. The mo-
ment accountant requires access to data-generation dis-
tribution to compute the bounds on the moment. Hence,
the privacy guarantees are speciﬁc to the dataset.

Cost models. [7, 15] propose game theoretic meth-
ods that provide the means to evaluate the monetary
cost of diﬀerential privacy. Pejó et al. [36] also propose
a game theoretic cost model in the setting of private
collaborative learning. Our approach is inspired by the
approach in the work of Hsu et al. [19]. They model
the cost under a scenario of a research study wherein
the participants are reimbursed for their participation.
Our cost modelling is driven by the scenario of secur-
ing a compensation budget in compliance with GDPR.
Our requirement diﬀers from the requirements of [19],
particularly in our model participants do not have any
monetary incentive to share their data.

ing the explicit noise distribution. Calibration of these
mechanisms require the knowledge of the sensitivity of
the query. Nissim et al. [32] consider the implicit ran-
domness in the data-generation distribution to compute
an estimate of the sensitivity. The authors propose the
smooth sensitivity function that is an envelope over the
local sensitivities for all individual datasets. Local sen-
sitivity of a dataset is the maximum change in the value
of the query over all of its neighboring datasets. In gen-
eral, it is not easy to analytically estimate the smooth
sensitivity function for a general query. Rubinstein et
al. [38] also study the inherent randomness in the data-
generation algorithm. We adopt their approach of sam-
pling the sensitivity from the empirical distribution of
the sensitivity. They use order statistics to choose a par-
ticular value of the sensitivity. We use the risk, which
provides a mediation tool for business entities to assess
the actual business risks, on the sensitivity distribution
to estimate the sensitivity.

Reﬁnements of diﬀerential privacy. In order to
account for both sources of randomness, reﬁnements of
ε-diﬀerential privacy are proposed in order to bound
the probability of occurrence of worst case scenarios.
Machanavajjhala et al. [27] propose probabilistic diﬀer-
ential privacy that considers upper bounds of the worst
case privacy loss for corresponding conﬁdence levels on
the noise distribution. Deﬁnition of probabilistic diﬀer-
ential privacy incorporates the explicit randomness in-
duced by the noise distribution and bounds the prob-
ability over the space of noisy outputs to satisfy the
ε-diﬀerential privacy deﬁnition. Dwork et al. [13] pro-
pose Concentrated diﬀerential privacy that considers
the expected values of the privacy loss random variables
for the corresponding. Deﬁnition of concentrated diﬀer-
ential privacy incorporates the explicit randomness in-
duced by the noise distribution but considering only the
expected value of privacy loss satisfying ε-diﬀerential
privacy deﬁnition instead of using the conﬁdence levels
limits its scope.

Hall et al. [17] propose random diﬀerential privacy
that considers the privacy loss for corresponding con-
ﬁdence levels on the implicit randomness in the data-
generation distribution. Deﬁnition of random diﬀeren-
tial privacy incorporates the implicit randomness in-
duced by the data-generation distribution and bounds
the probability over the space of datasets generated
from the given distribution to satisfy the ε-diﬀerential
privacy deﬁnition. Dwork et al. [9] deﬁne approximate
diﬀerential privacy by adding a constant bias to the pri-
vacy guarantee provided by the diﬀerential privacy. It is
not a probabilistic reﬁnement of the diﬀerential privacy.

8 Conclusion and Future Works

In this paper, we provide a means to ﬁne-tune the
privacy level of a privacy-preserving mechanism by
analysing various sources of randomness. Such a ﬁne-
tuning leads to probabilistic quantiﬁcation on privacy
levels with quantiﬁed risks, which we call as privacy at
risk. We also provide composition theorem that lever-
ages privacy at risk. We analytical calculate privacy at
risk for Laplace mechanism. We propose a cost model
that bridges the gap between the privacy level and the
compensation budget estimated by a GDPR compliant
business entity. Convexity of the cost function ensures
existence of unique privacy at risk that minimises com-
pensation budget. The cost model helps in not only re-
inforcing the ease of application in a business setting
but also providing stronger privacy guarantees on the
composition of mechanism.

It is possible to instantiate privacy at risk for Gaus-
sian mechanism. The mechanism is (ε, δ)-diﬀerential pri-
vate for γ = 0 and a non-zero risk calculated by account-
ing for the sources of randomness. We save it for future
work. Privacy at risk may be fully analytically com-
puted in cases where the data-generation, or the sensi-
tivity distribution, the noise distribution and the query
are analytically known and take convenient forms. We
are now looking at such convenient but realistic cases.

Acknowledgement

This work was supported by the National Research
Foundation (NRF) Singapore under its Corporate Labo-
ratory@University Scheme, National University of Sin-
gapore, and Singapore Telecommunications Ltd. This
research was also funded in part by the BioQOP project
of the French ANR (ANR-17-CE39-0006). We thank
Pierre Senellart for his help in reviewing derivations of
privacy at risk for the Laplace mechanism.

References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan

McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep
learning with diﬀerential privacy. In Proceedings of the 2016
ACM SIGSAC Conference on Computer and Communica-
tions Security, pages 308–318, 2016.

[2] Gergely Acs, Claude Castelluccia, and Rui Chen. Diﬀeren-
tially private histogram publishing through lossy compres-

Diﬀerential Privacy at Risk

16

sion. In Data Mining (ICDM), 2012 IEEE 12th International
Conference on, pages 1–10. IEEE, 2012.

[3] RA Askey and AB Olde Daalhuis. Generalized hypergeo-

metric functions and meijer g-function. NIST handbook of
mathematical functions, pages 403–418, 2010.

[4] Raef Bassily, Adam Groce, Jonathan Katz, and Adam Smith.
Coupled-worlds privacy: Exploiting adversarial uncertainty in
In 2013 IEEE 54th Annual Sympo-
statistical data privacy.
sium on Foundations of Computer Science, pages 439–448.
IEEE, 2013.

[5] Mark Bun and Thomas Steinke. Concentrated diﬀeren-

tial privacy: Simpliﬁcations, extensions, and lower bounds.
CoRR, 2016.

[6] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sar-
wate. Diﬀerentially private empirical risk minimization.
Journal of Machine Learning Research, 12(Mar):1069–1109,
2011.

[7] Yiling Chen, Stephen Chong, Ian A Kash, Tal Moran, and
Salil Vadhan. Truthful mechanisms for agents that value
privacy. ACM Transactions on Economics and Computation
(TEAC), 4(3):13, 2016.

[8] Damien Desfontaines and Balázs Pejó. Sok: Diﬀerential

privacies. Proceedings on Privacy Enhancing Technologies,
2020(2):288–313, 2020.

[9] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry,

Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy
via distributed noise generation. In Eurocrypt, volume 4004,
pages 486–503. Springer, 2006.

[10] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam

Smith. Calibrating noise to sensitivity in private data analy-
sis. In Theory of Cryptography Conference, pages 265–284.
Springer, 2006.

[11] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam
Smith. Calibrating Noise to Sensitivity in Private Data
Analysis, pages 265–284. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2006.

[12] Cynthia Dwork, Aaron Roth, et al. The algorithmic foun-
dations of diﬀerential privacy. Foundations and Trends® in
Theoretical Computer Science, 9(3–4):211–407, 2014.
[13] Cynthia Dwork and Guy N Rothblum. Concentrated diﬀer-
ential privacy. arXiv preprint arXiv:1603.01887, 2016.
[14] Simson L Garﬁnkel, John M Abowd, and Sarah Powazek.

Issues encountered deploying diﬀerential privacy. arXiv
preprint arXiv:1809.02201, 2018.

[15] Arpita Ghosh and Aaron Roth. Selling privacy at auction.
Games and Economic Behavior, 91:334–346, 2015.

[16] Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Ran-

dom diﬀerential privacy. Journal of Privacy and Conﬁdential-
ity, 4(2):43–59, 2012.

[17] Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Diﬀer-

ential privacy for functions and functional data. Journal of
Machine Learning Research, 14(Feb):703–727, 2013.

[18] Wassily Hoeﬀding. Probability inequalities for sums of

bounded random variables. In The Collected Works of Wass-
ily Hoeﬀding, pages 409–426. Springer, 1994.

[19] Justin Hsu, Marco Gaboardi, Andreas Haeberlen, Sanjeev

Khanna, Arjun Narayan, Benjamin C Pierce, and Aaron
Roth. Diﬀerential privacy: An economic method for choosing
epsilon.
(CSF), 2014 IEEE 27th, pages 398–410. IEEE, 2014.

In Computer Security Foundations Symposium

Diﬀerential Privacy at Risk

17

[20] Wolfram Research, Inc. Mathematica, Version 10. Cham-

2019.

paign, IL, 2014.

[21] Philippe Jorion. Value at risk: The new benchmark for man-

aging ﬁnancial risk, 01 2000.

[22] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The

composition theorem for diﬀerential privacy. In International
conference on machine learning, pages 1376–1385, 2015.

[37] William H Press. Numerical recipes 3rd edition: The art of
scientiﬁc computing. Cambridge university press, 2007.
[38] Benjamin IP Rubinstein and Francesco Aldà. Pain-free ran-
In Inter-

dom diﬀerential privacy with sensitivity sampling.
national Conference on Machine Learning, pages 2950–2959,
2017.

[23] Daniel Kifer and Bing-Rong Lin. An axiomatic view of sta-
tistical privacy and utility. Journal of Privacy and Conﬁden-
tiality, 4(1), 2012.

[39] Steven Ruggles, Katie Genadek, Ronald Goeken, Josiah
Integrated public use mi-

Grover, and Matthew Sobek.
crodata series: Version 6.0 [dataset], 2015.

[40] Aleksei Triastcyn and Boi Faltings. Federated learn-
ing with bayesian diﬀerential privacy. arXiv preprint
arXiv:1911.10071, 2019.

[41] Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and

Marianne Winslett. Functional mechanism: regression anal-
ysis under diﬀerential privacy. Proceedings of the VLDB
Endowment, 5(11):1364–1375, 2012.

[24] Daniel Kifer and Ashwin Machanavajjhala. A rigorous and
In Proceedings of the
customizable framework for privacy.
31st ACM SIGMOD-SIGACT-SIGAI symposium on Princi-
ples of Database Systems, pages 77–88. ACM, 2012.
[25] Jaewoo Lee and Chris Clifton. How much is enough? choos-
ing ε for diﬀerential privacy. In International Conference on
Information Security, pages 325–340. Springer, 2011.
[26] Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, and

Steven Z Wu. Accuracy ﬁrst: Selecting a diﬀerential privacy
In Advances in Neural
level for accuracy constrained erm.
Information Processing Systems, pages 2563–2573, 2017.
[27] Ashwin Machanavajjhala, Daniel Kifer, John Abowd, Jo-

hannes Gehrke, and Lars Vilhuber. Privacy: Theory meets
practice on the map. In Data Engineering, 2008. ICDE 2008.
IEEE 24th International Conference on, pages 277–286.
IEEE, 2008.

[28] Pascal Massart et al. The tight constant in the dvoretzky-
kiefer-wolfowitz inequality. The annals of Probability,
18(3):1269–1283, 1990.

[29] Sebastian Meiser. Approximate and probabilistic diﬀeren-
IACR Cryptology ePrint Archive,

tial privacy deﬁnitions.
2018:277, 2018.

[30] James P Moriarty, Megan E Branda, Kerry D Olsen, Nilay D

Shah, Bijan J Borah, Amy E Wagie, Jason S Egginton, and
James M Naessens. The eﬀects of incremental costs of
smoking and obesity on health care costs among adults:
a 7-year longitudinal study. Journal of Occupational and
Environmental Medicine, 54(3):286–291, 2012.

[31] Kevin P. Murphy. Machine Learning: A Probabilistic Per-

spective. The MIT Press, 2012.

[32] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.

Smooth sensitivity and sampling in private data analysis.
In Proceedings of the thirty-ninth annual ACM symposium
on Theory of computing, pages 75–84. ACM, 2007.
[33] Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian J.

Goodfellow, and Kunal Talwar. Semi-supervised knowledge
transfer for deep learning from private training data. In 5th
International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net, 2017.

[34] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth

Raghunathan, Kunal Talwar, and Úlfar Erlingsson. Scal-
able private learning with PATE. CoRR, abs/1802.08908,
2018.

[35] Athanasios Papoulis and S Unnikrishna Pillai. Probability,
random variables, and stochastic processes. Tata McGraw-
Hill Education, 2002.

[36] Balazs Pejo, Qiang Tang, and Gergely Biczok. Together or

alone: The price of privacy in collaborative learning. Pro-
ceedings on Privacy Enhancing Technologies, 2019(2):47–65,

A Proof of Theorem 2

(Section 3)

Proof. Let us ﬁx a pair of neighbouring datasets x and y,
and also ﬁx a subset of outputs Z ⊆ Range(M). Choose
OUT ⊆ Z such that

OUT (cid:44)

(cid:26)

z ∈ Z

(cid:12)
(cid:12)
(cid:12)

P(M(f, Θ)(x) = z)
P(M(f, Θ)(y) = z)

(cid:27)

> eε

Thus, we also obtain a complementary set of outputs
Z \ OUT, where the privacy loss is upper bounded i.e.
P(M(f, Θ)(x) = z) ≤ eεP(M(f, Θ)(y) = z). Thus,

P(M(f, Θ)(x) ∈ Z)
= P(M(f, Θ)(x) ∈ Z\OUT) + P(M(f, Θ)(x) ∈ OUT)

eεP(M(f, Θ)(y) ∈ Z\OUT) + P(M(f, Θ)(x) ∈ OUT)

≤
(a)

eεP(M(f, Θ)(y) ∈ Z \ OUT) + γ

≤
(b)
≤ eεP(M(f, Θ)(y) ∈ Z) + γ

(a) is obtained from the fact that the privacy loss is
upper bounded in the complimentary set of outputs
Z \ OUT. (b) is a consequence of the deﬁnition of (ε, γ)
privacy at risk. The deﬁnition upper bounds the prob-
ability measures of the subset OUT by γ.

B Proof of Theorem 4

(Section 4.1)

∆f
ε

Although a Laplace mechanism L
induces higher
amount of noise on average than a Laplace mechanism
∆f
for ε < ε0, there is a non-zero probability that
L
ε0
∆f
induces noise commensurate to L
. This non-zero
L
ε
probability guides us to calculate the privacy at risk γ1
for the privacy at risk level ε. In order to get an intu-
ition, we illustrate the calculation of the overlap between
two Laplace distributions as an estimator of similarity
between the two distributions.

∆f
ε0

[Overlap of Distributions,

Deﬁnition 8.
[35]] The
overlap, O, between two probability densities P1, P2 with
support X is deﬁned as

O =

Z

X

min[P1(x), P2(x)] dx.

Lemma 5. The overlap O between two probability dis-
), such that ε2 ≤ ε1, is
tributions, Lap(

) and Lap(

∆f
ε2

∆f
ε1

Diﬀerential Privacy at Risk

18

given by

O = 1 − (exp (−µε2/∆f ) − exp (−µε1/∆f )),

where µ =

∆f ln (ε1/ε2)
ε1−ε2

.

Using the result in Lemma 5, we note that the overlap
between two distributions with ε0 = 1 and ε = 0.6 is
∆f
0.6 induces noise that is more than 80%
0.81. Thus, L
∆f
times similar to the noise induced by L
1.0 . Therefore,
we can loosely say that at least 80% of the times a
∆f
Laplace Mechanism L
1.0 will provide the same privacy
as a Laplace Mechanism L

∆f
0.8 .
Although the overlap between Laplace distributions
with diﬀerent scales oﬀers an insight into the relation-
ship between diﬀerent privacy levels, it does not capture
the constraint induced by the sensitivity. For a given
query f , the amount of noise required to satisfy diﬀer-
ential privacy is commensurate to the sensitivity of the
query. This calibration puts a constraint on the noise
that is required to be induced on a pair of neighbouring
datasets. We state this constraint in Lemma 6, which we
∆f
further use to prove that the Laplace Mechanism L
ε0
satisﬁes (ε, γ1)-privacy at risk.

∆f
ε0 , the diﬀer-
Lemma 6. For a Laplace Mechanism L
ence in the absolute values of noise induced on a pair of
neighbouring datasets is upper bounded by the sensitivity
of the query.

Proof. Suppose that two neighbouring datasets x and y
are given input to a numeric query f : D → Rk. For any
output z ∈ Rk of the Laplace Mechanism L

,

∆f
ε0

k
X

i=1

(|f (yi) − zi| − |f (xi) − zi|) ≤

k
X

(|f (xi) − f (yi)|)

i=1
≤ ∆f .

We use triangular inequality in the ﬁrst step and Deﬁ-
nition 2 of sensitivity in the second step.

We write Exp(b) to denote a random variable sampled
from an exponential distribution with scale b > 0. We
write Gamma(k, θ) to denote a random variable sampled
from a gamma distribution with shape k > 0 and scale
θ > 0.

Lemma 7.
[[35]] If a random variable X follows
Laplace Distribution with mean zero and scale b, |X| ∼
Exp(b).

Lemma 8. [[35]] If X1, ..., Xn are n i.i.d. random vari-
ables each following the Exponential Distribution with
scale b, Pn

i=1 Xi ∼ Gamma(n, b).

Lemma 9. If X1 and X2 are two i.i.d. Gamma(n, θ)
random variables, the probability density function for the
random variable T = |X1 − X2|/θ is given by

PT (t; n, θ) =

22−ntn− 1
√

2 Kn− 1
2πΓ(n)θ

2

(t)

where Kn− 1
kind.

2

is the modiﬁed Bessel function of second

Proof. Let X1 and X2 be two i.i.d. Gamma(n, θ) random
variables. Characteristic function of a Gamma random
variable is given as

Diﬀerential Privacy at Risk

19

Proof. Let, x ∈ D and y ∈ D be two datasets such that
x ∼ y. Let f : D → Rk be some numeric query. Let
Px(z) and Py(z) denote the probabilities of getting the
∆f
output z for Laplace mechanisms L
ε0 (y)
respectively. For any point z ∈ Rk and ε 6= 0,

∆f
ε0 (x) and L

Px(z)
Py(z)

=

k
Y

exp

i=1

exp

(cid:16) −ε0|f (xi)−zi|
∆f
(cid:16) −ε0|f (yi)−zi|
∆f

(cid:17)

(cid:17)

=

k
Y

i=1

exp

(cid:18) ε0(|f (yi) − zi| − |f (xi) − zi|)
∆f

(cid:19)

φX1 (z) = φX2 (z) = (1 − ιzθ)−n.

= exp

ε

Therefore,

φX1−X2 (z) = φX1 (z)φ∗
X2

(z) =

1
(1 + (zθ)2)n

By Deﬁnition 4,

"

ε0 Pk

i=1(|f (yi) − zi| − |f (xi) − zi|)

ε∆f

#!

.

(14)

Probability density function for the random variable
X1 − X2 is given by,

PX1−X2 (x) =

∞
Z

1
2π

e−izxφX1−X2 (z)dz

−∞
θ |n− 1
21−n| x
√

2 Kn− 1
2πΓ(n)θ

2

=

(| x

θ |)

is the Bessel function of second kind. Let

where Kn− 1
T = | X1−X2

2

θ

|. Therefore,

(f (x) − z), (f (y) − z) ∼ Lap(∆f /ε0).

(15)

Application of Lemma 7 and Lemma 8 yields,

k
X

i=1

(|f (xi) − zi|) ∼ Gamma(k, ∆f /ε0).

(16)

Using Equations 15, 16, and Lemma 6, 10, we get

|(|f (yi) − z| − |f (xi) − z|)|

!

ε0
∆f

k
X

i=1

(t)

∼ PT 0 (t; k, ∆f /ε0, ∆f ).

(17)

PT (t; n, θ) =

21−ntn− 1
√

2 Kn− 1
2πΓ(n)θ

2

We use Mathematica [20] to solve the above integral.

Lemma 10. If X1 and X2 are two i.i.d. Gamma(n, θ)
random variables and |X1 − X2| ≤ M , then T 0 = |X1 −
X2|/θ follows the distribution with probability density
function:

PT 0 (t; n, θ, M ) =

PT (t0; n, θ)
PT (T ≤ M )

,

since, Pk
fore,

i=1 |(|f (yi) − z| − |f (xi) − z|)| ≤ ∆f . There-

 "

P

ε0
∆f

k
X

i=1

|(|f (yi) − z| − |f (xi) − z|)|

≤ ε

#

!

=

P(T ≤ ε)
P(T ≤ ε0)

,

(18)

where T follows the distribution in Lemma 9. We

use Mathematica [20] to analytically compute,

where PT is the probability density function of deﬁned
in Lemma 9.

P(T ≤ x) ∝

(cid:18)

1F2(

1
2

;

3
2

− k,

3
2

;

x2
4

√
)

(cid:19)

π4kx]

−

Lemma 11. For Laplace Mechanism L
f : D → Rk and for any output Z ⊆ Range(L
ε0,

∆f
ε0 with query
∆f
ε0 ), ε ≤

"

γ1 (cid:44) P

ln

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P(L
P(L

∆f
ε0 (x) ∈ Z)
∆f
ε0 (y) ∈ Z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

≤ ε

=

P(T ≤ ε)
P(T ≤ ε0)

,

where T follows
∆f
PT (t; k,
ε0

).

the distribution in Lemma 9,

(cid:18)

21F2(k;

1
2

+ k, k + 1;

(cid:19)

)x2kΓ(k)

x2
4

where 1F2 is the regularised generalised hypergeometric
function as deﬁned in [3]. From Equation 14 and 18,

"

P

ln

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P(L
P(L

∆f
ε0 (x) ∈ S)
∆f
ε0 (y) ∈ S)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

≤ ε

=

P(T ≤ ε)
P(T ≤ ε0)

.

 
 
This completes the proof of Theorem 4.

Corollary 3. Laplace Mechanism L
Rk

∆f
ε0 with f : D →
(ε, δ)-probabilistic diﬀerentially private

satisﬁes

where

δ =

(

1 −

P(T ≤ε)
P(T ≤ε0)

0

ε ≤ ε0

ε > ε0

and T follows BesselK(k, ∆f /ε0).

C Proof of Theorem 5

(Section 4.2)

Proof. Let, x and y be any two neighbouring datasets
sampled from the data generating distribution G. Let,
be the sampled sensitivity for query f : D → Rk.
∆Sf
Let, Px(z) and Py(z) denote the probabilities of get-
∆Sf
(x) and
ting the output z for Laplace mechanisms L
ε
(y) respectively. For any point z ∈ Rk and ε 6= 0,

∆Sf
L
ε

Px(z)
Py(z)

=

exp

k
Y

i=1

exp

(cid:16) −ε|f (xi)−zi|
∆Sf
(cid:16) −ε|f (yi)−zi|
∆Sf

(cid:17)

(cid:17)

= exp

≤ exp

ε Pk

i=1(|f (yi) − zi| − |f (xi) − zi|)

!

∆Sf

ε Pk

i=1 |f (yi) − f (xi)|

!

∆Sf

= exp

(cid:18) εkf (y) − f (x)k1
∆Sf

(cid:19)

(19)

We used triangle inequality in the penultimate step.

Using the trick in the work of [38], we deﬁne fol-
∆Sf denotes the set of pairs neigh-
lowing events. Let, B
bouring dataset sampled from G for which the sensitivity
∆Sf
random variable is upper bounded by ∆Sf
ρ
denotes the set of sensitivity random variable values for
which Fn deviates from the unknown cumulative distri-
bution of S, F , at most by the accuracy value ρ. These
events are deﬁned in Equation 20.

. Let, C

B

C

∆Sf (cid:44) {x, y ∼ G such that kf (y) − f (x)k1 ≤ ∆Sf }
∆Sf
ρ

S (∆) − FS(∆)| ≤ ρ

|F n

(cid:26)

(cid:27)

(cid:44)

(20)

sup
∆

Thus, we obtain

(cid:16)

P

∆Sf

B

(cid:17)

= P

(cid:16)

∆Sf

B

(cid:18)

∆Sf

B

+ P

∆Sf
ρ

(cid:17)

(cid:16)

∆Sf
ρ

C

P

(cid:17)

∆Sf
ρ

(cid:18)

(cid:19)

P

∆Sf
C
ρ

(cid:19)

(cid:12)
(cid:12)
(cid:12)C
(cid:12)
(cid:12)
C
(cid:12)
(cid:12)

Diﬀerential Privacy at Risk

20

(cid:17)

(cid:16)

∆Sf
ρ

C

P

(cid:17)

(cid:16)

B

∆Sf

≥ P

(cid:12)
∆Sf
(cid:12)
(cid:12)C
ρ
(cid:16)
= Fn(∆Sf )P
(cid:16)

C
1 − 2e−2ρ2n(cid:17)

∆Sf
ρ

≥ γ2 ·

(cid:17)

(21)

In the last step, we use the deﬁnition of the sampled
sensitivity to get the value of the ﬁrst term. The last
term is obtained using DKW-inequality, as deﬁned in
[28], where the n denotes the number of samples used
to build empirical distribution of the sensitivity, Fn.

From Equation 19, we understand that if kf (y) −
f (x)k1 is less than or equals to the sampled sensitivity
∆Sf
satisﬁes ε-diﬀerential
then the Laplace mechanism L
ε
privacy. Equation 21 provides the lower bound on the
probability of the event kf (y) − f (x)k1 ≤ ∆Sf
. Thus,
combining Equation 19 and Equation 21 completes the
proof.

D Proof of Theorem 6

(Section 4.3)

Proof of Theorem 6 builds upon the ideas from the
proofs for the rest of the two cases. In addition to the
events deﬁned in Equation 20, we deﬁne an additional
event A
that satisfy the constraint
of Laplace mechanism L
of ε-diﬀerential privacy for a speciﬁed privacy at risk
level ε.

, deﬁned in Equation 22, as a set of outputs

∆Sf
ε0

∆Sf
ε0

(

∆Sf
ε0

A

(cid:44)

z ∼ L

∆Sf
ε0

:

ln

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∆Sf
ε0
∆Sf
ε0

L

L

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(x)

(y)

≤ ε, x, y ∼ G

)

(22)

Corollary 4.

P(A

∆Sf
ε0

∆Sf ) =

|B

P(T ≤ ε)
P(T ≤ ηε0)

where T follows the distribution PT (t; ∆Sf /ε0)
Lemma 9 and η =

.

∆f
∆Sf

in

Proof. We provide the sketch of the proof. Proof follows
from the proof of Lemma 11. For a Laplace mechanism
calibrated with the sampled sensitivity ∆Sf
and privacy
level ε0, Equation 17 translates to,

|(|f (yi) − z| − |f (xi) − z|)|

∼

!

ε0
∆Sf

k
X

i=1

 
 
 
Diﬀerential Privacy at Risk

21

where n is the number of samples used to ﬁnd sampled
sensitivity, ρ ∈ [0, 1] is a accuracy parameter and η =
∆f
. The outer probability is calculated with respect to
∆Sf
support of the data-generation distribution G.

Proof. The proof follows from the proof of Lemma 11
and Lemma 13. Consider,

P(A

∆Sf
ε0

) ≥ P(A

∆Sf )P(B

∆Sf |C

∆Sf
ρ

)P(C

∆Sf
ρ

)

· γ2 · (1 − 2e−2ρ2n)

(24)

∆Sf
|B
ε0
P(T ≤ ε)
P(T ≤ ηε0)

≥

The ﬁrst term in the ﬁnal step of Equation 24 fol-
lows from the result in Corollary 4 where T follows
). It is the probability with which the
BesselK(k,

∆Sf
ε0

Laplace mechanism L
for a given value of sampled sensitivity.

satisﬁes ε-diﬀerential privacy

∆Sf
ε0

∆Sf
ε0

calculated by
Probability of occurrence of event A
accounting for both explicit and implicit sources of ran-
domness gives the risk for privacy level ε. Thus, the
proof of Lemma 13 completes the proof for Theorem 6.
in Theorem 6 and

Comparing the equations

Lemma 13, we observe that

γ3 (cid:44)

P(T ≤ ε)
P(T ≤ ηε0)

· γ2

(25)

The privacy at risk, as deﬁned in Equation 25, is free
from the term that accounts for the accuracy of sam-
pled estimate. If we know cumulative distribution of the
sensitivity, we do not suﬀer from the uncertainty of in-
troduced by sampling from the empirical distribution.

PT 0 (t; k, ∆Sf /ε0, ∆Sf ).

since, Pk

i=1 |(|f (yi) − z| − |f (xi) − z|)| ≤ ∆f . Using

Lemma 10 and Equation 18,

P(A

∆Sf
ε0

) =

P(T ≤ ε)
P(T ≤ ηε0)

.

where T follows the distribution PT (t; ∆Sf /ε0) and η =
∆f
∆Sf
For this case, we do not assume the knowledge of the
sensitivity of the query. Using the empirical estimation
presented in Section 4.2, if we choose the sampled sen-
sitivity for privacy at risk γ2 = 1, we obtain an approx-
imation for η.

Lemma 12. For a given value of accuracy parameter
ρ,

!

∆f
∆∗
Sf

= 1 + O

ρ
∆∗
Sf

where ∆∗
Sf
(cid:18)

i.e., O

n (1). O

= F −1
(cid:19)

ρ
∆∗

Sf

= k ρ
∆∗
Sf

(cid:19)

(cid:18)

ρ
∆∗

Sf

denotes order of

ρ
∆∗

Sf

,

for some k ≥ 1.

Proof. For a given value of accuracy parameter ρ and
any ∆ > 0,

Fn(∆) − F (∆) ≤ ρ
Since above inequality is true for any value of ∆, let
∆ = F −1(1). Therefore,

Fn(F −1(1)) − F (F −1(1)) ≤ ρ

Fn(F −1(1)) ≤ 1 + ρ

(23)

Since a cumulative distribution function is 1-Lipschitz
[[35]],

n (1) − F −1(1)|

|Fn(F −1
|Fn(F −1

n (1)) − Fn(F −1(1))| ≤ |F −1
n (1)) − Fn(F −1(1))| ≤ |∆∗
Sf
ρ ≤ ∆f − ∆∗
Sf
∆f
∆∗
Sf

ρ
∆∗
Sf

1 +

≤

− ∆f |

where we used result from Equation 23 in step 3. Intro-

ducing O

completes the proof.

(cid:18)

(cid:19)

ρ
∆∗

Sf

Lemma 13. For Laplace Mechanism L
with sam-
pled sensitivity ∆Sf of a query f : D → Rk and for any
∆Sf
Z ⊆ Range(L
ε

),

∆Sf
ε0

(cid:20)

P

ln

(cid:12)
(cid:12)
(cid:12)
(cid:12)

P(Lε0 (x) ∈ Z)
P(Lε0 (y) ∈ Z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

≤ ε

≥

P(T ≤ ε)
P(T ≤ ηε0)

γ2(1−2e−2ρ2n)

 
