A manifest-based framework for organizing the
management of personal data at the edge of the network
Riad Ladjel, Nicolas Anciaux, Philippe Pucheral, Guillaume Scerri

To cite this version:

Riad Ladjel, Nicolas Anciaux, Philippe Pucheral, Guillaume Scerri. A manifest-based framework for
organizing the management of personal data at the edge of the network. ISD 2019 - 28th International
Conference on Information Systems Development, Aug 2019, Toulon, France. ￿hal-02269203￿

HAL Id: hal-02269203

https://hal.science/hal-02269203

Submitted on 22 Aug 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

A manifest-based framework for organizing the management of 
personal data at the edge of the network 

Riad Ladjel 
Inria, UVSQ 
Versailles, France 

Nicolas Anciaux 
Inria, UVSQ 
Versailles, France 

Philippe Pucheral 
Inria, UVSQ 
Versailles, France 

Guillaume Scerri 
Inria, UVSQ 
Versailles, France 

riad.ladjel@inria.fr 

nicolas.anciaux@inria.fr 

philippe.pucheral@uvsq.fr 

guillaume.scerri@uvsq.fr 

Abstract 

Smart disclosure initiatives and new regulations such as GDPR allow individuals to get the 

control  back  on  their  data  by  gathering  their  entire  digital  life  in  a  Personal  Data 

Management Systems (PDMS). Multiple PDMS architectures exist, from centralized web 

hosting  solutions  to  self-data  hosting  at  home.  These  solutions  strongly  differ  on  their 

ability  to  preserve  data  privacy  and  to  perform  collective  computations  crossing  data  of 

multiple individuals (e.g., epidemiological or social studies) but none of them satisfy both 

objectives. The emergence of Trusted Execution Environments (TEE) changes the game. 

We propose a solution called Trusted PDMS, combining the TEE and PDMS properties to 

manage the data of each individual, and a Manifest-based framework to securely execute 

collective  computation  on  top  of  them.  We  demonstrate  the  practicality  of  the  solution 

through a real case-study being conducted over 10.000 patients in the healthcare field. 

Keywords: Trusted Execution Environment, Secure Distributed Computing, Data Privacy. 

1.  Introduction 

As Tim Berners Lee advocates, “time has come to restore the power of individuals on the 

web” [23]. Smart disclosure initiatives (e.g., Blue and Green Button in the US, MiData in 

UK, MesInfos in France) and new privacy-protection regulations (e.g., GDPR in Europe 

[12]) are a first step in this direction, allowing individuals to retrieve their personal data 

from  the  companies  and  administrations  hosting  them.  Hence,  they  can  gather  their 

complete digital environment in a single place, usually called Personal Cloud or Personal 

Data Management Systems (PDMS) [4] and manage it under their control.  

Several  companies  are  now  riding  this  Personal  Cloud  wave  with  highly  diverse 

 
 
 
 
 
solutions,  ranging  from  centralized  web  hosting  PDMS  solutions  (e.g.,  CozyCloud  or 

Digi.me) 

to  fully  decentralized  PDMS  hosted  at  home  (e.g.,  CloudLocker  or 

MyCloudHome). The architectural dimension of the PDMS concept raises two important 

and  potentially  conflicting  challenges:  (1)  gathering  personal  data  previously  scattered 

across distinct data silos generates new opportunities but also incurs new privacy threats 

depending on where and how these data are hosted and (2) giving the power back to each 

individual  on  his  data could  impede  the  development  of new collective services  of high 

societal interest (e.g., computing statistics or clustering data for an epidemiological study). 

Decentralized  PDMS  architectures  have  recognized  privacy  protection  virtues  by 

decreasing the Benefit/Cost ratio of an attack compared to their centralized web hosting 

counterparts. However, they make challenge (2) harder to tackle. How can we convince 

individuals who selected a decentralized PMDS setting to engage their personal data in a 

distributed process they do not control? Conversely, how could a service provider trust a 

processing performed by a myriad of untrusted participants? No existing work, including 

Multi-Party  Computation  (MPC)  [9],  gossip-based  [2],  homomorphic  encryption-based 

[13]  or  differential  privacy-based  [10]  protocols  fully  answer  this  dual  question  in  a 

practical  way. Existing solutions either  compute  a  limited set  of  operations (e.g., count, 

sum) in a scalable way or compute arbitrary functions on a limited number of participants. 

In this paper, we argue that the emergence of Trusted Execution Environments (TEE) [20] 

at the edge of the network  Intel SGX, ARM's TrustZone or TPM components are becoming 

omnipresent on every PC, tablets, smartphones and even smart objects  drastically changes 

the game. TEEs are able to efficiently compute arbitrary functions over sensitive data while 

providing  data  confidentiality  and  code  integrity.  However,  TEEs  have  been  designed  to 

protect individual device/server rather than large scale distributed edge computing processes. 

Moreover,  while  TEE  tamper-resistance  makes  attacks  difficult  and  costly,  side-channel 

attacks have been shown feasible [22]. Without appropriate counter-measures, a minority of 

corrupted participants in a distributed processing could endanger data from the majority. 

The  solution  proposed  in  this  paper  is  twofold.  First,  we  introduce  the  concept  of 

Trusted  PDMS  (TPDMS),  that  is  the  combination  of  a  TEE  and  a  PDMS  in  a  same 

dedicated box where only trusted code can be  installed. Second, we propose a Manifest-

based framework which provides a mutual trust between participants and service provider 

that a data processing task distributed among a large set of TPDMS will deliver a correct 

result while protecting the privacy of individuals. We finally demonstrate the practicality 

of the approach through an ongoing deployment of the technology in the medical-social 

field, over 10.000 patients receiving care at home. Our contributions are the following: 

  We  analyze  the  main  alternatives  in  terms  of  personal  cloud  architectures  and 

conclude from this analysis the need to deeply reconsider the way distributed computing 

is engineered compared to traditional cloud solutions; 

 
  We propose  a manifest-based  framework  reconciling  computation  generality  and 

scalability in the context of TPDMS; 

  We  present  a  concrete  instantiation  of  this  protocol  in  the  medical-social  field, 

evaluate it in terms of security, performance and societal impact. 

The rest of the paper is organized as follows. Sections 2 and 3 focus on the two first 

contributions mentioned above. Section 4 presents the medical-social instantiation of our 

framework and Section 5 is devoted to its evaluation. Finally, Section 6 concludes. 

2.  Related Works and Problem Formulation 

2.1.  Analysis of PDMS Architecture Alternatives 

The Personal Data Management Systems (PDMS) [4] concept, also called Personal Cloud, 

PIMS  [1],  Personal  Data  Server  [3]  or  Personal  Data  Store  [19],  attracts  significant 

attention  from  the  research  and  industrial  communities.  We  briefly  review  the  main 

families of solutions and compare their ability to tackle the two challenges identified in the 

introduction, namely privacy preservation and distributed collective computations.  

Centralized  web  hosting  solutions.  CozyCloud,  Digi.me,  Meeco,  or  Perkeep  and 

governmental  programs  like  MyData.org  (Finland),  MesInfos.fing.org  (France)  or 

MyDex.org  (UK)  are  representative  of  this  family.  Individuals  centralize  their  personal 

data in a server managed by the PDMS provider and can access them through the internet. 

These  approaches  rely  on  strong  security  hypotheses:  (i)  the  PDMS  provider  and  its 

employees  are  assumed  to  be  fully-honest,  and  (ii)  the  PDMS  code  as  well  as  all 

applications running on top of it must be trusted. This is  critical in a centralized context 

exacerbating the Benefit/Cost ratio of an attack. On the other hand, collective computations 

are simplified by the centralization but the security of such processing remains an issue. 

Zero-knowledge  personal  clouds  such  as  SpiderOak  or  Sync  and  to  a  certain  extent 

MyDex or Digi.me mentioned above, propose a variation of the centralized web hosting 

solutions where data is stored encrypted in the cloud and the user inherits the responsibility 

to store and manage the encryption keys elsewhere. The price to pay for this increase of 

security is the difficulty to develop advanced (local or distributed) services on top of zero-

knowledge personal clouds, reducing their use to a robust personal data safe. 

Home  cloud software  solutions (e.g.,  OpenPDS [19],  DataBox [8])  manage  personal 

data at the extremities of the network (e.g., within the user’s equipment) to circumvent the 

security risks of data centralization. Hence, queries on user’s data can be computed locally 

and only the result is sent back to the querier. However, these solutions implicitly assume 

that the computing platform at the user side is trusted and cannot be tampered with. 

Home cloud box (e.g., CloudLocker, MyCloudHome and many personal NAS solutions) 

go further in this direction by offering a dedicated box that can store TBs of data and run a 

server plugged on an individual’s home internet gateway. This solution alleviates the burden 

of  administrating  a  server  on  the  individual’s  device  and  logically  isolates  the  user’s 

computing environment from the box, they, however do not focus on security. Home cloud 

software nor home cloud box consider secure distributed processing as a primary target. 

The first conclusion that can be drawn from this analysis is that online personal cloud 

solutions  have  the  technical  ability  to  perform  distributed  computations  but  suffer  from 

very strong hypotheses in terms of security. Conversely, decentralized approaches are more 

likely to gain acceptance from the individuals but do not provide any  privacy preserving 

 solution to perform distributed computation (exporting their data on a central server to 

perform the computation would obviously hurt the decentralization principle). 

Decentralizing  the  processing  implies  to  temporarily  transfer  personal  data  among 

participants,  transforming  each  into  a  vulnerability  point.  Two  guarantees  must  then  be 

provided: (i) data confidentiality, i.e., any PDMS owner cannot access the data in transit 

of  other  participants,  and  (ii)  distributed  computation  integrity,  i.e.,  any  participant’s 

PDMS  can  attest  that  any  result  it  supplies  corresponds  to  the  assigned  computation. 

Classical distributed computation techniques used in enterprise systems do not apply here 

due to the unusual scale of the distribution (i.e., the computation may target a fraction of 

the population of a country). MPC works allow n users to perform computations involving 

their inputs while guaranteeing that only the final result of a computation will be disclosed 

but  they are  either  not  generic  in  terms  of  supported  computation  or  not  scalable  in  the 

number of participants [9]. Typically, MPC adaptations to distributed databases contexts, 

like  SMCQL  [7],  either  support  only  few  tens  of  participants  or  are  limited  to  specific 

database  operations.  Alternatively,  gossip-based  protocols  allow  to  work  on  fragmented 

clear-text data exchanged among nodes and, when communication may reveal data content, 

noise  is  added  to  provide  differentially  private  [10]  communication  patterns.  Gossip 

protocols scale well but are not generic in terms of computations (e.g., they can evaluate 

simple operations like sums, averages or ad-hoc data mining primitives like clustering [2]). 

The second conclusion that can be drawn from this analysis is that privacy-preserving 

distributed processing has traditionally been studied in the corporate cloud and database 

context  where  the  number  of  participants  (servers)  is  kept  small  or  in  large  scale 

environment  (e.g.,  sensor  networks,  smart  metering)  where  the  computation  to  be 

performed  is  basic.  Privacy-preserving  distributed  processing  must  thus  be  deeply 

rethought to tackle the PDMS context. 

2.2.  TEE as Game-changer and Related Trust Model 

The  emergence  of  Trusted  Execution  Environments  (TEE)  [20]  definitely  changes  the 

game. A TEE combines tamper-resistant hardware and software components to guarantee: 

(1) data confidentiality, meaning that private data residing in a TEE cannot be observed 

from the outside and (2)  code integrity, meaning that an attacker controlling a corrupted 

 
user environment (or OS) cannot  influence  the behavior  of a program  executing within a 

TEE. TEE are now omnipresent in end-user devices like PCs (e.g., Intel's Software Guard 

eXtention  (SGX)  in  Intel  CPUs  since  Skylake  version  in  2015),  mobile  devices  (e.g., 

ARM's  TrustZone  in  ARM  processors  equipping  smartphones  and  set-top  boxes)  and 

dedicated platforms (e.g., TPM combined with CPU or MCU). All these solutions provide 

the  two  properties  mentioned  above  with  different  levels  of  performance  and  different 

resilience  to  side-channel  attacks  which  could  compromise  data  confidentiality  [22]. 

Anyway,  side-channel  attacks  remain  complex  to  perform  and  require  physically 

instrumenting the TEE, which prevents large scale attacks. Code integrity is more difficult 

to compromise and not challenged today in most environments [16], [21]. 

In  this  paper,  we  assume  that  each  individual  is  equipped  with  a  trusted  PDMS 

(TPDMS) embedded in a dedicated hardware device. The proposed solution hence falls in 

the Home cloud box family (see section 2.1), with the salient difference that the box now 

provides defenses against attacks. More precisely, the box embeds a Trusted Computing 

Base,  i.e.  a  certified  software  composed  of:  (1)  a  personal  data  manager  managing  and 

protecting  the  individual's  data  (storing,  updating,  querying  data  and  enforcing  access 

control rules) and (2) a code loader ensuring the confidentiality and integrity of the code 

(in  the  TEE  sense)  executed  in  the  box.  Thus,  only  the  trusted  data  manager  and  code 

loader, and additional external code certified and verified by the code loader (through a 

signature of that code) can run in the box. Persistent personal data are stored outside the 

security  sphere,  in  a  stable  memory  attached  to  the  box  (e.g.,  a  SD  card  or  a  disk),  but 

encrypted by the TPDMS to protect them in confidentiality and  integrity. Compared to a 

regular Home cloud box, a TPDMS provides means to securely execute external code in 

the  box,  opening  the  door  to  the  design  of  secure  distributed  computation  protocols. 

Considering the omnipresence of TEE in most end-user devices today, building a TPDMS 

platform  is  more  than realistic.  Section  4  presents  a  concrete  instantiation  of  a  TPDMS 

platform combining a TPM (Trusted Platform Module) with a microcontroller but many 

other TPDMS instances could be devised (e.g., a Raspberry-Pi with ARM Trustzone or a 

personal cloud server or NAS with Intel SGX). We hence derive the following trust model: 

Large  set  of  trusted  TPDMS,  small  set  of  corrupted  TPDMS.  We  assume  that  each 

individual is equipped with a TPDMS managing his personal data. As mentioned above, 

despite the TEE tamper-resistance and the cost of such attacks, side-channel attacks have 

been shown feasible. Hence, in a large scale setting, we cannot totally exclude the fact that 

a small subset of TPDMS could have been instrumented by malicious participants opening 

the door to side-channel attacks compromising the confidentiality property. 

Trusted computation code. We consider that the code distributed to the participants to 

contribute  to  a  distributed  computation  has  been  carefully  reviewed  and  approved 

beforehand  by  a  regulatory  body  (e.g.,  an  association  or  national  privacy  regulatory 

agency) which signed this code. But the fact that the downloaded code is trusted does not 

imply that a whole distributed execution is trusted. 

Untrusted infrastructure. Besides the presence of TPDMS, no security assumptions can 

be made about the user's environment or the communication infrastructure. 

2.3.  Problem Statement  

The problem can be formulated as follows: how to translate the trust put in the computation 

code declaration, as certified by the regulatory body, into a mutual trust from all parties in 

the concrete distributed execution of that code under the trust model above? Solving this 

problem leads to satisfying the following properties: 

  Mutual trust: assuming that the declared code is executed within TPDMSs, mutual trust 

guarantees that: (1) only the data strictly specified for the computation is collected at each 

participants’ PDMS, (2) only the final result of the computation can be disclosed, i.e., none 

of  the  collected  raw  data  of  any  participant  is  leaked,  (3)  this  final  result  is  honestly 

computed  as  declared  and  (4)  the  computation  code  has  the  ability  to  check  that  any 

collected data is genuine. 

  Deterrence of side-channel attacks: assuming a small fraction of malicious participants 

are involved in the computation with instrumented TPDMS, the deterrence property must 

(1) guarantee that the leakage remains circumscribed to the data manipulated by the sole 

corrupted  TPDMS  and  (2)  prevent  the  attackers  from  targeting  a  specific  intermediate 

result (e.g., sensitive data or data of some targeted participants). 

To have a practical interest, the solution must finally: (1) be generic enough to support any 

distributed computations (e.g., from simple aggregate queries to advanced machine learning 

computations) and (2) scale to a large population (e.g., tens of thousands) of individuals. 

3.  Manifest-based Framework 

3.1.  Manifest-based Distributed Computation Framework 

To  ensure  a  collective  computation  that  scrupulously  respects  the  properties  described 

above, we propose a framework based on a Manifest describing the computation on which 

all  the  actors  agree  and  a  distributed  protocol  based  on  TPDMSs  performing  the 

computation in compliance with that Manifest. The solution is described in Figure 1. It is 

conducted in three main steps:  

Manifest  declaration.  The  entity  wishing  to  execute  a  distributed  computation  over 

personal data (e.g., a statistic agency, association of patients), called the Querier, acts as a 

data controller in the GDPR sense and produces a Manifest describing the computation. 

Individual contributors give their consent on the basis of the purpose of that manifest, and 

rely on regulatory bodies (e.g., WP29 members, CNIL) which validate the entire Manifest 

with regard to good confidentiality practices. To this end, the manifest indicates the identity 

 
of the Querier,  which  must be authorized  for the purpose.  It also provides  the  collection 

queries expressed in any easily interpretable declarative language (e.g., SQL), so that the 

regulatory body can verify that they reflect the principle of limited collection established 

by  the  legislation  for  the  intended  use.  The  code  of  the  implemented  operators  and  the 

organization of the data flow between them are also provided, and must correspond to the 

declared  purpose.  The  number  of  participants  plays  a  dual  role:  it  represents  both  a 

threshold  to  be  achieved  for  a  sufficiently  relevant  result  for  the  stated  purpose  and  a 

privacy  threshold  preventing  the  risk  of  re-identification  of  individual  data  in  the  final 

result, which the regulator must also check. Once certified, the Manifest is published in a 

Public store where it can be downloaded by individuals wishing to participate. Example 1 

shows the manifest of a distributed group-by query in the social-health context. 

Fig. 1. Manifest-based distributed computation framework. 

Random assignment of operators to participants. Participants download the manifest, 

and  when  a  sufficient  number  consent  to  contribute  with  their  data,  each  participant  is 

assigned  an  operator  of  the  Manifest.  Ensuring  a  random  assignment  is  critical  to  deter 

side-channel attacks on participants, by prohibiting corrupted participants from selecting 

specific operators in the execution process for malicious purpose (operators manipulating 

a large amount of data or receiving outputs from participants targeted by the attacker). 

Purpose: 
  Compute the avg number days of hospitalization prescribed 
  group by patient’s age and dependency-level (Iso-Resource Group, GIR) 
Operators code:   
  mapper source code  
  reducer source code  
Dataflow between the operators: 
  Number of mappers: 10000  
  Number of reducers: 10  
Collection queries:  
  SELECT GIR, to_year(sysdate-birthdate) FROM Patient; 
  SELECT avg(qty)FROM Prescription WHERE prescType = ‘hospitalization’;  
Number of participants: 10000 
Querier: ARS-Health-Agency, Public key: Rex2%ÃźHj6k7âĂę 

Example 1. ‘Group-by’ Manifest expressed by health organization. 

Secure distributed evaluation. Each participant’s TPDMS downloads the code of the 

operator  assigned  to  it  and  checks  the  code  signature,  authenticates  the  TPDMS  of 

participants supposed to exchange data with it (as specified in the random assignment) and 

establishes communication channels with them. The participant then executes his operator, 

potentially contributes personal data, and allows the computation to proceed by sending its 

Querier(Lab., Heatlhorg.)Regulatory Body(DPO, CNIL, ANSSI…)Manifest declarationRandom assignationoperators-to-TPDMSsSecure distributed computationManifest downloadand consentManifest validationIndividuals(and their TPDMS :    )Result transmission 
 
output to its successor. Once all participants have executed their operator, the end-result is 

published on the public store encrypted with the public key of the Querier. 

3.2.  Random Assignment Protocol 

Obtaining a random assignment of operators to participants is key to prevent any potential 

attackers  (Querier  or  any  participants)  colluding  with  corrupted  TPDMS  from  being 

assigned  a  targeted  operator  or  position  in  the  dataflow  at  execution.  While  existing 

solutions  have  been  proposed  to ensure  that a  random  number  is  chosen  and attested in 

distributed settings, e.g., [6], none can be applied to reach this specific goal as they assume 

the list of participants is known in advance, as opposed to our case where the participant 

list  is  chosen  based  on  collected  users’  consents.  We  propose  a  solution  to  produce  a 

provably random assignment,  detailed in Fig.  2.  As we  consider  TPDMS as  trusted,  the 

random assignment can be delegated to any TPDMS. However, the challenge is avoiding 

any malicious Participant or Querier aborting and replaying the assignment process a large 

number of times, picking the best one for a potential attack. 

Fig. 2. Random assignment of operators to participants. 

To avoid such attacks, we make sure, in a first step of our protocol, that the Querier 

commits  to  an  assigning  participant  among  the  consenting  participants.  More  precisely, 

each consenting participant first declares itself by publishing its identity and the hash of a 

random number used later to prove reception of the list of participants. Second, the protocol 

ensures  that  once  the  list  of  participants  has  been  fixed,  the  assignment  is  actually 

performed randomly, and that this randomness can be checked by every participant. Hence, 

once the Querier has gathered enough participants willing to participate, it broadcasts the 

full  list  of  participants  together  with  the  designated  assigning  participant,  which  is 

acknowledged by each participant by disclosing the random number chosen in the initial 

step. Following this, the designated assigning participant is sent the full list of participants 

QP1…Consent : hi,pkiPj…Pnpkjrandom({pki}){hi,pki},pkjri,pki{hi,ri,pki}Mas,SMas/*onallP1…Pn*/ifchecksign(Mas,SMas,pkj)and !tMas,t.r=rit.pk=pkithen execute(M, t.pos)/*onallP1…Pn*/ifPiconsents to M rirandom()hihash(ri) /*onlyonPj*/ifi,hi=hash(ri)thenseedrandom()Mas/*{ri,pki,posi}*/assign(M,seed)SMassign(Mas,skj)Mas,SMasManifest : M/*onallP1…Pn*/store pkjMthe manifestQthe QuerierPiwith i(1,n)the Participants pkitheir identity (public key)ria random chosen by Pihithe hash of riPjthe participant performing the random assignment skjits private key  Masthe random assignmentSMasthe signature Maswith skj(verifiable by any Pi )Legend: 
 
together  with  all  the  acknowledgements.  He  then  checks  that  all  acknowledgements  are 

valid, and performs a random assignment of operators to participants. Finally, he signs this 

assignment and sends it back to the Querier. 

Thus, the protocol ensures that when an individual consents to a manifest, the assignment 

can only be made once, at random (any attempt to replay the assignment would be visible to 

the participants and a restart require to obtain their consents again). 

3.3.  Global Assessment of the Manifest-based Framework 

We sum up by showing how the framework satisfies the properties identified in Section 2.3. 

Deterrence of attacks. This property first states that the data leakage due to an attack 

must  be  circumscribed  to  the  sole  data  manipulated  by  the  corrupted  TPDMS.  This  is 

intrinsically  achieved  by  never  sharing  any  cryptographic  information  among  different 

TPDMS.  Hence, any persistent  data residing in a TPDMS  is  encrypted with secret keys 

known only by that TPDMS and intermediate results in transit between a predecessor and 

a successor TPDMS is encrypted with a session key (see below) and managed in clear-text 

in that successor (inheriting the confidentiality property from the TEE processing it). The 

second requirement is to prevent any attacker from targeting specific personal data, which 

is the precise goal of the Random Assignment Protocol introduced in Section 3.2. 

These  requirements  satisfy  deterrence  of  attacks  by  drastically  increasing  the 

Cost/Benefit ratio of an attack. Indeed, even if a fraction of TPDMSs is instrumented with 

side-channel attacks compromising data confidentiality, such attack incurs a high cost of 

tampering with secure hardware (with physical intervention of the PDMS owner) with a 

benefit  limited  to  obtaining  the  data  manipulated  by  the  sole  corrupted  TPDMSs,  and 

negligible probability for gaining any personal data of interest. Indeed, in a computation 

manifest, we distinguish between participants assigned to a collection operator (which only 

extracts personal raw data from the participant) and participants assigned to a computation 

operator (which process personal data collected from others). Then, attacking any TPDMS 

running a collection operator is of no interest since the attacker only gains access to his 

own  data.  Moreover,  the  probability  of  a  corrupted  node  being  assigned  a  computation 

operator is negligible in practice (see security analysis in Section 5). Thus, although more 

elaborate  strategies  could  be  adopted  to  further  maximize  the  Cost/Benefit  ratio  (e.g., 

blurring data), they are considered unnecessary in our context. 

Mutual trust. The mutual trust property is guaranteed if two hypotheses hold: (H1) all 

data exchanged between the  participants’ TPDMSs are  encrypted with session keys and 

(H2) each TPDMS involved in the computation authenticates its neighboring participants 

as legitimate TPDMSs complying with the random assignment for that manifest. The first 

condition  for  mutual  trust  (see  Section  2.3)  stems  from  the  fact  that  (1)  the  collection 

queries  are  part  of  the  manifest  certified  by  the  Regulatory  body,  (2)  its  authenticity  is 

cryptographically checked by each TPDMS and (3) the TPDMS evaluating these queries 

is part of the Trusted Computing Base thereby guaranteeing the integrity of this collection 

phase. The second condition is satisfied by construction since each TPDMS guarantees the 

confidentiality of local data and H1 guarantees the confidentiality of intermediate data in 

transit. The final result is itself encrypted with the public key of the Querier so that no other 

data is ever leaked outside the TPDMS ecosystem. The third condition is again satisfied 

by construction by H2 guaranteeing that only genuine operators are computed and conform 

to the dataflow specified in the manifest. The last condition stems from the fact that (1) 

local  data  can  be  manipulated  in  clear-text  inside  each  TPDMS,  allowing  any  form  of 

verification  (e.g.,  check  signature  of  data  produced  by  a  smart  meter  or  quantified-self 

device, or issued by an employer or a bank) and (2) H2 guarantees the integrity of the data 

collection operator at each participant. Note that this guarantee holds even in the presence 

of corrupted TPDMSs which could compromise the confidentiality property. 

In  conclusion,  the  proposed  solution  is  generic  enough  to  capture  any  distributed 

execution plan where any node can be an operator of any complexity and edges are secure 

communication  channels  between  the  TPDMS  of  participants  executing  the  operators. 

Compared to the state of the art, our manifest-based approach has the ability to reconcile 

security  with  genericity  and  scalability.  First,  the  TEE  confidentiality  property  can  be 

leveraged to execute each operation over clear-text genuine data. Second, the number of 

messages exchanged among participants only results from the distributed computation to 

be  performed,  but  not  from  the  underlying  security  mechanism.  Hence,  unlike  secure 

Multiparty computations (MPC), homomorphic encryption, Gossip or Differential privacy 

approaches,  no  computational  constraint  hurting  genericity  nor  scalability  need  to  be 

introduced in the processing for security reasons. 

4.  A Trusted PDMS in the Medical-Social Field 

This section presents an on-going deployment of a TPDMS in the medical-social field and 

assesses the practicality of the Manifest framework. 

Overview. End of 2017, the Yvelines district in France launched a public call for tender 

to  deploy  an  Electronic  Healthcare  Record  (EHR)  infrastructure  facilitating  the  medical 

and social care at home for elderly people. 10.000 patients are primarily targeted by this 

initiative,  with  the  objective  to  use  it  as  a  testbed  for  a  larger  medium-term 

national/international deployment. The question raised by this initiative is threefold: 

  How  to  make  patients,  caregivers  and  professionals  trust  the  EHR  security  despite  the 

recent  and  massive  privacy  leakages  due  to  piracy,  scrutinization  and  opaque  business 

practices inherent to any data centralization process?  

  How to combine privacy expectations with the collective benefits of data analysis tools to 

rationalize care, improve business practices and predict disease evolution? 

 
  How  to  make  patient’s  healthcare  folder  available  even  in  a  disconnected  mode 

considering the low adoption of internet by elderly people? 

The  Hippocad  company,  in  partnership  with  the  Inria  research  institute  and  the 

University of Versailles (UVSQ), won this call for tender with a solution called hereafter 

THPC (Trusted Health Personal Cloud). THPC is based on a home box, pictured in Figure 

3, combining 3 usages: (1) effectiveness control and vigilance, (2) home care coordination 

and  (3)  supervision  (forthcoming).  The  hardware  incorporates  a  number  of  sensors  and 

communication modules (in particular SigFox) managed by a first microcontroller (called 

MCU1) devoted to the communication and sensing tasks. The data delivered by the box 

are used by the Yvelines district to cover usage (1), that is adjusting the care payment to 

their  duration  and  performing  a  remote  vigilance  of  the  patient  home.  A  second 

microcontroller (MCU2: STM32F417, 168 MHz, 192 KB RAM, 1 MB of NOR storage) 

is devoted to the PDMS managing the patient folder, a µ-SD card hosting the raw patient 

data  (encrypted  by  the  PDMS)  and  a  tamper-resistant  TPM  (Trusted  Platform  Module) 

securing the cryptographic keys and the boot phase of the PDMS in MCU2. As detailed 

next, the combination of a TPM with MCU2 forms a TPDMS. Care professionals interact 

with the PDMS (i.e., query and update the patient’s folder) through Bluetooth connected 

smartphone apps, covering usage (2). Finally, volunteer patients accepting to contribute to 

distributed computations (usage (3)), will be equipped with a box variant where the SigFox 

module is replaced by a GPRS module. 

Fig. 3. Architecture of the THPC solution. 

The  PDMS  engine  itself  has  been  specifically  designed  by  Inria/UVSQ  to 

accommodate the constraints of MCU2. This embedded PDMS is a full-fledged personal 

database  server  capable  of  storing  data  in  tables,  querying  them  in  SQL,  and  provides 

access control policies. Hence, care professionals can each interact with the patient’s folder 

according to the privileges associated to their role (e.g., a physician and a nurse will not 

get access to the same data). Finally, the patient’s data is replicated in an encrypted archive 

Usage 1Effectivenesscontrol & vigilanceSigfox/GPRSYvelines District8000 households262 counties10000 patientsUsage 3SupervisionBluetoothUsage 2Home carecoordination 
on a remote server to be able to recover it in case of crash. A specific master key recovery 

process (based on Shamir’s secret sharing) has been designed to guarantee that no one but 

the patient can recover this master key. 

THPC as an instance of Trusted PDMS. The THPC platform described above is an 

illustrative example of TPDMS. As introduced in Section 2.2, a TPDMS is a combination 

of a TEE and a PDMS software embedded in a same dedicated hardware device, providing 

confidentiality and integrity guarantees for the code running in this device. The presence 

of  two  separate  MCUs  answers  security  concerns,  indeed  the  Trusted  Computing  Base 

(TCB) is limited to the code located in MCU2 and does not include drivers and sensors 

(managed by MCU1) and is thus minimalistic. Additionally, the TCB is cryptographically 

signed. The TPM protecting the box is used at boot time (and NOR flash time) to check 

the genuineness of the PDMS code by checking the signature. The PDMS code in turn can 

download external pieces of code corresponding to the operators extracted from a Manifest, 

check their integrity thanks to the code signature provided by the Regulatory body, and run 

it. Hence, no code other than the TCB and signed operators can run in the box. The TPM 

also protects the cryptographic certificate that securely identifies the box and the master 

key protecting the personal database footprint on the µ-SD card. Note however that, while 

the  TPM  is  tamper-resistant,  the  MCU2  is  not.  Hence,  a  motivated  attacker  could 

physically instrument his box to spy the content of the RAM at run time. 

Distributed  computations  of  interest.  The  next  critical  step  of  the  project  is  to 

integrate  usage  (3)  (supervision).  GPRS  variant  of  the  boxes  are  under  development  to 

establish a communication network via a central server settled by the Hippocad company, 

which  plays  the  role  of  a  communication  gateway  between  the  THPC  boxes  (it  relays 

encrypted data bunches between THPC boxes but cannot access to the underlying data). 

Two essential distributed computations are considered, namely the Group-by and K-means 

computations.  Group-by  allows  computing  simple  statistics  by  evaluating  aggregate 

functions  (sum,  average,  min,  max,  etc.)  on  population  subsets  grouped  by  various 

dimensions (the level of dependence or GIR, age, gender, income, etc.). Such statistics are 

of general interest in their own and are also often used to calibrate more sophisticated data 

analysis techniques (e.g., accurately calibrate the k parameter of a K-means computation). 

K-means is one of the most popular clustering technique and is broadly used to analyze 

health data [17]. To date however, few studies was conducted on home care [15] because 

data  management  techniques  for  this  type  of  care  are  still  emerging.  Yet,  K-means 

techniques  already  delivered  significant  results  to  predict  the  evolution  of  patient 

dependency level after  a hip fracture [11] or Alzheimer symptoms, and derive from that 

the  required  evolution  of  the  home  cares  to  be  provided  and  their  cost.  The  first  two 

Manifest-based computations considered in the project cover these use cases as follows: 

 
  The Group-by manifest is the one presented in Example 1, using the usual map-reduce 

implementation of a Group-by computation, where operators executed by participants are 

the map and reduce task respectively. It computes the sum and average duration of home 

visits by professionals grouped by professional category and level of dependence (GIR) 

of the patient. Such statistics are expected to help adjusting the duration of interventions 

and the level of assistance according to the patients’ situation. 

  The  K-means  manifest  is  inspired  by  a  previous  study  conducted  in  Canada  with 

elderly people in home care. This study analyses 37 variables, and provides 7  centroids 

[5] that finely characterize the people cared for. On a similar map-reduce basis, we define 

K-means  manifests  computed  over  distributed  PDMSs  in  three  steps:  (1)  k  initial  means 

representing the centroid of k clusters are randomly generated by the Querier and sent to all 

participants to initialize the processing, (2) each participant playing a mapper role computes 

its  distance  with  these  k  means  and  sends  back  its  data  to the reducer  node  managing  the 

closest cluster, (3) each reducer recomputes the new centroid of the cluster it manages based 

on the data received from the mappers and sends it back to all participants. Steps 2 and 3 are 

repeated a given number of times or until convergence. 

In Section 5, we give preliminary measures obtained by a combination of real measures 

and simulations for these two manifests since they are not yet deployed. Running manifests 

in the THPC context has required an adaptation of the random assignation protocol to cope 

with the intrinsic limitation of GPRS in terms of communication bandwidth. 

Adaptation of the Random Assignment Protocol to the THPC context. Given the 

low bandwidth of the THPC boxes (GPRS communications), a critical problem is limiting 

the amount of data transmitted to all participants, as hundreds of KBs broadcasted to all 

thousands of participants would not be compatible with acceptable performance. In order 

to  reach  this  goal,  we  optimize  the  two  main  parts  of  the  random  assignation  protocol 

(Section 3.2) that lead to transmission of large amounts of data. The main optimization is 

making sure that we do not need to transmit neither the whole assignment nor the whole 

manifest to all participants as they only need their part of the assignment and the manifest 

related to their part of the computation. However, we need to make sure that the integrity 

of the whole manifest and assignment is ensured. In order to achieve these two seemingly 

antagonistic  goals,  we  make  use  of  Merkle  hash  trees  [18]  over  the  corresponding  data 

structures. The properties of the Merkle hash tree ensures that given the root of the hash 

tree, it is possible to provide a small checksum proving (in the cryptographic sense) that an 

element belongs to the corresponding hash tree, and it is computationally infeasible to forge 

such a proof. Note that the checksum is a logarithmic (in the number of values in the tree) 

number of hashed values and thus stays manageable (small size). Additionally, we avoid 

broadcasting  the  whole  list  of  participants  as  only  the  assigning  participant  needs  to 

perform checks on this list. We only broadcast a cryptographic hash of this list, and only 

send it in full to the assigning participant who actually needs  to check it.  The assigning 

participant however does not need to send back the full assignment, only a Merkle hash 

tree signed with its private key, and the random seed used to generate the assignment (so 

that the Querier can reconstruct it) is sent back. Finally, in order to perform its task in the 

manifest, any participant only needs its position together with the corresponding operator, 

collection  queries  and  data  flow  and  proof  of  membership  to  the  logical  manifest. 

Additionally, the participant needs to receive proof that the assignment is correct. 

Summing  up,  we  reduce  the  communication  load  during  assignment  building  phase 

from a few broadcasts of a few hundreds of KB (for tens of thousands of participants) to 

only one large download for the assigning participant (again a few hundreds of KB), and 

small  downloads/uploads  (a  few  tens  of  Bytes)  for  all  other  participants,  drastically 

reducing the overall communication load, and making it manageable in constrained setting. 

5.  Validation 

While the THPC platform is still under deployment over the 10.000 targeted patients, we 

can already draw interesting lessons learned and present preliminary performance and 

security results of the Manifest framework applied to the Group-by and K-means cases. 

5.1.  Lessons learned for the Deployment of THPC Solution 

An important criterion for the Yvelines District when selecting the THPC solution was its 

compliance with the new GDPR regulations and its ability to foster its adoption by 

patients and professionals.   

Adoption by patients. From the patients’ perspective, a crucial point was to provide 

differentiated views of their medical-social folder (e.g., a nurse is not supposed to see the 

income of the elderly person). To this end, a RBAC matrix (role-based access control) has 

been defined so that a professional owning a badge with a certificate attesting role R can 

play  this  role  with  the  appropriate  privileges  on  all  patients’  boxes.  Each  patient  can 

explicitly - and physically - express his consent (or not) to the access of a given professional 

by allowing access to his box during the consultation, as he would do with a paper folder. 

The patient can also express his consent, with the help of his referent, for each manifest. A 

notable effect of our proposal is to consent to a specific use of the data and to disclose only 

the  computed  result  rather  than  all  raw  data  as  usual  (e.g.,  consenting  to  an  Android 

application manifest provides an unconditional access to the raw data). 

Adoption by professionals. Professionals are reluctant to use an EHR solution which 

could  disclose  their  contact  details,  planning  and  statistical  information  that  may  reveal 

their professional practice (e.g., quantity of drugs prescribed or duration and frequency of 

home visits). A decentralized and secured solution is a great vector of adoption compared 

to any central server solution. Similarly, professionals are usually reluctant to see the data 

 
related  to  their  practice  involved  in  statistical  studies  unless  strict  anonymization 

guarantees  can  be  provided.  While  the  consent  of  the  professionals  is  not  requested  for 

distributed computations, a desirable effect of our proposal is to never disclose individual 

data referring to a given professional, and submit all computation to regulatory approval. 

5.2.  Performance and Security Evaluation of the Manifest-based Framework 

We validate the effectiveness of our approach on the Group-by and K-means use-cases.   

Experimental setting. We implemented the corresponding mappers and reducers code 

in the THPC box with a server used to route (encrypted) messages between participants, 

as  described  in  Section  4.  We  computed  the  execution  time  while  considering  different 

numbers of participants and amount of data transferred during the computations. We used 

a simulation to derive execution times with large numbers of participants. The results are 

shown  in  Fig.  4  (the  curves  are  in  log.  scale).  For  the  Group-by  case  we  consider  an 

implementation with 10 reducers and 50 different group keys, while for the K-means we 

consider 7 different clusters with 1 cluster per reducer as in [5] using a traditional distance 

metric [14]. We used synthetic datasets, as the objective is not to choose the most efficient 

implementation of a given computation, but rather to assess the efficiency of the manifest-

based  protocol  on  real  use-cases.  As  cryptographic  tools  we  used  ECC  256  bits  for 

asymmetric encryption, ECDSA signature scheme, AES 128 bits for symmetric encryption 

and SHA-2 as a hash function, leveraging the hardware acceleration provided by MCU2. 

(a) Execution time (assignment) 

(b) Execution time (computation) 

(c) Overall performance ratios 

(d) Exchanged data (by participant) 

(e) Exchanged data (in total) 

(f) Deterrence of attacks 

Fig. 4. Performance and security evaluation. 

Performance evaluation. Figures 4.a-b-d-e plot the various costs associated with our 

protocol. First, the optimization of our random assignment protocol has a strong impact on 

its execution time, from 75 seconds without optimization down to 22 seconds with 10000 

participants  (this  cost  depends  on  the  number  of  participants,  but  not  on  the  query 

performed),  as  well  as  on  the  volume  of  data  exchanges,  from  800  KB  per  participant 

11010020005000700010000Elapsed time (s)Number of participantsAssignmentOpt. assignment11010020005000700010000Elapsed time (s)Number of participantsGroupByKmeans58%31%11%CommunicationCrypto.Algorithm54%30%16%GroupByKmeans11010010001000020005000700010000Data transmission (KB)Number of participantsAssignmentOpt. assignment11010010001000020005000700010000Data transmission (MB)Number of participantsAssignmentOpt. assignment00,010,020,0315101520Probability of successNb of colluding corrupted TPDMSKmeansGroupByTargeted attack 
 
 
 
 
 
 
without  optimization  down  to  13  KB  (with  in  total,  7  GB  exchanged  data  down  to  130 

MB). Once the assignation is performed, the query computation time remains reasonable, 

with 18 seconds (resp. 40 seconds) for a Group-by (resp. K-means) over 10000 participants. 

Finally, the overhead incurred by the random assignation is limited (e.g., between 20 and 

30% of overall time), and the main part of the cost is due to communications (see Fig. 4.c). 

Fig. 4.f shows the tremendous impact of the random assignment protocol in terms of 

security. It plots the probability for a set of colluding malicious participants, acting with 

corrupted  TPDMSs  in  Group-by  and  K-means  computations,  to  be  assigned  a  reducer 

operator (hence gaining access to data produced by other participants) or to the data of a 

given  participant  of  interest  (targeted  attack).  This  probability  remains  very  low  (few 

percent) even if several participants successfully corrupt their TPDMS and collude. 

The  main  lessons  drawn  from  these  experiments  are:  First,  even  with  the  hardware 

limitations  of  the  box in terms  of  computing power and communication  throughput,  the 

global time is rather low and acceptable for this kind of study (less than a minute for 10000 

participants in comparison with manual epidemiological surveys which may last weeks). 

Second,  the  optimization  of  the  assignment  protocol  has  a  decisive  impact  on  both 

execution  times  and  data  volumes  exchanged,  with  a  significant financial  benefit  in  the 

context of pay-per-use communication services (such as GPRS network). 

6.  Conclusion 

The concept of TPDMS combined with a Manifest-based framework leverages the security 

properties provided by TEE to build a comprehensive personal data management solution. 

This solution reconciles the privacy preservation expected by individuals with the ability 

to perform collective computations of prime societal interest. We expect that such solution 

could pave the way to new research works and industrial initiatives tackling the privacy-

preserving distributed computing challenge with a new spirit and vision. 

Acknowledgements 

This research is supported by the ANR PerSoCloud grant no ANR-16-CE39-0014. 

References 

1.  Abiteboul, S., André, B., Kaplan, D.: Managing your digital life. CACM,58(5) (2015). 

2.  Allard, T., Hébrail, G., Pacitti, E., Masseglia, F.: Chiaroscuro: Transparency and privacy 

for massive personal time-series clustering. In: ACM SIGMOD, (2015). 

3.  Allard, T., Anciaux, N., Bouganim, L., Yanli, G., Le Folgoc, L., Nguyen, B., Pucheral, P., 

Ray, I., Yin, S.: Secure Personal Data Servers: a vision paper. In: VLDB, (2010). 

4.  Anciaux, N., Bonnet, P., Bouganim, L., Nguyen, et al.: Personal Data Management 

Systems: The Security and Functionality Standpoint. Information Systems, 80, (2019). 

 
5.  Armstrong, J., Zhu, M., Hirdes, J., Stolee, P.: K-means cluster analysis of rehabilitation 

service users in the home health care system of Ontario: Examining the heterogeneity of a 

complex geriatric population. Arch. of physical medicine and rehab., 93(12) (2012). 

6.  Backes, M., Druschel, P., Haeberlen, A., Unruh, D.: A Practical and Provable Technique 

to Make Randomized Systems Accountable. In: NDSS, 9 (2009). 

7.  Bater, J., Elliott, G., Eggen, C., Rogers, J.: SMCQL: secure query processing for private 

data networks. PVLDB, 10(6) (2017). 

8.  Chaudhry, A., Crowcroft, J., Howard, H., Haddadi, H., Howard, H., Madhavapeddy, A., 

Mortier, R.: Personal data: thinking inside the box. In: Critical Alternatives (2015). 

9.  Damgård, I., Keller, M., Larraia, E., Pastro, et al..: Practical covertly secure MPC for 

dishonest majority - or: Breaking the SPDZ limits. In: ESORICS (2013). 

10. Dwork, C.: Differential privacy. In: ICALP (2006). 

11. Elbattah, M., Molloy, O.: Clustering-Aided Approach for Predicting Patient Outcomes 

with Application to Elderly Healthcare in Ireland. In: Workshops at AAAI (2017). 

12. General Data Protection Regulation. (2016). https://gdpr-info.eu/. Accessed August, 2019 

13. Ge, T., Zdonik, S.: Answering Aggregation Queries in a Secure Model. In: VLDB (2007). 

14. Huang, Z.: Extensions to the k-means Algorithm for Clustering Large Data Sets with 

Categorical Values, pp 283–304. Data Mining and Knowledge Discovery (1998). 

15. Johnson, S., Bacsu, T., Jeffery, B., Novik, N.: No Place Like Home: A Systematic 

Review of Home Care for Older Adults. Canadian Journal on Aging, 37(4) (2018). 

16. Ladjel, R., Anciaux, N, Pucheral, P., Scerri, G.: Trustworthy Distributed Computations on 

Personal Data Using Trusted Execution Environments. In: TrustCom (2019)  

17. Liao, M., Li, Y, Kianifard, F, Obi, Z, Arcona, S.: Cluster analysis and its application to 

healthcare claims data: a study of end-stage renal disease patients who initiated 

hemodialysis. BMC Nephrology (2016). 

18. Merkle, C.: Protocols for public key cryptosystems. In: S&P (1980). 

19. De Montjoye, Y., Shmueli, E., Wang, S., Pentland, A.: OpenPDS: Protecting the privacy 

of metadata through SafeAnswers. PloS one, 9(7) (2014). 

20. Sabt, M., Achemlal, M., Bouabdallah, A.: Trusted Execution Environment: What it is, 

and what it is not. In: TrustCom/BigDataSE/ISPA (1) (2015).  

21. Tramèr, F., Zhang, F., Lin, H., Hubaux J., Juels, A., Shi, E.: Sealed-glass proofs: Using 

transparent enclaves to prove and sell knowledge. In: EuroS&P (2017). 

22. Wang, W., Chen, G., Pan, X., Zhang, Y., Wang, X., Tang, H., Gunter, A.: Leaky caul dron 

on the dark land: Understanding memory side-channel hazards in SGX. In: CCS (2017) 

23. www.inrupt.com/blog/one-small-step-for-the-web, Sept. (2018). Accessed August, 2019. 

