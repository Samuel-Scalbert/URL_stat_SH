Correlation Clustering with Adaptive Similarity Queries
Marco Bressan, Nicolo Cesa-Bianchi, Andrea Paudice, Fabio Vitale

To cite this version:

Marco Bressan, Nicolo Cesa-Bianchi, Andrea Paudice, Fabio Vitale. Correlation Clustering with Adap-
tive Similarity Queries. Conference on Neural Information Processing Systems, Dec 2019, Vancouver,
Canada. ￿hal-02376961￿

HAL Id: hal-02376961

https://inria.hal.science/hal-02376961

Submitted on 22 Nov 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

9
1
0
2

v
o
N
4

]

G
L
.
s
c
[

2
v
2
0
9
1
1
.
5
0
9
1
:
v
i
X
r
a

Correlation Clustering
with Adaptive Similarity Queries

Marco Bressan
Department of Computer Science
University of Rome Sapienza

Nicolò Cesa-Bianchi
Department of Computer Science & DSRC
Università degli Studi di Milano

Andrea Paudice
Department of Computer Science
Università degli Studi di Milano & IIT

Fabio Vitale
Department of Computer Science
University of Lille & Inria

Abstract

In correlation clustering, we are given n objects together with a binary similarity
score between each pair of them. The goal is to partition the objects into clusters
so to minimise the disagreements with the scores. In this work we investigate
correlation clustering as an active learning problem: each similarity score can be
learned by making a query, and the goal is to minimise both the disagreements
and the total number of queries. On the one hand, we describe simple active
learning algorithms, which provably achieve an almost optimal trade-off while
giving cluster recovery guarantees, and we test them on different datasets. On the
other hand, we prove information-theoretical bounds on the number of queries
necessary to guarantee a prescribed disagreement bound. These results give a rich
characterization of the trade-off between queries and clustering error.

1

Introduction

Clustering is a central problem in unsupervised learning. A clustering problem is typically represented
by a set of elements together with a notion of similarity (or dissimilarity) between them. When the
elements are points in a metric space, dissimilarity can be measured via a distance function. In more
general settings, when the elements to be clustered are members of an abstract set V , similarity is
deﬁned by an arbitrary symmetric function σ deﬁned on pairs of distinct elements in V . Correlation
Clustering (CC) [3] is a well-known special case where σ is a
1, +1
-valued function establishing
}
whether any two distinct elements of V are similar or not. The objective of CC is to cluster the points
in V so to maximize the correlation with σ. More precisely, CC seeks a clustering minimizing the
1 and belonging
number of errors, where an error is given by any pair of elements having similarity
to the same cluster, or having similarity +1 and belonging to different clusters. Importantly, there
are no a priori limitations on the number of clusters or their sizes: all partitions of V , including
the trivial ones, are valid. Given V and σ, the error achieved by an optimal clustering is known as
the Correlation Clustering index, denoted by OPT. A convenient way of representing σ is through
E iff σ(u, v) = +1. Note that OPT = 0 is equivalent to a
a graph G = (V, E) where
perfectly clusterable graph (i.e., G is the union of disjoint cliques). Since its introduction, CC has
attracted a lot of interest in the machine learning community, and has found numerous applications in
entity resolution [16], image analysis [18], and social media analysis [24]. Known problems in data
integration [13] and biology [4] can be cast into the framework of CC [25].

} ∈

u, v

{−

−

{

From a machine learning viewpoint, we are interested in settings when the similarity function σ is
not available beforehand, and the algorithm must learn σ by querying for its value on pairs of objects.
This setting is motivated by scenarios in which the similarity information is costly to obtain. For

Preprint. Under review.

 
 
 
 
 
 
example, in entity resolution, disambiguating between two entities may require invoking the user’s
help. Similarly, deciding if two documents are similar may require a complex computation, and
possibly the interaction with human experts. In these active learning settings, the learner’s goal is to
trade the clustering error against the number of queries. Hence, the fundamental question is: how
many queries are needed to achieve a speciﬁed clustering error? Or, in other terms, how close can we
get to OPT, under a prescribed query budget Q?

1.1 Our Contributions

In this work we characterize the trade-off between the number Q of queries and the clustering error
on n points. The table below here summarizes our bounds in the context of previous work. Running
time and upper/lower bounds on the expected clustering error are expressed in terms of the number of
(n2).
queries Q, and all our upper bounds assume Q = Ω(n) while our lower bounds assume Q =

O

Running time

Expected clustering error

Reference

Q + LP solver + rounding
Q
Exponential
Exponential (OPT = 0)

Unrestricted (OPT = 0)
0)
Unrestricted (OPT

(cid:29)

3(ln n + 1)OPT +

(cid:0)n5/2/√Q(cid:1)

3OPT +

O
(n3/Q)
O
(cid:0)n5/2/√Q(cid:1)
OPT +
O
(cid:0)n3/Q(cid:1)
(cid:101)
O
Ω(cid:0)n2/√Q(cid:1)
OPT + Ω(cid:0)n3/Q(cid:1)

[6]
Theorem 1 (see also [5])
Theorem 7
Theorem 7

Theorem 8
Theorem 9

O

Our ﬁrst set of contributions is algorithmic. We take inspiration from an existing greedy algorithm,
(n2) worst-case bound on the number
KwikCluster [2], that has expected error 3OPT but a vacuous
of queries. We propose a variant of KwikCluster, called ACC, for which we prove several desirable
(n3/Q), where Q = Ω(n) is
properties. First, ACC achieves expected clustering error 3OPT +
(cid:1), then it
a deterministic bound on the number of queries. In particular, if ACC is run with Q = (cid:0)n
becomes exactly equivalent to KwikCluster. Second, ACC recovers adversarially perturbed latent
clusters. More precisely, if the input contains a cluster C obtained from a clique by adversarially
perturbing a fraction ε of its edges (internal to the clique or leaving the clique), then ACC returns a
cluster (cid:98)C such that E(cid:2)
denotes symmetric difference. This
|
means that ACC recovers almost completely all perturbed clusters that are large enough to be “seen”
with Q queries. We also show, under stronger assumptions, that via independent executions of ACC
one can recover exactly all large clusters with high probability. Third, we show a variant of ACC,
called ACCESS (for Early Stopping Strategy), that makes signiﬁcantly less queries on some graphs.
For example, when OPT = 0 and there are Ω(cid:0)n3/Q(cid:1) similar pairs, the expected number of queries
made by ACCESS is only the square root of the queries made by ACC. In exchange, ACCESS
makes at most Q queries in expectation rather than deterministically.

+ n2/Q(cid:1), where

(cid:3) =
|

C
|

(cid:0)ε

(cid:98)C

O

O

⊕

⊕

C

2

|

Our second set of contributions is a nearly complete information-theoretic characterization of the
query vs. clustering error trade-off (thus, ignoring computational efﬁciency). Using VC theory,
we prove that for all Q = Ω(n) the strategy of minimizing disagreements on a random subset of
(cid:0)n5/2/√Q(cid:1), which
pairs achieves, with high probability, clustering error bounded by OPT +
(cid:0)n3/Q(cid:1) when OPT = 0. The VC theory approach can be applied to any efﬁcient
reduces to (cid:101)
O
approximation algorithm, too. The catch is that the approximation algorithm cannot ask the similarity
of arbitrary pairs, but only of pairs included in the random sample of edges. The best known
approximation factor in this case is 3(ln n + 1) [14], which gives a clustering error bound of
(cid:0)n5/2/√Q(cid:1) with high probability. This was already observed in [6] albeit in a
3(ln n + 1)OPT +
slightly different context.

O

O

We complement our upper bounds by developing two information-theoretic lower bounds; these
(n2) queries, possibly chosen in an adaptive
lower bounds apply to any algorithm issuing Q =
way. For the general case, we show that any algorithm must suffer an expected clustering error of at
least OPT + Ω(cid:0)n3/Q(cid:1). In particular, for Q = Θ(n2) any algorithm still suffers an additive error of
order n, and for Q = Ω(n) our algorithm ACC is essentially optimal in its additive error term. For
the special case OPT = 0, we show a lower bound Ω(cid:0)n2/√Q(cid:1).
Finally, we evaluate our algorithms empirically on real-world and synthetic datasets.

O

2

2 Related work

Minimizing the correlation clustering error is APX-hard [8], and the best efﬁcient algorithm found so
far achieves 2.06 OPT [9]. This almost matches the best possible approximation factor 2 achievable
via the natural LP relaxation of the problem [8]. A very simple and elegant algorithhm for approxi-
mating CC is KwikCluster [2]. At each round, KwikCluster draws a random pivot πr from V , queries
the similarities between πr and every other node in V , and creates a cluster C containing πr and all
points u such that σ(πr, u) = +1. The algorithm then recursively invokes itself on V
C. On any
instance of CC, KwikCluster achieves an expected error bounded by 3OPT. However, it is easy to
see that KwikCluster makes Θ(n2) queries in the worst case (e.g., if σ is the constant function
1).
Our algorithms can be seen as a parsimonious version of KwikCluster whose goal is reducing the
number of queries.

−

\

The work closest to ours is [5]. Their algorithm runs KwikCluster on a random subset of 1/(2ε)
Π is assigned to the cluster
nodes and stores the set Π of resulting pivots. Then, each node v
identiﬁed by the pivot π
Π with smallest index and such that σ(v, π) = +1. If no such pivot is
found, then v becomes a singleton cluster. According to [5, Lemma 4.1], the expected clustering
(cid:0)εn2(cid:1), which can be compared to our bound for ACC by setting
error for this variant is 3OPT +
Q = n/ε. On the other hand our algorithms are much simpler and signiﬁcantly easier to analyze.
This allows us to prove a set of additional properties, such as cluster recovery and instance-dependent
query bounds. It is unclear whether these results are obtainable with the techniques of [5].

O

∈

∈

V

\

Another line of work attempts to circumvent computational hardness by using the more powerful
same-cluster queries (SCQ). A same-cluster query tells whether any two given nodes are clustered
together according to an optimal clustering or not. In [? ] SCQs are used to design a FPTAS for
a variant of CC with bounded number of clusters. In [? ] SCQs are used to design algorithms for
solving CC optimally by giving bounds on Q which depend on OPT. Unlike our setting, both works
assume all (cid:0)n
(cid:1) similarities are known in advance. The work [21] considers the case in which there is
a latent clustering with OPT = 0. The algorithm can issue SCQs, however the oracle is noisy: each
query is answered incorrectly with some probability, and the noise is persistent (repeated queries give
the same noisy answer). The above setting is closely related to the stochastic block model (SBM),
which is a well-studied model for cluster recovery [1, 19, 22]. However, few works investigate SBMs
with pairwise queries [11]. Our setting is strictly harder because our oracle has a budget of OPT
adversarially incorrect answers.

2

A different model is edge classiﬁcation. Here the algorithm is given a graph
with hidden binary
labels on the edges. The task is to predict the sign of all edges by querying as few labels as
possible [6, 10, 12]. As before, the oracle can have a budget OPT of incorrect answers, or a latent
clustering with OPT = 0 is assumed and the oracle’s answers are affected by persistent noise. Unlike
correlation clustering, in edge classiﬁcation the algorithm is not constrained to predict in agreement
with a partition of the nodes. On the other hand, the algorithm cannot query arbitrary pairs of nodes
in V , but only those that form an edge in

G

.

G

{

u, v

≡ {

the set of input nodes, by

1, . . . , n
of distincts nodes in V , and by σ :

}
is a partition of V in disjoint clusters Ci : i = 1, . . . , k. Given

Preliminaries and notation. We denote by V
the set of all pairs
the binary similarity
and σ, the
function. A clustering
C
set ΓC of mistaken edges contains all pairs
1 and u, v belong to same
and all pairs
cluster of
.
is (cid:12)
(cid:12)
C
(cid:12). The correlation clustering index is OPT = minC ∆C, where the minimum
The cost ∆C of
E is an edge
. We often view V, σ as a graph G = (V, E) where
u, v
is over all clusterings
V we let G[U ] be the subgraph of G
if and only if σ(u, v) = +1. In this case, for any subset U
induced by U , and for any v

}
E → {−
such that σ(u, v) =

such that σ(u, v) = +1 and u, v belong to different clusters of

v be the neighbor set of v.

1, +1
}

V we let

E ≡

(cid:12)ΓC

u, v

u, v

} ∈

⊆

−

C

C

C

C

}

{

{

}

{

(cid:1)

(cid:0)V
2

∈

N
u, v, w

A triangle is any unordered triple T =
edge; we write e
+, +,
are
{
see that the number of edge-disjoint bad triangles is a lower bound on OPT.

a generic triangle
e. We say T is a bad triangle if the labels σ(u, v), σ(u, w), σ(v, w)
the set of all bad triangles in V . It is easy to

\
(the order is irrelevant). We denote by

V . We denote by e =

u, w
{

T and v

} ⊆

−}

⊂

∈

T

T

}

{

Due to space limitations, here most of our results are stated without proof, or with a concise proof
sketch; the full proofs can be found in the supplementary material.

3

3 The ACC algorithm

We introduce our active learning algorithm ACC (Active Correlation Clustering).

→
1) then RETURN

|

|

πr

Vr

V1

= 0

← {

r > f (
|

Algorithm 1 Invoked as ACC(V1, 1) where V1
Parameters: Query rate function f : N
N.
1: if
| −
∨
2: Draw pivot πr u.a.r. from Vr
3: Cr
4: Draw a random subset Sr of f (
Vr
|
Sr do query σ(πr, u)
5: for each u
u
Sr such that σ(πr, u) = +1 then
6: if
Query all remaining pairs (πr, u) for u
7:
Cr
8:
∪ {
←
9: Output cluster Cr
10: ACC(Vr

u : σ(πr, u) = +1
}

Cr, r + 1)

| −

Cr

∈

∈

∃

}

\

V and r = 1 is the index of the recursive call.

≡

(cid:46) Create new cluster and add the pivot to it

1) nodes from Vr

πr

}

\ {

∈

Vr

(cid:0)

\

{

(cid:46) Check if there is at least a positive edge
πr

Sr
(cid:46) Populate cluster based on queries

} ∪

(cid:1)

(cid:46) Recursive call on the remaining nodes

∈

≤

Vr

Vr

nr other nodes u (lines 4–5), where nr =

ACC has the same recursive structure as KwikCluster. First, it starts with the full instance V1 = V .
Then, for each round r = 1, 2, . . . it selects a random pivot πr
Vr, queries the similarities between
πr and a subset of Vr, removes πr and possibly other points from Vr, and proceeds on the remaining
residual subset Vr+1. However, while KwikCluster queries σ(πr, u) for all u
, ACC
queries only f (nr)
1. Thus, while KwikCluster
always ﬁnds all positive labels involving the pivot πr, ACC can ﬁnd them or not, with a probability
that depends on f . The function f is called query rate function and dictates the tradeoff between the
clustering cost ∆ and the number of queries Q, as we prove below. Now, if any of the aforementioned
f (nr) queries returns a positive label (line 6), then all the labels between πr and the remaining
u
Vr are queried and the algorithm operates as KwikCluster until the end of the recursive call;
otherwise, the pivot becomes a singleton cluster which is removed from the set of nodes. Another
important difference is that ACC deterministically stops after f (n) recursive calls (line 1), declaring
all remaining points as singleton clusters. The intuition is that with good probability the clusters not
found within f (n) rounds are small enough to be safely disregarded. Since the choice of f is delicate,
we avoid trivialities by assuming f is positive, integral, and smooth enough. Formally:
Deﬁnition 1. f : N
for all n

N is a query rate function if f (1) = 1 and f (n)

→
N. This implies f (n+k)

(cid:0)1 + 1

f (n + 1)

(cid:1)f (n)

| −

\ {

πr

≤

≤

1.

∈

∈

}

n

|

f (n)
n for all k

n+k ≤

≥

∈

We can now state formally our bounds for ACC.
Theorem 1. For any query rate function f and any labeling σ on n nodes, the expected cost E[∆A]
of the clustering output by ACC satisﬁes

E[∆A]

≤

3OPT +

1
1)

n2
f (n)

+

n
2e

.

2e
2(e

−
−

The number of queries made by ACC is deterministically bounded as Q
case f (n) = n for all n
Q

N, ACC reduces to KwikCluster and achieves E[∆A]

n2.

≤

∈

nf (n). In the special
3OPT with

≤

≤

Note that Theorem 1 gives an upper bound on the error achievable when using Q queries: since
(n3/Q). Furthermore, as one expects, if the
Q = nf (n), the expected error is at most 3OPT +
learner is allowed to ask for all edge signs, then the exact bound of KwikCluster is recovered (note
that the ﬁrst formula in Theorem 1 clearly does not take into account the special case when f (n) = n,
which is considered in the last part of the statement).

O

Vr. The essence is
Proof sketch. Look at a generic round r, and consider a pair of points
} ∈
that ACC can misclassify
1, ACC can choose as
}
pivot πr a node v such that σ(v, u) = σ(v, w) = +1. In this case, if the condition on line 6 holds,
. If instead σ(u, w) = +1,
then ACC will cluster v together with u and w, thus mistaking

in one of two ways. First, if σ(u, w) =

u, w
{

u, w
{

u, w

−

{

}

4

}

u, w
{

u, w, v
{

by pivoting on a node v such that σ(v, u) = +1 and σ(v, w) =

1,
then ACC could mistake
and clustering together only v and u. Crucially, both cases imply the existence of a bad triangle
T =
. We charge each such mistake to exactly one bad triangle T , so that no triangle is
}
charged twice. The expected number of mistakes can then be bound by 3OPT using the packing
argument of [2] for KwikCluster. Second, if σ(u, w) = +1 then ACC could choose one of them, say
u, as pivot πr, and assign it to a singleton cluster. This means the condition on line 6 fails. We can
then bound the number of such mistakes as follows. Suppose πr has cn/f (n) positive labels towards
0. Loosely speaking, we show that the check of line 6 fails with probability e−c,
Vr for some c
(cid:0)n/f (n)(cid:1)
in which case cn/f (n) mistakes are added. In expectation, this gives cne−c/f (n) =
(cid:0)n2/f (n)(cid:1). (The actual proof has to take
mistakes. Over all f (n)
into account that all the quantities involved here are not constants, but random variables).

n rounds, this gives an overall

O

O

−

≤

≥

3.1 ACC with Early Stopping Strategy

O

We can reﬁne our algorithm ACC so that, in some cases, it takes advantage of the structure of the
input to reduce signiﬁcantly the expected number of queries. To this end we see the input as a graph
G with edges corresponding to positive labels (see above). Suppose then G contains a sufﬁciently
(n2/f (n)) of edges. Since ACC deterministically performs f (n
1) rounds, it
small number
could make Q = Θ(f (n)2) queries. However, with just Q = f (n) queries one could detect that G
(n2/f (n)) edges, and immediately return the trivial clustering formed by all singletons.
contains
(n2/f (n)), i.e. the same of Theorem 1.
The expected error would obviously be at most OPT +
More generally, at each round r with f (nr) queries one can check if the residual graph contains at
least n2/f (n) edges; if the test fails, declaring all nodes in Vr as singletons gives expected additional
(n2/f (n)). The resulting algorithm is a variant of ACC that we call ACCESS (ACC with
error
Early Stopping Strategy). The pseudocode can be found in the supplementary material.

O

O

O

−

First, we show ACCESS gives guarantees virtually identical to ACC (only, with Q in expectation).
Formally:
Theorem 2. For any query rate function f and any labeling σ on n nodes, the expected cost E[∆A]
of the clustering output by ACCESS satisﬁes

E[∆A]

≤

3OPT +

n2
ef (n)

+

n
2e

.

Moreover, the expected number of queries performed by ACCESS is E[Q]

n(2f (n) + 1).

≤

Theorem 2 reassures us that ACCESS is no worse than ACC. In fact, if most edges of G belong to
relatively large clusters (namely, all but O(n2/f (n)) edges), then we can show ACCESS uses much
fewer queries than ACC (in a nutshell, ACCESS quickly ﬁnds all large clusters and then quits). The
following theorem captures the essence. For simplicity we assume OPT = 0, i.e. G is a disjoint
union of cliques.
Theorem 3. Suppose OPT = 0 so G is a union of disjoint cliques. Let C1, . . . , C(cid:96) be the cliques of
G in nondecreasing order of size. Let i(cid:48) be the smallest i such that (cid:80)i
= Ω(n2/f (n)), and
let h(n) =

(cid:0)n2 lg(n)/h(n)(cid:1) queries.

. Then ACCESS makes in expectation E[Q] =
|

ECj |

j=1 |

Ci(cid:48)

O

|

As an example, say f (n) = √n and G contains n1/3 cliques of n2/3 nodes each. Then for ACC The-
(n4/3 lg(n)).
orem 1 gives Q

(n3/2), while for ACCESS Theorem 3 gives E[Q] =

nf (n) =

≤

O

O

4 Cluster recovery

In the previous section we gave bounds on E[∆], the expected total cost of the clustering. However,
in applications such as community detection and alike, the primary objective is recovering accurately
the latent clusters of the graph, the sets of nodes that are “close” to cliques. This is usually referred to
as cluster recovery. For this problem, an algorithm that outputs a good approximation (cid:98)C of every
latent cluster C is preferrable to an algorithm that minimizes E[∆] globally. In this section we show
that ACC natively outputs clusters that are close to the latent clusters in the graph, thus acting as a
cluster recovery tool. We also show that, for a certain type of latent clusters, one can amplify the
accuracy of ACC via independent executions and recover all clusters exactly with high probability.

5

To capture the notion of “latent cluster”, we introduce the concept of (1
view V, σ as a graph G = (V, E) with e
induced by C

ε)-knit set. As usual, we
E iff σ(e) = +1. Let EC be the edges in the subgraph

−

∈

V and cut(C, C) be the edges between C and C = V
ε)-knit if (cid:12)

ε)(cid:0)|C|

V is (1

(1

C.
\
(cid:12)cut(C, C)(cid:12)
(cid:1) and (cid:12)
(cid:12)

Deﬁnition 2. A subset C

(cid:12)EC

⊆

(cid:12)
(cid:12)

⊆

−

≥

−

2

ε(cid:0)|C|

(cid:1).

2

≤

−

⊕

(cid:12) (cid:98)C

C(cid:12)
(cid:12) + (cid:12)
(cid:12)C
=
C

(cid:12) = (cid:12)
C(cid:12)
(cid:12) (cid:98)C
(cid:98)C

ε)-knit set C in the graph, a cluster (cid:98)C with

Suppose now we have a cluster (cid:98)C as “estimate” of C. We quantify the distance between C and (cid:98)C as
the cardinality of their symmetric difference, (cid:12)
(cid:98)C(cid:12)
(cid:12). The goal is to obtain,
\
) for some small ε. We
for each (1
C
⊕
|
= o(n/f (n)),
prove ACC does exactly this. Clearly, we must accept that if C is too small, i.e.
= Ω(n/f (n)), we can prove E[
then ACC will miss C entirely. But, for
).
(ε
] =
(cid:98)C
|
|
|
|
We point out that the property of being (1
ε)-knit is rather weak for an algorithm, like ACC, that is
completely oblivious to the global topology of the cluster — all what ACC tries to do is to blindly
cluster together all the neighbors of the current pivot. In fact, consider a set C formed by two disjoint
cliques of equal size. This set would be close to 1/2-knit, and yet ACC would never produce a single
cluster (cid:98)C corresponding to C. Things can only worsen if we consider also the edges in cut(C, C),
which can lead ACC to assign the nodes of C to several different clusters when pivoting on C. Hence
it is not obvious that a (1

ε)-knit set C can be efﬁciently recovered by ACC.

C
|
⊕

\
(ε
|

C
|

|
−

|
C

O

O

C

|

|

Note that this task can be seen as an adversarial cluster recovery problem. Initially, we start with a
disjoint union of cliques, so that OPT = 0. Then, an adversary ﬂips the signs of some of the edges
of the graph. The goal is to retrieve every original clique that has not been perturbed excessively.
Note that we put no restriction on how the adversary can ﬂip edges; therefore, this adversarial setting
subsumes constrained adversaries. For example, it subsumes the high-probability regime of the
stochastic block model [17] where edges are ﬂipped according to some distribution.

−

We can now state our main cluster recovery bound for ACC.

Theorem 4. For every C
+ min(cid:8) 2n
3ε

f (n) , (cid:0)1

C
|

|

−

⊆
f (n)
n

V that is (1
(cid:9) +
C
|

(cid:1)

|

−
C
|
|

ε)-knit, ACC outputs a cluster (cid:98)C such that E(cid:2)
|
e−|C|f (n)/5n.

C

(cid:98)C

⊕

(cid:3)

|

≤

The min in the bound captures two different regimes: when f (n) is very close to n, then E(cid:2)
C
|

C

(ε

) independently of the size of C, but when f (n)
|

O
must be large enough to be found by ACC.

|

(cid:28)

n we need

C

|

|

(cid:98)C
|
= Ω(n/f (n)), i.e.,

⊕

(cid:3) =
C
|

|

4.1 Exact cluster recovery via ampliﬁcation

C
|

For certain latent clusters, one can get recovery guarantees signiﬁcantly stronger than the ones given
natively by ACC (see Theorem 4). We start by introducing strongly (1
ε)-knit sets (also known as
v is the neighbor set of v in the graph G induced by the positive labels.
quasi-cliques). Recall that
Deﬁnition 3. A subset C
C and
V is strongly (1
1).

ε)-knit if, for every v

C, we have

N
⊆

v
N

ε)(

(1

−

−

⊆

∈

v

−

| −

| ≥

|N
We remark that ACC alone does not give better guarantees on strongly (1
(1
strongly (1
(cid:98)C
with

v
|
ε)-knit, and yet when pivoting on any v

ε)-knit subsets. Suppose for example that

, since the pivot has edges to less than (1
|

−
C
| ≥

ε)
|

= (1

C
|

| −

ε)(

|N

−

−

−

⊕

C

∈

ε

|

|

ε)-knit subsets than on
C. Then C is
1) for all v
C ACC will inevitably produce a cluster (cid:98)C

C
|

−

∈

other nodes of C.

{

u

∈

(cid:98)C

(cid:98)C = min

is the id of (cid:98)C, we will set id(v) = u

To bypass this limitation, we run ACC several times to amplify the probability that every node in
C is found. Recall that V = [n]. Then, we deﬁne the id of a cluster (cid:98)C as the smallest node of (cid:98)C.
The min-tagging rule is the following: when forming (cid:98)C, use its id to tag all of its nodes. Therefore,
if u
(cid:98)C. Consider now the
following algorithm, called ACR (Ampliﬁed Cluster Recovery). First, ACR performs K independent
runs of ACC on input V , using the min-tagging rule on each run. In this way, for each v
V we
obtain K tags id1(v), . . . , idK(v), one for each run. Thereafter, for each v
V we select the tag that
v has received most often, breaking ties arbitrarily. Finally, nodes with the same tag are clustered
together. One can prove that, with high probability, this clustering contains all strongly (1
ε)-knit
sets. In other words, ACR with high probability recovers all such latent clusters exactly. Formally,
we prove:

(cid:98)C for every v

−

∈

∈

∈

}

6

Theorem 5. Let ε
with probability at least 1

≤

1

10 and ﬁx p > 0. If ACR is run with K = 48 ln n
p , then the following holds
f (n) , the algorithm
C

p: for every strongly (1

ε)-knit C with

> 10 n

−

−

|

|

outputs a cluster (cid:98)C such that (cid:98)C = C.

ε)-knit
It is not immediately clear that one can extend this result by relaxing the notion of strongly (1
set so to allow for edges between C and the rest of the graph. We just notice that, in that case, every
node v
C that is smaller than every node of C. In this case,
V
∈
when pivoting on v ACC would tag v with x rather than with uC, disrupting ACR.

C could have a neighbor xv

−

∈

\

5 A fully additive scheme

In this section, we introduce a(n inefﬁcient) fully additive approximation algorithm achieving cost
OPT + n2ε in high probability using order of n
ε sufﬁces.
Our algorithm combines uniform sampling with empirical risk minimization and is analyzed using
VC theory.

ε2 queries. When OPT = 0, Q = n

ε ln 1

C

C

E → {−
u, v
}
{

∈
is P(hC(e)

1, +1
}
of distinct elements u, v
. Let

First, note that CC can be formulated as an agnostic binary classiﬁcation problem with binary
classiﬁers hC :
of V (recall that
denotes the
associated with each clustering
V ), and we assume hC(u, v) = +1 iff u and v
set of all pairs
n be the set of all such hC. The risk of a classiﬁer hC with
belong to the same cluster of
H
. It is
respect to the uniform distribution over
E
easy to see that the risk of any classiﬁer hC is directly related to ∆C, P(cid:0)hC(e)
(cid:1).
(cid:14)(cid:0)n
2
= σ(e)(cid:1). Now, it is well known —see, e.g., [23,
(cid:1) minh∈Hn
Hence, in particular, OPT = (cid:0)n
Theorem 6.8]— that we can minimize the risk to whithin an additive term of ε using the following
n, and ﬁnd
procedure: query
the clustering
n with zero
(cid:0)(d/ε) ln(1/ε)(cid:1) random queries sufﬁce. A trivial upper bound on the VC dimension of
C
risk, then
O
n is log2 |H
H
Theorem 6. The VC dimension of the class

such that hC makes the fewest mistakes on the sample. If there is h∗

(cid:0)n ln n). The next result gives the exact value.

(cid:0)d/ε2(cid:1) edges drawn u.a.r. from

= σ(e)) where e is drawn u.a.r. from

, where d is the VC dimension of

n of all partitions of n elements is n

E
= σ(e)(cid:1) = ∆C

H
∈ H

P(cid:0)h(e)

O

O

=

1.

E

E

n

2

|

H

−

H

Proof. Let d be the VC dimension of
n. We view an instance of CC as the complete graph Kn with
edges labelled by σ. Let T be any spanning tree of Kn. For any labeling σ, we can ﬁnd a clustering
of V such that h perfectly classiﬁes the edges of T : simply remove the edges with label
1 in T and
consider the clusters formed by the resulting connected components. Hence d
1 because any
1 edges. On the other hand, any set of n edges must contain at least a
spanning tree has exactly n
makes hC consistent with the labeling σ that gives positive
cycle. It is easy to see that no clustering
labels to all edges in the cycle but one. Hence d < n.

≥

−

−

−

n

C

An immediate consequence of the above is the following.
Theorem 7. There exists a randomized algorithm A that, for all 0 < ε < 1, ﬁnds a clustering
(cid:0)n2ε(cid:1) with high probability while using Q =
satisfying ∆C
O
(cid:1) queries are enough to ﬁnd a clustering
(cid:0) n
ε ln 1
OPT = 0, then Q =

(cid:1) queries. Moreover, if
C
(cid:0)n2ε(cid:1).

(cid:0) n
ε2
satisfying ∆C =

OPT +

≤

ε

O
C

O

O

6 Lower bounds

In this section we give two lower bounds on the expected clustering error of any (possibly randomized)
algorithm. The ﬁrst bound holds for OPT = 0, and applies to algorithms using a deterministically
bounded number of queries. This bound is based on a construction from [7, Lemma 11] and related
to kernel-based learning.
Theorem 8. For any ε > 0 such that 1
learning algorithm asking fewer than 1
n

ε is an even integer, and for every (possibly randomized)
50ε2 queries with probability 1, there exists a labeling σ on

ε nodes such that OPT = 0 and the expected cost of the algorithm is at least n2ε
8 .
Our second bound relaxed the assumption on OPT. It uses essentially the same construction of [5,
Lemma 6.1], giving asymptotically the same guarantees. However, the bound of [5] applies only to

ε ln 1

≥

16

7

(cid:54)
(cid:54)
(cid:54)
∈

a very restricted class of algorithms: namely, those where the number qv of queries involving any
speciﬁc node v
V is deterministically bounded. This rules out a vast class of algorithms, including
KwikCluster, ACC, and ACCESS, where the number of queries involving a node is a function of the
random choices of the algorithm. Our lower bound is instead fully general: it holds unconditionally
for any randomized algorithm, with no restriction on what or how many pairs of points are queried.
Theorem 9. Choose any function ε = ε(n) such that Ω(cid:0) 1
randomized) learning algorithm and any n0 > 0 there exists a labeling σ on n
that the algorithm has expected error E[∆]
satisﬁes E[Q] < n

N. For every (possibly
n0 nodes such
80 whenever its expected number of queries

OPT + n2ε

2 and 1

ε ∈

≤

≥

≤

≥

ε

(cid:1)

n

1

80 ε .

1, the
In fact, the bound of Theorem 9 can be put in a more general form: for any constant c
OPT + A(c) where A(c) = Ω(n2ε) is an additive term with constant
expected error is at least c
factors depending on c (see the proof). Thus, our algorithms ACC and ACCESS are essentially
optimal in the sense that, for c = 3, they guarantee an optimal additive error up to constant factors.

≥

·

7 Experiments

We tested ACC on six datasets from [21, 20]. Four of these datasets are obtained from real-world
data and the remaining two are synthetic. In Figure 2 we show our results for one real-world dataset
(cora, with 1879 nodes and 191 clusters) and one synthetic dataset (skew, with 900 nodes and 30
clusters). Similar results for the remaining four datasets can be found in the supplementary material.
Every dataset provides a ground-truth partitioning of nodes with OPT = 0. To test the algorithm for
OPT > 0, we perturbed the dataset by ﬂipping the label of each edge indipendently with probability
p (so the results for p = 0 refer to the original dataset with OPT = 0).

(a) skew.

(b) cora.

Figure 1: Clustering cost vs. number of queries. The curves show the average value of ∆. The
circular outliers mark the performance of KwikCluster.

∈ {

Figure 2 shows the measured clustering cost ∆ against the number of queries Q performed by ACC.
0, 0.1, 0.2, 0.3
For each value of p
, each curve in the plot is obtained by setting the query rate
}
f (n) to nα for 20 distinct values of α
[0, 3/4]. For each value of α we ran ACC ﬁfty times. The
∈
curve shows the average value of ∆ (standard deviations, which are small, are omitted to avoid
cluttering the ﬁgure). The performance of KwikCluster is shown by the circular marker. On both
datasets, the error of ACC shows a nice sublinear drop as the number of queries increases, quickly
approaching the performance of KwikCluster. Ignoring lower order terms, Theorem 1 gives an
expected cost bounded by about 3.8n3/Q for the case OPT = 0 (recall that OPT is unknown).
Placing this curve in our plots, shows that ACC is a factor of two or three better than the theoretical
bound (which is not shown in Figure 2 due to scaling issues).

8

0123NumberofQueries×10401234ClusteringCost×105p=0p=0.1p=0.2p=0.30246NumberofQueries×1040.000.250.500.751.001.251.501.75ClusteringCost×106p=0p=0.1p=0.2p=0.3Acknowledgments

The authors gratefully acknowledge partial support by the Google Focused Award “Algorithms and
Learning for AI” (ALL4AI). Marco Bressan and Fabio Vitale are also supported in part by the ERC
Starting Grant DMAP 680153 and by the “Dipartimenti di Eccellenza 2018-2022” grant awarded to
the Department of Computer Science of the Sapienza University of Rome. Nicolò Cesa-Bianchi is
also supported by the MIUR PRIN grant Algorithms, Games, and Digital Markets (ALGADIMAR)

References

[1] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
Fundamental limits and efﬁcient algorithms for recovery. In 2015 IEEE 56th Annual Symposium
on Foundations of Computer Science, pages 670–688. IEEE, 2015.

[2] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information:

Ranking and clustering. J. ACM, 55(5):23:1–23:27, 2008.

[3] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine learning, 56

(1-3):89–113, 2004.

[4] Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. Clustering gene expression patterns. Journal

of computational biology, 6(3-4):281–297, 1999.

[5] Francesco Bonchi, David García-Soriano, and Konstantin Kutzkov. Local correlation clustering.

arXiv preprint arXiv:1312.5105, 2013.

[6] Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and Giovanni Zappella. A correlation
clustering approach to link classiﬁcation in signed networks. In Annual Conference on Learning
Theory, volume 23, pages 34–1. Microtome, 2012.

[7] Nicolo Cesa-Bianchi, Yishay Mansour, and Ohad Shamir. On the complexity of learning with

kernels. In Conference on Learning Theory, pages 297–325, 2015.

[8] Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative

information. Journal of Computer and System Sciences, 71(3):360–383, 2005.

[9] Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near
optimal lp rounding algorithm for correlationclustering on complete and complete k-partite
graphs. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,
STOC ’15, pages 219–228, New York, NY, USA, 2015. ACM.

[10] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs
via convex optimization. The Journal of Machine Learning Research, 15(1):2213–2238, 2014.

[11] Yuxin Chen, Govinda Kamath, Changho Suh, and David Tse. Community recovery in graphs
with locality. In International Conference on Machine Learning, pages 689–698, 2016.

[12] Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit S Dhillon, and Ambuj Tewari.
Prediction and clustering in signed networks: a local to global perspective. The Journal of
Machine Learning Research, 15(1):1177–1213, 2014.

[13] William W Cohen and Jacob Richman. Learning to match and cluster large high-dimensional
In Proceedings of the eighth ACM SIGKDD international

data sets for data integration.
conference on Knowledge discovery and data mining, pages 475–480. ACM, 2002.

[14] Erik D Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in

general weighted graphs. Theoretical Computer Science, 361(2-3):172–187, 2006.

[15] Devdatt Dubhashi and Alessandro Panconesi. Concentration of Measure for the Analysis of
Randomized Algorithms. Cambridge University Press, New York, NY, USA, 1st edition, 2009.

[16] Lise Getoor and Ashwin Machanavajjhala. Entity resolution: theory, practice & open challenges.

Proceedings of the VLDB Endowment, 5(12):2018–2019, 2012.

9

[17] Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels:
First steps. Social Networks, 5(2):109 – 137, 1983. ISSN 0378-8733. doi: https://doi.org/10.
1016/0378-8733(83)90021-7. URL http://www.sciencedirect.com/science/article/
pii/0378873383900217.

[18] Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and Chang D Yoo. Higher-order
correlation clustering for image segmentation. In Advances in neural information processing
systems, pages 1530–1538, 2011.

[19] Laurent Massoulié. Community detection thresholds and the weak ramanujan property. In
Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 694–703.
ACM, 2014.

[20] Arya Mazumdar and Barna Saha. Query complexity of clustering with side information. In

Advances in Neural Information Processing Systems 30. 2017.

[21] Arya Mazumdar and Barna Saha. Clustering with noisy queries.

In Advances in Neural

Information Processing Systems, pages 5788–5799, 2017.

[22] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture.

Combinatorica, 38(3):665–708, 2018.

[23] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory
to Algorithms. Cambridge University Press, New York, NY, USA, 2014. ISBN 1107057132,
9781107057135.

[24] Jiliang Tang, Yi Chang, Charu Aggarwal, and Huan Liu. A survey of signed network mining in

social media. ACM Computing Surveys (CSUR), 49(3):42, 2016.

[25] Anthony Wirth. Correlation clustering. In Claude Sammut and Geoffrey I Webb, editors,
Encyclopedia of machine learning and data mining, pages 227–231. Springer US, 2010.

10

A Probability bounds

We give Chernoff-type probability bounds can be found in e.g. [15] and that we repeatedly use in
our proofs. Let X1, . . . , Xn be binary random variables. We say that X1, . . . , Xn are non-positively
correlated if for all I

we have:

1, . . . , n

P[
i
∀

∈

⊆ {
I : Xi = 0]

}
(cid:89)

≤

i∈I

P[Xi = 0]

and P[
i
∀

∈

I : Xi = 1]

(cid:89)

i∈I

≤

P[Xi = 1]

(1)

The following holds:
Lemma 1. Let X1, . . . , Xn be independent or, more generally, non-positively correlated binary
random variables. Let a1, . . . , an

i=1 aiXi. Then, for any δ > 0, we have:

[0, 1] and X = (cid:80)n

∈

δ)E[X]] < e− δ2
P[X < (1
P[X > (1 + δ)E[X]] < e− δ2

−

2

2+δ

E[X]

E[X]

(2)

(3)

B Supplementary Material for Section 3

B.1 Pseudocode of ACC

→
1) then RETURN

|

|

}

πr

Vr

V1

= 0

← {

r > f (
|

Algorithm 2 Invoked as ACC(V1, 1) where V1
Parameters: Query rate function f : N
N.
1: if
| −
∨
2: Draw pivot πr u.a.r. from Vr
3: Cr
4: Draw a random subset Sr of f (
Vr
|
Sr do query σ(πr, u)
5: for each u
u
Sr such that σ(πr, u) = +1 then
6: if
Query all remaining pairs (πr, u) for u
7:
Cr
8:
∪ {
←
9: Output cluster Cr
10: ACC(Vr

u : σ(πr, u) = +1
}

Cr, r + 1)

| −

Cr

∈

∈

∃

\

V and r = 1 is the index of the recursive call.

≡

(cid:46) Create new cluster and add the pivot to it

1) nodes from Vr

πr

}

\ {

Vr

∈

(cid:0)

\

{

πr

} ∪

(cid:1)

(cid:46) Check if there is at least an edge
Sr
(cid:46) Populate cluster based on queries

(cid:46) Recursive call on the remaining nodes

B.2 Proof of Theorem 1

We refer to the pseudocode of ACC ( Algorithm 2). We use Vr to denote the set of remaining nodes
at the beginning of the r-th recursive call. Hence V1 = V . If the condition in the if statement on
line 6 is not true, then Cr is a singleton cluster. We denote by Vsing the set nodes that are output as
singleton clusters.
Let ΓA be the set of mistaken edges for the clustering output by ACC and let ∆A = (cid:12)
(cid:12)
(cid:12) be the cost
of this clustering. Note that, in any recursive call, ACC misclassiﬁes an edge e =
if and only
if e is part of a bad triangle whose third node v is chosen as pivot and does not become a singleton
cluster, or if σ(e) = +1 and at least one of u, w becomes a singleton cluster. More formally, ACC
misclassiﬁes an edge e =

if and only if one of the following three disjoint events holds:

(cid:12)ΓA
u, w

u, w

{

}

}

{
f (n

≤

B1(e): There exists r

1) and a bad triangle T

v

Vsing.

(cid:54)∈
B2(e): There exists r
B3(e): The algorithm stops after f (n

f (n

−

≤

−
1) such that u, w

−

+1.

u, v, w

≡ {

} ⊆

Vr such that πr = v and

Vsing.
1) calls without removing neither u nor w, and σ(u, w) =

Vr with σ(u, w) = +1 and πr

u, w

∈ {

} ∩

∈

Therefore the indicator variable for the event “e is mistaken” is:

I

e

{

∈

ΓA

}

= I

B1(e)
{

}

+ I

{

B2(e)

+ I

}

B3(e)
}
{

11

The expected cost of the clustering is therefore:

E[∆A] =

(cid:88)

e∈E

P(B1(e)) +

P(B2(e)) +

(cid:88)

e∈E

P(B3(e))

(cid:88)

e∈E

(4)

We proceed to bound the three terms separately.

Bounding (cid:80)
. Note that, if B1(e) occurs, then T
}
is unique, i.e. exactly one bad triangle T in V satisﬁes the deﬁnition of B1(e). Each occurrence of
B1(e) can thus be charged to a single bad triangle T . We may thus write

P(B1(e)). Fix an arbitrary edge e =

u, w
{

e∈E

(
∃

{

r)(

T

∃

∈ T

) : T

Vr

e

T

πr

T

e

\

∈

∧

∧

πr

(cid:54)∈

⊂

∧

⊆

Vsing

}

(cid:88)

I

e∈E

B1(e)
}
{

=

=

≤

(cid:88)

I

e∈E
(cid:88)

T ∈T
(cid:88)

T ∈T

I

I

r) : T

(
∃

{

Vr

πr

T

∈

∧

∧

πr

(cid:54)∈

⊆

Vsing

}

AT

{

}

∈

∧

⊆

πr

Vr

(cid:8)(
T (cid:9). Let us then bound (cid:80)
r) : T
∃
≡
T (cid:48)
. We use the following fact extracted from the proof of [2, Theorem 6.1]. If
is a set of weights on the bad triangles such that (cid:80)
∈
}
T ∈T (e) βT
,
∈ T }
∈ E
, let FT (e) be the event corresponding to T being
OPT. Given e
≤
e for some r. Now if FT (e) holds

≡
: e
0 : T
T ∈T βT

where AT
T (cid:48)
{
∈ T
βT
≥
{
then (cid:80)
and T
∈ E
(e) such that T
the ﬁrst triangle in the set
then AT holds and no other AT (cid:48) for T (cid:48)
(cid:88)

\
holds. Therefore

Vr and πr
T

P(AT ). Let

∈ T
∈
(e)

1 for all e

(e)

\ {

T ∈T

≤

∈

T

T

T

∈ T
I

AT

{

∧

}
FT (e)
}

= 1 .

T ∈T (e)

If AT holds for some r0, then it cannot hold for any other r > r0 because πr0 ∈
all r > r0 we have πr0 (cid:54)∈
too, then it holds for the same r0 by construction. This implies that P(cid:0)FT (e)
ACC chooses the pivot u.a.r. from the nodes in Vr0 . Thus, for each e

T implies that for
Vr. Hence, given that AT holds for r0, if FT (e) holds
3 because

|
E we can write

Vr implying T

(cid:1) = 1

AT

(cid:54)⊆

∈

1 =

(cid:88)

P(cid:0)AT

T ∈T (e)

∧

FT (e)(cid:1) =

(cid:88)

P(cid:0)FT (e)

T ∈T (e)

AT

(cid:1)P(AT ) =

(cid:88)

T ∈T (e)

|

1
3

P(AT ) .

(5)

T ∈T

P(AT ) we get (cid:80)

P(AT )
Choosing βT = 1
3
In the proof of KwikCluster, the condition (cid:80)
1 was ensured by considering events
GT (e) = AT
are disjoint,
∈ T
(e) whose node opposite to e is chosen
because GT (e) holds iff T is the ﬁrst and only triangle in
as pivot. For ACC this is not true because a pivot can become a singleton cluster, which does not
cause e

ΓA. Indeed, in KwikCluster the events

≤
T ∈T (e) βT

GT (e) : T
{

ΓA necessarily to hold.

3OPT.

(e)

≤

∧

∈

T

}

e

∈

Bounding (cid:80)

e∈E

P(B2(e)). For any u

Vr, let d+

r (u) = (cid:12)
(cid:12)

v
{

Vr : σ(u, v) = +1
}

∈

(cid:12)
(cid:12). We have:

(cid:88)

I

e∈E

B2(e)
}

{

=

1
2

∈
f (n−1)
(cid:88)

(cid:88)

u∈V

r=1

I

πr = u
{

∧

πr

∈

Vsing

}

d+
r (u) .

Taking expectations with respect to the randomization of ACC,

P(cid:0)B2(e)(cid:1) =

(cid:88)

e∈E

=

1
2

1
2

(cid:88)

f (n−1)
(cid:88)

(cid:104)
E

I

u∈V

r=1

(cid:88)

f (n−1)
(cid:88)

(cid:104)
E

I

u∈V

r=1

πr = u
{

∧

πr

∈

Vsing

}

(cid:105)
d+
r (u)

πr
{

∈

Vsing

}

d+
r (u)

(cid:12)
(cid:105)
(cid:12)
(cid:12) πr = u

P(πr = u)

12

For any round r, let Hr−1 be the sequence of random draws made by the algorithm before round
r. Then P(cid:0)πr
1 and
d−
r (u) < f (nr). Otherwise,

r (u) = 0 if either d+

(cid:12)
(cid:12) πr = u, Hr−1

r (u) = 0, or d+

r (u)

(cid:1)d+

Vsing

≥

∈

P(cid:0)πr

∈

Vsing

(cid:12)
(cid:12) πr = u, Hr−1

f (nr)−1
(cid:89)

(cid:1) =

j=0

where the inequality holds because d−

r (u)

(cid:104)
E

I

πr
{

∈

Vsing

}

d+
r (u)

d−
r (u)
nr

j
−
j ≤

−

(cid:18) d−
r (u)
nr

(cid:19)f (nr)
=

(cid:18)

1

d+
r (u)
nr

−

(cid:19)f (nr)

(6)

≤

(cid:12)
(cid:12)
(cid:12) πr = u, Hr−1

nr. Therefore, when d+
r (u)
≥
(cid:12)
(cid:12) πr = u, Hr−1

Vsing

1 and d−

r (u)
(cid:1)d+

≥
r (u)

(cid:105)

f (nr),

= P(cid:0)πr
(cid:18)
1

=

∈
d+
r (u)
nr

−

(cid:19)f (nr)

d+
r (u)

(cid:19)f (nr)

d+
r (u)
nr
d+
r (u)f (nr)
nr
z f (nr)
nr

−

(cid:18)

−

d+
r (u)

(cid:19)

d+
r (u)

(cid:19)

z

(cid:18)
1

=

−
(cid:18)

exp

≤

≤

≤

exp

max
z>0
nr
ef (nr)

.

Combining with the above, this implies

P(cid:0)B2(e)(cid:1)

(cid:88)

e∈E

where we used the facts that nr

1
2e

f (n−1)
(cid:88)

r=1

E

(cid:21)

(cid:20) nr
f (nr)

≤

1
2e

f (n−1)
(cid:88)

r=1

n
f (n)

<

n
2e

n and the properties of f .

≤

≤

Bounding (cid:80)
e∈E
Vﬁn
> 1 (so that there is at least a query left). Let nﬁn =
assume
|
|
ﬁn(u) = (cid:12)
let d+
v
(cid:12)
{
for any r > f (n

P(B3(e)). Let Vﬁn be the remaining vertices in Vr after the algorithm stops and
Vﬁn,
Vﬁn

1 and, for any u
|
(cid:12)
(cid:12). In what follows, we conventionally assume Vr

∈
≡

Vﬁn

| −

ﬁn. We have

Vﬁn : σ(u, v) = +1
∈
1), and similarly for nﬁn and d+
−
1
2

d+
ﬁn(u)

(cid:88)

(cid:88)

1
2

(cid:32)

}

≤

nﬁn
f (nﬁn)

(cid:88)

I

B3(e)
}

{

=

(cid:88)

+

(cid:26)

I

d+
ﬁn(u) >

(cid:27)

(cid:33)

d+
ﬁn(u)

.

nﬁn
f (nﬁn)

≤

e∈E

f (n

u∈Vfin

u∈Vfin

1). Given any vertex v

u∈Vfin
Vr with d+
Fix some r
r (v)
event when at round r, ACC queries σ(v, u) for all u
Vr
Sr = (cid:80)
d+
r (u) with Sr = Sﬁn for all r > f (n), and let δr = nr
be the number of nodes that are removed from Vr at the end of the r-th recursive call. Then
(cid:26)

nr
f (nr) , let Er(v) be the
Introduce the notation
nr+1

r (u) > nr
d+
f (nr)

≥
v
.
}
\ {

u∈Vr

−

−

(cid:27)

(cid:111)

∈

∈

(cid:110)

I

I

Er(πr)
{

}

d+
r (πr)

I

d+
r (πr) >

≥

nr
f (nr)

I

Er(πr)
}

{

d+
r (πr)

δr

≥

and

E[δr

|

Hr−1]

≥

(cid:26)

(cid:88)

I

v∈Vr

d+
r (v) >

(cid:27)

nr
f (nr)

P(cid:0)Er(v)

|

πr = v, Hr−1

(cid:1)P(πr = v

Hr−1)d+

r (v) .

|

Using the same argument as the one we used to bound (6),

P(cid:0)Er(v)

|
and P(πr = v

πr = v, Hr−1

1

(cid:1)

−

≥
nr+1 for any v
(cid:18)

Hr−1) = 1

|
E[δr

Hr−1]

1

−

≥

|

(cid:18)

1

d+
r (v)
nr

−

(cid:19)f (nr)

(cid:18)
1

1
f (nr)

−

1

−

≥

(cid:19)f (nr)

1

−

≥

1
e

Vr, we may write
(cid:18)

Hr−1]

∈
(cid:19) E[Sr

|
nr + 1

1

−

≥

1
e

(cid:19) E[Sr

1
e

Hr−1]
|
n

.

13

n

1 and Sr is monotonically nonincreasing in r.

E[Sr]

f (n

(cid:18)

1)

(cid:19)

1
e

1

−

E[Sﬁn]

−
n

≥

Observe now that (cid:80)f (n−1)
Thus

r=1

δr

n1

≤

−
≤
(cid:19) f (n)
(cid:88)

nﬁn

−
(cid:18)

1

1
e

−
(cid:0) e

e−1

n

1

−

≥

f (n−1)
(cid:88)

r=1

E[δr]

1
n

≥
(cid:1) n(n−1)

≤
P(cid:0)B3(e)(cid:1)

(cid:0) e

e−1
(cid:32)

1
2

≤

f (n−1) ≤
(cid:88)

E

(cid:20) nﬁn
f (nﬁn)

u∈Vfin

which implies E[Sﬁn]

(cid:88)

e∈E

as claimed.

r=1
(cid:1) n2
f (n) . So we have
(cid:21)

(cid:33)

+ E[Sﬁn]

1
2

(cid:18) n2
f (n)

+

e

(cid:19)

n2
f (n)

1

e

−

≤

Bounding the number of queries.
f (nr)
queries is at most max (cid:8)n, f (n)(cid:9)f (n)

≤

Vsing and
f (n) queries otherwise. Since the number of rounds is at most f (n), the overall number of

In round r, ACC asks nr

n queries if πr

≤

(cid:54)∈

nf (n).

≤

∈

\ {

KwikCluster as special case. When f (r) = r for all r, ACC issues all queries σ(πr, u) for
u
in each round r, and builds a cluster just like KwikCluster would. At the end of
πr
Vr
}
1 rounds, there can be at most a single node left, which is then declared a
V
f (n) = n =
|
singleton cluster. Hence, ACC and KwikCluster behaves identically for any sequence of pivot draws.
Moreover, it is easy to check that the events B2(e) and B3(e) can never occur when f (n) = n.
Therefore, the only contribution to ∆A is (cid:80)
T ∈T AT which is bounded by 3OPT for any choice of
f .

| −

B.3 Pseudocode of ACCESS

V and r = 1 is the index of the recursive call.

≡

(cid:1)f (n)/n2 edges chosen u.a.r. from (cid:0)Vr

(cid:1)

2

2

→

N.

Vr as singleton

STOP and declare every v

Algorithm 3 Invoked as ACCESS(V1, 1) where V1
Parameters: Query rate function f : N
1: Sample the labels of (cid:0)|Vr|
2: if no label is positive then
3:
4: Draw pivot πr u.a.r. from Vr
5: Cr
6: Draw a random subset Sr of f (
Vr
|
7: for each u
Sr do query σ(πr, u)
Sr such that σ(πr, u) = +1 then
u
8: if
Query all remaining pairs (πr, u) for u
9:
Cr
10:
∪ {
11: Output cluster Cr
12: ACCESS(Vr

u : σ(πr, u) = +1
}

Cr, r + 1)

← {

| −

Cr

πr

←

Vr

∈

∈

∈

∈

∃

}

\

(cid:46) Create new cluster and add the pivot to it

1) nodes from Vr

πr

}

\ {

(cid:0)

πr
{

\

} ∪

(cid:1)

(cid:46) Check if there is at least an edge
Sr
(cid:46) Populate cluster based on queries

(cid:46) Recursive call on the remaining nodes

B.4 Proof of Theorem 2

We refer to the pseudocode of ACCESS (Algorithm 3).

Let Gr be the residual graph at round r. The total cost of the clustering produced by ACCESS is
clearly bounded by the sum of the cost of ACC without round restriction, plus the number of edges
in the residual graph Gr if r is the round at which ACCESS stops. The ﬁrst term is, in expectation,
at most 3OPT + n/2e, as one can easily derive from the proof of Theorem 1. For the second term
note that, if Gr contains k edges, then the probability that ACCESS stops is at most:

(cid:33)(|Vr |

2 )f (n)/n2

(cid:32)
1

k
(cid:0)|Vr|
2

(cid:1)

−

e−kf (n)/n2

≤

Thus the expected number of edges in the residual graph when ACCESS returns is bounded from
above by maxk≥1(ke−kf (n)/n2

)

n2
ef (n) .

≤

14

Let us then move to the bound on the number of queries. The queries performed at line 1 are
deterministically at most nf (n). Concerning the other queries (line 7 and line 9), we divide the
algorithm in two phases: the “dense” rounds r where Gr still contains at least n2/2f (n) edges, and
the remaining “sparse” rounds where Gr contains less than n2/2f (n) edges.
Consider ﬁrst a “dense” round r. We see Gr as an arbitrary ﬁxed graph: for all random variables
mentioned below, the distribution is thought solely as a function of the choices of the algorithm in the
current round (i.e., the pivot node πr and the queried edges). Now, let Qr be the number of queries
performed at lines 7 and 9), and Rr =
be the number of nodes removed. Let πr be
Vr+1
the pivot, and let Dr be its degree in Gr. Let Xr be the indicator random variable of the event that
σ(πr, u) = +1 for some u

| − |
Sr. Observe that:

Vr
|

|

Vr
1) + Xr(
|

1)

| −

and

Rr = 1 + Xr Dr

f (

Thus E[Qr]
Vr
|
increasing in Dr, so E[XrDr] = E[Xr]E[Dr] + Cov(Xr, Dr)
hypothesis E[Dr]

, while E[Rr] = 1+E[XrDr]. However, Xr is monotonically
|
E[Xr]E[Dr]. Moreover, by
Vr

n/f (n). Thus:

≥

≤

∈
Vr

Qr

≤

f (
|

| −
1)+E[Xr]

Vr
|
|−
2(cid:0)n2/2f (n)(cid:1)/
|

≥

| ≥

E[Rr]

1 + E[Xr]E[Dr]

≥
But then, since obviously (cid:80)

1 + E[Xr]

≥

n

f (n) ≥

1 + E[Xr]

Vr
|
Vr

|
f (
|

) ≥
|

E[Qr]
Vr
f (
|

|

) ≥

E[Qr]
f (n)

n:

≤

r Rr
(cid:34)

E

(cid:88)

Qr

r dense

(cid:35)

f (n)E

≤

(cid:34)

(cid:35)

(cid:88)

Rr

r dense

nf (n)

≤

Consider now the “sparse” rounds, where Gr contains less than n2/2f (n) edges. With probability at
least 1/2 ACCESS ﬁnds no edge and thus stops right after lines 1–2. Hence the number of sparse
rounds completed by ACCESS is at most one in expectation; the corresponding expected number of
queries is then at most n.

B.5 Proof of Theorem 3

(n2/f (n)) edges, from r onwards ACCESS
First of all, note that if the residual graph Gr contains
stops at each round independently with constant probability. The expected number of queries
(n), and the expected error incurred is obviously at most
performed before stopping is therefore

O

(n2/f (n)).

O

O
We shall then bound the expected number of queries required before the residual graph contains
(n2/f (n)) edges. In fact, by deﬁnition of i(cid:48), if ACCESS removes Ci(cid:48), . . . , C(cid:96), then the residual
(n2/f (n)) edges. We therefore bound the expected number of queries before

O
graph contains
Ci(cid:48), . . . , C(cid:96) are removed.

O

j=1

(cid:0)Cj
2

First of all recall that, when pivoting on a cluster of size c, the probability that the cluster is not
removed is at most e−cf (n)/n. Thus the probability that the cluster is not removed after Ω(c) of
its nodes have been used as pivot is e−Ω(c2)f (n)/n. Hence the probability that any of Ci(cid:48), . . . , C(cid:96)
is not removed after Ω(c) of its nodes are used as pivot is, setting c = Ω(cid:0)h(n)(cid:1) and using a
union bound, at most p = ne−Ω(h(n)2)f (n)/n. Observe that h(n) = Ω(cid:0)n/f (n)(cid:1), for otherwise
(cid:80)i(cid:48)
ne−Ω(h(n)). Note also that we can
(cid:0)ne−ω(ln n)(cid:1) =
assume h(n) = ω(ln n), else the theorem bound is trivially O(n2). This gives p =
o(cid:0)1/ poly(n)(cid:1). We can thus condition on the events that, at any point along the algorithm, every
cluster among Ci(cid:48), . . . , C(cid:96) that is still in the residual graph has size Ω(cid:0)h(n)(cid:1); the probability of any
(p), which can be ignored.
other event changes by an additive

(cid:1) = o(cid:0)n2/f (n)(cid:1), a contradiction. Therefore p

i(cid:48) + 1, and suppose at a generic point k(cid:48)

O
Let now k = (cid:96)
k of the clusters Ci(cid:48), . . . , C(cid:96) are
(cid:0)n/k(cid:48)h(n)(cid:1) rounds in
in the residual graph. Their total size is therefore Ω(cid:0)k(cid:48)h(n)(cid:1). Therefore
expectation are needed for the pivot to fall among those clusters. Each time this happens, with
e−Ω(h(n))f (n)/n = Ω(1) the cluster containing the pivot is removed. Hence, in
probability 1
(cid:0)n/k(cid:48)h(n)(cid:1) rounds. By summing
expectation a new cluster among Ci(cid:48), . . . , C(cid:96) is removed after

O

O

≤

≤

−

−

O

15

over all values of k(cid:48), the number of expected rounds to remove all of Ci(cid:48), . . . , C(cid:96) is

(cid:32) k

(cid:88)

O

k(cid:48)=1

(cid:33)

n
k(cid:48)h(n)

=

O

(cid:0)n(ln n)/h(n)(cid:1)

Since each round involves

O

(n) queries, the bound follows.

C Supplementary Material for Section 4

C.1 Proof of Theorem 4

Fix any C that is (1

ε)-knit. We show that ACC outputs a (cid:98)C such that

E(cid:2)
|

(cid:98)C

C

(cid:3)
|

∩

≥

max

(cid:19)
ε

5
2

C

|

| −

2

n
f (n)

,

1

−

(cid:18) f (n)

n −

5
2

ε

(cid:19)

(cid:27)

C
|

|

and E(cid:2)
|

(cid:98)C

C

(cid:3)
|

∩

≤

ε
2 |

C

|

(7)

−
(cid:26)(cid:18)

|

|

|

2

2

∩

C

C

−

(cid:98)C

(1

| ≥

EC

ε)(cid:0)|C|

= (cid:0)|C|

(cid:3) from above. Finally, we add the

∩
(cid:1), and bound E(cid:2)
|

(cid:3) for KwikCluster assuming
|

One can check that these two conditions together imply the ﬁrst two terms in the bound. We start by
deriving a lower bound on E(cid:2)
(cid:1). Along the way we
(cid:98)C
|
introduce most of the technical machinery. We then port the bound to ACC, relax the assumption to
e−|C|f (n)/5n part
EC
|
C
of the bound. To lighten the notation, from now on C denotes both the cluster and its cardinality
.
|

|
For the sake of analysis, we see KwikCluster as the following equivalent process. First, we draw a
random permutation π of V . This is the ordered sequence of candidate pivots. Then, we set G1 = G,
Gi, then πi is used as an actual pivot; in
and for each i = 1, . . . , n we proceed as follows. If πi
this case we let Gi+1 = Gi
Gi,
N
then we let Gi+1 = Gi. Hence, Gi is the residual graph just before the i-th candidate pivot πi is
processed. We indicate the event πi
Pi = I

v is the set of neighbors of v. If instead πi /
∈

Gi by the random variable Pi:

πi) where

}
More in general, we deﬁne a random variable indicating whether node v is “alive” in Gi:

πi is used as pivot
}
{

∈
πi
{

C
|

= I

∪ N

(πi

Gi

(8)

∈

∈

\

|

X(v, i) = I

v
{

∈

Gi

}

= I (cid:8)v /

∈ ∪

j<i : Pj =1 (πj

πj )(cid:9)

∪ N

(9)

i : πi
Let iC = min
{

C

}

∈

be the index of the ﬁrst candidate pivot of C. Deﬁne the random variable:

SC =

C
|

GiC |

∩

=

(cid:88)

v∈C

X(v, iC)

(10)

In words, SC counts the nodes of C still alive in GiC . Now consider the following random variable:
S = PiC ·

(11)

SC

|

|

|

∩

∩

∩

∩

C

(cid:98)C

(cid:98)C

(cid:98)C

| ≥

| ≥

∩
(cid:3)

C
|

C
|

| ≥
(cid:98)C
∩

S.
PiC SC = S. If instead PiC = 0, then
E[S].

(cid:98)C
GiC , so
| ≥
S, and E(cid:2)
0. Hence in any case
C
C
|
|
(cid:3) from below by bounding E[S] from below.

Let (cid:98)C be the cluster that contains πiC in the output of KwikCluster. It is easy to see that
Indeed, if PiC = 1 then (cid:98)C includes C
(cid:98)C
S = 0 and obviously
∩
Therefore we can bound E(cid:2)
C
|
Before continuing, we simplify the analysis by assuming KwikCluster runs on the graph G after
all edges not incident on C have been deleted. We can easily show that this does not increase S.
First, by (9) each X(v, iC) is a nonincreasing function of
. Second, by (10) and (11),
}
S is a nondecreasing function of
. Hence, S is a nonincreasing function of
∈
. Now, the edge deletion forces Pi = 1 for all i < iC, since any πi : i < iC has no
Pi : i < iC
{
}
neighbor πj : j < i. Thus the edge deletion does not increase S (and, obviously, E[S]). We can then
assume G[V
C] is an independent set. At this point, any node not adjacent to C is isolated and
can be ignored. We can thus restrict the analysis to C and its neighborhood in G. Therefore we let
C =
We turn to bounding E[S]. For now we assume G[C] is a clique; we will then relax the assumption to
(cid:1). Since by hypothesis cut(C, C) < εC 2, the average degree of the nodes in C is
EC
|
less than εC 2/C. This is also a bound on the expected number of edges between C and a node drawn

denote both the neighborhood and the complement of C.

Pi : i < iC
{

X(v, iC) : v

C, v /
∈

v :
{

ε)(cid:0)C

E, u

u, v

} ∈

| ≥

(1

≥

−

C

C

∈

{

\

{

}

}

2

16

1 = i the nodes π1, . . . , πiC −1 are indeed
u.a.r. from C. But, for any given i, conditioned on iC
drawn u.a.r. from C, and so have a total of at most iεC 2/C edges towards C in expectation. Thus,
over the distribution of π, the expected number of edges between C and π1, . . . , πiC −1 is at most:

−

n
(cid:88)

i=0

iεC 2
C

P(iC

−

1 = i) =

εC 2
C

E[iC

1] =

−

εC 2
C

C
C + 1

< εC

(12)

−

where we used the fact that E[iC
1] = C/(C + 1). Now note that (12) is a bound on C
expected number of nodes of C that are adjacent to π1, . . . , πiC −1. Therefore, E[SC]
Recall that PiC indicates whether πiC is not adjacent to any of π1, . . . , πiC −1. Since the distribution
of πiC is uniform over C, P(PiC |
SC] = (SC)2/C,
and thus E[S] = E(cid:2)(SC)2(cid:3)/C. Using E[SC]
ε)C and invoking Jensen’s inequality we obtain
E[SC]2

SC) = SC/C. But S = PiC SC, hence E[S

E[SC], the
ε)C.
(1

−
≥

(1

−

−

≥

|

(13)

which is our bound on E(cid:2)
|

C

(cid:98)C

|

∩

E[S]

C ≥
≥
−
(cid:3) for KwikCluster.

(1

ε)2C

(1

2ε)C

≥

−

−

1)
1 queries, and that ACC stops after
1) rounds. We start by addressing the ﬁrst issue, assuming for the moment ACC has no

Let us now move to ACC. We have to take into account the facts that ACC performs f (
|
queries on the pivot before deciding whether to perform
f (n
restriction on the number of rounds.
Recall that P(PiC |
is easy to check that the probability that ACC ﬁnds some of them is at least 1
this event occurs, then S = SC. Thus

1 edges incident on πiC . It
e−f (n) SC −1
and, if

SC) = SC/C. Now, if PiC = 1, then we have SC

Gr
|

| −

| −

Gr

−

−

n

E[S

SC] = P(PiC |

|

SC)SC

(cid:16)

1

−

≥

e−f (n) SC −1

n

(cid:17) S2
C
C ≥

S2
C
C −

SC

2n
f (n)C

(14)

where we used the facts that for SC
≤
for x > 0, and that 1/x < 2/(x + 1) for all x
inequality and an application of E[SC]

(1

≥
ε)C, give

≥

−

1 the middle expression in (14) vanishes, that e−x < 1/x
2. Simple manipulations, followed by Jensen’s

E[S]

n
f (n)
(cid:1). To this end note that, since at most ε(cid:0)C
(cid:1)
We next generalize the bound to the case EC
(1
−
edges are missing from any subset of C, then any subset of SC nodes of C has average degree at least

f (n)C ≥
ε)(cid:0)C

ε)2C

2ε)C

(15)

ε)C

2n

(1

(1

(1

−

−

−

−

≥

≥

−

2

2

2

(cid:26)

max

0, SC

(cid:27)

(cid:18)C
2

(cid:19) 2ε
SC

1

−

−

SC

≥

−

1)

εC(C

−
2SC

1

−

We can thus re-write (14) as

E[S

SC]

|

≥

(cid:16)

1

SC
C

−

e−f (n) SC −1

n

(cid:17) (cid:18)

SC

−

εC(C

−
2SC

(cid:19)

1)

(16)

(17)

Standard calculations show that this expression is bounded from below by S2
C −
which by calculations akin to the ones above leads to E[S]
2 n
f (n) .

C

(cid:0) f (n)

Similarly, we can show that E[S]
≥
the remaining cluster nodes are found with probability at least f (n)
when such a probability is indeed 1). In (14), we can then replace 1
leads to E[S]

−
2 ε(cid:1)C. This proves the ﬁrst inequality in (7).

n −

(cid:0) f (n)

5

5

−

≥

(1

5
2 ε)C
2 ε(cid:1)C. To this end note that when ACC pivots on πiC all
1,
n , which

n (this includes the cases SC

n with f (n)

e−f (n) SC −1

−

≤

SC

2n
f (n)C −

εC
2 ,

For the second inequality in (7), note that any subset of SC nodes has cut(C, C)
is be incident to at most ε
SC
ACC assigns to (cid:98)C, as a function of SC, can thus be bounded by SC
C

(cid:1). Thus, πiC
(cid:1) such edges in expectation. The expected number of nodes of C that

(cid:1) < ε

ε(cid:0)C

2 C.

(cid:0)C
2

(cid:0)C
2

≤

2

ε
SC

≥

n −

17

(Ce−Cf (n)/n) part of the bound is concerned, simply note that the bounds obtained
As far as the
O
1), in which case ACC stops before ever reaching the ﬁrst node of C.
so far hold unless iC > f (n
C
1) is the event that no node
If this happens, (cid:98)C =
(cid:98)C
πiC }
|
of C is drawn when sampling f (n
1) nodes from V without replacement. We can therefore apply
Chernoff-type bounds to the random variable X counting the number of draws of nodes of C and get
P(cid:0)X < (1
/n, and
exp(
we have to bound the probability that X equals 0 < (1

β2E[X]/2(cid:1) for all β > 0. In our case E[X] = f (n

. The event iC > f (n
|

β)E[X])

⊕
−

−
and

C
|

1)

<

−

−

−

≤

−

C

{

|

|

|

P(X = 0)

(cid:19)

(cid:18)

β2E[X]
2

−

exp

≤

= exp

−

(cid:19)

1)

C
|

|

−
2n

−

β)E[X]. Thus
(cid:18)
β2f (n

−

1)

Since f (n
≥
P(X = 0) < exp (cid:0)
E[
].
C
|

(cid:98)C

⊕

|

f (n)/2 (otherwise n = 1 and V is trivial), choosing, e.g., β > (cid:112)4/5 yields
f (n)/5n) to
|

f (n)/5n(cid:1). This case therefore adds at most
|

exp(

− |

−|

C

C

C

|

|

C.2 Proof of Theorem 5

Before moving to the actual proof, we need some ancillary results. The next lemma bounds the
probability that ACC does not pivot on a node of C in the ﬁrst k rounds.
Lemma 2. Fix a subset C
C let Xv = I
of V . For any v
and P(XC = 0) < e− k|C|
3n .

1, and let π1, . . . , πn be a random permutation
, and let XC = (cid:80)
v∈C Xv. Then E[XC] = k|C|
n ,

V and an integer k
π1, . . . , πk

≥
}}

∈ {

⊆

∈

{

v

n . Therefore E[Xv] = k

Proof. Since π is a random permutation, then for each v
C and each each i = 1, . . . , k we have
n and E[XC] = k|C|
P(πi = v) = 1
n . Now, the process is exactly equivalent to
sampling without replacement from a set of n items of which
are marked. Therefore, the Xv’s are
non-positively correlated and we can apply standard concentration bounds for the sum of independent
binary random variables. In particular, for any η

(0, 1) we have:

C
|

∈

|

P(XC = 0)

P(XC < (1

−

≤

which drops below e− k|C|

3n by replacing E[XC] and choosing η

≥

∈
η)E[XC]) < exp

(cid:16)

η2E[XC]
2

(cid:17)

−
(cid:112)2/3.

The next lemma is the crucial one.
Lemma 3. Let ε
C

≤
be the id of C. Then, for any v

1
10 . Consider a strongly (1

−

}

∈

ε)-knit set C with

C, in any single run of ACC we have P(id(v) = uC)

C
|

|

> 10n

f (n) . Let uC = min
v
{
2
3 .

∈

≥

Proof. We bound from above the probability that any of three “bad” events occurs. As in the proof
of Theorem 4, we equivalently see ACC as going through a sequence of candidate pivots π1, . . . , πn
that is a uniform random permutation of V . Let iC = min
i : πi
be the index of the ﬁrst
{
node of C in the random permutation of candidate pivots. The ﬁrst event, B1, is
iC > f (n
.
}
Note that, if B1 does not occur, then ACC will pivot on πiC . The second event, B2, is the event that
Vsing if ACC pivots on πiC (we measure the probability of B2 conditioned on B1). The third
πiC ∈
v. If none among B1, B2, B3 occurs, then ACC forms
event, B3, is
πiC /
{
∈
a cluster (cid:98)C containing both uC and v, and by the min-tagging rule sets id(v) = minu∈ (cid:98)C = uC. We
shall then show that P(B1
∪
For B1, we apply Lemma 2 by observing that iC > f (n
k = f (n

1) corresponds to the event XC = 0 with

where P =

uC ∩ N

1). Thus

1/3.

B3)

B2

1)

N

−

−

≤

C

∈

∪

P

}

{

}

P(iC > f (n

1)) < e− f (n−1)|C|

3n

e− f (n−1)

3n

10 n

f (n) = e− f (n−1)

f (n)

10

3 < e−3

−
where we used the fact that n
deﬁnition every v

C has at least (1

≥ |

C

∈

P(πiC ∈

Vsing)

≤

(cid:16)

exp

−

f (n
n

≤
11 and therefore f (n
1)
ε)c edges. Thus, if ACC pivots on πiC , we have:
(cid:17)
f (n
n

(cid:1) 10 n
f (n)

(cid:17)
ε)c

1
10

exp

(cid:0)1

(1

≥

−

(cid:16)

−

≤

−

−

1)
−
1
−

| ≥
−
1)
−
1
−

e−9

≤

10
11 f (n). For B2, recall that by

−

18

where we used the fact that f (n−1)
n−1
over C. Now, let
v be the neighbor sets of uC and v in C, and let P =
N
call P the set of good pivots. Since C is strongly (1
C
neighbors in C. But then

1. For B3, note that the distribution of πiC is uniform
v. We
ε)c

uC ∩ N
ε)-knit, both uC and v have at least (1

n
f (n) ≥

uC and

2εc and

N

N

−

−

P

|

\

| ≤
P(πiC /
∈

P ) = |

C

P

|

\
C
|

|

2ε

≤

≤

1/5

e−3 + e−9 + 1/5 < 1/3.

By a union bound, then, P(B1

B2

∪

∪

B3)

≤

ln(n/p)
We are now ready to conclude the proof. Suppose we execute ACC independently K = 48
(cid:101)
(cid:100)
G let Xv be the number of executions giving
times with the min-tagging rule. For a ﬁxed v
∈
id(v) = uC. On the one hand, by Lemma 3, E[Xv]
2
3 K. On the other hand, v will not be assigned
E[Xv](1
δ) where δ = 1
to the cluster with id uC by the majority voting rule only if Xv
4 .
−
By standard concentration bounds, then, P(Xv
K
48 ). By setting
) = exp(
≤
K = 48 ln(n/p), the probability that v is not assigned id uC is thus at most p/n. A union bound
over all nodes concludes the proof.

1
2 K
≤
δ2E[Xv]
2

≤
exp(

1
2 K)

≥

−

≤

−

D Supplementary Material for Section 6

D.1 Proof of Theorem 8

{

1, . . . , n

8 . Yao’s minimax principle then implies the claimed result.

We prove that there exists a distribution over labelings σ with OPT = 0 on which any deterministic
algorithm has expected cost at least nε2
Given V =
2 isolated cliques
, we deﬁne σ by a random partition of the vertices in d
}
T1, . . . , Td such that σ(v, v(cid:48)) = +1 if and only if v and v(cid:48) belong to the same clique. The cliques are
V to a clique Iv drawn uniformly at random with replacement
formed by assigning each node v
V : Iv = i
v
from
. Consider a deterministic algorithm making queries
}
{
}
{
. Let Ei be the event that the algorithm never queries a pair of nodes in Ti with
st, rt
{
} ∈ E
2d > 5. Apply Lemma 4 below with d = 1
n
Ti
ε . This implies that the expected number of
|
2 = 1
non-queried clusters of size at least n
2ε . The overall expected cost of ignoring these
clusters is therefore at least
n2
8d

2d is at least d
(cid:16) n
2d

, so that Ti =

εn2
8

1, . . . , d

∈
∈

| ≥

d
2

(cid:17)2

=

≥

=

and this concludes the proof.

Lemma 4. Suppose d > 0 is even, n
algorithm making at most B queries,

≥

16d ln d, and B < d2

50 . Then for any deterministic learning

d
(cid:88)

i=1

P(Ei) >

d
2

.

Proof. For each query
Ti and some edge
containining both st and a node of Ti was previously queried. The set Rt is deﬁned similarly using
rt. Formally,

we deﬁne the set Lt of all cliques Ti such that st

st, rt

(cid:54)∈

{

}

Lt =
Rt =

i : (
{
i : (
{

∃
∃

τ < t) sτ = st
τ < t) rτ = rt

rτ
sτ

Ti
Ti

∈
∈

∧
∧

∧
∧

σ(sτ , rτ ) =
σ(sτ , rτ ) =

1
}
−
1
}
−

.

Let Dt be the event that the t-th query discovers a new clique of size at least n
max(cid:8)
|

(cid:9). Using this notation,

Rt
|

Lt

|

|

,

2d , and let Pt =

B
(cid:88)

I

t=1

Dt

{

}

=

B
(cid:88)

t=1

I

Dt
{

Pt < d/2
}

+

∧

B
(cid:88)

t=1
(cid:124)

I

Dt
{

Pt

≥

.

d/2
}
(cid:125)

∧
(cid:123)(cid:122)
N

(18)

We will now show that unless B

≥

d2
50 , we can upper bound N deterministically by √2B.

19

2 , and let t1, . . . , tN be the times tk such that I

Suppose N > d
= 1. Now ﬁx
some k and note that, because the clique to which stk and rtk both belong is discovered, neither stk
nor rtk can occur in a future query
) that discovers a new clique. Therefore, in order to have
st, rt
{
}
I
= 1 for N > d
d/2
2 times, at least
(cid:19)

Dtk ∧
{

Ptk ≥

d/2
}

Dt
{

Pt

≥

∧

}

(cid:18)N
2

d2
8

≥

queries must be made, since each one of the other N
at most a query to making Pt
of size at least two, which contradicts the lemma’s assumption that B

1
d
2 . So, it takes at least B

d
2 discovered cliques can contribute with
d2
8 queries to discover the ﬁrst d
2 cliques
d2
d
16 . Therefore, N
2 .

≥
≥

−

≥

≤
= 1 for N

≤
d
2 times, at least

≤

Using the same logic as before, in order to have I

d
2

+

(cid:18) d

2 −

(cid:19)
1

+

queries must be made. So, it must be

Dt

{

+

· · ·

∧
(cid:18) d

Pt

≥

d/2
}
(cid:19)

N + 1

2 −

B

N
(cid:88)

(cid:18) d

≥

k=1

2 −

(k

−

(cid:19)

1)

= (d + 1)

N
2 −

N 2
2

or, equivalently, N 2
d
2 , we have that N

(d + 1)N + 2B

−

hypothesis N
get that N

≤
√2B.

≤

≤

0. Solving this quadratic inequality for N , and using the

≥
(d+1)−√(d+1)2−8B
2

. Using the assumption that B

d2
50 we

≤

Rt, where for any S

We now bound the ﬁrst term of (18) in expectation. The event Dt is equivalent to st, rt
1, . . . , d
S.
Lt
i
∩ ¬
∈ ¬
Let Pt = P(cid:0)
Pt < d/2(cid:1). For L(cid:48), R(cid:48) ranging over all subsets of
d
2 ,
Pt(Dt) =

(cid:12)
(cid:12) Lt = L(cid:48), Rt = R(cid:48)(cid:1) Pt(Lt = L(cid:48)

{
1, . . . , d

S to denote

1, . . . , d

we use

(cid:0)st

⊆ {

(cid:88)

(cid:88)

} \

Pt

Ti

Ti

· |

rt

¬

{

}

}

Ti for some

∈

of size strictly less than

Rt = R(cid:48))

∈

∧
∈
(cid:12)
(cid:12) Lt = L(cid:48)(cid:1) Pt

Pt

(cid:0)st

Ti

∈

(cid:0)rt

Ti

∈

∧
(cid:12)
(cid:12) Rt = R(cid:48)(cid:1) Pt(Lt = L(cid:48)

∧

L(cid:48),R(cid:48)
(cid:88)

i∈¬L(cid:48)∩¬R(cid:48)
(cid:88)

L(cid:48),R(cid:48)

i∈¬L(cid:48)∩¬R(cid:48)

(cid:88)

(cid:88)

|¬

i∈¬L(cid:48)∩¬R(cid:48)
L(cid:48)
L(cid:48)

∩ ¬
| |¬

|¬
|¬

R(cid:48)
R(cid:48)

|
|

L(cid:48),R(cid:48)

(cid:88)

L(cid:48),R(cid:48)
2
d

.

=

=

=

≤

1
L(cid:48)

1
R(cid:48)

|

|

|¬
Pt(Lt = L(cid:48)

Pt(Lt = L(cid:48)

Rt = R(cid:48))

∧

Rt = R(cid:48))

∧

Rt = R(cid:48))

(19)

(20)

(21)

Equality (19) holds because Pt = max
2 implies that there are at least two remaining
cliques to which st and rt could belong, and each node is independently assigned to one of these
cliques. Equality (20) holds because, by deﬁnition of Lt, the clique of st is not in Lt, and there were
Lt (similarly for rt). Finally,
no previous queries involving st and a node belonging to a clique in
L(cid:48)
R(cid:48)
min
(21) holds because

. Therefore,

Lt, Rt

R(cid:48)

R(cid:48)

L(cid:48)

L(cid:48)

}

{

,

< d

|¬

B
(cid:88)

d
2 ,

| ≥
P(cid:0)Dt

d
2 , and

|¬

∧

| ≥
Pt < d/2(cid:1)

|¬
B
(cid:88)

∩ ¬
P(cid:0)Dt

≤

t=1

|

¬
| ≤
{|¬
Pt < d/2(cid:1)

|}

|

|¬
2B
d

≤

t=1

Putting everything together,

.

(22)

(cid:34) B
(cid:88)

E

t=1

(cid:35)

2B
d

≤

I

Dt
{

}

+ √2B .

20

On the other hand, we have

B
(cid:88)

I

t=1

Dt
{

}

=

d
(cid:88)

(cid:16)

I (cid:8)

i=1

Ti
|

| ≥

(cid:9)

n
2d

I

Ei
{

}

−

(cid:17)

= d

d
(cid:88)

(cid:16)
I (cid:8)

i=1

−

< n
2d

(cid:9) + I

Ti
|

|

(cid:17)

Ei

{

}

(23)

Combining (22) and (23), we get that

d
(cid:88)

i=1

P(Ei)

d

−

≥

d
(cid:88)

P(cid:0)

i=1

(cid:1)

< n
2d

Ti

|

|

2B
d −

−

√2B .

By Chernoff-Hoeffding bound, P(cid:0)
Therefore,

|

Ti

|

(cid:1)

< n
2d

≤

1
d2 for each i = 1, . . . , d when n

16d ln d.

≥

d
(cid:88)

i=1

P(Ei)

d

−

≥

2B + 1
d

−

√2B .

To ﬁnish the proof, suppose on the contrary that (cid:80)d
we would get that

i=1

P(Ei)

≤

d
2 . Then from the inequality above,

2B + 1
d

d

−

−

√2B

d
2 ≥
d2 > d2

50 , contradicting the assumptions. Therefore, we must have

which implies B
(cid:80)d
P(Ei) > d

i=1

≥

2 as required.

√

(cid:16) 2−
4

2

(cid:17)2

D.2 Proof of Theorem 9

|

|

B

−

A
|

= (1

1. Finally, for each v

Choose a suitably large n and let V = [n]. We partition V in two sets A and B, where
= αn
α)n; we will eventually set α = 0.9, but for now we leave it free to have a clearer
and
proof. The set A is itself partitioned into k = 1/ε subsets A1, . . . , Ak, each one of equal size αn/k
(the subsets are not empty because of the assumption on ε). The labeling σ is the distribution deﬁned
as follows. For each i = 1, . . . , k, for each pair u, v
B,
B we have a random variable iv distributed uniformly over [k].
σ(u, v) =
Aiv . Note that the distribution
Then, σ(u, v) = +1 for all u
A
of iv is independent of the (joint) distributions of the iw’s for all w
B
Let us start by giving an upper bound on E[OPT]. To this end consider the (possibly suboptimal)
v
Ci : i
is a partition
clustering
{
of V . The expected cost E[∆C] of
can be bound as follows. First, note the only mistakes are due to
pairs u, v
B. However, for any such ﬁxed pair u, v, the probability of a mistake (taken over σ) is
P(iu

B : iv = i
. One can check that
}

Ai, σ(u, v) = +1; for each u, v

∈
1 for all u

= iv) = 1/k. Thus,

Aiv and σ(u, v) =

where Ci = Ai

[k]
}

\
∈

∪ {

.
}

\ {

−

=

−

∈

∈

∈

∈

∈

∈

∈

C

C

C

v

|

E[OPT]

≤

E[∆0] < |

2
B
|
k

(1

=

−

α)2n2
k

(24)

∈

B let Qv be the total number of distinct queries the algorithm makes to pairs

Let us now turn to the lower bound on the expected cost of the clustering produced by an algorithm.
For each v
}
B. Let Q be the total number of queries made by the algorithm; obviously,
with u
(cid:80)
∈
Q
v∈B Qv. Now let Sv be the indicator variable of the event that one of the queries involving v
returned +1. Both Qv and Sv as random variables are a function of the input distribution and of the
choices of the algorithm. The following is key:

u, v
{

A and v

≥

∈

P(Sv

∧

Qv < k/2) <

1
2

(25)

The validity of (25) is seen by considering the distribution of the input limited to the pairs
.
}
Indeed, Sv
Qv < k/2 implies the algorithm discovered the sole positive pair involving v in less than
k/2 queries. Since there are k pairs involving v, and for any ﬁxed j the probability (taken over the
input) that the algorithm ﬁnds that particular pair on the j-th query is exactly 1/k. Now,

u, v
{

∧

P(Sv

∧

Qv < k/2) + P(Sv

∧

Qv < k/2) + P(Qv

k/2) = 1

≥

(26)

21

(cid:54)
and therefore

P(Sv

∧

Qv < k/2) + P(Qv

k/2) >

≥

1
2

(27)

|

Let us now consider Rv, the number of mistakes involving v made by the algorithm. We analyse
E[Rv
v indicate the event that, for some u
Ai, the algorithm
Sv
∈
; thus I contains all i such that the algorithm did
v = 0
i
queried the pair
{
}
not query any pair u, v with u
Qv < k/2 occurs. On the one hand,
Ai. Suppose now the event Sv
Sv implies that:

Qv < k/2]. For all i
. Let I =
}

[k] let Qi
[k] : Qi

u, v
{

∈

∧

∈

∧

∈

P(σ(u, v) = +1

(cid:26) 1/|I| u
u

0

I) =

|

Ai, i
Ai, i

I
[k]

∈
∈

I

\

∈
∈

(28)

Informally speaking, this means that the random variable iv is distributed uniformly over the (random)
Qv < k/2, whatever label s the
set I. Now observe that, again conditioning on the joint event Sv
algorithm assigns to a pair u, v with u
I, the distribution of σ(u, v) is independent
of s. This holds since s can obviously be a function only of I and of the queries made so far, all of
which returned

∈
1, and possibly of the algorithm’s random bits. In particular, it follows that:
min (cid:8)1/|I|, 1

Ai where i

P(σ(u, v)

1/|I|(cid:9)

= s

I)

(29)

−

∧

∈

However, Qv < k/2 implies that
1/|I|. Therefore, P(σ(u, v)
= s

|

≥

k
−
1/|I| for all u

I
| ≥
I)
≥

|
|

−

∈

Ai with i

I.

∈

Qv > k/2 = 2/ε > 2, which implies min

1/|I|, 1
{

−

1/|I|

} ≥

We can now turn to back to Rv, the number of total mistakes involving v. Clearly, Rv
(cid:80)k

(cid:80)

≥

i=1

u∈Ai

I

σ(u, v)
{

= s
. Then:
}

E[Rv

|

(cid:104) k
(cid:88)

E] = E

(cid:88)

u∈Ai

i=1
(cid:104) k
(cid:88)

(cid:104)
E
= E

σ(u, v)

I

{

= s
}

(cid:12)
(cid:12)
(cid:12) Sv

∧

(cid:105)

Qv < k/2

(cid:88)

u∈Ai
(cid:88)

u∈Ai
(cid:88)

i=1
(cid:104) (cid:88)

(cid:104)
E
E

i∈I
(cid:104) (cid:88)

(cid:104)
E
E

≥

≥

(cid:104)
E
= E

i∈I
(cid:104) αn
k

u∈Ai
(cid:105) (cid:12)
(cid:12)
(cid:12) Sv

I

I

σ(u, v)
{

= s
}

σ(u, v)
{

= s
}

(cid:12)
(cid:12)
(cid:12) I

(cid:105) (cid:12)
(cid:12)
(cid:12) Sv

(cid:12)
(cid:12)
(cid:12) I

(cid:105) (cid:12)
(cid:12)
(cid:12) Sv

∧

∧

(cid:105)
Qv < k/2

(cid:105)
Qv < k/2

(cid:105)

Qv < k/2

(cid:12)
(cid:12)
(cid:12) I

(cid:105) (cid:12)
(cid:12)
(cid:12) Sv

∧
|
(cid:105)
Qv < k/2

1
I

|

∧

=

αn
k

And therefore:

E[Rv]

E[Rv
αn
k ·
This concludes the bound on E[Rv]. Let us turn to E[Qv]. Just note that:

·
Qv < k/2)

Sv
|
∧
P(Sv

Qv < k/2]

P(Sv

≥
>

∧

∧

Qv < k/2)

By summing over all nodes, we obtain:

E[Qv]

k
2 ·

≥

P(Qv

k/2)

≥

E[Q]

E[∆]

(cid:88)

v∈B
(cid:88)

v∈B

≥

≥

E[Qv]

k
2

≥

(cid:16) (cid:88)

P(Qv

(cid:17)

k/2)

≥

E[Rv] >

αn
k

v∈B
(cid:16) (cid:88)

v∈B

(cid:17)

Qv < k/2)

P(Sv

∧

22

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
to which, by virtue of (27), applies the constraint:

(cid:16) (cid:88)

v∈B

P(Qv

≥

(cid:17)

+

k/2)

(cid:16) (cid:88)

P(Sv

v∈B

Qv < k/2)

∧

(cid:17)

>

1
2

|

B

|

=

(1

α)n
−
2

(39)

(1−α)n

(1−α)n

4 = (1−α)nk

then E[∆] > αn
k

This constrained system gives the bound. Indeed, by (37), (38) and (39), it follows that if E[Q] <
2 = α(1−α)n2
k
. It just remains to set α and k properly so
2
8
to get the statement of the theorem.
Let α = 9/10 and recall that k = 1/ε. Then, ﬁrst, (1−α)nk
E[OPT] < (1−α)2n2
100k = εn2
above statement hence becomes: if E[Q] < n
Yao’s minimax principle completes the proof.

8
= 9n2
80ε , then E[∆] > E[OPT] + εn2

80 ε . Second, (24) gives
80 . The
80 . An application of

= nk
400k = 9εn2

400 > E[OPT] + εn2

100 . Third, α(1−α)n2

80 = n

= n2

4k

4k

k

As a ﬁnal note, we observe that for every c
Ω(n2ε) by choosing α

c/(c + 1/4).

≥

1 the bound can be put in the form E[∆]

E[OPT] +

c

·

≥

≥

E Supplementary Material for Section 7

We report the complete experimental evaluation of ACC including error bars (see the main paper for
a full description of the experimental setting). The details of the datasets are found in Table 1.

Table 1: Description of the datasets.

Datasets

Type

captchas
cora
gym
landmarks
skew
sqrt

Real
Real-world
Real
Real
Synthetic
Synthetic

V

|
|
244
1879
94
266
900
900

#Clusters

69
191
12
12
30
30

23

Figure 2: Clustering cost vs. number of queries. The dotted curves are the average cost and the shaded
areas around them measure the standard deviation. The trailing diamonds refer to KwikCluster’s
performance.

(a) skew.

(b) sqrt.

(c) cora.

(d) landmarks.

(e) gym.

(f) captchas.

24

0123NumberofQueries×10401234ClusteringCost×105p=0p=0.1p=0.2p=0.30.00.51.01.5NumberofQueries×10401234ClusteringCost×105p=0p=0.1p=0.2p=0.30246NumberofQueries×1040.000.250.500.751.001.251.501.75ClusteringCost×106p=0p=0.1p=0.2p=0.3024NumberofQueries×1030.00.51.01.52.02.53.0ClusteringCost×104p=0p=0.1p=0.2p=0.30.00.51.0NumberofQueries×10301234ClusteringCost×103p=0p=0.1p=0.2p=0.30.00.20.40.60.8NumberofQueries×1040.00.51.01.52.02.53.0ClusteringCost×104p=0p=0.1p=0.2p=0.3