À quoi sert la spécialisation en évolution culturelle de la
connaissance?
Andreas Kalaitzakis, Jérôme Euzenat

To cite this version:

Andreas Kalaitzakis, Jérôme Euzenat. À quoi sert la spécialisation en évolution culturelle de la con-
naissance?. JFSMA 2023 - 31e journées francophones sur Systèmes multi-agent, Jun 2023, Strasbourg,
France. pp.76-85. ￿hal-04351105￿

HAL Id: hal-04351105

https://hal.science/hal-04351105

Submitted on 18 Dec 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

À quoi sert la spécialisation en évolution culturelle
de la connaissance?

Andreas Kalaitzakisa
andreas.kalaitzakis@inria.fr

Jérôme Euzenata
jerome.euzenat@inria.fr

aUniv. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, F-38000 Grenoble

Résumé
Des agents peuvent faire évoluer leurs ontologies
en accomplissant conjointement une tâche. Nous
considérons un ensemble de tâches dont chaque
agent ne considère qu’une partie. Nous suppo-
sons que moins un agent considère de tâches,
plus la précision de sa meilleure tâche sera éle-
vée. Pour le vérifier, nous simulons différentes
populations considérant un nombre de tâches
croissant. De manière contre-intuitive, l’hypo-
thèse n’est pas vérifiée. D’une part, lorsque les
agents ont une mémoire illimitée, plus un agent
considère de tâches, plus il est précis. D’autre
part, lorsque les agents ont une mémoire limi-
tée, les objectifs de maximiser la précision de
leur meilleures tâches et de s’accorder entre
eux sont mutuellement exclusifs. Lorsque les
sociétés favorisent la spécialisation, les agents
n’améliorent pas leur précision. Cependant, ces
agents décideront plus souvent en fonction de
leurs meilleures tâches, améliorant ainsi la per-
formance de leur société.

Mots-clés : Évolution culturelle, Spécialisation

Abstract
Agents can evolve their ontologies by jointly per-
forming a task. We consider a set of tasks of
which each agent considers only a part. We as-
sume that the fewer tasks an agent considers, the
higher the accuracy of its best task. To verify this,
we simulate different populations considering an
increasing number of tasks. Counter-intuitively,
the assumption is not verified. On the one hand,
when agents have an unlimited memory, the more
tasks an agent considers, the more accurate it is.
On the other hand, when agents have limited me-
mory, the goals of maximizing the accuracy of
their best tasks and agreeing with each other are
mutually exclusive. When societies favor specia-
lization, agents do not improve their accuracy.
However, these agents will decide more often
based on their best tasks, thus improving the
performance of their society.
Keywords: Cultural evolution, specialization

1

Introduction

En communiquant entre eux, des agents font
évoluer leurs représentations des connaissances,
un phénomène qui se manifeste par un chan-
gement de comportement. Ce phénomène est
étudié dans le cadre de l’évolution culturelle
des connaissances, où les agents acquièrent et
font évoluer leurs connaissances, en fonction de
leur perception et du retour qu’ils reçoivent des
autres agents. Ici, nous modélisons des agents
qui représentent leurs connaissances en utilisant
des ontologies. Jusqu’à présent, la plupart des
travaux sur l’évolution culturelle des connais-
sances se concentrent sur des agents qui (1) font
évoluer leurs connaissances dans des environne-
ments mono-tâche et (2), disposent de ressources
illimitées. [15] propose un jeu d’alignement ité-
ratif, permettant aux agents de classer progres-
sivement les nombres dans des ensembles iden-
tiques. Ainsi, à la fin de l’expérience, les agents
partagent la même représentation des connais-
sances. Dans [2], les agents interagissent sur une
seule tâche de décision. Après un nombre fini
d’interactions, les agents se mettent d’accord sur
toutes les décisions, mais pas nécessairement sur
la même base.

Cependant, notre monde n’est pas monolithique
et les ressources des agents ne sont pas illimi-
tées. Les agents doivent être capables d’utiliser
des ressources limitées afin d’effectuer plusieurs
tâches de manière efficace. Pourtant, on sait peu
de choses sur la façon dont la réalisation de plu-
sieurs tâches affecte l’évolution culturelle des
connaissances. On en sait encore moins sur les
agents encouragés à se spécialiser dans des envi-
ronnements multitâches. Peuvent-ils se spéciali-
ser tout en restant en accord les uns avec les
autres ? Les sociétés d’agents bénéficient-elles
de la spécialisation des agents ? Nous abordons
ces questions en étendant le cadre introduit dans
[2]. Dans ce cadre, les agents effectuent la même
tâche consistant à prendre une décision abstraite
dans un domaine abstrait. Dans notre cadre, les
agents sont affectés à une ou plusieurs tâches et

sont donc capables de prendre des décisions dans
plusieurs domaines abstraits. Notre hypothèse
principale est que les agents qui entreprennent
moins de tâches atteignent une précision plus
élevée sur leurs meilleures tâches, que les agents
qui entreprennent toutes les tâches. Nous suppo-
sons qu’en interagissant sur un ensemble limité
de tâches, les agents seront plus précis dans cer-
taines tâches que dans d’autres.

Nous organisons notre travail en deux expé-
riences. Dans la première expérience [8], des
agents disposant d’une mémoire illimitée entre-
prennent un ensemble limité de tâches, que nous
appellerons par la suite le scope. Dans notre tra-
vail, la prise de décisions pour chaque tâche re-
pose sur différentes propriétés. Par exemple, dé-
cider de prendre un parapluie peut reposer sur la
propriété humidité alors que décider de porter un
t-shirt peut reposer sur la propriété température.
Dans la deuxième expérience [9], les agents en-
treprennent les mêmes tâches, tout en faisant face
à des limitations de mémoire. À cette fin, nous
fixons un nombre maximum de classes à main-
tenir dans l’ontologie d’un agent. Lorsque les
agents atteignent ces limites, ils essaient d’ou-
blier des connaissances qui ne sont pas perti-
nentes pour leur scope et rendent ainsi l’espace
à nouveau disponible. Nous évaluons les ontolo-
gies des agents par leur contribution à (1) la pro-
motion de l’accord entre les agents et (2) la prise
de décisions correctes pour différentes tâches.

l’environnement,
Les entités qui constituent
ainsi que la notation qui le décrit, sont présen-
tés dans le §3. La section 4 présente les grandes
lignes de l’expérience. La section 5 présente un
ensemble d’hypothèses ainsi que le protocole
utilisé pour les tester. Les résultats des tests sont
présentés dans la section 6.

2 État de l’art

Il a été montré que les jeux référentiels [11], faci-
litent l’établissement de protocoles de communi-
cation entre des agents communicants qui tentent
d’obtenir une rétribution partagée. [13] sou-
tient qu’un protocole de communication émerge
lorsque les agents tentent de minimiser la com-
plexité computationnelle de l’interprétation sé-
mantique. [7] et [3] présentent différents cadres
permettant l’émergence d’un langage partagé.
[7] étudie un cadre où deux agents développent
un langage dans le but de réussir dans un jeu réfé-
rentiel. [3] propose un cadre permettant la trans-
mission intra-générationnelle, en se concentrant
sur la compositionnalité de la langue émergente.

[6] montre que la transmission culturelle impli-
cite conduit à une plus grande compositionna-
lité de la langue. Notre travail repose également
sur les agents qui essaient de se mettre d’accord
entre eux. Cependant, les travaux présentés ici se
concentrent sur les caractéristiques du protocole
de communication émergé. Bien que des carac-
téristiques similaires soient également présentes
dans notre travail, nous ne nous concentrons pas
sur le protocole de communication mais plutôt
sur l’impact de l’accord des agents sur la réali-
sation de plusieurs tâches.

Il a été montré que l’apprentissage multitâche
améliore considérablement la classification dans
différents domaines, par exemple, la robustesse
des adversaires [12], la similarité visuelle inter-
concept [4], l’apprentissage du phénotype [5].
Les agents ont également été utilisés pour étudier
l’impact de l’apprentissage multitâche sur les
protocoles de communication émergents. Dans
[14], l’apprentissage par renforcement multi-
agent coopératif est considéré. La littérature pré-
sentée dans ce paragraphe est liée à notre travail
car elle considère des agents qui effectuent plu-
sieurs tâches. Cependant, tout changement dans
le comportement et les connaissances de ces
agents n’est pas le résultat d’une évolution cultu-
relle.

L’évolution culturelle des connaissances a été
étudiée dans [2] et [15]. Dans [15], les auteurs
cherchent à comprendre comment les concepts
sont organisés et comment un comportement
collectif peut être établi de manière autonome.
Ils montrent qu’il existe un large éventail de
conditions sous lesquelles un consensus col-
lectif se produit. Dans [2], une expérience en
deux étapes est utilisée où les agents apprennent
d’abord un classificateur et interagissent ensuite
par paires. Grâce à un mécanisme d’adapta-
tion, il est montré que les agents parviennent
à une meilleure connaissance, sans nécessaire-
ment aboutir à des ontologies identiques. Nous
nous différencions de ceux-ci en introduisant des
agents qui font évoluer leurs connaissances par
rapport à des tâches multiples.

Enfin, il est à souligner qu’aucun des travaux
présentés dans cette section n’impose de limites
aux agents.

3 Cadre expérimental

Nous étendons [2] en introduisant des agents qui
entreprennent plusieurs tâches. Ici nous fournis-
sons des définitions préliminaires concernant les

agents, les différentes tâches, les objets, ainsi que
leur environnement.

3.1 Environnement

Les agents évoluent dans un environnement peu-
plé d’objets décrits par un ensemble P de pro-
priétés booléennes. Les objets sont donc décrits
par la présence ou l’absence d’une propriété
p ∈ P, dénotée par p et ¬p respectivement. Il
n’existe donc que 2|P| types d’objets, qui sont
rassemblés dans un ensemble I.

3.2 Tâches

Le terme tâche fait référence à un travail effec-
tué par un agent. Ici, nous nous concentrerons
sur un ensemble de tâches de décision : prendre
une décision abstraite dans un domaine abstrait,
c’est-à-dire choisir parmi d1, d2 . . . dn. Il peut y
avoir différentes tâches t ∈ T associées à un en-
semble différent de décisions possibles Dt. Dans
ce contexte, chaque objet o peut être considéré
par rapport à toute tâche t ∈ T . Une fonction
h∗(o, t) → Dt fournit la décision correcte, in-
connue des agents, pour un objet o par rapport à
une tâche t.

3.3 Agents

Les agents sont des entités autonomes, coexis-
tantes, capables de percevoir et de distinguer des
objets en fonction de leurs propriétés. Dans ce
contexte, une société d’agents multitâches A,
réalise de multiples tâches. À cette fin, les agents
construisent et font évoluer des connaissances
sous la forme d’ontologies, privées à chaque
agent, exprimées en ALC [1]. Chaque agent α
utilise ses connaissances pour calculer une fonc-
tion hα(o, t) → Dt qui, étant donné un objet
o et une tâche t, fournit une décision hα(o, t).
La figure 1 montre un exemple de connaissance
multi-tâche construite par un agent α. La partie
inférieure représente l’ontologie privée Oα de
l’agent α, lui permettant de classifier les ob-
jets de l’environnement. La partie supérieure
présente deux ontologies de décision, chacune
contenant les décisions valides pour les tâches
t1 et t2. Étant donné qu’un agent α apprend au
plus une décision pour un objet o et une tâche
t, chaque feuille de Oα ne peut être alignée plus
d’une fois avec la même ontologie de décision.

Dt1

Dt2

⊑

⊑

d1

⊕

⊑

⊕

d3

⊑

⊑

d1

⊕

⊑

⊕

d3

d2
⊕

⊑

⊑

⊑

¬p1

Oα

d2
⊕

⊑

⊑

⊑
p2

⊑

⊑

p1 ⊓ ¬p2

p1 ⊓ p2

Figure (1) Exemple d’une ontologie multitâche pour un
agent α. Chaque couleur représente une décision diffé-
rente. Le symbole ⊕ indique que deux décisions sont dis-
jointes.

4 Aperçu de l’expérience

L’expérience est initialisée par une phase d’ap-
prentissage, à la fin de laquelle chaque agent a
appris une ontologie privée. Une fois leurs onto-
logies apprises, les agents passent par un nombre
fixe d’interactions. En fonction du résultat d’une
interaction, un agent peut adapter son ontologie.
La figure 2 illustre le déroulement de l’expé-
rience du point de vue de l’agent α. De plus
amples détails sur la façon dont les agents ap-
prennent, définissent leur scope, interagissent,
oublient et adaptent leurs ontologies sont pré-
sentés dans les sous-sections 4.1, 4.2, 4.3, 4.4 et
4.5 respectivement.

4.1 Apprentissage initial

abordons

Nous
l’apprentissage multitâche
comme un problème d’induction d’une ontolo-
gie capable de fournir une décision pour toute
tâche t ∈ T . L’induction d’une telle ontologie
peut être vue dans la figure 3. Nous observons
qu’il est possible pour un agent d’être capable
de classifier un objet mais d’être incapable de
prendre une décision concernant une ou plu-
sieurs tâches. Par exemple, étant donné l’ontolo-
gie de la figure 1 et un objet décrit par p1 ⊓ p2, il
est impossible de prendre une décision par rap-
port à la tâche t1.

4.2 Définition du scope

Nous implémentons ici des agents qui accom-
plissent différents sous-ensembles de tâches, en
donnant la priorité à certaines tâches par rap-

Exemples étiquetés

Apprentissage

Ontologie

tache, objet

Interaction
suivante

Interagir

Non

Continuer

Adaptation

Oui

Prise de
décision

Non

Accord ?

Oui

Figure (2) Schéma de l’expérience. Les éléments en cou-
leur bisque constituent des entrées pour différentes activi-
tés (e.g., les exemples étiquetés). Les éléments en violet
constituent soit des activités (rectangles), soit des déci-
sions (diamants).

port à d’autres. À cette fin, nous introduisons la
fonction priorα(t) → N , qui renvoie un entier
unique attribué à la tâche t par un agent α. Cet
entier va de 1 à |T | et représente la priorité de
la tâche t parmi toutes les tâches existantes. Mu-
nis de leurs classements de priorité, les agents
définissent l’ensemble des tâches qu’ils entre-
prennent. Afin de décider si une tâche doit être
incluse dans leur scope, les agents comparent la
priorité de chaque tâche t avec le rang de prio-
rité maximum accepté m. Si priorα(t) ≤ m,
alors la tâche est dans le scope de l’agent. Si
priorα(t) > m, alors la tâche est en dehors du
scope de l’agent.

4.4 Limitations et oubli des connaissances

Nos agents sont limités en termes de nombre
maximum de classes présentes dans leurs on-
tologies. Lorsque cette limitation est atteinte,
ils essaient d’oublier certaines parties de leurs
connaissances afin de libérer de l’espace mé-
moire en faveur des tâches qu’ils priorisent.
Nous supposons que si elle est suffisamment res-
treinte, une ontologie ne pourra contenir que les
propriétés nécessaires pour être précis sur une
seule tâche. Par exemple, décidons si l’agent
doit prendre un parapluie (d1) ou non (d2), en
s’appuyant sur la propriété humidité. Si l’on-
tologie de l’agent ne contient pas la propriété
humidité, alors la probabilité qu’une décision
associée soit correcte est de 50%. Nous implé-
mentons le processus d’oubli comme suit. Nous
supprimons les nœuds parents qui satisfont aux
critères suivants : (a) leurs descendants sont des
nœuds feuilles et (b) leurs descendants sont as-
sociés à la même décision concernant toutes les
tâches dans le scope de l’agent. Le processus est
répété de manière récursive, tant que des nœuds
parents satisfaisant aux critères (a) et (b) existent.

4.5 Adaptation

Nous étendons
le mécanisme d’adaptation
présenté dans [2], en introduisant l’adaptation
en présence de plusieurs tâches (figure 5). Un
agent peut soit remplacer une décision exis-
tante, soit diviser une classe en 2 sous-classes.
L’agent le fait sur la base d’une propriété qui
permet de distinguer l’objet actuel des objets
classés par la classe à diviser. Seules les déci-
sions concernant la tâche en cours sont affectées.

4.3

Interaction

Algorithm 1 Adaptation d’ontologie

À chaque interaction, deux agents α et β sélec-
tionnés aléatoirement se voient présenter un ob-
jet o et une tâche t. Les agents vont alors fournir
des décisions basées sur leurs fonctions respec-
tives hα(o, t) et hβ(o, t). Si un des agents n’est
pas en mesure de fournir une décision, alors cet
agent en choisit une au hasard. Les agents ré-
vèlent alors leurs décisions les uns aux autres.
Si hα(o, t) = hβ(o, t), les agents sont d’accord et
leur interaction se termine avec succès. Dans le
cas contraire, on considère qu’il s’agit d’un échec
et l’un des deux agents adaptera ses connais-
sances.

1: α demande à β la définition de la classe classant l’objet o.
2: β répond avec Cβ.
3: if Cα ̸⊑ Cβ then
4:

α demande à β sa décision hβ(o, t).
β répond avec hβ(o, t)
une propriété p : p ∈ Cβ, p /∈ Cα est sélectionnée.
α divise Cα en Cα
1 ≡ Cα ⊓ ¬p et Cα
α associe Cα

2 ≡ Cα ⊓ p

1 à toutes les décisions précédemment as-

sociées à Cα.

if Aucune décision concernant t n’est associée à Cα.

α associe Cα à la décision hβ(o, t).

α associe Cα

2 à la décision hβ(o, t).

Cα = Cα ⊓ p
Associer Cα à toutes les décisions précédemment as-

sociées à Cβ.

5:

6:
7:
8:

9:

10:

then

11:
12: else
13:
14:

Object
o1
o2
o3
o4
o5
o6
o7
o8

p1
1
1
0
0
1
1
0
1

p2
1
0
1
0
1
1
1
0

p3
0
0
0
1
0
0
1
1

p4 Tâche Décision
d1
t1
0
d1
t2
1
d2
t1
1
d2
t2
0
d1
t1
1
d1
t2
0
d2
t1
0
d2
t2
1

p1

False

T

r

u

e

=⇒

dt12 dt22

p3

False

o7 o4 o3

dt11 dt21

o1 o2 o5 o6

Dt1

Dt2

⊑

⊑

d1

⊕

⊑

⊕

d3

⊑

⊑

d1

⊕

⊑

⊕

d3

d2
⊕

⊑

⊑

⊑

¬p1

Oα

d2
⊕

⊑

⊑

⊑
p2

⊑

⊑

p1 ⊓ ¬p2

p1 ⊓ p2

=⇒

T

r

u

e

dt22

o8

Figure (3) Vue d’ensemble de la méthode d’apprentissage proposée. Les agents commencent par induire un arbre de
décision basé sur un ensemble d’exemples étiquetés. Cet arbre de décision est ensuite converti en une ontologie.

Dt1

T 2

Dt1

Dt2

⊑

⊑

d1

⊕

⊑

⊕

d3

⊑

⊑

d1

⊕

⊑

⊕

d3

⊑

⊑

d1

⊕

⊑

⊕

d3

⊑

⊑

d1

⊕

⊑

⊕

d3

d2
⊕

⊑

⊑

¬p4

d2
⊕

⊑

⊑

⊑

Oα

⊑

⊑

p1

⊑

p4 ⊓ ¬p1

p4 ⊓ p1

=⇒

⊑

⊑

d2
⊕

¬p4

Oα

⊑

⊑

⊑

p4

d2
⊕

⊑

Figure (4) Exemple de libération de mémoire par généralisation lors d’une interaction par rapport à la tâche t2. Soit la
tâche t2 reposant sur l’ensemble de propriétés Pt2 . La propriété p1 /∈ Pt2 , donc p1 ne permet pas de distinguer différentes
décisions pour la tâche t2. Dans cet exemple, l’agent a associé la même décision (en rouge), à la fois à p4 ⊓ ¬p1 et à p4 ⊓ p1.
Ces deux classes peuvent être fusionnées sans aucune perte de connaissance par rapport à t2, comme on le voit à droite.

5 Protocole

5.1 Hypothèses

Nous testons les hypothèses suivantes :

• Hypothèse 1 Les agents convergent sur leurs
décisions après un nombre fini d’interactions.

• Hypothèse 2 Les agents qui entreprennent
moins de tâches atteindront une précision plus
élevée sur leur meilleurs tâches, que les agents
qui entreprennent toutes les tâches.

Une exécution consiste en 80000 interactions,
chaque interaction ayant lieu entre deux agents
choisis au hasard parmi 18 agents. Leur envi-
ronnement contient 64 types d’objets différents,
chacun étant perceptible à travers 6 propriétés.
Les agents sont initialement formés à toutes les
tâches |T | = {3}, chaque tâche faisant appel
à 2 des 6 propriétés. L’induction de l’ontologie
initiale est basée sur un échantillon aléatoire de
10 % de tous les exemples étiquetés existants.
Pour chaque tâche, il existe 4 décisions diffé-
rentes. L’évaluation du score entre deux interac-
tions consécutives se base sur les 60 % de tous
les exemples étiquetés existants.

5.2 Paramètres

5.3 Mesures

Nous réalisons deux expériences. Dans l’expé-
rience 1, les 18 agents ne sont soumis à aucune
limitation de mémoire. Dans l’expérience 2, les
ontologies des 18 agents sont limitées à un maxi-
mum de 4 classes feuilles. Chaque expérience est
exécutée sous 3 configurations, correspondant à
des agents qui entreprennent 1 à 3 tâches respec-
tivement. Chaque configuration est exécutée 20
fois et la moyenne de ses résultats est calculée.

Le taux de réussite (srate), tel qu’introduit dans
[2] évalue l’interopérabilité entre les agents. Il
est défini comme la proportion d’interactions
réussies, sur toutes les interactions réalisées jus-
qu’à la nème interaction.

Le taux de décisions correctes (cdrate) évalue
la performance d’une société d’agents. Il est dé-
fini comme la proportion de décisions correctes

DHunting

DCooking

⊑

⊑

⊑

⊑

Hunt

⊕

Avoid

Discard

⊕

Cook

Oα

⊑

⊑

Oβ

⊑

⊑

¬nails

nails

¬poison

poison

Rabbit

nails, ¬poison

DHunting

DCooking

⊑

⊑

⊑

⊑

Hunt

⊕

Avoid

Discard

⊕

Cook

Oα

⊑

⊑

¬nails

nails

⊑

⊑

nails ⊓ ¬poison nails ⊓ poison

Oβ

⊑

⊑

¬poison

poison

Figure (5) Suite à un échec de communication, l’agent α divise la classe nails en 2 sous-classes en effectuant une
division supplémentaire sur la propriété poison. La 1ère sous-classe, nails ⊓ poison sera associée à la même décision que
celle de l’agent β (Hunt). La 2ème décision sera associée à la décision précédemment associée à la classe nails (Avoid).

Rabbit

nails, ¬poison

prises par une société d’agents, sur l’ensemble
des interactions réalisées jusqu’à la nème interac-
tion. Lorsque les agents sont d’accord, cette dé-
cision est celle prise par les deux agents. Lorsque
les agents sont en désaccord, la décision prise en
compte est celle de l’agent ayant le score le plus
élevé parmi les deux agents en interaction.

La précision de la tâche (tacc) évalue la qua-
lité de la représentation des connaissances d’un
agent. Elle adapte la mesure de précision in-
troduite dans [2] à différentes tâches. Elle est
définie comme la proportion de types d’objets
pour lesquels une décision correcte serait prise
par rapport à une tâche t, par un agent α à la ne
itération de l’expérience.

|{o ∈ I : hα

n(o, t) = h∗(o, t)}|

tacc(α, n, t) =

|I|
tacc est utilisé pour mesurer
la précision
moyenne sur la meilleure tâche des agents, ainsi

que leur précision moyenne sur toutes les tâches
existantes ou entreprises.

6 Résultats et discussion

Nous présentons et discutons ici les résultats ob-
tenus dans le cadre des deux expériences décrites
précédemment. Chacune des figures 6, 7 et 9
est divisée en deux sous-figures. La sous-figure
(a) correspond aux résultats acquis lors de l’ex-
périence 1, tandis que la sous-figure (b) corres-
pond aux résultats acquis lors de l’expérience 2.
Puisque les agents qui ne font face à aucune li-
mitation ne bénéficient pas de la spécialisation,
les autres figures concernent uniquement les ré-
sultats de l’expérience 2.

Interopérabilité inter-agent. La figure 6
montre l’évolution du taux de réussite moyen
pour différentes tailles du scope. La figure 6a

(a) Agents avec une mémoire illimitée

(b) Agents dont la mémoire est limitée à 4 classes

Figure (6) Évolution du taux de réussite moyen pour différentes tailles du scope des agents.

montre qu’une population d’agents en interac-
tion apprendra à se mettre d’accord sur toutes
leurs décisions, indépendamment de la taille
de leur scope. Ceci est justifié par le fait que
les agents représentés ici ne sont confrontés à
aucune limitation. Par conséquent, ils adopte-
ront progressivement les mêmes propriétés dans
leurs ontologies, même si leurs classes feuilles
ne coïncident pas. Nos observations confirment
les résultats qui ont été présentés précédemment
dans [2]. De plus, elles indiquent que la taille du
scope a un impact sur le taux de réussite obtenu.
Plus la taille du scope est grande, plus il faut
d’interactions pour se mettre d’accord sur tout,
donc plus le taux de réussite moyen à la conver-
gence est faible. La figure 6b montre que le taux
de réussite se stabilise, sans toutefois converger
à 1. Cela indique que soit les agents continuent
à adapter leurs ontologies, soit leurs ontologies
finales ne leur permettent pas de s’accorder sur
toutes les décisions. Dans la section 4.4, nous
avons supposé que si elle est suffisamment res-
treinte, une ontologie sera capable de contenir
uniquement les propriétés requises pour être pré-
cis sur une seule tâche. Par conséquent, il faut
s’attendre à ce que les agents interagissant sur
un nombre de tâches nécessitant plus de mé-
moire, ne pourront pas se mettre d’accord sur
toutes les décisions. Il est montré que cela est
vrai même pour les agents qui interagissent sur
une seule tâche. Nous ne pouvons donc soutenir
l’hypothèse 1 que pour les agents qui ne sont
confrontés à aucune limitation. Cependant, nous
observons que plus le scope des agents est réduit,
plus leur taux de réussite est élevé. En raison de
l’algorithme d’induction de l’ontologie initiale,
l’ontologie d’un agent qui entreprend la tâche t

peut contenir des propriétés qui n’appartiennent
pas à Pt. Rien ne garantit que les agents parvien-
dront à remplacer ces propriétés par des proprié-
tés appartenant à Pt. Deux cas distincts peuvent
être envisagés. Dans le premier cas, un agent éli-
minera progressivement toutes les propriétés qui
ne sont pas liées à la tâche qu’il entreprend. Cet
agent va potentiellement apprendre une ontolo-
gie qui lui permettra de prendre des décisions
correctes par rapport à tous les types d’objets
rencontrés. Dans le second cas, un agent ne par-
viendra pas à éliminer toutes les propriétés qui
ne sont pas liées à la tâche qu’il entreprend. Les
agents qui relèvent du deuxième cas remplace-
ront de manière répétée les propriétés qui appar-
tiennent à Pt par des propriétés différentes qui
appartiennent également à Pt. Par conséquent,
ces agents sont capables de prendre des déci-
sions correctes pour différents sous-ensembles
de types d’objets existants à un moment donné.

Précision des ontologies. La figure 7 repré-
sente l’évolution de la précision moyenne pour
différentes tailles du scope des agents. Dans la
section 1, nous supposons que les agents inter-
agissant sur un scope limité de tâches seront
plus précis sur certaines tâches que sur d’autres,
au détriment de leur précision moyenne. Cepen-
dant, nous ne l’observons dans aucune des deux
sous-figures de la figure 7. 7a montre que la réa-
lisation de tâches supplémentaires améliore si-
gnificativement la précision moyenne des agents.
7b montre que la taille du scope des agents a une
incidence minimale sur leur précision moyenne.
Une fois de plus, cela se justifie par les limita-
tions appliquées. Deux cas peuvent être distin-
gués. Dans le premier cas, un agent devient très

01000020000300004000050000600007000080000itération0.00.20.40.60.81.0taux de réussiteFigure 1: Success rate depending on the maximum adapting rank |maxAdaptingRank| = {1,2,3}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches01000020000300004000050000600007000080000itération0.00.20.40.60.81.0taux de réussiteFigure 1: Success rate depending on the maximum adapting rank |maxAdaptingRank| = {1,2,3}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches(a) Agents avec une mémoire illimitée

(b) Agents dont la mémoire est limitée à 4 classes

Figure (7) Évolution de la précision moyenne pour différentes tailles du scope des agents.

base desquelles la précision moyenne des tâches
effectuées est calculée est élevé. Par conséquent,
les agents qui s’attaquent à une seule tâche font
preuve d’une précision moyenne sur les tâches
effectuées supérieure. La figure 9 dépeint l’évo-
lution de la précision sur la meilleure tâche pour
différentes tailles du scope des agents. En exa-
minant 9a, deux observations peuvent être ti-
rées. La première observation est que la préci-
sion moyenne des agents à mémoire illimitée
(figure 7a) est toujours inférieure à la préci-
sion moyenne sur la meilleure tâche des mêmes
agents montrés ici. Ces agents sont donc ca-
pables de se spécialiser en restreignant le scope
de leurs tâches. La deuxième observation est que
les agents qui s’attaquent à moins de tâches, ont
une précision moyenne sur leur meilleure tâche
plus faible que les agents qui s’attaquent à toutes
les tâches. Plus précisément, plus le scope des
agents est restreint, plus cette précision est faible.
Ainsi, nous pouvons conclure que la spécialisa-
tion des agents disposant d’une mémoire illimi-
tée n’apporte aucun avantage en termes de pré-
cision. Dans la configuration examinée, chaque
configuration dépend de propriétés différentes.
Par conséquent, notre observation n’est pas liée
à la transférabilité des connaissances d’une tâche
à l’autre, puisque l’apprentissage de la décision
par rapport à une tâche n’est pas lié à l’appren-
tissage de la décision pour une autre tâche. Au
contraire, elle est justifiée par le fait que les
agents s’attaquant à toutes les tâches construisent
des ontologies plus complètes et par conséquent
associent les décisions apprises à une classifica-
tion plus détaillée. La figure 9b montre que les
agents à mémoire limitée se spécialisent égale-
ment. Cependant, cette spécialisation n’est pas

Figure (8) Évolution de la précision moyenne des tâches
entreprises pour différentes tailles du scope des agents.

précis dans une tâche et nettement moins pré-
cis dans les autres. Dans le second cas, un agent
apprend une ontologie lui permettant de devenir
moyennement précis sur plusieurs tâches. Cela
se traduit par des agents qui démontrent des pré-
cisions moyennes qui sont statistiquement indis-
cernables les unes des autres, indépendamment
de la taille de leur scope.

La figure 8 montre l’évolution de la précision
moyenne sur les tâches que les agents entre-
prennent pour différentes tailles de leur scope.
Elle montre que plus le scope des agents est
petit, plus leur précision moyenne sur les tâches
effectuées est élevée. Ce résultat est attendu pour
la raison suivante. Un agent dans la configura-
tion examinée peut devenir très précis sur une
tâche au maximum. Cependant, plus le scope des
agents est grand, plus le nombre de tâches sur la

01000020000300004000050000600007000080000itération0.00.20.40.60.81.0précision moyenne sur toutes les tâchesFigure 3: Average accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches01000020000300004000050000600007000080000itération0.00.20.40.60.81.0précision moyenne sur toutes les tâchesFigure 3: Average accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches01000020000300004000050000600007000080000itération0.00.20.40.60.81.0précision moyenne des tâches entreprisesFigure 5: Average specialization accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches(a) Agents avec une mémoire illimitée
Figure (9) Précision moyenne sur la meilleure tâche en fonction du scope des agents.

(b) Agents dont la mémoire est limitée à 4 classes

connaissances indépendamment de la taille de
leur scope et (2) leur précision moyenne ne
dépend pas du nombre de tâches qu’ils entre-
prennent. La figure 10 nous permet d’examiner
si les sociétés d’agents bénéficient d’un scope
réduit. Les résultats montrent que plus ce scope
est réduit, plus le taux de décisions correctes
pour une société est élevé. En d’autres termes, si
la modification de la taille du scope des agents
ne les rend ni moins, ni plus spécialisés, elle per-
met aux agents de prendre des décisions pour les
tâches qu’ils maîtrisent mieux.

Analyse statistique. Notre analyse de va-
riance (one-way ANOVA) montre que la taille
du scope a un impact statistiquement significatif
(p ≤ 0.01) sur (1) le taux de réussite, (2) la pré-
cision moyenne sur les tâches effectuées et (3)
le taux moyen de décisions correctes. La taille
du scope n’a pas d’impact statistiquement signi-
ficatif (p > 0.01) sur (1) la précision sur leur
meilleure tâche et (2), la précision moyenne sur
toutes les tâches existantes.

7 Conclusions

Ici nous examinons si les sociétés d’agents béné-
ficient de la spécialisation. Nous proposons une
expérience où les agents font évoluer des onto-
logies en interagissant sur un ensemble limité de
tâches. En exploitant ce cadre, nous montrons
que les agents à mémoire limitée spécialiseront
leurs connaissances quel que soit le nombre de
tâches qu’ils effectuent. Cependant, nos résul-
tats montrent qu’en assignant différentes tâches
à différents agents, les sociétés améliorent leur
taux de décision correcte. Jusqu’à présent, nous

Figure (10) Évolution du taux de décision correcte pour
différentes tailles du scope des agents.

liée à la taille de leur scope, mais plutôt aux li-
mites de la mémoire. Les agents deviendront très
précis au maximum sur une tâche, qu’ils s’at-
taquent à une ou plusieurs tâches. En d’autres
termes, le fait que les agents puissent s’abstenir
de toute interaction, sauf celles qui concernent la
seule tâche qu’ils entreprennent, ne leur permet
pas d’améliorer leur précision sur leur meilleure
tâche. Nous avons vérifié que cela est également
vrai pour des capacités de mémoire plus élevées
(8 et 12 classes respectivement). L’hypothèse 2
est donc soutenue uniquement pour des agents
qui ne sont confrontés à aucune limitation.

La figure 10 représente l’évolution du taux
moyen de décisions correctes pour différentes
tailles du scope des agents. Jusqu’à présent, les
résultats montrent que les agents confrontés à
des limitations de mémoire (1) spécialisent leurs

01000020000300004000050000600007000080000itération0.00.20.40.60.81.0précision moyenne sur la meilleure tâcheFigure 4: Maximum accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches01000020000300004000050000600007000080000itération0.00.20.40.60.81.0précision moyenne sur la meilleure tâcheFigure 4: Maximum accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.1 tâche2 tâches3 tâches01000020000300004000050000600007000080000iteration0.00.20.40.60.81.0taux de décisions correctesFigure 6: Average correct decision rate depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.1 tâche2 tâches3 tâchesConference on Empirical Methods in Natu-
ral Language Processing and the 9th In-
ternational Joint Conference on Natural
Language Processing (EMNLP-ĲCNLP),
pages 3700–3710, Hong Kong, China,
2019.

[7] Serhii Havrylov and Ivan Titov. Emer-
gence of language with multi-agent games :
Learning to communicate with sequence
In 5th International Confe-
of symbols.
rence on Learning Representations (ICLR
17, workshop track), Toulon, France, 2017.
20230110-MTOA
experiment description, 2023. URL : https:
//sake.re/20230110-MTOA.

[8] Andreas Kalaitzakis.

[9] Andreas Kalaitzakis.

20230120-MTOA
experiment description, 2023. URL : https:
//sake.re/20230120-MTOA.

[10] Lazy lavender. 2020. URL : https://gitlab.

inria.fr/moex/lazylav.

[11] David Lewis. Convention : A philosophical
study. Synthese, 26(1) :153–157, 1969.
[12] Chengzhi Mao, Amogh Gupta, Vikram Ni-
tin, Baishakhi Ray, Shuran Song, Junfeng
Yang, and Carl Vondrick. Multitask lear-
ning strengthens adversarial robustness.
In Andrea Vedaldi, Horst Bischof, Tho-
mas Brox, and Jan-Michael Frahm, editors,
Computer Vision – ECCV 2020, pages 158–
174, Cham, 2020.

[13] Luc Steels. What triggers the emergence
of grammar ? In AISB’05 : Proceedings
of the Second International Symposium on
the Emergence and Evolution of Linguis-
tic Communication (EELC’05), pages 143–
150, Hatfield, United Kingdom, 2005.
[14] Jonathan Thomas, Raul Santos-Rodriguez,
Mihai Anca, and Robert Piechocki. Multi-
lingual agents through multi-headed neu-
ral networks. volume 4, Tromsø, Norway,
2023.

[15] Jun Wang and Les Gasser. Kmutual on-
line ontology alignment. In Proc. 1st ACM
international conference on Autonomous
Agents and Multi-Agent Systems (AAMAS),
Bologna, Italy, 2002.

n’avons examiné que les sociétés homogènes,
c’est-à-dire les sociétés où tous les agents effec-
tuent le même nombre de tâches. Ce travail peut
être un tremplin vers l’exploration de sociétés
d’agents plus complexes. Par exemple, il serait
intéressant d’examiner des sociétés d’agents où
des agents multitâches coexistent avec des agents
qui se concentrent sur des tâches spécifiques.

Remerciements

Ce travail a été partiellement supporté par la
chaire MIAI “Knowledge communication and
evolution” (ANR-19-P3IA-0003).

Références

[1] Franz Baader, Diego Calvanese, Deborah
McGuinness, Daniele Nardi, and Peter F.
Patel-Schneider, editors. The Description
Logic Handbook : Theory, Implementation,
and Applications. Cambridge University
Press, 2003.

[2] Yasser Bourahla, Manuel Atencia, and
Jérôme Euzenat. Knowledge improve-
ment and diversity under interaction-driven
adaptation of learned ontologies. In Proc.
20th ACM international conference on Au-
tonomous Agents and Multi-Agent Systems
(AAMAS), London, United Kingdom, pages
242–250, 2021.

[3] Michael Cogswell, Jiasen Lu, Stefan Lee,
Devi Parikh, and Dhruv Batra. Emer-
gence of compositional
language with
deep generational transmission. CoRR,
arXiv:1904.
abs/1904.09067, 2019.
09067.

[4] Jianping Fan, Yuli Gao, and Hangzai Luo.
Integrating concept ontology and multitask
learning to achieve more effective classifier
training for multilevel image annotation.
IEEE Transactions on Image Processing,
17(3) :407–426, 2008.

[5] Mohamed Ghalwash, Zĳun Yao, Prithwish
Chakraporty, James Codella, and Daby
Sow. Phenotypical ontology driven fra-
In Pro-
mework for multi-task learning.
ceedings of the Conference on Health, In-
ference, and Learning, CHIL ’21, page
183–192, New York, USA, 2021.

[6] Laura Harding Graesser, Kyunghyun Cho,
linguis-
and Douwe Kiela.
tic phenomena in multi-agent communica-
In Proceedings of the 2019
tion games.

Emergent

