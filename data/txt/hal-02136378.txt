Improving Domain Adaptation By Source Selection
Kevin Bascol, Rémi Emonet, Elisa Fromont

To cite this version:

Improving Domain Adaptation By Source Selec-
ICIP 2019 - IEEE International Conference on Image Processing, Sep 2019, Taïpei, Taiwan.

Kevin Bascol, Rémi Emonet, Elisa Fromont.
tion.
￿10.1109/ICIP.2019.8803325￿. ￿hal-02136378￿

HAL Id: hal-02136378

https://hal.science/hal-02136378

Submitted on 22 May 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

IMPROVING DOMAIN ADAPTATION BY SOURCE SELECTION

Kevin Bascol † (cid:91)

R´emi Emonet †

´Elisa Fromont (cid:63)

† Laboratoire Hubert Curien UMR 5516, Univ Lyon, UJM-Saint-Etienne
F-42023, Saint-Etienne, France
(cid:63)IRISA/INRIA rba, Uni Rennes 35042 Rennes cedex, France
(cid:91)Bluecime inc. 38330 Montbonnot Saint-Martin, France

ABSTRACT

Domain adaptation consists in learning from a source data
distribution a model that will be used on a different target
data distribution. The domain adaptation procedure is usually
unsuccessful if the source domain is too different from the
target one. In this paper, we study domain adaptation for im-
age classiﬁcation with deep learning in the context of multiple
available source domains. We propose a multisource domain
adaptation method that selects and weights the sources based
on inter-domain distances. We provide encouraging results
on both classical benchmarks and a new real world applica-
tion with 21 domains.

Index Terms— Domain Adaptation, Negative Transfer,

Deep Learning, Image Classiﬁcation.

1. INTRODUCTION AND RELATED WORK

Domain adaptation [1] consists in learning from a (labeled)
source data distribution, a model that will be used on a differ-
ent (but related and often unlabeled) target data distribution.
Many real word tasks require the use of domain adaptation
simply because of a lack of (target) labeled data or because of
some shift between the source and the target data distribution
that prevents from successfully using the learned model on the
target data. When using deep learning, the most common do-
main adaptation algorithmic setting is to construct a common
representation space for the two domains while keeping good
performance on the source labeling task. This can be achieved
through the use of adversarial techniques where feature rep-
resentations from samples in different domains are encour-
aged to be indistinguishable [2], [3]. Whatever the technique,
the domain adaptation procedure is usually unsuccessful if the
source domain is too different from the target one. In [4] for
example, the authors have empirically identiﬁed positive and
negative transfer situations. We study domain adaptation for
image classiﬁcation with deep learning in the context of mul-
tiple available source domains and when no label are available
on the target domain.

Notations We consider having D source domains. Data
of the ith domain are noted Zi and are composed of examples

Xi and labels Yi. The target domain is given the index j.

A number of related works propose to select or weight
(elements of) the source domains in order to improve the test
accuracy on the target domain but none of these works ex-
plicitly evaluate and propose solutions to overcome the effect
of negative transfer during the adaptation process. For exam-
ple, the work from [5] considers transfer learning from only
one source domain and when the target task is a sub-task of
the source task (as for us, no target label is available). They
also extend the work of [2] but they decompose the domain
classiﬁer according to each source classes. During the adap-
tation phase, each target example is weighted according to
the class-domain classiﬁer loss. The works from [6], [7], [8]
and [9] tackle the problem of multi-source domain adapta-
tion but their selection scheme makes use of a few target la-
bels and is used to select one single source domain. The un-
published work from [10] is the closest to ours. The authors
propose to select multiple domains according to four possi-
ble distances (the χ2-divergence, the Maximum Mean Dis-
crepancy, the Wassertein distance and the Kullback-Liebler
divergence) and according to the classiﬁcation performance
on each single source domain. Both the distance and the per-
formance features are weighted by a parameter β computed
as:

β = arg min

D
(cid:88)

D
(cid:88)

β≥0

i=1

k=1;k(cid:54)=i

|ξ(Zi, Zk) − βf (Zi, Zk)|

(1)

with f the set of considered features and ξ(Zi, Zk) the
performance of the classiﬁer trained on Zi and tested on
Zk. The authors show that on a homogeneous dataset, their
method is better than randomly selecting the domains but not
better than when using all of them. However, on a hetero-
geneous dataset, selecting the sources with their proposed
distance is better than both selecting all the domains and se-
lecting them randomly. Note that to optimize β, D classiﬁers
should be trained which can be costly in practice (especially
for deep neural networks). Besides, the authors do not pro-
vide any criterion to set the number of selected sources.

In Section 2, we show our strategy to automatically se-
lect the best sources to avoid negative transfer during do-

main adaptation. Section 3 shows extensive experiments and
promising results on both classical benchmarks and a new real
world application related to ski-resort chairlift security. We
conclude in Section 4.

2. DOMAIN SELECTION AND WEIGHTING

Considering a target domain j and D source domains, we pro-
pose an approach that automatically computes a weight vec-
tor pj ∈ ∆D−1 ⊂ RD (probability simplex) and uses pj
to reweight the domains (when sampling minibatches) during
“domain-adversarial training [2]”. This training phase is usu-
ally done to ﬁne-tune a pre-trained network. The proposed
approach is modular as we decompose the computation of the
domains weight vector pj in three conﬁgurable steps :

1. the distance vector dj =

(cid:110)

dj
i

(cid:111)D

is computed (dis-

i=1

tance of each source domain, i, to the target one, j),

2. it is mapped to a score vector sj = score(dj),

3. it is normalized to a probability vector pj = sj/ (cid:80)

i sj
i .

Computing pairwise dataset distances We focus here
on a distance based on optimal-transport but other distances
could be considered. For instance, one could use the average
minimal Euclidean distance (dj
or a distance based on auto-encoder where an autoencoder
AEj is trained with the points of domain j and dj
i =
(cid:107)x − AEj(x)(cid:107)2 (average squared reconstruc-
averagex∈Xi
tion error).

i = averagex∈Xi

The optimal transport problem aims at ﬁnding the min-
imal cost for transforming a data distribution into another
one [11]. This minimal cost is deﬁned as a sum of the (proba-
bility) mass to displace multiplied by the given displacement
price (for instance the euclidean distance). The minimal cost
is called the Wasserstein distance and constitutes a distance
between distribution. In our discrete case (samples of points),
the distance can be expressed as:

(cid:88)

(cid:88)

dj
i =

γ∗
x,yCx,y

x∈Xj

y∈Xi

(2)

where C is a distance matrix between all pair of elements
of the two domains. The optimal transport plan γ∗ is obtain
by solving the optimal transport problem:

γ∗ = arg min
γ∈Π(ˆµj ,ˆµi)

(cid:104)γ, C(cid:105)F

(3)

(cid:110)

where F is the Frobenius inner product and the constraint
(cid:111)

set Π(ˆµj, ˆµi) =
ensures that the transport plan γ does not create or remove
mass (by ensuring that the marginal distributions ˆµj and ˆµi
are preserved).

| γ1 = ˆµS, γT 1 = ˆµT

γ ∈ R|Xj |×|Xi|
+

Two set of points, even if drawn from the same distribu-
tion, will exhibit a variable non-zero Wasserstein distance.
To compensate for this sampling-induced bias and variance,
we normalize the Wasserstein distance dj
i by subtracting
the mean and dividing by the variance, of dj
j, obtained by
sampling subsets of the source domain and computing their
Wasserstein distance.

Transforming distances into scores Possible score func-
d ) or the inverse squared
d2 ). Here, we focus on the negative exponential

tions include the inverse distances ( 1
distances ( 1
scoring function:

i

i = e−λd j
sj
The parameter λ allows a smooth interpolation between
putting all the weight on the closest domain and having a uni-
form distribution of all domains. Thanks to this parameter,
we will be able to control the variety of the subset of domains
we are considering (see below).

(4)

Ensuring training set variety In case of many source do-
mains and when some of them have a very small number of
training examples, it becomes important to avoid selecting too
few domains in the process (e.g., a single one). For general-
ization (i.e. avoiding overﬁtting) and transfer purpose, the
training set should exhibit enough variety. For domain sizes
n ∈ ND, a probability vector pj and a draw of N training
samples (with replacement), we deﬁne the training set variety
as the expected number of distinct samples we will use for
training. The variety can be approximated as:

variety(pj, n, N ) ≈



(cid:32)

ni ·

1 −

1 −

(cid:33)N 


pj
i
ni

(cid:88)

i

(5)

Our probability vector pj depends on the λ parameter. As
such, by varying λ from ∞ to 0, we can move from a min-
imal variety (sampling from the closest domain i, getting a
diversity of approximately ni, the number of examples in this
closest domain) to a maximal one (using a uniform pj, getting
a variety of N − N (cid:0) N −1
≈ 0.63N in case of a balanced
n). In the experiments, we consider one “epoch” (with re-
placement) of N = (cid:80)
i ni samples. We use pj and n to ﬁnd
the highest value of λ for which the variety is below a target
variety.

(cid:1)N

N

3. EXPERIMENTS

Our backbone classiﬁer is a residual network (ResNet50) [12]
pre-trained on the ImageNet dataset [13]. We report the av-
erage test-accuracy on the target domain using a 5-fold cross
validation procedure. We use the following names to report
the model performance:
– Target (only): models trained on the labeled target dataset.
Note that this is an ideal but unrealistic situation since, in our
actual applications, there is no target label. This setting re-
quires the target domain to be split into training/test sets.

miny∈Xj (cid:107)x − y(cid:107))

– Only near.: models trained using only the nearest domain
(according to our distance measure) to the target one;
– Only far.: models trained using only the farthest domain
(according to our distance measure) to the target one;
– LODO: models trained using all domains but the target
one, using domain adaptation on the remaining target domain,
without using our domain selection method;
– w/o near.: models trained using all domains but two: the
target and the closest domain to the target one (according to
our distance measure) are not used.
– w/o far.: models trained using all domains but two: the tar-
get and the farthest domain to the target one (according to our
distance measure) are not used.
– OURS: models trained by weighting the source domains
with our method, using the variety criterion (with a target va-
riety of half its maximum value).

3.1. Datasets

Ofﬁce-Caltech (O-C) [14] is a classical domain adaptation
benchmark with four domains: Amazon (A), DSLR (D), We-
bcam (W) and Caltech (C). It is composed of the 10 classes
(Backpack, Bike, Calculator, Headphones, Keyboard, Lap-
top, Monitor, Mouse, Mug, and Video-projector) common be-
tween Ofﬁce-31 [15] and Caltech256 [16].

ImageNet-Caltech (I-C). To control the discrepancy be-
tween each domain and validate our chosen domain similar-
ity measure, we have designed a dataset using images from
Caltech101 [17] (C1), Caltech256 [16] (C2) and from Ima-
geNet [13] (IN) with mixed labels (bird, car, chair, dog, and
person, following, among others, [18, 19]) described in Ta-
ble 1. This dataset is composed of three different types of
domain. The ”Good” (G ) domains are created with the true
original classes, the ”Bad” (B ) domains are created with dif-
ferent but similar original classes, and the ”Random” (R ) do-
mains are created with randomly chosen classes in the cor-
responding datasets. With this design, the ”Good” domains
are expected to be closer to each other since the original la-
bels are the same. The ”Bad” domains should be farther away
from the ”Good” ones and the ”Random” datasets should be
the farthest (and are expected to be far from each others too).
Bluecime. In [20], we introduced an image dataset for the
classiﬁcation of risky situations on chairlifts. The task is to
detect if a chairlift vehicle is empty, with passengers in a safe
situation, or with passengers in an unsafe situation (security
railing not put down, children alone, ...). The images come
from 21 different chairlifts: we consider that each chairlift
represents a different domain. This dataset is currently not
publicly available, so the actual sky resort names are replaced
by letters in the corresponding performance table.

3.2. Results

In Figure 1, we show the probabilities we obtain using the
selection process described in Section 2 on the 3 datasets.

Domains

G C1

G IN

B C1

B C2

B IN

R C1

R IN

Class
bird
car
chair
dog
person
bird
car
chair
dog
person
bird
car
chair
dog
person
bird
car
chair
dog
person
bird
car
chair
dog
person
bird
car
chair
dog
person
bird
car
chair
dog
person

Dataset Classes
”pigeon”, ”ﬂamingo”, ”ibis”, ”rooster”, ”emu”
”car side”
”chair”; ”windsor chair”
”dalmatian”
”Faces”; ”Faces easy”
”birds” (56)
”motorcar” (8)
”chair” (3)
”domestic dog” (117)
”individual” (2)
”butterﬂy”; ”dragonﬂy”
”Motorbikes”
”grand piano”
”cougar body”
”buddha”
”024.butterﬂy”
”072.ﬁre-truck”, ”178.school-bus”
”011.billiards”
”105.horse”
”038.chimp”; ”090.gorilla”
”butterﬂy” (6)
”motortruck” (9)
”dining table, board”
”domestic cat” (5)
”apes” (5)
”brain”
”chandelier”
”watch”
”ketch”
”bonsai”
”saltshaker”
”ﬁddler crab”
”brown bear”
”banana”
”dough”

# Ex
294
123
118
67
870
1456
1496
1500
1404
1500
159
798
99
47
85
112
216
278
270
322
1500
1494
1500
1500
1500
98
107
239
114
128
1473
1182
1500
1409
1249

Table 1: Classes used in each dataset to create the different
domains (G = ”good”; B = ”bad”; R = ”random”, C1: from
Caltech101; C2: from Caltech256; IN: from ImageNet). In
parentheses: number of classes composing the superclass that
has been used (e.g. 56 classes of birds).

Fig. 1: Probabilities used to reweight
the different do-
mains: ImageNet-Caltech (top-left), Ofﬁce-Caltech (bottom-
left), Bluecime (right).

In most datasets, only up to three domains have a selection
probably greater than 0.1 and more than half the source do-
mains are totally unused. For instance, during training with
”Bad” ImageNet as the target domain, 67.2% of the training
images come from ”Bad” Caltech256, 21.6% from ”Good”
ImageNet, 6.7% from ”Good” Caltech101, and only 4.6%
from the three other source domains.

G_C1G_INB_C1B_C2B_INR_C1R_ING_C1G_INB_C1B_C2B_INR_C1R_IN2.56.10.76.752.10.096.65.62.121.66.20.00.00.00.12.020.20.01.90.061.767.27.10.01.597.522.497.112.41000.00.04.20.00.90.00.00.00.00.01.71.9AmznC256DSLRWcamAmznC256DSLRWcam10012.617.21009.311.70.00.071.10.00.078.2DCABDCAB0.80.40.70.90.00.23.40.24.70.10.01.50.50.25.60.60.10.40.00.62.839.50.661.10.20.218.81.01.80.424.30.31.30.060.91.814.10.40.10.62.46.40.69.50.00.19.00.47.40.733.07.21.40.22.54.71.80.50.20.56.21.11.05.73.10.51.72.30.112.30.01.62.38.02.87.21.92.60.52.90.433.415.11.60.00.32.70.40.30.10.10.12.20.04.80.21.00.60.10.31.34.90.933.82.436.71.136.12.144.60.80.92.530.55.737.111.73.912.434.80.00.00.00.00.20.00.00.127.40.00.00.00.50.10.10.60.00.10.70.52.04.53.10.11.50.00.00.30.00.00.045.31.00.02.70.047.30.50.00.10.21.00.12.10.926.70.61.40.06.86.10.21.80.80.33.00.43.34.830.50.80.20.20.00.20.027.30.10.00.00.02.30.70.01.21.70.00.10.00.03.92.515.741.53.218.83.02.110.70.523.35.64.627.45.537.53.49.310.412.00.00.90.80.00.40.00.20.10.60.30.10.02.90.00.00.50.01.30.80.20.10.00.20.00.00.00.015.70.01.50.00.01.60.11.00.03.30.40.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.060.40.00.00.90.10.13.20.40.52.60.41.00.12.00.03.32.90.40.40.23.85.63.269.837.43.81.79.10.20.510.40.419.70.80.012.50.80.20.814.30.50.00.51.91.817.66.20.84.68.30.32.333.413.87.20.31.80.20.90.11.21.01.30.13.70.70.51.90.10.031.50.40.40.20.017.93.90.14.60.21.20.00.20.00.00.00.10.20.00.00.20.50.00.30.10.462.10.30.00.10.11.10.50.00.10.00.10.20.44.90.02.20.01.24.10.03.05.80.00.60.04.111.47.31.30.87.01.445.414.41.141.00.216.50.90.42.126.01.02.90.45.462.3Setting

A

Target
Only near.
Only far.

LODO
w/o near.
w/o far.
OURS

95.10
95.30
83.80

94.41
91.06
95.38
94.98

C

94.44
91.32
84.19

92.76
83.20
91.52
92.70

D

92.11
100.0
95.07

99.26
93.92
100.0
100.0

W

92.46
96.43
98.01

98.44
96.22
98.57
98.57

AVG

93.53
95.76
90.27

96.22
91.10
96.37
96.56

G C1 G IN B C1

99.81
98.09
0.88

94.40
77.77
97.20
97.54

96.17
68.84
14.39

87.24
75.56
86.66
84.93

99.56
34.85
38.97

93.86
95.72
96.90
97.38

B C2

98.32
83.79
28.42

91.80
81.88
90.16
94.73

B IN

98.23
81.61
24.95

91.88
86.43
91.99
91.13

AVG

98.42
86.03
21.52

91.67
83.47
92.58
93.14

R C1 R IN

100.0
0.00
3.17

11.76
11.99
18.27
8.89

98.21
2.66
12.53

9.01
5.68
12.40
16.81

AVG

98.61
61.83
17.62

68.56
62.15
70.51
70.20

Table 2: Accuracy averaged over 5 experiments on the Ofﬁce-Caltech datasets (ﬁrst 5 columns) and the datasets created from
ImageNet and Caltech (last 9 columns). We use a ResNet50 pretrained on ImageNet, and train it for 50 epochs (batch-size 64,
learning rate 10−5). The last column is grayed-out as it gives the average including the “Random” domains.

Setting

A

LODO
OURS

95.70
95.63

B

98.26
98.38

C

98.28
98.07

D

98.69
98.94

All 21

95.94
97.06

Table 3: Accuracy on the Bluecime dataset, averaged on 5
experiments, on a selection of 4 domains (the same A/B/C/D
as in [20]) and on all 21 domains.

In Table 2, we show the results on the Ofﬁce-Caltech (O-
C, ﬁrst ﬁve columns) and ImageNet-Caltech (I-C, last nine
columns) datasets. On both datasets, using only the nearest
source domains is beneﬁcial compared to using the farthest
one (+5.49 accuracy points on O-C and +44.21 pts on I-C)
which suggests that our distance is meaningful. On Ofﬁce-
Caltech, using the target domain as source (Target line) gives
worst performance than using the nearest source (-2.23 pts),
which can be explained by the lack of training data which in-
duces overﬁtting phenomena. Since ImageNet-Caltech con-
tains more data, the Target setting is much more suited, as
expected, than the Only nearest one (+36.78 pts).

Using the LODO setting (all source domains are used dur-
ing training), we observe better performance than with the
Only nearest and Only farthest ones thanks to a more diverse
training set (respectively, on O-C, +0.46 pts and +5.95 pts,
on I-C, +6.73 pts and +50.94 pts). This means that there is
a real trade-off between the domain selection and the num-
ber of remaining training data. However, if we remove the
nearest source domain, the performance becomes worst than
for the LODO setting (-5.12 pts on O-C and -6.41pts on I-C),
on Ofﬁce-Caltech we even get worst performance than using
the Only nearest setting (-4.66pts). If we remove the farthest
source domain, we do obtain better performance than with the
LODO setting (+0.15 pts on O-C and +1.95 pts on I-C). We
can conclude that using many data (LODO setting) is impor-
tant and always better than choosing a single domain (even
the most similar one) but selecting a good number of sources
can be beneﬁcial.

Our distance-based weighting approach provides bet-
ter performance than removing the farthest source domain
on Ofﬁce-Caltech (+0.19pts). On ImageNet-Caltech, we

get worst results than when removing the farthest domain
(-0.31pts), but, still notably better than with the LODO set-
ting (+1.64 pts). However, if we ignore the results on the
”Random” domains (which are close to random by design),
on average, the LODO setting gives 91.67% of accuracy,
w/o farthest 92.58%, and our approach allows us to get the
best accuracy performance of 93.14% which conﬁrms the
relevance of our approach.

In Table 3, we show the results on the Bluecime dataset.
Due to the page limit, we only detail the results on 4 domains
(the same ones as in [20]) and give the averaged results over
the 21 domains. As with the other datasets, we obtain bet-
ter results by selecting the source domains (+1.12 pts). This
shows that the proposed method works well even when there
are much more source domains to select from.

4. CONCLUSION

We have shown that unsupervised domain adaptation can be
improved by selecting and weighting a good subset of the
sources that are the most similar to the target domain. Our
approach weights the sources according to the Wasserstein
distance between unlabeled domain distributions and accord-
ing to the variety of the data in the selected sources. Extensive
experiments showed the relevance of our proposed weighting
scheme. Future work involves exploring and reporting the
behavior of our approach with different settings (e.g. combi-
nation of distances, scoring functions, variety criterion), that
were left out due to the page limit.

Acknowledgements

This work is supported by the FUI MIVAO project.

5. REFERENCES

[1] Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan, “A theory of learning from different domains,”

Machine Learning, vol. 79, no. 1, pp. 151–175, May
2010.

Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei,
“Imagenet: A large-scale hierarchi-
cal image database,” in Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on.
Ieee, 2009, pp. 248–255.

[14] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grau-
man, “Geodesic ﬂow kernel for unsupervised domain
adaptation,” in Computer Vision and Pattern Recogni-
tion (CVPR), 2012 IEEE Conference on. IEEE, 2012,
pp. 2066–2073.

[15] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Dar-
rell, “Adapting visual category models to new domains,”
in European conference on computer vision. Springer,
2010, pp. 213–226.

[16] Gregory Grifﬁn, Alex Holub, and Pietro Perona,

“Caltech-256 object category dataset,” 2007.

[17] Li Fei-Fei, Rob Fergus, and Pietro Perona, “Learning
generative visual models from few training examples:
An incremental bayesian approach tested on 101 object
categories,” Computer vision and Image understanding,
vol. 106, no. 1, pp. 59–70, 2007.

[18] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz,
Alexei A Efros, and Antonio Torralba, “Undoing the
damage of dataset bias,” in European Conference on
Computer Vision. Springer, 2012, pp. 158–171.

[19] Chen Fang, Ye Xu, and Daniel N Rockmore, “Unbiased
metric learning: On the utilization of multiple datasets
and web images for softening bias,” in Proceedings of
the IEEE International Conference on Computer Vision,
2013, pp. 1657–1664.

[20] Kevin Bascol, R´emi Emonet, Elisa Fromont, and Raluca
Debusschere, “Improving chairlift security with deep
in International Symposium on Intelligent
learning,”
Data Analysis. Springer, 2017, pp. 1–13.

[2] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, Franois Laviolette,
“Domain-
Mario Marchand, and Victor Lempitsky,
adversarial training of neural networks,” JMLR, 2016.

[3] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor
“Adversarial discriminative domain adapta-
Darrell,
tion,” in 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR, 2017, pp. 2962–2971.

[4] Michael T Rosenstein, Zvika Marx, Leslie Pack Kael-
bling, and Thomas G Dietterich, “To transfer or not to
transfer,” in NIPS 2005 workshop on transfer learning,
2005, vol. 898, pp. 1–4.

[5] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and
Michael I Jordan, “Partial transfer learning with selec-
tive adversarial networks,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, 2018, pp. 2724–2732.

[6] Rita Chattopadhyay, Qian Sun, Wei Fan, Ian David-
son, Sethuraman Panchanathan, and Jieping Ye, “Mul-
tisource domain adaptation and its application to early
detection of fatigue,” ACM Transactions on Knowl-
edge Discovery from Data (TKDD), vol. 6, no. 4, pp.
18, 2012.

[7] Lixin Duan, Dong Xu, and Shih-Fu Chang, “Exploiting
web images for event recognition in consumer videos:
A multiple source domain adaptation approach,”
in
Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on. IEEE, 2012, pp. 1338–1345.

[8] Liang Ge, Jing Gao, Hung Ngo, Kang Li, and Aidong
Zhang, “On handling negative transfer and imbalanced
distributions in multiple source transfer learning,” Sta-
tistical Analysis and Data Mining: The ASA Data Sci-
ence Journal, vol. 7, no. 4, pp. 254–271, 2014.

[9] Muhammad Jamal Afridi, Arun Ross, and Erik M
Shapiro,
“On automated source selection for trans-
fer learning in convolutional neural networks,” Pattern
Recognition, vol. 73, pp. 65–75, 2018.

[10] Lex Razoux Schultz, Marco Loog, and Peyman Mo-
hajerin Esfahani, “Distance based source domain se-
arXiv preprint
lection for sentiment classiﬁcation,”
arXiv:1808.09271, 2018.

[11] C´edric Villani, Optimal transport: old and new, vol.
338, Springer Science & Business Media, 2008.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in

