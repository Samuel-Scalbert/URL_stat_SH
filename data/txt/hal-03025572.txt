NegPSpan: eﬀicient extraction of negative sequential
patterns with embedding constraints
Thomas Guyet, René Quiniou

To cite this version:

Thomas Guyet, René Quiniou. NegPSpan: eﬀicient extraction of negative sequential patterns with em-
bedding constraints. Data Mining and Knowledge Discovery, 2020, 34, pp.563-609. ￿10.1007/s10618-
019-00672-w￿. ￿hal-03025572￿

HAL Id: hal-03025572

https://inria.hal.science/hal-03025572

Submitted on 26 Nov 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Data Mining and Knowledge Discovery manuscript No.
(will be inserted by the editor)

NegPSpan: eﬃcient extraction of negative sequential patterns
with embedding constraints

Thomas Guyet · René Quiniou

Received: date / Accepted: date

Abstract Mining frequent sequential patterns consists in extracting recurrent behav-
iors, modeled as subsequences, in a big sequence dataset. Such patterns inform about
which events are frequently observed in sequences, i.e. events that really happen.
Sometimes, knowing that some speciﬁc event does not happen is more informative
than extracting observed events. Negative sequential patterns (NSPs) capture recur-
rent behaviors by patterns having the form of sequences mentioning both observed
events and the absence of events. Few approaches have been proposed to mine such
NSPs. In addition, the syntax and semantics of NSPs diﬀer in the diﬀerent methods
which makes it diﬃcult to compare them. This article provides a uniﬁed framework
for the formulation of the syntax and the semantics of NSPs. Then, we introduce a new
algorithm, NegPSpan, that extracts NSPs using a PreﬁxSpan depth-ﬁrst scheme, en-
abling maxgap constraints that other approaches do not take into account. The formal
framework highlights the diﬀerences between the proposed approach and the methods
from the literature, especially with the state of the art approach eNSP. Intensive ex-
periments on synthetic and real datasets show that NegPSpan can extract meaningful
NSPs and that it can process bigger datasets than eNSP thanks to signiﬁcantly lower
memory requirements and better computation times.

Keywords Sequential patterns mining · pattern semantics · absence modeling ·
negative containment

T. Guyet
Agrocampus Ouest/IRISA-UMR6074
65 rue de Saint Brieuc, 35042 Rennes, France
E-mail: thomas.guyet@irisa.fr

R. Quiniou
Inria, Univ Rennes, CNRS, IRISA

2

1 Introduction

Thomas Guyet, René Quiniou

In many application domains such as diagnosis or marketing, decision makers show
a strong interest for rules that associate speciﬁc events (a context) to undesirable
events to which they are correlated or that are frequently triggered in such a context.
In the pattern mining ﬁeld, such rules are extracted from dataset and formalized by
patterns. Patterns are substructures that appear in a structured dataset. The patterns can
refer to diﬀerent structural forms: itemsets, subsequences, subgraphs. For example, a
sequential pattern is a subsequence, such as buying ﬁrst milk, then bread, and then
chocolate. This subsequence of purchases is interesting when it occurs in a shopping
history database (the structure dataset). Sequential pattern mining algorithms can
extract such hidden rules from execution traces or transactions. In the classical setting,
sequential patterns contain only positive events, i.e. really observed events. However,
the absence of a speciﬁc action or event can often better explain the occurrence
of an undesirable situation (Cao et al., 2015). For example in diagnosis, if some
maintenance operations have not been performed, e.g. damaged parts have not been
replaced, then a fault will likely occur in a short delay while if these operations
would have been performed in time the fault would not occur. In marketing, if some
market-place customer has not received special oﬀers or coupons for a long time then
she/he has a high probability of churning while if she/he would have been provided
such special oﬀers she/he should remain loyal to her/his market-place. In these two
cases, mining speciﬁc events, some present and some absent, to discover under which
context some undesirable situation occurs or not, may provide interesting so-called
actionable information for determining which action should be performed to avoid
the undesirable situation, i.e. fault in diagnosis, churn in marketing.

We aim at discovering sequential patterns that take into account the absence of
some events called negative events (Cao et al., 2015). Moreover, we want to take into
account some aspects of the temporal dimension as well, maximal span of pattern
occurrences or maximal gap between the occurrences of pattern events. For example,
suppose that from a sequence dataset, we want to mine a sequential pattern (cid:104)𝑎 𝑏(cid:105) with
the additional negative constraint telling that the event 𝑐 should not appear between
events 𝑎 and 𝑏 (the scope of the constraint). The corresponding negative pattern is
represented as (cid:104)𝑎 ¬𝑐 𝑏(cid:105), where the logical sign ¬ indicates the absence of the event or
the set of events in its scope. Once the general idea of introducing negative events in a
pattern has been stated, the syntax and semantics of such negative patterns should be
clearly formulated since they have a strong impact both on algorithms outcome and
their computational eﬃciency. As we will see, the few algorithms from literature do
not use the same syntactical constraints and rely on very diﬀerent semantical principles
(see Section 7). More precisely, the eﬃciency of eNSP (Cao et al., 2016), the state-of-
the-art algorithm for NSP mining, comes from a semantic for negation that enables
eﬃcient operations on the sets of supported sequences. The two major computational
limits of eNSP are memory requirements and the impossibility for eNSP to handle
embedding constraints such as the classical maxgap and maxspan constraints. Maxgap
and maxspan constraints enforce the events of a pattern occurrence to not be too distant
in time. Intuitively, if events are far from each others, they are not related/ Thus, they

NegPSpan: eﬃcient extraction of negative sequential patterns

3

might not match the pattern. In addition, such constraints can prune eﬃciently the
exploration of occurrences during the sequence dataset scan.
The two main contributions of this article are as follows:

– a clariﬁcation of the syntactic deﬁnition of negative sequential patterns and dif-

ferent negation semantics with their associated properties.

– a complete and correct algorithm called NegPSpan, inspired by algorithm PreﬁxS-
pan, to extract negative sequential patterns with maxgap and maxspan constraints.

Intensive experiments compare, on synthetic and real datasets, the performance of
NegPSpan and eNSP as well as the pattern sets extracted by each of them. As results,
they show that algorithm NegPSpan is more time-eﬃcient than eNSP for mining long
sequences thanks to the maxgap constraint and that its memory requirement is several
orders of magnitude lower, enabling to process much larger datasets. In addition, they
highlight that eNSP misses interesting patterns on real datasets due to semantical
restrictions.

2 Frequent sequential pattern mining

This section introduces frequent sequential pattern mining, its basic concepts and
main results. This data mining task was introduced at the early ages of the pattern
mining ﬁeld (Srikant and Agrawal, 1996).

Let (I, <) be the ﬁnite set of items (alphabet) associated with a total order
(e.g. lexicographic order). In the sequel, [𝑛] = {1, · · · , 𝑛} denotes the set of the ﬁrst
𝑛 strictly positive integers.

Deﬁnition 1 (Sequence) An itemset 𝐴 = {𝑎1 𝑎2 · · · 𝑎𝑚} ⊆ I is a set of items. | 𝐴|
denotes the size of the itemset 𝐴. A sequence 𝒔 is a ﬁnite set of sequentially ordered
itemsets 𝒔 = (cid:104)𝑠1 𝑠2 · · · 𝑠𝑛(cid:105). This means that 𝑠𝑖 appears before 𝑠 𝑗 in sequence 𝒔 for
all 𝑖, 𝑗 ∈ [𝑛], 𝑖 < 𝑗. This sequence starts by 𝑠1 and ﬁnishes by 𝑠𝑛. 𝑛 is the length
of the sequence, denotes |𝒔|. The size of the sequence is the total number of items it
contains.

2 · · · 𝑠(cid:48)
𝑠(cid:48)

Deﬁnition 2 (Subsequence and embedding) Let 𝒔 = (cid:104)𝑠1 𝑠2 · · · 𝑠𝑛(cid:105) and 𝒔(cid:48) =
𝑚(cid:105) be two sequences, 𝒔(cid:48) is a subsequence of 𝒔, denoted 𝒔(cid:48) (cid:22) 𝒔, iﬀ there
(cid:104)𝑠(cid:48)
1
exists an increasing sequence of 𝑚 indexes 𝑒𝑖 ∈ [𝑛] such that 𝑒𝑖 < 𝑒𝑖+1 for all
𝑖 ⊆ 𝑠𝑒𝑖 for all 𝑖 ∈ [𝑚]. (𝑒𝑖)𝑖 ∈ [𝑚] ∈ [𝑛] 𝑚 is called an embedding of
𝑖 ∈ [𝑚 − 1] and 𝑠(cid:48)
𝒔(cid:48) in 𝒔.

Deﬁnition 3 (Sequential pattern) A sequential pattern 𝒑 = (cid:104)𝑝1 𝑝2 · · · 𝑝𝑚(cid:105) is a
sequence over the set of items I. Let 𝒔 be a sequence and 𝒑 be a sequential pattern.
Sequence 𝒔 supports pattern 𝒑 (or 𝒑 occurs in sequence 𝒔) iﬀ 𝒑 is a subsequence of
𝒔, i.e. 𝒑 (cid:22) 𝒔.

Example 1 (Sequence and sequential patterns) In this example, we illustrate the
diﬀerent notions of frequent sequential pattern mining. We assume that I = {𝑎, 𝑏, 𝑐, 𝑑,

4

Thomas Guyet, René Quiniou

𝑒, 𝑓 } and we consider the following dataset of six sequences:

D =

𝒔1 = (cid:104)𝑎 (𝑒 𝑓 ) 𝑏 𝑏 (𝑐𝑒)(cid:105)
𝒔2 = (cid:104)𝑒 𝑎 (𝑐𝑑) 𝑏 𝑑 𝑏 (𝑐𝑒) (𝑑𝑒)(cid:105)
𝒔3 = (cid:104)𝑎 𝑑 𝑏 (𝑐𝑒) 𝑓 (cid:105)
𝒔4 = (cid:104)𝑏 (𝑐𝑒) 𝑎 𝑑 𝑓 𝑏(cid:105)
𝒔5 = (cid:104)(𝑎𝑐𝑑) 𝑐 𝑏 𝑒 𝑏 (𝑐𝑒) 𝑑(cid:105)
𝒔6 = (cid:104)𝑐 𝑒 𝑏 𝑑(cid:105)






.






The length of sequence 𝒔1 is 5. (𝑒 𝑓 ) and (𝑐𝑒) denote itemsets containing 2 items

occurring at the same time in the sequence.

Let 𝒑 = (cid:104)𝑎 𝑏 𝑏 (𝑐𝑒)(cid:105) be a sequential pattern, 𝒑 occurs in sequences 𝒔1, 𝒔2, 𝒔5.
The embedding of 𝒑 in 𝒔5 is (1, 3, 5, 6). Deﬁnition 2 requires that each itemset of the
pattern is a subset of some sequence itemset and respects the pattern and the sequence
orderings. In addition, according to the deﬁnition of a subsequence (Deﬁnition 2),
successive itemsets of a sequential pattern do not need to occur in a row in the
sequence. Some items or itemsets can appear between the sequence occurrences of
two successive itemsets of the sequential pattern. Indeed, 𝑎, the ﬁrst item of 𝒑, is a
subset of (𝑎𝑐𝑑) in 𝒔5. Also, item 𝑐 appears between the occurrences of 𝑎 and 𝑏 in 𝒔5,
and item 𝑒 appears between the occurrences of the second and third itemsets of 𝒑.
But, 𝒑 does not occur in 𝒔6 because 𝒔6 does not contain any item 𝑎. 𝒑 does not occur
in 𝒔4 because the items of the pattern 𝒑 are not in the correct order. 𝒑 does not occur
in 𝒔3 because item 𝑏 must occur twice (the embedding must strictly increase).

Let us now consider the problem of mining frequent sequential patterns from
a dataset of sequences, denoted D. The main idea behind mining frequent patterns
assumes that the more frequent a pattern the more interesting it is. In practice, a pattern
is said to be frequent when it occurs more frequently than a user-deﬁned threshold 𝜎.
The following deﬁnitions formalize these intuitive deﬁnitions.

Deﬁnition 4 (Sequential pattern support) Let 𝒑 be a sequential pattern and D =
{𝒔𝒊 } a dataset of sequences where 𝒔𝒊 is the 𝑖-th sequence of D.

The support of 𝒑 in D, denoted 𝑠𝑢 𝑝 𝑝( 𝒑), is the number of sequences that

support 𝒑:

𝑠𝑢 𝑝 𝑝( 𝒑) = |{𝒔𝒊 ∈ D | 𝒑 (cid:22) 𝒔𝒊 }| .

The support of some sequential pattern 𝒑 is the number of sequences from the
dataset in which 𝒑 occurs. It is worth noting that the support is the number of
sequences that contains a pattern and not the number of embeddings of the pattern in
the sequences. If the pattern occurs several times in a sequence, this sequence counts
only for 1 in the support.

Deﬁnition 5 (Frequent sequential pattern mining) Let D be a dataset of sequences
and 𝜎 be a threshold, mining frequent sequential patterns consists in extracting the
complete list of patterns having a support greater than a given threshold 𝜎:

{ 𝒑 | 𝑠𝑢 𝑝 𝑝( 𝒑) ≥ 𝜎}.

NegPSpan: eﬃcient extraction of negative sequential patterns

5

A naive approach to solve the problem of mining frequent sequential pattern
mining consists in enumerating all potential sequential patterns and to evaluate their
support. But, this approach is practically intractable on large datasets due to the
large size of the set of sequential patterns. The algorithmic strategies that have been
developed in the ﬁeld of sequential pattern mining are based on the anti-monotonicity
property of the support measure.

Proposition 1 (Support anti-monotonicity (Srikant and Agrawal, 1996)) Let D a
dataset of sequences and 𝒑, 𝒑(cid:48) two sequential patterns, the anti-monotonicity states
that:

𝒑 (cid:22) 𝒑(cid:48) =⇒ 𝑠𝑢 𝑝 𝑝( 𝒑) ≥ 𝑠𝑢 𝑝 𝑝( 𝒑(cid:48))

The anti-monotonicity property of the support with respect to pattern inclusion
states that if a pattern 𝒑(cid:48) is a super-pattern of 𝒑 (in other words, that 𝒑 is a subsequence
of 𝒑(cid:48)), then the support of 𝒑(cid:48) is lower than the support of 𝒑(cid:48). The intuition behind
this property, is that each time the super-pattern 𝒑(cid:48) occurs in a sequence of D, then
any of its sub-pattern 𝒑 also occurs in the same sequence, thus the support of 𝒑 is at
least equal to or greater than the support of 𝒑(cid:48).

Example 2 (Anti-monotonicity of the support) Continuing Example 1, let 𝒑(cid:48) = (cid:104)𝑎 𝑏 𝑏
(𝑐𝑒) 𝑑(cid:105) be a super-pattern of 𝒑, 𝒑 (cid:22) 𝒑(cid:48). Indeed, 𝒑(cid:48) occurs in sequence 𝒔2 and 𝒔5, but
not in 𝒔1. 𝑠𝑢 𝑝 𝑝( 𝒑) = 3 whereas 𝑠𝑢 𝑝 𝑝( 𝒑(cid:48)) = 2 and so 𝑠𝑢 𝑝 𝑝( 𝒑(cid:48)) ≤ 𝑠𝑢 𝑝 𝑝( 𝒑). It is
useless to look at other sequences, if 𝒑 does not occur in some sequence, 𝒑(cid:48) does not
as well.

It follows from Proposition 1 that if 𝒑 and 𝒑(cid:48) are two sequential patterns such that
𝒑 (cid:22) 𝒑(cid:48) then if 𝒑(cid:48) is frequent (𝑠𝑢 𝑝 𝑝( 𝒑(cid:48)) ≥ 𝜎), then 𝒑 is also frequent (𝑠𝑢 𝑝 𝑝( 𝒑) ≥ 𝜎).
Conversely, if 𝒑 is not frequent, then 𝒑(cid:48) is not frequent.

The anti-monotonicity property of the support is used by all mining algorithms to
make the exploration of the pattern space eﬃcient. Intuitively, each time the algorithm
encounters a not frequent pattern 𝒑, it avoids the exploration of the super-patterns
of 𝒑. Decades of research have proposed many algorithmic strategies to exploit this
anti-monotonicity property and numerous algorithms have been proposed. They can
be classiﬁed in three main approaches:

– bread-ﬁrst search: all potential frequent patterns of size 𝑛 + 1 are generated from
frequent patterns of size 𝑛 and then evaluated. GSP (Srikant and Agrawal, 1996)
uses this strategy.

– depth-ﬁrst search: a frequent pattern is extended until being not frequent, and
then alternative extensions are explored. FreeSpan (Han et al., 2000) and PreﬁxS-
pan (Pei et al., 2004) uses this strategy.

– vertical dataset: this strategy uses a vertical representation of the dataset of se-
quences to explore the search space using equivalent classes of patterns. This
strategy has been introduced by SPADE (Zaki, 2001).

Several variations of the problem of sequential pattern mining have been proposed.
We refer the reader wishing to have a broad and deep view of this speciﬁc ﬁeld to a
survey of the literature, such as Mooney and Roddick (2013).

6

Thomas Guyet, René Quiniou

Sequential pattern mining has many applications but sometimes a simple sequence
of items is not precise enough to capture the behaviors that are looked for. In addition,
under the general setting, sequential pattern mining may yield too many patterns
for low support thresholds or trivial patterns for high thresholds. To address these
limitations, the user would like to enrich the pattern syntax to obtain the information
she/he is looking for.

Some of these variations consist in integrating constraints in sequential pattern
mining (Pei et al., 2007), e.g. constraints on the size of pattern embeddings (maxspan)
or on the number of sequence itemsets that may be skipped between the occurrences
of two successive pattern itemsets (maxgap). The constraints that preserve the anti-
monotonicity property of the support reduce the number of patterns and thus decrease
the computation times. Some other variations consist in using alternative domains
of sequential patterns. A pattern domain speciﬁes new pattern shapes. For instance,
temporally-annotated sequences (Giannotti et al., 2006) are sequential patterns with
inter-event durations. There are many such extensions of sequential patterns devoted
to answering speciﬁc questions.

3 Negative sequential patterns

This section introduces negative sequential pattern mining which aims at capturing the
notion of the absence of some items in sequences. First, we introduce a syntax for the
domain of negative patterns. Then, we present several possible semantics for patterns
based on this syntax and show that some enjoys the anti-monotonicity property. Finally,
we extend negative sequential patterns to constrain negative sequential patterns.

3.1 Syntax of negative sequential patterns

The negative sequential pattern (NSP) model extends classical sequential patterns
by enabling the speciﬁcation of the absence of some itemsets. For example, 𝒑 =
(cid:104)𝑎 𝑏 ¬𝑐 𝑒 𝑓 (cid:105) is a negative pattern. The symbol ¬ before 𝑐 denotes that item 𝑐 must be
absent. ¬𝑐 is called a negative item. Semantically, 𝒑 speciﬁes that items 𝑎, 𝑏, 𝑒 and 𝑓
occur in a row, but no item 𝑐 occurs between the occurrence of 𝑏 and the occurrence
of 𝑒.

In the ﬁeld of string matching, negation is classically deﬁned for regular ex-
pression. In this case, a pattern is an expression that can hold any kind of negated
pattern. The same principle gives the following most generic deﬁnition of nega-
tive sequential patterns: Let N be the set of negative patterns. A negative pattern
𝒑 = (cid:104)𝑝1 · · · 𝑝𝑛(cid:105) ∈ N is a sequence where, for all 𝑖 ∈ [𝑛], 𝑝𝑖 is a positive itemset
(𝑝𝑖 ⊆ I) or a negated pattern (𝑝𝑖 = ¬{𝑞𝑖 }, 𝑞𝑖 ∈ N ).

Due to its inﬁnite recursive deﬁnition, N appears to be too huge to be an interesting
and tractable search space for pattern mining. For instance, with I = {𝑎, 𝑏, 𝑐}, it is
possible to express simple patterns like (cid:104)𝑎 ¬𝑏 𝑐(cid:105) but also complex patterns like
(cid:104)𝑎 ¬ (cid:104)𝑏 𝑐(cid:105)(cid:105). The combinatorics for such patterns is inﬁnite.

Now we provide our deﬁnition of negative sequential patterns (NSP) which intro-
duces some syntactic restrictions compared with the most generic case. These simple

NegPSpan: eﬃcient extraction of negative sequential patterns

7

restrictions are broadly pointed in the literature (Kamepalli et al., 2014) and enable
us to propose an eﬃcient algorithm.

Deﬁnition 6 (Negative sequential patterns (NSP)) A negative pattern 𝒑 is a se-
quence (cid:104)𝑝1 · · · 𝑝𝑛(cid:105) where, for all 𝑖 ∈ [𝑛], 𝑝𝑖 is a positive itemset (𝑝𝑖 = {𝑝 𝑗
𝑖 } 𝑗 ∈ [𝑚𝑖 ], 𝑝 𝑗
I) or a negated itemset (𝑝𝑖 = ¬{𝑞 𝑗
𝑖 ∈ I) under the two following con-
straints: consecutive negative itemsets are forbidden as well as negative itemsets at
the pattern boundaries. 𝑚𝑖 (resp. 𝑚(cid:48)
𝑖) is the size of the 𝑖-th positive (resp. negative)
itemset. 𝒑+, the positive part1 of pattern 𝒑 denotes the subsequence of 𝒑 restricted to
its positive itemsets.

𝑖 ], 𝑞 𝑗

𝑖 } 𝑗 ∈ [𝑚(cid:48)

𝑖 ∈

According to the non consecutive negative itemsets constraint, a negative pattern
𝒑 has the general form 𝒑 = (cid:104)𝑝1 ¬𝑞1 𝑝2 ¬𝑞2 · · · 𝑝𝑛−1 ¬𝑞𝑛−1 𝑝𝑛(cid:105) where 𝑝𝑖 ∈
2𝐼 \ {∅} and 𝑞𝑖 ∈ 2𝐼 for all 𝑖 ∈ [𝑛] (𝑞𝑖 may be empty). Under this notation, 𝒑+ =
(cid:104)𝑝1 𝑝2 · · · 𝑝𝑛−1 𝑝𝑛(cid:105).

Let us illustrate the syntactic restrictions by some pattern counterexamples that

our approach does not extract:

– ﬁrst of all, a pattern is a sequence of positives and negative itemsets. It is not

possible to have patterns such as (cid:104)𝑎 ¬ (cid:104)𝑏 𝑐(cid:105)(cid:105)

– then, successive negated itemsets such as (cid:104)𝑎 ¬𝑏 ¬𝑐 𝑑(cid:105) are not allowed.
– ﬁnally, a pattern starting or ﬁnishing by a negated itemsets such as (cid:104)¬𝑏 𝑑(cid:105) is not

allowed as well.

Finally, it is worth noticing that the syntax of NSP introduced in Deﬁnition 6 can

not handle patterns with positive and negative events in the same itemset.

3.2 Semantics of negative sequential patterns

The semantics of negative sequential patterns relies on negative containment: a se-
quence 𝒔 supports pattern 𝒑 if 𝒔 contains a sub-sequence 𝒔(cid:48) such that every positive
itemset of 𝒑 is included in some itemset of 𝒔(cid:48) in the same order and for any negative
itemset ¬𝑞𝑖 of 𝒑, 𝑞𝑖 is not included in any itemset occurring in the sub-sequence of
𝒔(cid:48) located between the occurrence of the positive itemset preceding ¬𝑞𝑖 in 𝒑 and the
occurrence of the positive itemset following ¬𝑞𝑖 in 𝒑.

So far in the literature, the absence or non-inclusion of itemsets (represented
here as a negative itemset) has been speciﬁed by loose formulations. The authors of
PNSP (Hsueh et al., 2008) have proposed the set symbol (cid:42) to specify non-inclusion.
This symbol is misleading since it does not correspond to the associated semantics
given in PNSP: an itemset 𝐼 is absent from an itemset 𝐼 (cid:48) if the entire set 𝐼 is absent
from 𝐼 (cid:48) (as opposed to at least some item from 𝐼 is absent from 𝐼 (cid:48)) which corresponds
to 𝐼 ∩ 𝐼 (cid:48) = ∅ in standard set notation, i.e. disjoint sets, and not 𝐼 (cid:42) 𝐼 (cid:48). We will call
PNSP interpretation total non inclusion, i.e. disjointness. It should be distinguished
from partial non inclusion which corresponds (correctly) to the set symbol (cid:42). The

1 Called the maximal positive subsequence in PNSP (Hsueh et al., 2008) and NegGSP (Zheng et al.,

2009) or the positive element id-set in eNSP.

8

Thomas Guyet, René Quiniou

Table 1 Lists of supported sequences in D by negative patterns 𝒑𝒊, 𝑖 = 1..4 under the total and partial non
inclusion semantics. Every pattern has the shape (cid:104)𝑎 ¬𝑞𝑖 𝑏(cid:105) where 𝑞𝑖 are itemsets such that 𝑞𝑖 ⊂ 𝑞𝑖+1.

partial

non inclusion
(cid:42)𝐺

𝒑1 = (cid:104)𝑏¬𝑐𝑎(cid:105)
𝒑2 = (cid:104)𝑏¬(𝑐𝑑) 𝑎(cid:105)
𝒑3 = (cid:104)𝑏¬(𝑐𝑑𝑒) 𝑎(cid:105)
𝒑4 = (cid:104)𝑏¬(𝑐𝑑𝑒𝑔) 𝑎(cid:105)

{𝒔1, 𝒔3, 𝒔4 }
{𝒔1, 𝒔2, 𝒔3, 𝒔4 }
{𝒔1, 𝒔2, 𝒔3, 𝒔4 }
{𝒔1, 𝒔2, 𝒔3, 𝒔4, 𝒔5 }

total

non inclusion
(cid:42)𝐷

{𝒔1, 𝒔3, 𝒔4 }
{𝒔1, 𝒔4 }
{𝒔1 }
{𝒔1 }

monotonic

anti monotonic

symbol (cid:42) was further used by the authors of NegGSP (Zheng et al., 2009) and
eNSP (Cao et al., 2016). The semantics of non inclusion is not detailed in NegGSP
and one cannot determine whether it means total or partial non inclusion.2 eNSP does
not deﬁne explicitly the semantics of non inclusion but, from the procedure used to
compute the support of patterns, one can deduce that it uses total non inclusion.

Deﬁnition 7 (Non inclusion) We introduce two relations between itemsets 𝑃 and 𝐼:

– partial non inclusion: 𝑃 (cid:42)𝐺 𝐼 ⇔ ∃𝑒 ∈ 𝑃, 𝑒 ∉ 𝐼
– total non inclusion: 𝑃 (cid:42)𝐷 𝐼 ⇔ ∀𝑒 ∈ 𝑃, 𝑒 ∉ 𝐼

Choosing one non inclusion interpretation or the other has consequences on ex-
tracted patterns as well as on pattern search. Let us illustrate this on related pattern
support in sequence dataset D

D =

𝒔1 = (cid:104)(𝑏𝑐) 𝑓 𝑎(cid:105)
𝒔2 = (cid:104)(𝑏𝑐) (𝑐 𝑓 ) 𝑎(cid:105)
𝒔3 = (cid:104)(𝑏𝑐) (𝑑𝑓 ) 𝑎(cid:105)
𝒔4 = (cid:104)(𝑏𝑐) (𝑒 𝑓 ) 𝑎(cid:105)
𝒔5 = (cid:104)(𝑏𝑐) (𝑐𝑑𝑒 𝑓 ) 𝑎(cid:105)






.






Table 1 compares the support of progressively extended patterns under the two se-
mantics to show whether anti-monotonicity is respected or not. Let us consider the
inclusion of pattern 𝒑2 = (cid:104)𝑏 ¬(𝑐𝑑) 𝑎(cid:105) in sequence 𝒔2. Since the positive part of 𝒑2
is in 𝒔2, 𝒑2 occurs in sequence 𝒔2 iﬀ (𝑐𝑑) (cid:42)∗ (𝑐 𝑓 ) ((𝑐𝑑) is not included in (𝑐 𝑓 )). In
case of total non inclusion, it is false that (𝑐𝑑) (cid:42)𝐷 (𝑐 𝑓 ) because 𝑐 occurs in (𝑐 𝑓 ),
and thus 𝒑2 does not occur in 𝒔2. But in case of partial non inclusion, it is true that
(𝑐𝑑) (cid:42)𝐺 (𝑐 𝑓 ), because 𝑑 does not occur in (𝑐 𝑓 ), and thus 𝒑2 occurs in 𝒔2.

Obviously, partial non inclusion satisﬁes anti-monotonicity while total non inclu-
sion satisﬁes monotonicity. In the sequel we will denote the general form of itemset
non inclusion by the symbol (cid:42)∗, meaning either (cid:42)𝐺 or (cid:42)𝐷.

2 Actually, though not clearly stated, it seems that the negative elements of NegGSP patterns consist of

items rather than itemsets. In this case, total and partial inclusion are equivalent (see Proposition 6).

NegPSpan: eﬃcient extraction of negative sequential patterns

9

Now, we formulate the notions of sub-sequence, non inclusion and absence by

means of the concept of embedding borrowed from (Negrevergne and Guns, 2015).

Deﬁnition 8 (Positive pattern embedding) Let 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛(cid:105) be a sequence and
𝒑 = (cid:104)𝑝1 · · · 𝑝𝑚(cid:105) be a (positive) sequential pattern. 𝒆 = (𝑒𝑖)𝑖 ∈ [𝑚] ∈ [𝑛] 𝑚 is an
embedding of pattern 𝒑 in sequence 𝒔 iﬀ 𝑝𝑖 ⊆ 𝑠𝑒𝑖 for all 𝑖 ∈ [𝑚] and 𝑒𝑖 < 𝑒𝑖+1 for all
𝑖 ∈ [𝑚 − 1]
Deﬁnition 9 (Strict and soft embeddings of negative patterns) Let 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛(cid:105)
be a sequence, 𝒑 = (cid:104)𝑝1 · · · 𝑝𝑚(cid:105) be a negative sequential pattern and (cid:42)∗∈ {(cid:42)𝐺, (cid:42)𝐷}.
𝒆 = (𝑒𝑖)𝑖 ∈ [𝑚] ∈ [𝑛] 𝑚 is a soft-embedding of pattern 𝒑 in sequence 𝒔 iﬀ for all

𝑖 ∈ [𝑚]:
– 𝑝𝑖 ⊆ 𝑠𝑒𝑖 if 𝑝𝑖 is positive
– 𝑝𝑖 (cid:42)∗ 𝑠 𝑗 , ∀ 𝑗 ∈ [𝑒𝑖−1 + 1, 𝑒𝑖+1 − 1] if 𝑝𝑖 is negative

𝒆 = (𝑒𝑖)𝑖 ∈ [𝑚] ∈ [𝑛] 𝑚 is a strict-embedding of pattern 𝒑 in sequence 𝒔 iﬀ for all

𝑖 ∈ [𝑚]:
– 𝑝𝑖 ⊆ 𝑠𝑒𝑖 if 𝑝𝑖 is positive
– 𝑝𝑖 (cid:42)∗ (cid:208) 𝑗 ∈ [𝑒𝑖−1+1,𝑒𝑖+1−1] 𝑠 𝑗 if 𝑝𝑖 is negative
Proposition 2 soft- and strict-embeddings are equivalent when (cid:42)∗
Proof The proofs of all propositions are in Appendix A.

def

=(cid:42)𝐷.

Let 𝒑+ = (cid:104)𝑝𝑘1 · · · 𝑝𝑘𝑙 (cid:105) be the positive part of some pattern 𝒑, where 𝑙 denotes the
number of positive itemsets in 𝒑. If 𝒆 is an embedding of pattern 𝒑 in some sequence
𝒔, then 𝒆+ = (cid:104)𝑒𝑘1 · · · 𝑒𝑘𝑙 (cid:105) is an embedding of the positive sequential pattern 𝒑+ in 𝒔.
The following examples illustrate the impact of the itemset non-inclusion relation

and of the embedding type on pattern occurrence.

Example 3 (Itemset absence semantics) Let 𝒑 = (cid:104)𝑎 ¬(𝑏𝑐) 𝑑(cid:105) be a pattern and four
sequences:

Sequence

(cid:42)𝐷 (cid:42)𝐺 / strict-embedding (cid:42)𝐺 / soft-embedding

𝒔1 = (cid:104)𝑎 𝑐 𝑏 𝑒 𝑑(cid:105)
𝒔2 = (cid:104)𝑎 (𝑏𝑐) 𝑒 𝑑(cid:105)
𝒔3 = (cid:104)𝑎 𝑏 𝑒 𝑑(cid:105)
𝒔4 = (cid:104)𝑎 𝑒 𝑑(cid:105)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

def

One can notice that each sequence contains a unique occurrence of (cid:104)𝑎 𝑑(cid:105), the
positive part of pattern 𝒑. Using the soft-embedding and partial non-inclusion se-
=(cid:42)𝐺), 𝒑 occurs in sequences 𝒔1, 𝒔3 and 𝒔4 but not in 𝒔2. Using the
mantics ((cid:42)∗
strict-embedding and partial non-inclusion semantics, 𝒑 occurs in sequences 𝒔3 and
𝒔4 considering that items 𝑏 and 𝑐 occur between occurrences of 𝑎 and 𝑑 in sequences
𝒔1 and 𝒔2. With total non inclusion ((cid:42)∗
=(cid:42)𝐷) and either type of embeddings, the
absence of an itemset is satisﬁed if any of its item is absent. As a consequence, 𝒑
occurs only in sequence 𝒔4.

def

10

Thomas Guyet, René Quiniou

Another point that determines the semantics of negative containment concerns
the multiple occurrences of some pattern in a sequence: should every or only one
occurrence of the pattern positive part in the sequence satisfy the non inclusion
constraints? This point is not discussed in previous propositions for negative sequential
pattern mining. Actually, PNSP (Hsueh et al., 2008) and NegGSP (Zheng et al., 2009)
require a weak absence (at least one occurrence should satisfy the non inclusion
constraints) while eNSP requires a strong absence (every occurrence should satisfy
non inclusion constraints).

Deﬁnition 10 (Negative pattern occurrence) Let 𝒔 be a sequence, 𝒑 be a negative
sequential pattern. 𝒑+ is the positive part of 𝒑.
– Pattern 𝒑 weakly-occurs in sequence 𝒔, denoted 𝒑 (cid:22) 𝒔, iﬀ there exists at least one

(strict/soft)-embedding of 𝒑 in 𝒔.

– Pattern 𝒑 strongly-occurs in sequence 𝒔, denoted 𝒑 (cid:118) 𝒔, iﬀ for any embedding 𝒆(cid:48)

of 𝒑+ in 𝒔 there exists an embedding 𝒆 of 𝒑 in 𝒔 such that 𝒆(cid:48) = 𝒆.

Deﬁnition 10 allows for formulating two notions of absence semantics for negative

sequential patterns depending on the occurrence of the positive part:

– strong occurrence: a negative pattern 𝒑 occurs in a sequence 𝒔 iﬀ there exists at
least one occurrence of the positive part of pattern 𝒑 in sequence 𝒔 and every such
occurrence satisﬁes the negative constraints,

– weak occurrence: a negative pattern 𝒑 occurs in a sequence 𝒔 iﬀ there exists at
least one occurrence of the positive part of pattern 𝒑 in sequence 𝒔 and one of
these occurrences satisﬁes the negative constraints.

Example 4 (Strong vs weak occurrence semantics) Let 𝒑 = (cid:104)𝑎 𝑏 ¬𝑐 𝑑 (cid:105) be a pattern
and 𝒔1 = (cid:104)𝑎 𝑏 𝑒 𝑑(cid:105) and 𝒔2 = (cid:104)𝑎 𝑏 𝑐 𝑎 𝑑 𝑒 𝑏 𝑑(cid:105) be two sequences. The positive part of
𝒑 is (cid:104)𝑎 𝑏 𝑑(cid:105). It occurs once in 𝒔1 so there is no diﬀerence for occurrences under the
two semantics. But, it occurs three times in 𝒔2 with embeddings (1, 2, 5), (1, 2, 8) and
(4, 7, 8). The two ﬁrst occurrences do not satisfy the negative constraint (¬𝑐) while
the third occurrence does. Under the weak occurrence semantics, pattern 𝒑 occurs in
sequence 𝒔2 whereas under the strong occurrence semantics it does not.

The deﬁnitions of pattern support, frequent pattern and pattern mining task derive
naturally from the notion of negative sequential pattern occurrence, no matter the
choices for embedding (soft or strict), non inclusion (partial or total) and occurrence
(weak or strong). However, these choices concerning the semantics of NSPs impact
directly the number of frequent patterns (under the same minimal threshold) and
further the computation time. The stronger the negative constraints, the fewer the
number of sequences that hold some pattern, and the fewer the number of frequent
patterns.

Finally, we introduce a partial order on NSPs that is the foundation of our eﬃcient

NSP mining algorithm.

Deﬁnition 11 (NSP partial order) Let 𝒑 = (cid:104)𝑝1 ¬𝑞1 𝑝2 ¬𝑞2 · · · 𝑝𝑘−1 ¬𝑞𝑘−1 𝑝𝑘 (cid:105)
𝑘(cid:48)(cid:105) be two NSPs s.t. 𝑝𝑖 ≠ ∅ for all
and 𝒑(cid:48) = (cid:104)𝑝(cid:48)
𝑝(cid:48)
𝑖 ∈ [𝑘] and 𝑝(cid:48)

𝑘(cid:48)−1 ¬𝑞(cid:48)
1 ¬𝑞(cid:48)
𝑖 ≠ ∅ for all 𝑖 ∈ [𝑘 (cid:48)]. We write 𝒑 (cid:67) 𝒑(cid:48) iﬀ 𝑘 ≤ 𝑘 (cid:48) and:

2 · · · 𝑝(cid:48)

2 ¬𝑞(cid:48)
𝑝(cid:48)

𝑘(cid:48)−1

1

NegPSpan: eﬃcient extraction of negative sequential patterns

11

1. ∀𝑖 ∈ [𝑘 − 1], 𝑝𝑖 ⊆ 𝑝(cid:48)
2. 𝑝𝑘 ⊆ 𝑝(cid:48)
𝑘
3. 𝑘 (cid:48) = 𝑘 =⇒ 𝑝𝑘 ≠ 𝑝(cid:48)

𝑖 and 𝑞𝑖 ⊆ 𝑞(cid:48)
𝑖

𝑘 (irreﬂexive)

Intuitively, 𝒑 (cid:67) 𝒑(cid:48) if 𝒑 is shorter than 𝒑(cid:48) and the positive and negative itemsets of
𝒑 are pairwise included into the itemsets of 𝒑(cid:48). The classical pattern inclusion fails
to be anti-monotonic (Zheng et al., 2009) because pattern extension may change the
scope of negative itemsets. We illustrate what is happening on two examples. Let us
ﬁrst consider the case of a pattern ending with a negated itemset illustrated by Zheng
et al. (2009). Let 𝒑(cid:48) = (cid:104)𝑏 ¬𝑐 𝑎(cid:105) and 𝒑 = (cid:104)𝑏 ¬𝑐(cid:105) be two NSPs. Removing 𝑎 from 𝒑(cid:48)
makes the constraint of the pattern positive part weaker, and so the positive part is
more frequent. But, removing 𝑎 extends also the scope of the negative constraint and
makes it stronger and so, the negative pattern may be less frequent which violates the
anti-monotonicity property. This speciﬁc case does not impact our framework. In fact,
our deﬁnition of NSP (see Deﬁnition 6) does not allow ending an NSP with negated
itemsets. Let us now consider patterns 𝒑(cid:48) = (cid:104)𝑏 ¬𝑐 𝑑 𝑎(cid:105) and 𝒑 = (cid:104)𝑏 ¬𝑐 𝑎(cid:105), and the
sequence 𝒔 = (cid:104)𝑏 𝑒 𝑑 𝑐 𝑎(cid:105). 𝒑(cid:48) occurs in 𝒔 but not 𝒑 since the scope of the negated
itemset ¬𝑐 is wider: it is restricted to the interval between the occurrences of 𝑏 and 𝑑
for 𝒑(cid:48), but to the interval between the occurrences of 𝑏 and 𝑎 for 𝒑.

What is important to note for the partial order (cid:67) of Deﬁnition 11, is that the
embedding of the pattern positive part yields an embedding for 𝒑 that imposes the
negative constraints on the exact same scope than the negative constraints of 𝒑(cid:48).
Thanks to the anti-monotonicity of (cid:42)𝐷, additional itemsets in negative patterns lead
to stronger constrain the containment relation. These remarks give some intuition
behind the following anti-monotonicity property (Proposition 3).

Proposition 3 (Anti-monotonicity of NSP support) The support of NSP is anti-
monotonic with respect to (cid:67) when (cid:42)∗
=(cid:42)𝐷 and weak-occurrences ((cid:22)) are considered.
Proof see Appendix A.

def

We can note that when the strong occurrence semantic ((cid:118)) is used, (cid:67) violates
the anti-monotonicity. Considering 𝒑(cid:48) = (cid:104)𝑎 (𝑏𝑐) ¬𝑐 𝑑(cid:105), 𝒑 = (cid:104)𝑎 𝑏 ¬𝑐 𝑑(cid:105) and 𝒔 =
(cid:104)𝑎 b (𝑏𝑐) 𝑒 𝑑(cid:105), then it is true that 𝒑(cid:48) (cid:118) 𝒔, but not that 𝒑 (cid:118) 𝒔. There are two possible
embeddings for 𝒑(cid:48) in 𝒑(cid:48): (1, 2, 5) and (1, 3, 5). The ﬁrst one ((1, 2, 5), which does
not derive from the embedding of 𝒑(cid:48)) does not satisfy the negative constraint.

A second example illustrates another case that is encountered when the postﬁx
of a sequence restricts the set of valid embeddings: 𝒑(cid:48) = (cid:104)𝑎 ¬𝑏 𝑑 c(cid:105), 𝒑 = (cid:104)𝑎 ¬𝑏 𝑑(cid:105)
and 𝒔 = (cid:104)𝑎 𝑒 𝑑 𝑐 𝑏 𝑑(cid:105). Again, 𝒑(cid:48) occurs only once while 𝒑 occurs twice and one
of its embeddings does not satisfy the negated itemset ¬𝑏. This example shows that
a simple postﬁx extension of an NSP leads to violate the anti-monotonicity property
under the strong occurrence semantics.

3.3 Gap constraints over negative sequential patterns

Numerical constraints have been introduced early in sequential constraints. Such
constraints can also be introduced in NSPs yielding constrained negative sequen-
tial patterns. We consider the two most common constraints on sequential patterns:

12

Thomas Guyet, René Quiniou

maxgap (𝜃 ∈ N) and maxspan (𝜏 ∈ N) constraints. These constraints impact NSP
embeddings whatever their semantic. An embedding 𝒆 = {𝑒𝑖, · · · , 𝑒𝑛} of a pattern 𝒑
in some sequence 𝒔 satisﬁes the maxgap (resp. maxspan) constraint iﬀ 𝒆 satisﬁes the
following constraint: 𝑒𝑖+1 − 𝑒𝑖 ≤ 𝜃 (resp. 𝑒𝑛 − 𝑒1 ≤ 𝜏) for all 𝑖 ∈ [𝑛 − 1].

Example 5 (Embedding of a constrained NSP) Let 𝒑 = (cid:104)𝑎 ¬𝑏 𝑐 𝑑(cid:105) be a NSP and
we consider the following constraints: not more than one itemset in between positive
itemsets occurrences (𝜃 = 2) and not more than three itemsets in between the ﬁrst and
the last itemset occurrences (𝜏 = 4). Table below illustrates on ﬁve sequences whether
the sequence supports pattern 𝒑 or not. These simple patterns and sequences lead to
the same results whatever the containment relation between NSP and sequences.

Sequence

(cid:104)𝑎 𝑏 𝑐 𝑑(cid:105)
(cid:104)𝑎 𝑒 𝑐 𝑑(cid:105)
(cid:104)𝑎 𝑒 𝑒 𝑐 𝑑(cid:105)
(cid:104)𝑎 𝑒 𝑐 𝑒 𝑑(cid:105)
(cid:104)𝑎 𝑒 𝑐 𝑏 𝑑(cid:105)

𝜏 = ∞
𝜃 = ∞

𝜏 = 4
𝜃 = ∞

𝜏 = ∞
𝜃 = 2

𝜏 = 4
𝜃 = 2

(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)

It is worth noticing that the anti-monotonicity of the support is preserved with a
maxspan constraint but not with a maxgap constraint for frequent sequential patterns.
Nonetheless, the support becomes anti-monotonic for both constraints with the partial
order that takes only into consideration extensions at the end of sequential patterns
and not elsewhere. The partial order speciﬁed in Deﬁnition 11 for negative sequential
patterns is based on the same idea of having extensions only at the end of a pattern.
Thus, the support of constrained NSP also enjoys the anti-monotonicity property.

Proposition 4 (Anti-monotonicity of constrained NSP support) The support of
NSP with maxgap or maxspan constraints is anti-monotonic with respect to (cid:67) when
(cid:42)∗

=(cid:42)𝐷 and weak-occurrences ((cid:22)) are considered.

def

Proof see Appendix A.

4 Algorithm NegPSpan

def

In this section, we introduce algorithm NegPSpan for mining NSPs from a dataset of
sequences under maxgap and maxspan constraints and under a soft absence semantics
with (cid:42)∗
=(cid:42)𝐷 for itemset inclusion. As stated in Proposition 2, no matter the embedding
strategy, they are equivalent under total itemset inclusion. Considering occurrences,
NegPSpan uses the weak-occurrence semantics: at least one occurrence of the negative
pattern is suﬃcient to consider that it is supported by the sequence.

For computational reasons, we make an additional assumption for admissible
itemsets as negative itemsets: the negative itemsets are restricted to one element

NegPSpan: eﬃcient extraction of negative sequential patterns

13

belonging to some language L− ⊆ IN. Considering all possible itemsets (i.e. IN) for
negatives is computationally costly. The deﬁnition of a subset L− allows to control
partially the combinatorics introduced by negative itemsets. In algorithm NegPSpan
presented below, L− = {𝐼 = {𝑖1, · · · , 𝑖𝑛}|∀𝑘, 𝑠𝑢 𝑝 𝑝(𝑖𝑘 ) ≥ 𝜎} denotes the set of
itemsets that can be built from frequent items. But this set could be arbitrarily deﬁned
when the user is interested in the absence of some speciﬁc events. For instance, the
deﬁnition of L− could be changed into the set of frequent itemsets, which would be
more restrictive than the set of itemsets made of frequent items.

4.1 Main algorithm

NegPSpan is based on algorithm PreﬁxSpan (Pei et al., 2004) which implements a
depth ﬁrst search and uses the principle of database projection to reduce the number
of sequence scans. NegPSpan adapts the pseudo-projection principle of PreﬁxSpan
which uses a projection pointer to avoid copying the data. For NegPSpan, a projection
pointer of some pattern 𝒑 is a triple (cid:104)𝑠𝑖𝑑, 𝑝 𝑝𝑟𝑒𝑑, 𝑝𝑜𝑠(cid:105) where 𝑠𝑖𝑑 is a sequence
identiﬁer in the dataset, 𝑝𝑜𝑠 is the position in sequence 𝑠𝑖𝑑 that matches the last
itemset of the pattern (necessarily positive) and 𝑝 𝑝𝑟𝑒𝑑 is the position of the previous
positive pattern. In the PreﬁxSpan algorithm, the projection pointer is the couple
(cid:104)𝑠𝑖𝑑, 𝑝𝑜𝑠(cid:105). NegPSpan adds the position of the previous positive pattern to locate
the region of the sequence in which evaluate the presence or absence of a candidate
extensions of the last negative itemset.

Algorithm 1 details the main recursive function of NegPSpan for extending
a current pattern 𝒑. The principle of this function is similar to PreﬁxSpan. Every
pattern 𝒑 is associated with a pseudo-projected database represented by both the
original set of sequences S and a set of projection pointers 𝑜𝑐𝑐𝑠. First, the function
evaluates the size of 𝑜𝑐𝑐𝑠 to determine whether pattern 𝒑 is frequent or not. If so, it is

Algorithm 1: NegPSpan: recursive function for negative sequential pattern
extraction

in: S: set of sequences, 𝒑: current pattern of length 𝑘, 𝜎: minimum support threshold, 𝑜𝑐𝑐𝑠: list

of occurrences, I 𝑓 : set of frequent items, 𝜃: maxgap, 𝜏: maxspan

1 Function NegPSpan (S, 𝜎, 𝒑, 𝑜𝑐𝑐𝑠, I 𝑓 , 𝜃, 𝜏):

//Support evaluation of pattern 𝒑
if |𝑜𝑐𝑐𝑠 | ≥ 𝜎 then

OutputPattern(𝒑, 𝑜𝑐𝑐𝑠);

else

return;

//Positive itemset composition
PositiveComposition(S, 𝜎, 𝒑, 𝑜𝑐𝑐𝑠, I 𝑓 , 𝜃, 𝜏);
//Positive sequential extension
PositiveSequence(S, 𝜎, 𝒑, 𝑜𝑐𝑐𝑠, I 𝑓 , 𝜃, 𝜏);
if |𝒑 | ≥ 2 and | 𝑝𝑘 | = 1 then

//Negative sequential extension
NegativeExtension(S, 𝜎, 𝒑, 𝑜𝑐𝑐𝑠, I 𝑓 , 𝜃, 𝜏);

2

3

4

5

6

7

8

9

14

Thomas Guyet, René Quiniou

outputted, otherwise, the recursion is stopped because no larger patterns are possible
(anti-monotonicity property).

Then, the function explores three types of pattern extensions of pattern 𝒑 into a

pattern 𝒑(cid:48):

– the positive sequence composition ((cid:32)𝑐) consists in adding one item to the last
itemset of 𝒑 (following the notations of Deﬁnition 11, this extension corresponds
to the case in which 𝒑(cid:48) is the extension of 𝒑 where 𝑘 (cid:48) = 𝑘, 𝑞𝑖 = 𝑞(cid:48)
𝑖 for all 𝑖 ∈ [𝑘 −1]
and | 𝑝(cid:48)

𝑘 | = | 𝑝𝑘 | + 1),

– the positive sequence extension ((cid:32)𝑠) consists in adding a new positive singleton

itemset at the end of 𝒑 (𝑘 (cid:48) = 𝑘 + 1, 𝑞𝑖 = 𝑞(cid:48)

𝑖 for all 𝑖 ∈ [𝑘 − 1] and | 𝑝(cid:48)

𝑘(cid:48) | = 1),

– the negative sequence extension ((cid:32)𝑛) consists in appending a negative itemset
𝑖 for all 𝑖 ∈ [𝑘 − 2],
𝑘 = 𝑝𝑘 ). In addition, NSP are negatively extended iﬀ

between the last two positive itemsets of 𝒑 (𝑘 (cid:48) = 𝑘, 𝑞𝑖 = 𝑞(cid:48)
|𝑞(cid:48)
| 𝑝𝑘 | = 1. It prevents redundant exploration of same patterns (see Section 4.3).

𝑘−1| = |𝑞𝑘−1| + 1 and 𝑝(cid:48)

The negative pattern extension is speciﬁc to our algorithm and is detailed in
next section. The ﬁrst two extensions are identical to PreﬁxSpan pattern extensions,
including their gap constraints management, i.e. maxgap and maxspan constraints on
positive patterns.

Proposition 5 The proposed algorithm is correct and complete.

Proof see Appendix A.

Intuitively, the algorithm is complete considering that the three extensions enable
to generate any NSP. For instance, pattern (cid:104)𝑎 ¬𝑒 𝑏 (𝑐𝑒) ¬(𝑏𝑑) 𝑎(cid:105) would be generated
after evaluating the following patterns: (cid:104)𝑎(cid:105) (cid:32)𝑠 (cid:104)𝑎 𝑏(cid:105) (cid:32)𝑛 (cid:104)𝑎 ¬𝑒 𝑏(cid:105) (cid:32)𝑠 (cid:104)𝑎 ¬𝑒 𝑏 𝑐(cid:105) (cid:32)𝑐
(cid:104)𝑎 ¬𝑒 𝑏 (𝑐𝑒)(cid:105) (cid:32)𝑠 (cid:104)𝑎 ¬𝑒 𝑏 (𝑐𝑒) 𝑎(cid:105) (cid:32)𝑛 (cid:104)𝑎 ¬𝑒 𝑏 (𝑐𝑒) ¬𝑏 𝑎(cid:105) (cid:32)𝑛 (cid:104)𝑎 ¬𝑒 𝑏 (𝑐𝑒) ¬(𝑏𝑑) 𝑎(cid:105).
Secondly, according to Proposition 3, the pruning strategy is correct.

4.2 Extension of patterns with negated itemsets ((cid:32)𝑛)

Algorithm 2 extends the current pattern 𝒑 with negative items. It generates new
candidates by inserting an item 𝑖𝑡 ∈ I 𝑓 , the set of frequent items. Let 𝑝𝑘−1 and
𝑝𝑘 denote respectively the penultimate itemset and the last itemset of 𝒑. If 𝑝𝑘−1 is
positive, then a new negated itemset is inserted between 𝑝𝑘−1 and 𝑝𝑘 . Otherwise, if
𝑝𝑘−1 is negative, item 𝑖𝑡 is appended to 𝑝𝑘−1. To prevent the redundant enumeration
of negative itemsets, only items 𝑖𝑡 (lexicographically) greater than the last item of
𝑝𝑘−1 can be added.

Once the pattern has been extended, lines 10 to 20 evaluate the candidate by
computing the pseudo-projection of the current database. According to the selected
semantics associated with (cid:42)𝐷, i.e. total non inclusion (see Deﬁnition 9), it is suﬃcient
to check the absence of 𝑖𝑡 in the subsequence included between the occurrences of
the two positive itemsets surounding 𝑖𝑡. To achieve this, the algorithm checks the
sequence positions in the interval [𝑜𝑐𝑐.𝑝 𝑝𝑟𝑒𝑑 + 1, 𝑜𝑐𝑐.𝑝𝑜𝑠 − 1]. If 𝑖𝑡 does not occur
in itemsets from this interval, then the extended pattern occurs in the sequence 𝑜𝑐𝑐.𝑠𝑖𝑑.

NegPSpan: eﬃcient extraction of negative sequential patterns

15

Algorithm 2: NegPSpan: negative extensions

in: S: set of sequences, 𝒑: current pattern of length 𝑘, 𝜎: minimum support threshold, 𝑜𝑐𝑐𝑠: list

of occurrences, I 𝑓 : set of frequent items, 𝜃: maxgap, 𝜏: maxspan

1 Function NegativeExtension(S, 𝜎, 𝒑, 𝑜𝑐𝑐𝑠, I 𝑓 , 𝜃, 𝜏):
2

for 𝑖𝑡 ∈ I 𝑓 do

if 𝑝𝑘−1 is pos then

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

//Insert the negative item at the penultimate position
𝒑.𝑖𝑛𝑠𝑒𝑟 𝑡 (¬𝑖𝑡);

else

if 𝑖𝑡 > 𝑝𝑘−1.𝑏𝑎𝑐𝑘 () then

//Append an item to the penultimate (negative) itemset
𝑝𝑘−1.𝑎 𝑝 𝑝𝑒𝑛𝑑 (¬𝑖𝑡);

else

continue;

𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ← ∅;
for 𝑜𝑐𝑐 ∈ 𝑜𝑐𝑐𝑠 do

𝑓 𝑜𝑢𝑛𝑑 ← 𝑓 𝑎𝑙𝑠𝑒;
for 𝑠 𝑝 = [𝑜𝑐𝑐. 𝑝𝑟 𝑒𝑑 + 1, 𝑜𝑐𝑐. 𝑝𝑜𝑠 − 1] do

if 𝑖𝑡 ∈ 𝒔𝒐𝒄𝒄.𝒔𝒊𝒅 [𝑠 𝑝] then
𝑓 𝑜𝑢𝑛𝑑 ← 𝑡𝑟 𝑢𝑒;
break;

if not 𝑓 𝑜𝑢𝑛𝑑 then

𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ← 𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ∪ {𝑜𝑐𝑐 };

else

//Look for an alternative occurrence
𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ← 𝑛𝑒𝑤𝑜𝑐𝑐𝑠∪ Match(𝒔𝒐𝒄𝒄.𝒔𝒊𝒅, 𝒑, 𝜃, 𝜏);

NegPSpan (D, 𝜎, 𝒑, 𝑛𝑒𝑤𝑜𝑐𝑐𝑠, I 𝑓 );
𝑝𝑘−1. 𝑝𝑜 𝑝 ();

Otherwise, to ensure the completeness of the algorithm, another occurrence of the
pattern has to be searched in the sequence (cf. Match function that takes into account
gap constraints).

For example, the ﬁrst occurrence of pattern 𝒑 = (cid:104)𝑎 𝑏 𝑐(cid:105) in sequence (cid:104)𝑎 𝑏 𝑒 𝑐 𝑎 𝑏 𝑐(cid:105)
is 𝑜𝑐𝑐 𝒑 = (cid:104)𝑠𝑖𝑑, 2, 4(cid:105). Let us now consider 𝒑(cid:48) = (cid:104)𝑎 𝑏 ¬𝑒 𝑐(cid:105), a negative extension of
𝒑. The extension of the projection-pointer 𝑜𝑐𝑐 𝒑 does not satisfy the absence of 𝑒. So
a new occurrence of 𝒑 has to be searched for. (cid:104)𝑠𝑖𝑑, 6, 7(cid:105), the next occurrence of 𝒑,
satisﬁes the negative constraint. Then, NegPSpan is called recursively for extending
the new current pattern (cid:104)𝑎 𝑏 ¬𝑒 𝑐(cid:105).

We can note that the gap constraints 𝜏 and 𝜃 are not mentioned explicitly in this
algorithm (except when a complete matching is required), but they impact indirectly
the algorithm by narrowing the interval of admissible sequence positions (line 13).

4.2.1 Extracting NSP without surrounding negations

An option may be used to restrict negated items to not be surrounded by itemsets
containing this item. This option is motivated by the objective to ease pattern under-

16

Thomas Guyet, René Quiniou

standing. A pattern (cid:104)𝑎 ¬𝑏 𝑏 𝑐(cid:105) may be interpreted as “there is exactly one occurrence
of 𝑏 between 𝑎 and 𝑐”. But, this may also lead to redundant patterns. For instance,
(cid:104)𝑎 𝑏 ¬𝑏 𝑐(cid:105) matches exactly the same sequences as (cid:104)𝑎 ¬𝑏 𝑏 𝑐(cid:105) (see section 4.3). This
second restriction can be disabled in our algorithm implementation. If so and for the
sake of simplicity, we choose to yield only pattern (cid:104)𝑎 𝑏 ¬𝑏 𝑐(cid:105).

The set of such restricted NSPs can be extracted using the same algorithm, simply
by changing the candidate generation in Algorithm 2, line 2 by 𝑖𝑡 ∈ I 𝑓 \ ( 𝑝𝑘 ∪ 𝑝𝑘−1).
The items to be added to a negative itemset are among frequent items where items of
surrounding itemsets are ﬁltered out.

4.2.2 Extracting NSP with partial non inclusion ((cid:42)∗

def

=(cid:42)𝐺)

def

Algorithm 3 presents the variant of the negative extension algorithm for the partial
non-inclusion ((cid:42)∗
=(cid:42)𝐺). The backbone of the algorithm is similar: a candidate pattern
with a negated itemset at the penultimate position is generated and it assesses whether
this candidate is frequent or not. The absence of the itemset 𝑖𝑠 is checked in the
itemsets of the sequence at positions between the occurrence of the preceding itemset
and the occurrence of last itemset. The test in line 8 assesses that it is false that
𝑖𝑠 (cid:42)𝐺 𝒔𝒐𝒄𝒄.𝒔𝒊𝒅 [𝑠𝑝]: 𝑖𝑠 is not partially non-included in one sequence itemset iﬀ 𝑖𝑠 is a
subset of this itemset.

Contrary to Algorithm 2, it is not possible to use negative sequence extension
((cid:32)𝑛) to extend negative itemsets. With partial non inclusion the support is monotonic
(and not anti-monotonic). Thus, candidate patterns extended with negative itemsets
are generated based on a user deﬁned L− ⊆ IN. We remind that the default L− is
the set of itemsets that can be built from frequent items, i.e. L− = {𝐼 = {𝑖1, · · · , 𝑖𝑛}|
∀𝑘, 𝑠𝑢 𝑝 𝑝(𝑖𝑘 ) ≥ 𝜎}. According to the monotony property if a pattern (cid:104)𝑎 ¬(𝑏𝑐) 𝑑(cid:105)
is not frequent, the pattern (cid:104)𝑎 ¬𝑏 𝑑(cid:105) is not frequent either. But, in practice, both
candidates will be evaluate by Algorithm 3. Thus, the combinatorics of this variant is
signiﬁcantly higher in practice because each element of L− is evaluated.

4.3 Redundancy avoidance

Algorithm NegPSpan is syntactically non-redundant but it can in practice generate
patterns that are semantically redundant.

For instance, pairs of patterns like (cid:104)𝑎 ¬𝑏 𝑏 𝑐(cid:105) and (cid:104)𝑎 𝑏 ¬𝑏 𝑐(cid:105) are syntactically
diﬀerent but match exactly the same sequences. Semantically, such pattern could be
interpreted as “there is not more than one occurrence of 𝑏 between 𝑎 and 𝑐”. It is
possible to avoid generating both type of patterns eﬃciently. Our solution is to avoid
the generation of candidate patterns with negative items that are in the last itemset.
Thus, only (cid:104)𝑎 𝑏 ¬𝑏 𝑐(cid:105) would be generated. In Algorithm 2 line 2, the list of frequent
items I 𝑓 is replaced by I 𝑓 \ 𝑝𝑘 . But, this modiﬁcation leads to loose the completeness
of the algorithm. In fact, the pattern (cid:104)𝑎 ¬𝑏 𝑏(cid:105) is not generated neither its semantically
equivalent pattern (cid:104)𝑎 𝑏 ¬𝑏(cid:105) because of the syntactic constraint on NSP that can not
end with a negative itemset. In practice, we do not manage this kind of redundancy.

NegPSpan: eﬃcient extraction of negative sequential patterns

17

Algorithm 3: NegPSpan: negative extensions with partial non-inclusion
(alternative to Algorithm 2)

in: S: set of sequences, 𝒑: current pattern, 𝜎: minimum support threshold, 𝑜𝑐𝑐𝑠: list of

occurrences, I 𝑓 : set of frequent items, 𝜃: maxgap, 𝜏: maxspan

1 Function NegativeExtension(S, 𝜎, 𝒑, 𝑜𝑐𝑐𝑠, I 𝑓 , 𝜃, 𝜏):
2

for 𝑖𝑠 ∈ L− do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

//Insert the negative itemset at the penultimate position
𝒑.𝑖𝑛𝑠𝑒𝑟 𝑡 (¬𝑖𝑠);
𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ← ∅;
for 𝑜𝑐𝑐 ∈ 𝑜𝑐𝑐𝑠 do

𝑓 𝑜𝑢𝑛𝑑 ← 𝑓 𝑎𝑙𝑠𝑒;
for 𝑠 𝑝 = [𝑜𝑐𝑐. 𝑝𝑟 𝑒𝑑 + 1, 𝑜𝑐𝑐. 𝑝𝑜𝑠 − 1] do

if 𝑖𝑠 ⊆ 𝒔𝒐𝒄𝒄.𝒔𝒊𝒅 [𝑠 𝑝] then
𝑓 𝑜𝑢𝑛𝑑 ← 𝑡𝑟 𝑢𝑒;
break;

if ! 𝑓 𝑜𝑢𝑛𝑑 then

𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ← 𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ∪ {𝑜𝑐𝑐 };

else

//Look for an alternative occurrence
𝑛𝑒𝑤𝑜𝑐𝑐𝑠 ← 𝑛𝑒𝑤𝑜𝑐𝑐𝑠∪ Match(𝒔𝒐𝒄𝒄.𝒔𝒊𝒅, 𝒑, 𝜃, 𝜏);

NegPSpan (D, 𝜎, 𝒑, 𝑛𝑒𝑤𝑜𝑐𝑐𝑠, I 𝑓 );
𝑝𝑘−1 = ∅;

We prefer the sound and correct option of not surrounding negative itemsets (see
section 4.2.1).

A syntactic redundancy is introduced by extending patterns with negative items.
For instance, the pattern (cid:104)𝑎 ¬𝑏 (𝑐𝑑)(cid:105) may be reached by two distinct paths 𝑝1 :
(cid:104)𝑎 𝑐(cid:105) (cid:32)𝑐 (cid:104)𝑎 (𝑐𝑑)(cid:105) (cid:32)𝑛 (cid:104)𝑎 ¬𝑏 (𝑐𝑑)(cid:105) or 𝑝2 : (cid:104)𝑎 𝑐(cid:105) (cid:32)𝑛 (cid:104)𝑎 ¬𝑏 𝑐(cid:105) (cid:32)𝑐 (cid:104)𝑎 ¬𝑏 (𝑐𝑑)(cid:105). To
solve this problem, the algorithm ﬁrst speciﬁes the negative itemsets as a composition
of negative items and then it composes the last itemset with new items. This discards
the path 𝑝1. In Algorithm 1, line 8 enables negative extension only if the last (positive)
itemset is of size 1.

4.4 Execution example

This section illustrates the execution of the algorithm on a small example. Let us
consider the dataset of sequences illustrated in Table 2 and the minimal support
threshold 𝜎 = 2. In this example, we consider the following semantics for negative
patterns: total non inclusion and strict absence. No gap constraints are considered
(𝜃 = ∞ and 𝜏 = ∞) and I = {𝑎, 𝑏, 𝑐, 𝑑, 𝑒, 𝑓 }. In this example, candidate negative
itemsets are simply frequent items: L− = {𝑎, 𝑏, 𝑐, 𝑑, 𝑒}. Event 𝑓 is not in L− because
it occurs only once and thus is not frequent under the minimal support 𝜎.

Figure 1 illustrates the execution of NegPSpan algorithm on the dataset of Table 2
starting from pattern (cid:104)𝑎(cid:105). The search tree illustrates the successive patterns explored by
the depth-ﬁrst search strategy. Each node details both the pattern and the corresponding

18

Thomas Guyet, René Quiniou

Table 2 Dataset of sequences used in the execution example.

SID

Sequence

𝒔1
𝒔2
𝒔3
𝒔4

(cid:104)𝑎 𝑐 𝑏 𝑒 𝑑 (cid:105)
(cid:104)𝑎 (𝑏𝑐) 𝑒(cid:105)
(cid:104)𝑎 𝑏 𝑒 𝑑 (cid:105)
(cid:104)𝑎 𝑒 𝑑 𝑓 (cid:105)

projected database. For the sake of space, the tree is simpliﬁed and some nodes are
missing.

For patterns longer than two, projected sequences are represented in two gray
levels: the bold part of the sequence can be used to make positive extensions and the
gray part of the sequence is used to assess the absence of items for negative extension.
Two markers locate the projection pointer positions. The second pointer is the same
as the one computed by PreﬁxSpan.

Let us consider projected sequences of pattern (cid:104)𝑎 𝑒(cid:105). In the ﬁrst sequence, 𝑑 is
green as it is the part of the sequence ending the sequence after the position of 𝑒. 𝑐
and 𝑏 are in red because these events are in between occurrences of 𝑎 and 𝑒. Pattern
(cid:104)𝑎 𝑒(cid:105) can be extended in two ways:

– with negative items among L \ {𝑎, 𝑒} (𝑎 and 𝑒 are removed if the restriction on

second restriction is activated),

– with positive items among items that are frequent in the bold parts.

Considering extension of pattern (cid:104)𝑎 𝑒(cid:105) with a negative item, e.g. ¬𝑐, each sequence
whose gray part contains the item is discarded, the others remain identical. The
extension by ¬𝑐 leads to pattern (cid:104)𝑎 ¬𝑐 𝑒(cid:105) which is supported by sequences 𝒔3 and 𝒔4
only.

The extension of pattern (cid:104)𝑎 𝑒(cid:105) by a positive item follows the same strategy as
PreﬁxSpan. In this case, the algorithm explores only the extension by item 𝑑 and the
projected pointers are updated to reduce further scanning.

Adding a new negative item while the penultimate item is negative consists in
appending it to the negative itemset. In case of pattern (cid:104)𝑎 ¬𝑐 𝑒(cid:105), 𝑑 is the only candidate
because 𝑒 is one of the surrounding events and 𝑏 is before 𝑐 in the lexicographic order.
Under total non inclusion, again, we simply have to discard sequences that contain
the item 𝑑 within their gray part.
For extending (cid:104)𝑎 ¬𝑑 𝑒 𝑑(cid:105), we can see that every combination of itemsets may quickly
satisfy all the negation constraints. This suggests ﬁrst to carefully select the appropriate
L− and second to use as big as possible negative itemsets to avoid pattern explosion.
Finally, we also notice that extensions with negative items are not terminal recur-
sive steps. Once negative items have been inserted, new positive items can be append
to the pattern. We encounter this case with pattern (cid:104)𝑎 ¬𝑑 𝑒(cid:105) which is extended by
item 𝑑.

NegPSpan: eﬃcient extraction of negative sequential patterns

19

Fig. 1 Example of algorithm NegPSpan search tree on the dataset of Table 2. Each node tree represents the
pattern on the left and the projected database on the right. The gray part of sequences is used to assess future
negations while the bold part of sequences is used for sequential ((cid:32)𝑠). Dashed arrows represent negative
extensions ((cid:32)𝑛) while plain arrows are sequential ((cid:32)𝑠) or compositional extensions ((cid:32)𝑐). Arrow label
holds the item that is used in the extension.

5 Experiments

This section presents experiments on synthetic and real data. Experiments on synthetic
data aims at exploring and comparing NegPSpan and eNSP for negative sequential
pattern mining. The other experiments were conducted on real dataset to illustrate
results for negative patterns. NegPSpan and eNSP have been implemented in C++.3

5.1 Benchmark on synthetic datasets

This section presents experiments on synthetically generated data. We ﬁrst provide
some details about the random sequence generator. This sequence generator enables
to study the behavior of eNSP and NegPSpan for some speciﬁc dataset features and

3 Code, data generator and synthetic benchmark datasets can be tested online and downloaded here:

http://people.irisa.fr/Thomas.Guyet/negativepatterns/.

20

Thomas Guyet, René Quiniou

algorithm parameters. First, we study the time-eﬃciency and the number of extracted
patterns with respect to the minimal frequency threshold. Then, we complement the
analysis of the number of patterns with a study of the capacity of eNSP or NegPSpan
to extract accurately some NSPs known to be frequent in the synthetic dataset. Note
that additional experiments on the behavior of the algorithms with respect to the
vocabulary size and the average sequence length can be found in Appendix C.

5.1.1 Random sequence generator

The principle of our sequence generator is the following: generate random negative
patterns and randomly plant or not some of their occurrences into randomly generated
sequences. The main parameters are the total number of sequences (𝑛, default value
is 𝑛 = 500), the mean sequences length (𝑙 = 20), the vocabulary size i.e. the number
of items (𝑑 = 20), the total number of patterns to plant (3), their mean length (4) and
the pattern minimal occurrence frequency in the dataset ( 𝑓 = 10%).

In addition, the generator ensures that the positive partners of a NSP occurs in
a controlled number of the generated sequences. The number of occurrences of the
positive partners is in [𝜆𝑚𝑖𝑛 × 𝑓 × 𝑛, 𝜆𝑚𝑎𝑥 × 𝑓 × 𝑛].

For sake of fairness of our evaluation, the generated sequences ensures that eNSP
extracts less patterns than NegPSpan on our synthetic datasets. It is the case under
two speciﬁc conditions. First, the generated sequences are sequences of items (not
itemsets). In this case, Proposition 6 (see Appendix B) shows that any frequent NSP
with the semantic of eNSP is also frequent with the semantic of NegPSpan. Second,
we restrict L− to the set of frequent items also for fairness of evaluation. Indeed, for
sequences of items, patterns extracted by eNSP are only made of items and not itemsets
because positive partners must be a frequent sequential pattern in sequences of items.
This means that for sequence of items, eNSP does not manage any conjunction of
negative items. The restriction of L− for NegPSpan ensures that NegPSpan will not
extract patterns with negative itemsets of size strictly larger than 1.

The occurrences of negative pattern are randomly generated without restriction
on the gaps between item occurrences of a pattern. This means that the number of
occurrences of planted NSP may be higher than its evaluated support while taking
gap constraints into consideration.

5.1.2 Computation time and number of patterns

Figure 2 displays the computation time and the number of patterns extracted by eNSP
and NegPSpan on sequences of length 20 and 30, under three minimal thresholds
(𝜎 = 10%, 15% and 20%) and with diﬀerent values for the maxgap constraint (𝜏 = 4,
7, 10 and ∞). For eNSP, the minimal support of positive partners, denoted 𝜍, is set to
70% of the minimal threshold 𝜎. For both approaches, we bound the pattern length to
5 items. Each boxplot has been obtained with a 20 diﬀerent sequence datasets. Each
run has a timeout of 5 minutes (300 𝑠).

The main conclusion from Figure 2 is that NegPSpan is more eﬃcient than
eNSP when maxgap constraints are used. As expected, eNSP is more eﬃcient than
NegPSpan without any maxgap constraint. This is mainly due to the number of

NegPSpan: eﬃcient extraction of negative sequential patterns

21

Fig. 2 Comparison of eNSP and NegPSpan on number of extracted patterns (left) and computation time
(right) for diﬀerent values of maxgap (𝜏). Top (resp. bottom) ﬁgures correspond to datasets with average
sequence length equal to 20 (resp. 30). Boxplot colors correspond to diﬀerent values of 𝜎 (10%, 15% and
20%).

extracted patterns. NegPSpan extracts signiﬁcantly more patterns than eNSP because
of diﬀerent choices for the semantics of NSPs. First, eNSP uses a stronger negation
semantics. Without maxgap constraints, the set of patterns extracted by NegPSpan is
a superset of those extracted by eNSP (see proof in Appendix B).

An interesting result is that, for reasonably long sequences (20 or 30), even a weak
maxgap constraint (𝜏 = 10) signiﬁcantly reduces the number of patterns and makes
NegPSpan more eﬃcient. 𝜏 = 10 is said to be a weak constraint because it does not
cut early the search of a next occurring item compared to the length of the sequence
(20 or 30). This is of particular interest because maxgap is a quite natural constraint
when mining long sequences. It prevents from taking into account long distance
correlations that are likely irrelevant. Note that section 5.1.4 discusses the fairness
of this setting as NegPSpan extracts more planted patterns that eNSP does. Another
interesting question raised by these results is the real meaning of extracted patterns by
eNSP. In fact, under low frequency thresholds, it extracts numerous patterns that are
not frequent when weak maxgap constraints are considered. As a consequence, the
signiﬁcance of most of the patterns extracted by eNSP seems low when processing
datasets containing “long” sequences. Extensive experiments on the inﬂuence of
sequence length can be found in Appendix C.2.

5.1.3 Inﬂuence of minimum threshold

Figure 3 illustrates computation time and memory consumption with respect to min-
imum threshold for diﬀerent settings: eNSP is run with diﬀerent values for 𝜍, the
minimal frequency of the positive partner of negative patterns (100%, 80% and 20%

llllllllllllllllllllllll=20l=30eNSPNegPSpanno constraintNegPSpant =15NegPSpant =10NegPSpant =7NegPSpant =41e+011e+031e+051e+011e+031e+05Nb negative patternsllllllllllllllllllllllllllll=20l=30eNSPNegPSpanno constraintNegPSpant =15NegPSpant =10NegPSpant =7NegPSpant =40.110.00.110.0Time (s)s0.10.150.222

Thomas Guyet, René Quiniou

Fig. 3 Comparison of eNSP and NegPSpan computation time (left) and memory consumption (right) wrt
minimal support.

of the minimal frequency threshold) and NegPSpan is run with maxgap = 10 or
maxgap = 0. Computation times show similar behaviors as in previous experiments:
NegPSpan becomes as eﬃcient as eNSP with a (weak) maxgap constraint. We can
also notice that the minimal frequency of the positive partners does not impact eNSP
computation times neither memory requirements.

The main result illustrated by Figure 3 is that NegPSpan consumes signiﬁcantly
less memory than eNSP. This comes from the depth-ﬁrst search strategy used by
NegPSpan which prevents from storing in memory many patterns. On the opposite,
eNSP requires to keep in memory all frequent positive patterns and their occurrence
lists. The lower the threshold, the more memory is required. When 𝜍 is low, then
the number of positive patterns is huge. We can see that the memory consumption
for 𝜍 = .2𝜎 is 2 orders of magnitude larger than for 𝜍 = .8𝜎. The strategy of eNSP
appears to be practically intractable for large or dense datasets for which the number
of 𝜍-frequent positive patterns is very high and requires a huge amount of memory.
This conclusion is consolidated by the experiment on the inﬂuence of vocabulary size
in Appendix C.1.

5.1.4 Study of the pattern recall

In this section, we study and compare the recall of eNSP and NegPSpan on simulated
data. The recall is the proportion of planted patterns in the dataset that are actually
extracted by the mining algorithm. The higher the recall, the better the mining al-
gorithm. NegPSpan is complete (i.e. a recall 1.0), but in case of the use of maxgap
constraints, it may miss some of the planted patterns. Conversely, eNSP is incomplete
due to the constraint on positive partners (that must be frequent). A negative pattern
can be frequent without satisfying this constraint and could be missed by algorithm
eNSP.

The recall is computed as the number of negative patterns extracted from 10
datasets of 𝑛 = 1000 random sequences each in which at least 10 negative patterns
were planted in at least 25, 5% of the sequences (𝜆𝑚𝑖𝑛 = 0.15 and 𝜆𝑚𝑎𝑥 = 0.2) and
with positive partners in 30% of the data.

The incompleteness of eNSP comes from the minimal support of positive partner
which is 𝜍 × 𝑑. The incompleteness of NegPSpan comes from the maxgap constraint

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●−5.0−2.50.02.55.00.10.20.3Frequency ThresholdTime (s)●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●1012140.10.20.3Frequency ThresholdMemory (bytes)lNegPSpanNegPSpant =10eNSP ( V = s )eNSP ( V = 0.8 s )eNSP ( V = 0.2 s )NegPSpan: eﬃcient extraction of negative sequential patterns

23

Fig. 4 Recall of algorithms eNSP (on the left) and NegPSpan (on the right) wrt rthe minimal threshold
(𝜎) and parameters 𝜍 (minimal support of positive partners) or 𝜏 (maxgap constraint).

𝜏 and also the minimal support threshold. In fact, the maxgap constraint reduces the
number of occurrences of an NSP. If the minimal support threshold is suﬃciently low,
all the patterns are eﬀectively extracted.

Figure 4 illustrates the recall wrt the minimal support threshold (𝜎). Each curve
corresponds to one value of some algorithm parameter – maxgap (𝜏) for NegPSpan
and the positive pattern threshold (𝜍) for eNSP – that impacts the recall.

We notice that:

– For eNSP and NegPSpan: the lower the support, the higher the recall but the
longer the mining process. In fact, if the minimal support is low enough, it is
always possible to ﬁnd the planted pattern (recall is 1 with very low 𝜎). But
decreasing the support increases computational costs (see previous experiments).
– For eNSP: curves with lower 𝜍 are below, so the lower 𝜍, the higher the recall.
In fact, low 𝜍 values (e.g. .1𝜎) means that the constraint on the positive partner
is very weak. But, this choice may lead to generate many positive partners that
eNSP must analyse. Low 𝜍 values make the computation time increase and make
the memory requirement explode.

– In addition, we can note that eNSP does not extract the complete set of patterns
with a minimal threshold set to 25% where NegPSpan does. This means that
eNSP requires to set the minimal support threshold lower than actually required
by the dataset to reach the perfect recall of 1.

– For NegPSpan, curves with small maxgap value, 𝜏, are below. The smaller 𝜏, the

lower the support of a pattern and the fewer patterns are frequent.

– Finally, curves with 𝜍 = .1𝜎 or 𝜏 = 5 exhibit a perfect recall whatever the minimal
support between 1% and 25%. This means that NegPSpan extracts all the planted
patterns. We can note that the recall could decrease for higher support. In fact,
the generated datasets ensure that planted patterns occur in at least 25.5% of the
sequences, if the minimal frequency threshold 𝜎 is above this value, some planted
patterns may not be frequent at all.

Figure 5 compares the recall of eNSP with 𝜍 = 0.7𝜎 and NegPSpan with 𝜏 = 10.
We can see on Figure 5 left that the recall curve of NegPSpan is above the recall curve
of eNSP and, at the same time, on Figure 5, on the right side, that the computation
times are lower. This demonstrates that NegPSpan extracts eﬃciently and accurately
the planted negative patterns.

llllllllll0.000.250.500.751.000.100.150.200.25Frequency threshold ( s )recallllV =.1 sV =.2 sV =.5 sV =.8 sV = sllllllllll0.000.250.500.751.000.100.150.200.25Frequency threshold ( s )recallllt =5t =8t =12t =15No maxgap24

Thomas Guyet, René Quiniou

Fig. 5 Recall of algorithms eNSP (on the left) and NegPSpan (on the right) wrt the minimal threshold (𝜎)
and parameters 𝜍 (ratio of positive patterns) or 𝜏 (maxgap constraint).

Let us now sum up the results on simulated datasets:

– NegPSpan extracts a correct and complete set of NSP while eNSP is correct but

incomplete

– NegPSpan with 𝜏 = 10 is more time-eﬃcient than eNSP with 𝜍 = .7𝜎 and has a

better recall

– NegPSpan has a memory consumption several orders of magnitude lower than
eNSP, and its memory consumption increases several orders of magnitude slower
than eNSP with the average sequence length

– NegPSpan is more time-eﬃcient than eNSP on dense datasets (small vocabulary

size)

NegPSpan is then the best compromise for extracting eﬃciently a set of negative

sequential patterns as much complete as possible.

These conclusions were drawn from synthetic datasets. Now, we supplement them

with an analysis of results obtained from real datasets.

5.2 Experiments on real datasets

In this section, we present experiments on real datasets coming from the SPMF
repository.4 These datasets consist of click-streams or texts represented as sequences
of items. For every dataset, we have computed the negative sequential patterns with
a maximum length 𝑙 = 5 and a minimal frequency threshold set to 𝜎 = 5%. we set
maxgap to 𝜏 = 10 for NegPSpan and 𝜍 is set to .7𝜎 for eNSP. Table 3 provides the
computation time, the memory consumption and the numbers of positive and negative
extracted patterns. Note that the numbers of positive patterns for eNSP are given for
𝜍 threshold, i.e. the support threshold for positive partners used to generate negative
patterns.

For the sign dataset, the execution has been stopped after 10𝑚𝑖𝑛𝑠 to avoid running
out of memory. The number of positive patterns extracted by eNSP considering the 𝜎
threshold is not equal to NegPSpan simply because of the maxgap constraint.

The results presented in Table 3 conﬁrm the results from experiments on synthetic
datasets. First, it highlights that NegPSpan requires signiﬁcant less memory for mining

4 http://www.philippe-fournier-viger.com/spmf/index.php?link=datasets.php

llllllllll0.000.250.500.751.000.100.150.200.25Frequency threshold ( s )recall●●●●●●●●●●−2−1010.100.150.200.25Frequency threshold (σ)Time (s)lleNSPNegPSpanNegPSpan: eﬃcient extraction of negative sequential patterns

25

Table 3 Results on real datasets with settings 𝜎 = 5%, 𝑙 = 5, 𝜏 = 10, 𝜍 = .7𝜎. Bold faces highlight the
lowest computation times or memory consumptions.

NegPSpan

eNSP

time (𝑠) mem (𝑘𝑏) #pos

#neg

time (𝑠) mem (𝑘𝑏)

#pos

#neg

Sign
Leviathan
Bible
BMS1
BMS2
kosarak25k
MSNBC

15.51
6.07
38.82
0.16
0.37
0.92
40.97

6,220
19,932
68,944
22,676
39,704
24,424
41,560

348 1,357,278
39797
110
43,701
102
0
5
0
1
409
23
56,418
613

349.84 13,901,600 1,190,642 1,257,177
17,220
28.43
2,621
27.38
7
0.18
2
0.35
51
0.53
5,439
41.44

428,916
552,288
34,272
53,608
43,124
808,744

7,691
1,364
8
3
50
2,441

every dataset. Second, NegPSpan outperforms eNSP for datasets having a long mean
sequence length (Sign, Leviathan, and MSNBC). In case of the Bible dataset, the
number of extracted patterns by eNSP is very low compared to NegPSpan due to
the constraint on minimal frequency of positive partners. NegPSpan fails to identify
negative patterns from BMS datasets, but eNSP extracts few negative patterns. It can
be explained by the maxgap constraint for NegPSpan that lowers the support of the
few eNSP patterns below the frequency threshold.

6 Case studies

In this section, we presents the use of NSPs on three case studies that show both the
wide range of applications of NSPs and some beneﬁts of NegPSpan for extracting
NSPs. It presents the output of the algorithms and analyze them. The computational
performances of the algorithms on these case studies datasets are presented in Ap-
pendix C.3.

First, we present brieﬂy some outputs on the classical market basket dataset, then
we present a complete study of care pathway data to compare patterns extracted by
eNSP and NegPSpan, and ﬁnally we apply NSP mining for customer relationship
management purpose and we present how to take decisions with extracted NSPs.

6.1 Instacart data

Instacart is an online grocery service. It has published a dataset containing information
on 3 million grocery orders from more than 200,000 users from 2017.5 The dataset
contains information on what products users purchased, the sequence they bought
them in and the amount of time between Instacart orders. The number of items is
above 50, 000 with a lot of items that are rarely bought. The objective of Instacart
was to foster research on algorithms that can predict what items customers will buy
or may be interested in.

5 The

Instacart

dataset

is

available

on

Kaggle:

https://www.kaggle.com/c/

instacart-market-basket-analysis

26

Thomas Guyet, René Quiniou

The motivation for extracting negative sequential patterns from purchase data is to
identify which products are surprisingly absent from sequences of orders. A product
is surprisingly absent when it is absent but the analyst assumes it should not. It is
the case when retailers assume that some products are very correlated. For instance,
customers that bought breakfast cereal without buying milk at any time could be
surprising. This information can help retailers to better understand customer habits
and to suggest commercial actions to increase sales. For instance, it could be proﬁtable
to advertise some milk products to the above customers.

In these experiments, NSPs were extracted with two minimal frequency thresh-
olds: 0.5% and 1%. The vocabulary size requires to set such very low thresholds
to identify non-trivial patterns, but such low thresholds prevent eNSP from process-
ing the whole dataset. For this reason, we analyzed only the ﬁrst 50, 000 customers.
The main parameter values of each algorithm were set to 𝜏 ∈ {2, 3, 4, 6, 8} and
𝜍 ∈ {0.4, 0.5, 0.6, 0.7}.

Let us now analyze the set of patterns extracted with a minimal support 𝜎 = 1% and
the settings 𝜍 = 0.4 for eNSP and 𝜏 = 4 for NegPSpan. eNSP extracted 6, 499 patterns,
while NegPSpan extracted 8, 096 patterns. With higher values for 𝜏, the number of
NSPs makes the comparison diﬃcult.

Despite the low threshold, a large part of the patterns extracted by NegPSpan falls

into one of the following shapes of patterns (𝑋 and 𝑌 represent itemsets):

– 𝒑𝑰

– the pattern is positive (contains no negation),
– 𝒑𝑰
– 𝒑𝑰

1 = (cid:104)13176 ¬𝑋 13176(cid:105) where 13176 is bag (86 patterns)
2 = (cid:104)21903 ¬𝑋 21903(cid:105) where 21903 is organic baby spinach ravioli
(86 patterns)
3 = (cid:104)24852 ¬𝑋 24852 ¬𝑌 24852(cid:105) where 24852 is banana (7, 830 patterns)
The patterns of type 𝒑𝑰
3

represent more than 90% of the extracted patterns. Among
these NSPs, some seems interesting because they involve diﬀerent fruits. It is inter-
esting to identify customers purchasing lots of bananas and who do not purchase
any other fruit during the same period. Nevertheless, most of these patterns involves
products that are not fruits, and are not really meaningful. In this case, the posi-
tive partner constraint of eNSP would discard the patterns whose positive pattern,
(cid:104)24852 ¬𝑋 24852 ¬𝑌 24852(cid:105), is not 𝜍-frequent. And we can expect that eNSP would
extract a small set of meaningful patterns, but eNSP extracts a similar amount of
patterns (6, 499).

eNSP extracts similar patterns but also a collection of more diverse patterns. The
diversity comes from the absence of gap constraints. The embedding of a sequential
pattern without gap constraint may involve purchases that are very distant in the
sequence of purchases. This case study questions the signiﬁcance of such patterns.
Retailers are in reality not interested in relationships involving client purchases that
are very distant in time. In some way, we can say that the absence of gap constraints
makes eNSP over-estimates the support of patterns.

The conclusion of this case study is that eNSP has better computation time results
on this dataset, but it did not succeed in processing the whole dataset. NegPSpan is
time-consuming but ﬁt in memory even on a large and sparse dataset. The computation

NegPSpan: eﬃcient extraction of negative sequential patterns

27

costs of NegPSpan comes from the enumeration of patterns with the same positive
shape: it combines positive patterns with all possible negative items that could make
them frequent. At the end, the patterns extracted by each algorithm are diﬃcult to
interpret. The patterns extracted by eNSP may be somehow more signiﬁcant thanks
to the positive partner constraint. But without gap constraint, most of the extracted
NSPs are irrelevant for retailers.

To process such large but sparse dataset, it would be interesting to beneﬁt from the
best of the two approaches: a relevant NSP semantics including gap constraints and
a reasonable number of diverse NSPs. Thanks to the anti-monotonicity property of
its semantics for negative patterns, NegPSpan could be adapted to ﬁt in some recent
frameworks for mining diverse patterns eﬃciently (Bosc et al., 2018). Alternatively,
the frequency constraint on positive partners could be applied as a post-processing
step to prevent NegPSpan from extracting useless patterns. It would not be eﬃcient,
but could provide more interpretable results for this case study.

6.2 Care pathway analysis

This section presents the use of NSPs for analyzing epileptic patient care pathways.
Recent studies suggest that medication changes may be associated with epileptic
seizures for patients with long term treatment with anti-epileptic medication (Polard
et al., 2015). NSP mining algorithms are used to extract patterns of drugs deliveries
that may inform the suppression of a drug from a patient treatment. In (Dauxais et al.,
2017), we studied discriminant temporal patterns but it does not explicitly extract the
information about medication absence as a possible explanation of epiletic seizures.
Our dataset was obtained from the french insurance database (Moulis et al., 2015).
8,379 epileptic patients were identiﬁed by their hospitalization related to an epileptic
seizure. For each patient, we built a sequence of drugs deliveries within the 90 days
before the ﬁrst epileptic seizure. For each drug delivery, an event is a tuple (𝑚, 𝑔)
where 𝑚 is the ATC6 code of the active molecule, 𝑔 ∈ {0, 1} is the brand-name (0) vs
generic (1) status of the drug. For the sake of readability, an identiﬁer is assigned to
each event tuple. Table 4 gives the mapping between event identiﬁers and event tuples.
For example, the following sequence representing a sequence of drug deliveries with
a switch from generic to brand-name valproic acid:

(cid:104) (valproic acid, 𝑔𝑒𝑛𝑒𝑟𝑖𝑐) ((valproic acid, 𝑔𝑒𝑛𝑒𝑟𝑖𝑐), (paracetamol, 𝑔𝑒𝑛𝑒𝑟𝑖𝑐)) . . .
(valproic acid, 𝑔𝑒𝑛𝑒𝑟𝑖𝑐) (valproic acid, 𝑏𝑟𝑎𝑛𝑑) (valproic acid, 𝑏𝑟𝑎𝑛𝑑) (cid:105)

is translated into the sequence:

(cid:104)383 (383, 86) 383 114 114(cid:105)

The dataset contains 251,872 events over 7,180 diﬀerent drugs. The mean length of
a sequence is 7.89±8.44 itemsets. The sequence length variance is high. This is due to
the heterogeneous nature of care pathways. Some of them represent complex therapies

6 ATC: Anatomical Therapeutic Chemical Classiﬁcation System is a drug classiﬁcation system that

classiﬁes the active ingredients of drugs.

28

Thomas Guyet, René Quiniou

Table 4 Selection of events identiﬁers that are used in the discussed patterns.

Event tuple

Event id

(levetiracetam, generic)
(paracetamol, generic)
(zolpidem, generic)
(valproic acid, brand)
(clobazam, generic)
(zopiclone, generic)
(phenobarbital, generic)
(valproic acid, generic)

7
86
112
114
115
151
158
383

Table 5 Patterns involving valproic acid switches with their supports computed by eNSP and NegPSpan.
Empty supports indicate that the pattern has not been extracted. Drug names associated with identiﬁers in
the ﬁgures are given within the text.

Pattern

𝒑𝑪𝑷
1
𝒑𝑪𝑷
2
𝒑𝑪𝑷
3
𝒑𝑪𝑷
4
𝒑𝑪𝑷
5
𝒑𝑪𝑷
6
𝒑𝑪𝑷
7
𝒑𝑪𝑷
8

= (cid:104)383 ¬(86, 383) 383(cid:105)
= (cid:104)383 ¬86 383(cid:105)
= (cid:104)383 ¬112 383(cid:105)
= (cid:104)383 ¬114 383(cid:105)
= (cid:104)383 ¬115 383(cid:105)
= (cid:104)383 ¬151 383(cid:105)
= (cid:104)383 ¬158 383(cid:105)
= (cid:104)383 ¬7 383(cid:105)

support
eNSP

support
NegPSpan

1,579
1,251
1,610
1,543
1,568
1,611
1,605

1,243

1,232
1,236

1,243

involving the consumption of many diﬀerent drugs while others are simple cases
consisting of few deliveries of anti-epileptic drugs. In the french health system, drug
deliveries are renewed every month. For epileptic patients that require a continuous
medication, we expect to have mostly sequences with three or four deliveries of anti-
epileptic drugs. Other items are additional medical treatments that are not necessarily
related to epilepsy.

Let us now compare the pattern sets extracted by eNSP and NegPSpan. In fact,
expressible pattern constraints are diﬀerent (maxgap constraints for NegPSpan and
minimal support of positive partners for eNSP) and the extracted pattern sets are
diﬀerent. In this qualitative experiment, the parameters is set to 𝜎 = 14.3% (1, 200
sequences), 𝑙 = 3 (maximal pattern length), 𝜏 = 3 for NegPSpan and 𝜍 = .1 × 𝜎 the
minimal support for positive partners for eNSP. eNSP extracted 1,120 patterns and
NegPSpan only 10 patterns (including positive and negative patterns). Due to a low 𝜍
threshold, many positive patterns were extracted by eNSP leading to generate a lot of
patterns having a single negated item. In this analysis, we pay attention to the specialty
of valproic acid which exists in generic form (event 383) or brand-named form
(event 114). Table 5 presents all patterns starting and ﬁnishing with event 383. Other
events correspond to alternative anti-epileptic drugs (levetiracetam, phenobarbital)
or psycholeptic drugs (zolpidem, clobazam, zopiclone) except paracetamol.

NegPSpan: eﬃcient extraction of negative sequential patterns

29

5

2

, 𝒑𝑪𝑷
4

and 𝒑𝑪𝑷

It is interesting to note that only 3 patterns ( 𝒑𝑪𝑷

) are extracted
by both algorithms. Their supports are lower with NegPSpan because of the maxgap
constraint. This constraint also explains that patterns 𝒑𝑪𝑷
are not extracted
by NegPSpan. These patterns illustrate that in some cases, the patterns extracted by
eNSP may not be really interesting because they involve distant events in the sequence.
is not extracted by NegPSpan due to the syntactic constraints. On the
Pattern 𝒑𝑪𝑷
opposite, NegPSpan extracts patterns that eNSP misses. For instance, pattern 𝒑𝑪𝑷
is
not extracted by eNSP because its positive partner, (cid:104)383 7 383(cid:105), is not frequent. In this
case, it leads eNSP to miss a potentially interesting pattern involving two anti-epileptic
drugs.

and 𝒑𝑪𝑷

1

6

8

3

Next, we examine a particular case to illustrate that NSPs are essential to have true
insights about the meaning of frequent positive patterns. Remind that our objective
is to conclude about the potential link between switches in epileptic drug deliver-
ies and epileptic seizures. Our point is that positive patterns can be misinterpreted,
because they only show what actually happens. Negative patterns may avoid some
misinterpretation.

For this experiment, we changed the NegPSpan settings to focus on patterns
involving a switch from the generic form to the brand-named form of valproic acid.
The settings were 𝜎 = 1.2%, 𝑙 = 3 and 𝜏 = 5. The only two frequent positive
patterns are (cid:104)114 383 114(cid:105) and (cid:104)114 114(cid:105). In our experiment, it means that these
patterns occur frequently in patient having an epileptic seizure. Because (cid:104)114 114(cid:105) is
a positive pattern, it does not means that there is no switches in sequences that hold
this pattern. Thus, positive patterns are not conclusive about the impact of a switch
from 114 to 383 on epileptic seizures.

NegPSpan also extracts the pattern (cid:104)114 ¬383 114(cid:105), which speciﬁes that the
absence of a switch is frequent for patients having epileptic seizure. This time, it
sheds light on the fact that switches and non-switches from generic form to brand-
named form of valproic acid are both frequent behaviors in care pathways of patients
having epileptic seizure. We can conclude that pattern mining techniques does not
establish a statistical link between epileptic drug switches and epileptic seizure. This
result is consistent with (Polard et al., 2015).

6.3 Customer Relationship Management

Customer Relationship Management (CRM) refers to a set of tools that is used for
managing the interactions between a company and its customers. As its main objective
is to maintain regular customers, by developing long-term relationships with them, and
to acquire new ones, CRM is being used by any organization to identify the problems
of customers and to improve the consistency with them. To fullﬁll the objectives
above, it is required from the company to perform customer relationship analysis in
order to meet customer needs and improve customer services. However, due to the
increased number of business data generated by companies during the last years, the
most useful and valuable information required for customer analysis is often hidden
in the large CRM databases and is not easily accessible.

30

Thomas Guyet, René Quiniou

Table 6 Frequent contact reasons in the CRM database.

Id

Name

Frequency (%)

1CTR
2VIE
1RES
6DIS
8DOS
3SER
4FAC
5REC
Unknown
3RCTR

Contract subscription
Contract modiﬁcation
Contract termination
Relationship with supplier
File handling
Advices and services
Billing
Recovery
Unknown
Contract

30.73%
20.67%
12.42%
6.62%
6.47%
4.56%
4.08%
3.04%
2.4%
1.9%

According to Ngai et al. (2009), a common set of supporting tools (statistical
analysis, cluster analysis, probability theory, artiﬁcial neural networks, etc) widely
used from companies for making relevant CRM decisions is data mining, which are
good at extracting and identifying useful knowledge from large customer databases.
Speciﬁc data mining techniques like association rules mining or sequential pattern
mining are especially useful for analyzing customer data. Frequent sequential pattern
mining is particularly useful in commercial applications, as it can be used to discover
customers’ behavioral and purchasing patterns over time (Mallick et al., 2013).

The negative events are meaningful in CRM to inform stakeholders about which
actions was absent in the customer interactions. An absent interaction may be an action
missed by customer services. This way, NSPs enable decision makers to identify
potential improvements in the customer relationship management.

In this case study, we illustrate how some extracted NSPs can be interpreted
to make decisions for improving customer relationship management. We analyze
sequences corresponding to the interactions of customers with the services of an
electricity supplier in order to prevent contract cancellations.

The dataset has 375, 142 customers (one sequence per customer) with a vocabulary
of 93 diﬀerent items. Items are tags that label the contact reasons (for instance invoicing
procedures, technical issues or commercial purposes). Table 6 presents the 16 items
that occurs more than 1% in the dataset. The dataset does not specify whether a contact
was initiated by the company or by the customer. A contact between the company and
the customer can be labeled with several tags. The average number of tags per contact
is 1.21±0.34. This means that most of the itemsets hold only 1 item (a contact has
only one contact reason), but it raises up to 10 tags for a unique contact. The average
length of the sequences is 2.52±7.26: most of the sequences are short (2 or 3 contacts
within a period of one year) but some customers have much more interactions with
the company.

The frequent NSPs is run with a minimal support threshold 𝜎 = 0.5% (at least
1,875 occurrences of an NSP), 𝜍 = 0.4𝜎 for eNSP and 𝜏 = 4 NegPSpan. NegPSpan
(resp. eNSP) extracts 18, 652 (resp. 6, 746) patterns. The two sets of extracted NSPs
have 1, 463 patterns in common. So, NegPSpan extracts more NSPs than eNSP does

NegPSpan: eﬃcient extraction of negative sequential patterns

31

Table 7 Selection of patterns involving 3SER (on the left) or 2VIE (on the right) negative items with their
supports computed by NegPSpan but not by eNSP.

Pattern

𝒑𝑪 𝑹𝑴
1
𝒑𝑪 𝑹𝑴
2
𝒑𝑪 𝑹𝑴
3
𝒑𝑪 𝑹𝑴
4
𝒑𝑪 𝑹𝑴
5
𝒑𝑪 𝑹𝑴
6
𝒑𝑪 𝑹𝑴
7

= (cid:104)6𝐷𝐼 𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
= (cid:104)1𝑅𝐸𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
= (cid:104)(1𝐶𝑇 𝑅, 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
= (cid:104)(1𝐶𝑇 𝑅, 2𝑉 𝐼 𝐸 , 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸 𝑆 (cid:105)
= (cid:104)(3𝑆𝐸 𝑅, 8𝐷𝑂𝑆) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
= (cid:104)(1𝐶𝑇 𝑅, 3𝑆𝐸 𝑅), ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
= (cid:104)1𝐶𝑇 𝑅 2𝑉 𝐼 𝐸 8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)

support
NegPSpan

5,352
5,265
6,597
1,946
3,032
2,258
1,912

in previous case studies but it does it faster. NegPSpan requires 113𝑠 computation time
while eNSP requires 786𝑠. In the following, we do not compare the set of extracted
patterns, but we focus our attention on the interpretation of patterns extracted by
NegPSpan.

Among the 18, 652 patterns extracted by NegPSpan, we selected only the patterns
ending by the contract termination item (1𝑅𝐸 𝑆) and which have only one negative
event. There are 281 patterns satisfying these conditions. This amount of patterns is
still too large to be analyzed. Then, the practical objective of this case study suggests to
focus our interest on actionable patterns. The information extracted aims at suggesting
actions to improve customer relation management. The customer services can impact
customer decisions mainly by providing services (3SER item) or by modifying the
customer contract (2VIE item). For this reason, we look at the NSPs that involve the
3SER item or the 2VIE item in the negative part.

Table 7 illustrates some interesting patterns extracted only by NegPSpan. The
complete lists of selected patterns is in Appendix D: 20 NSP are extracted by both
eNSP and NegPSpan, 18 are extracted only by eNSP and 20 are extracted only by
NegPSpan.

1

, 𝒑𝑪 𝑹𝑴
2

and 𝒑𝑪 𝑹𝑴
3

Patterns 𝒑𝑪 𝑹𝑴

are interesting because they are almost frequent
but are not extracted by eNSP. It seems that the support of their positive partner does
not exceed 𝜍 = 0.4𝜎 (750 sequences). Nonetheless, the following comments show
that they are insightful and can provide interesting information about the behavior of
customers:

– 𝒑𝑪 𝑹𝑴
1

are customers that contacted the company with a concern about distribution
of the electricity (6𝐷 𝐼𝑆) and that did not get any advice or service (3𝑆𝐸 𝑅). For the
5, 368 customers, it leads to a contract termination. It can be notice that customers
that experience this pattern amounts to about 2% but contacts for the 6𝐷 𝐼𝑆 purpose
amounts to 6.62% of the contact reasons. Thus, it is a relatively frequent pathway
for customers having this contact reason. This pattern highlights that customer
services have to provide advice or services to prevent customer churn in this case.
are customers who contacted the company about contract cancellation but
were not contacted for advice or services proposal afterwards, and then contacted
again the customer services about contract cancellation (maybe to ﬁnalize their

– 𝒑𝑪 𝑹𝑴
2

32

Thomas Guyet, René Quiniou

contract cancellation procedure). This pattern identiﬁes customers undergoing a
potential failure in the customer relationship management: if a customer contacts
the customer services about contract cancellation, it seems quite normal to investi-
gate this customer’s situation with a 3𝑆𝐸 𝑅 contact. This pattern shows that 5, 265
customers were not contacted back after asking for cancellation information.

– 𝒑𝐶 𝑅𝑀
3

4

(and its similar pattern 𝒑𝐶 𝑅𝑀

) represents customers who subscribed to
a contract and ﬁnally canceled their contract without having been contacted for
advice. The maximum gap constraint that have been imposed by NegPSpan, 𝜏 = 4,
indicates that the customer relationship was faulty: for these customers there are
at most three interactions between their contract subscription and their contract
cancellation.

5

and 𝒑𝑪 𝑹𝑴
6

Patterns 𝒑𝑪 𝑹𝑴

involve absence of contract modiﬁcations (2𝑉 𝐼 𝐸)
because advice or service oﬀers do not necessarily meet customers’ needs. In these
cases, they do not modify their contract. A possible explanation of these two patterns
is that customers received some advice (3𝑆𝐸 𝑅) but did not modify their contracts
(2𝑉 𝐼 𝐸) and ﬁnally churned. For the customer services, these patterns can be used to
identify which advice or services have been oﬀer to these particular customers and to
assess whether an alternative advice would not lead to a contract termination.

7

Pattern 𝒑𝑪 𝑹𝑴

is relatively frequent regarding its length. We interpret this pattern
as a failure in the ﬁle handling procedure. The customer subscribed to a contract, then
she/he had a contact to modify it. This modiﬁcation may not have been correctly taken
into consideration and the customer had another contact about her/his ﬁle handling.
Without eﬀective contract modiﬁcation after the last contact, the customer churned.

Let us now conclude about the three case studies. The ﬁrst case study is a dataset on
which both algorithms have diﬃculties to extract meaningful NSPs. The case study on
medical care pathways highlights two important conclusions about the interpretation
of sequential patterns. First, NSPs provide meaningful information about sequential
patterns and may prevent from their misinterpretation. Second, the diﬀerent semantics
adopted by the two algorithms lead to diﬀerent sets of extracted pattern. Without
maxgap constraints, eNSP outputs some patterns whose frequency is over-estimated
compared to the frequency computed by NegPSpan. But, eNSP misses some possibly
important patterns at the boundary of the frequent NSPs. Finally, the third case study
illustrates some complex interpretations of NSPs that would be useful for further
analysis of customer relationship data and management procedures.

7 Related work

Kamepalli et al. (2014) and more recently Wang and Cao (2019) provide surveys
of the approaches proposed for mining negative patterns. The three most signiﬁcant
algorithms appear to be PNSP (Hsueh et al., 2008), NegGSP (Zheng et al., 2009) and
eNSP (Cao et al., 2016). We brieﬂy review each of them in the following paragraphs.
PNSP (Positive and Negative Sequential Patterns mining) (Hsueh et al., 2008) is
the ﬁrst algorithm proposed for mining full negative sequential patterns where nega-
tive itemsets are not only located at the end of the pattern. PNSP extends algorithm

NegPSpan: eﬃcient extraction of negative sequential patterns

33

GSP (Srikant and Agrawal, 1996) to cope with mining negative sequential patterns.
PNSP consists of three steps: i) use algorithm GSP for mining frequent positive
sequential patterns, ii) preselect negative sequential itemsets — for PNSP, negative
itemsets must not be too infrequent (should have a support lower than a threshold
miss_freq) — iii) generate candidate negative sequences levelwise and scan the se-
quence dataset again to compute the support of these candidates and prune the search
when the candidate is infrequent. This algorithm is incomplete: the second parameter
reduces the set of potential negative itemsets. Moreover, the pruning strategy of PNSP
is not correct (Zheng et al., 2009) and PNSP misses potentially frequent negative
patterns.

Zheng et al. (2009) also proposed a negative version of algorithm GSP, called
NegGSP, to extract negative sequential patterns. They showed that traditional Apriori-
based negative pattern mining algorithms relying on support anti-monotonicity have
two main problems. The ﬁrst one is that the Apriori principle does not apply to negative
sequential patterns. They gave an example of sequence that is frequent even if one of
its sub-sequence is not frequent. The second problem has to do with the eﬃciency
and the eﬀectiveness of ﬁnding frequent patterns due to a vast candidate space. Their
solution was to prune the search space using the support anti-monotonicity over
positive parts. This pruning strategy is correct but incomplete and it is not really
eﬃcient considering the huge number of remaining candidates whose support has to
be evaluated. To improve the eﬃciency of their approach, the authors proposed an
incomplete heuristic search based on Genetic Algorithm to ﬁnd negative sequential
patterns (Zheng et al., 2010).

eNSP (eﬃcient NSP) has been recently proposed by Cao et al. (2016). It identiﬁes
NSPs by computing only frequent positive sequential patterns and deducing nega-
tive sequential patterns from positive patterns. Precisely, Cao et al. showed that the
support of some negative pattern can be computed by arithmetic operations on the
support of its positive sub-patterns, thus avoiding additional sequence dataset scans
to compute the support of negative patterns. However, this necessitates to store all the
(positive) sequential patterns with their set of covered sequences (tid-lists) which may
be impossible in case of big dense datasets and low minimal support thresholds. This
approach makes the algorithm more eﬃcient but it hides some restrictive constraints
on the extracted patterns. First, a frequent negative pattern whose so-called positive
partner (the pattern where all negative events have been switched to positive) is not
frequent will not be extracted. Second, every occurrence of a negative pattern in a
sequence should satisfy absence constraints. We call this strong absence semantics
(see Section 3.2). These features lead eNSP to extract fewer patterns than previous
approaches. In some practical applications, eNSP may miss potentially interesting
negative patterns from the dataset.

The ﬁrst constraint has been partly tackled by Dong et al. with algorithm eNSPFI,
an extension of eNSP which mines NSPs from frequent and some infrequent positive
sequential patterns from the negative border (Gong et al., 2017). E-msNSP (Xu et al.,
2017b) is another extension of eNSP which uses multiple minimum supports: an NSP
is frequent if its support is greater than a local minimal support threshold computed
from the content of the pattern and not a global threshold as in classical approaches.
A threshold is associated with each item, and the minimal support of a pattern is

34

Thomas Guyet, René Quiniou

deﬁned from the most constrained item it contains. Such kind of adaptive support
prevents from extracting some useless patterns still keeping the pattern support anti-
monotonic. The same authors also proposed high utility negative sequential patterns
based on the same principles (Xu et al., 2017a) and applied the framework on smart
city data (Xu et al., 2018). An alternative approach has been proposed by Lin et al.
(2016) consisting in mining high-utility itemsets with negative unit proﬁts but it
was not applied on sequential patterns. It is worth noting that this algorithm relies
basically on the same principle as eNSP and so, presents the same drawbacks, heavy
memory requirements, strong absence semantics for negation. F-NSP+ (Dong et al.,
2018b) extends the eNSP algorithm to use bitmap representations of itemsets. Using
bitmap representations enables to speed up algorithm eNSP, thanks to very eﬃcient set
operations on bitmaps. Algorithm F-NSP has a poor memory usage, while F-NSP+,
which adapts the bitmap size to the dataset, requires slightly less memory.

e-RNSP (Dong et al., 2018a) extracts repetition NSP (RNSP). A RNSP is a NSP
for which the support takes into account the repetition of the pattern in a sequence
of the sequence dataset. Thus, it is not really a new pattern domain but more a new
deﬁnition of support measure. The e-RNSP uses the same strategy as eNSP with a hash
map to store all occurrences of the positive patterns (all repetition must be stored).
The experiments have been conducted on small datasets with a maximum number
of 10𝑘 sequences (of at most 15 itemsets). Unsurprisingly, the computation times of
e-RNSP are slightly above e-RNSP. The memory consumption of the approach is not
discussed but it is at most as expensive as eNSP.

SAPNSP (Liu et al., 2015) tackles the problem of large amount of patterns by
selecting frequent negative and positive patterns that are actionable. Patterns are
actionable if they conform to special rules.

NegI-NSP (Qiu et al., 2017) proposes additional syntactic constraints on negative

itemsets and uses the same strategy as e-NSP.

To conclude this state of the art section, we provide in Table 8 a comparison of
several negative sequential pattern mining approaches wrt several features investigated
in section 3. It is also important to emphasize that one semantics is “more correct”
than another one. Its relevance depends on the information the data scientists want
to capture in datasets, and the nature of the data at hand. In this work, one of our
objectives is to provide a sound and insightful framework for negative patterns to
enable users to choose the tool to use and to make this choice according to the
semantics of the negation they want to use. Execution time is obviously an important
choice criteria but it must be consistent with semantical choices to provide interesting,
intuitive and sound results.

8 Conclusion and perspectives

This article has investigated negative sequential pattern mining (NSP). It highlights
that state of the art algorithms do not extract the same patterns, not only depending on
their syntax and algorithm speciﬁcities, but also depending on the semantical choices
adopted for each of them. In this article, we have proposed deﬁnitions that clarify the

NegPSpan: eﬃcient extraction of negative sequential patterns

35

Table 8 Comparison of negative pattern mining proposals. Optional constraints are speciﬁed in Italic.
Question marks indicates that the article does not clearly state their semantic.

PNSP (Hsueh
et al., 2008)

NegGSP
(Zheng et al.,
2009)

eNSP (Cao
et al., 2016)

NegPSpan

Negative elements

Itemsets

Embeddings

Occurrences

Constraints on negative
itemsets

Global constraints on pat-
terns

itemsets

(cid:42)𝐺?
strict

weak

not too in-
frequent
(𝑠𝑢 𝑝 𝑝 (cid:54)
𝑙𝑒𝑠𝑠_ 𝑓 𝑟 𝑒𝑞)

items?

(cid:42)𝐺
strict?

weak

itemsets

(cid:42)𝐷
strict

strong

itemsets

(cid:42)𝐷
strict/soft
weak

frequent items

positive part-
ner is frequent

frequent items,
bounded size

positive part
is frequent
(second greater
threshold)

maxspan,
maxgap

negation semantics encountered in the literature. We have shown that the support of
NSP depends on the semantics of itemset non-inclusion, two possible alternatives for
considering negation of itemsets and two ways for considering multiple embeddings
in a sequence. So, we could point out the limits of the state of the art algorithm eNSP
that imposes a minimum support for positive partners and that is not able to deal with
embedding constraints, and more especially maxgap constraints.

We have proposed NegPSpan, a new algorithm for mining negative sequential
patterns that overcomes these limitations. The experiments show that NegPSpan is
more eﬃcient than eNSP on datasets with medium-long sequences (more than 20
itemsets) even when weak maxgap constraints are applied and that it prevents from
missing possibly interesting patterns.

In addition, NegPSpan is based on well-founded theoretical principles that makes
possible to extend it to the extraction of closed or maximal patterns to reduce the
number of extracted patterns even more.

Acknowledgements The authors would like to thank REPERES Team from Rennes University Hospital
for spending time to discuss our case study results. We also would like to thanks M. Boumghar, L. Pierre and
D. Lagarde for raising interesting issues and providing the dataset about customer relationship management.
Finally, we would also like to thanks the reviewers for their insightful comments.

References

Bosc G, Boulicaut JF, Raïssi C, Kaytoue M (2018) Anytime discovery of a diverse set
of patterns with monte carlo tree search. Data Mining and Knowledge Discovery
32(3):604–650

Cao L, Yu PS, Kumar V (2015) Nonoccurring behavior analytics: A new area. Intel-

ligent Systems 30(6):4–11

36

Thomas Guyet, René Quiniou

Cao L, Dong X, Zheng Z (2016) e-NSP: Eﬃcient negative sequential pattern mining.

Artiﬁcial Intelligence 235:156–182

Dauxais Y, Guyet T, Gross-Amblard D, Happe A (2017) Discriminant chronicles
mining - application to care pathways analytics. In: Proceedings of 16th Conference
on Artiﬁcial Intelligence in Medicine (AIME), pp 234–244

Dong X, Gong Y, Cao L (2018a) e-RNSP: An eﬃcient method for mining repetition
negative sequential patterns. IEEE Transactions on Cybernetics pp 1–13, DOI
10.1109/TCYB.2018.2869907

Dong X, Gong Y, Cao L (2018b) F-NSP+: A fast negative sequential patterns mining

method with self-adaptive data storage. Pattern Recognition 84:13–27

Giannotti F, Nanni M, Pedreschi D (2006) Eﬃcient mining of temporally annotated
sequences. In: Proceedings of the SIAM International Conference on Data Mining,
pp 348–359

Gong Y, Xu T, Dong X, Lv G (2017) e-NSPFI: Eﬃcient mining negative sequential
pattern from both frequent and infrequent positive sequential patterns. International
Journal of Pattern Recognition and Artiﬁcial Intelligence 31(02):1750002

Han J, Pei J, Mortazavi-Asl B, Chen Q, Dayal U, Hsu MC (2000) FreeSpan: fre-
quent pattern-projected sequential pattern mining. In: Proceedings of the sixth
international conference on Knowledge discovery and data mining (SIGKDD), pp
355–359

Hsueh SC, Lin MY, Chen CL (2008) Mining negative sequential patterns for e-
commerce recommendations. In: Proceedings of Asia-Paciﬁc Services Computing
Conference, pp 1213–1218

Kamepalli S, Sekhara R, Kurra R (2014) Frequent negative sequential patterns – a
survey. International Journal of Computer Engineering and Technology 5, 3:115–
121

Lin JCW, Fournier-Viger P, Gan W (2016) FHN: An eﬃcient algorithm for min-
ing high-utility itemsets with negative unit proﬁts. Knowledge-Based Systems
111:283–298

Liu C, Dong X, Li C, Li Y (2015) SAPNSP: Select actionable positive and negative
sequential patterns based on a contribution metric. In: Proceedings of the 12th
International Conference on Fuzzy Systems and Knowledge Discovery, pp 811–
815

Mallick B, Garg D, Grover PS (2013) CRM customer value based on constrained se-
quential pattern mining. International Journal of Computer Applications 64(9):21–
29

Mooney CH, Roddick JF (2013) Sequential pattern mining – approaches and algo-

rithms. ACM Computing Survey 45(2):1–39

Moulis G, Lapeyre-Mestre M, Palmaro A, Pugnet G, Montastruc JL, Sailler L (2015)
French health insurance databases: What interest for medical research? La Revue
de Médecine Interne 36:411–417

Negrevergne B, Guns T (2015) Constraint-based sequence mining using constraint
programming. In: Michel L (ed) Proceedings of the conference on Integration of AI
and OR Techniques in Constraint Programming (CPAIOR), Springer International
Publishing, pp 288–305

NegPSpan: eﬃcient extraction of negative sequential patterns

37

Ngai E, Xiu L, Chau D (2009) Application of data mining techniques in customer
relationship management: A literature review and classiﬁcation. Expert Systems
with Applications 36(2, Part 2):2592 – 2602

Pei J, Han J, Mortazavi-Asl B, Wang J, Pinto H, Chen Q, Dayal U, Hsu MC (2004)
Mining sequential patterns by pattern-growth: The PreﬁxSpan approach. IEEE
Transactions on knowledge and data engineering 16(11):1424–1440

Pei J, Han J, Wang W (2007) Constraint-based Sequential Pattern Mining: The Pattern-

growth Methods. Journal of Intelligent Information Systems 28(2):133–160

Polard E, Nowak E, Happe A, Biraben A, Oger E (2015) Brand name to generic
substitution of antiepileptic drugs does not lead to seizure-related hospitalization:
a population-based case-crossover study. Pharmacoepidemiology and drug safety
24:1161–1169

Qiu P, Zhao L, Dong X (2017) NegI-NSP: Negative sequential pattern mining based
on loose constraints. In: Proceedings of the 43rd Annual Conference of the IEEE
Industrial Electronics Society (IECON), pp 3419–3425

Srikant R, Agrawal R (1996) Mining sequential patterns: Generalizations and perfor-
mance improvements. In: Proceedings of the International Conference on Extending
Database Technology (EDBT), Springer, pp 1–17

Wang W, Cao L (2019) Negative sequences analysis: A review. ACM Computing

Survey 52

Xu T, Dong X, Xu J, Dong X (2017a) Mining high utility sequential patterns with
negative item values. International Journal of Pattern Recognition and Artiﬁcial
Intelligence 31(10):1750035

Xu T, Dong X, Xu J, Gong Y (2017b) E-msNSP: Eﬃcient negative sequential patterns
mining based on multiple minimum supports. International Journal of Pattern
Recognition and Artiﬁcial Intelligence 31(02):1750003

Xu T, Li T, Dong X (2018) Eﬃcient high utility negative sequential patterns mining

in smart campus. IEEE Access 6:23839–23847

Zaki MJ (2001) SPADE: An Eﬃcient Algorithm for Mining Frequent Sequences.

Machine Learning 42(1/2):31–60

Zheng Z, Zhao Y, Zuo Z, Cao L (2009) Negative-GSP: An eﬃcient method for min-
ing negative sequential patterns. In: Proceedings of the Australasian Data Mining
Conference, pp 63–67

Zheng Z, Zhao Y, Zuo Z, Cao L (2010) An eﬃcient GA-based algorithm for mining
negative sequential patterns. In: Proceedings of the Paciﬁc-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD), Springer, pp 262–273

38

A Proofs

Thomas Guyet, René Quiniou

Proof (Proof of Proposition 2) Let 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛 (cid:105) be a sequence and 𝒑 = (cid:104) 𝑝1 · · · 𝑝𝑚 (cid:105) be a
negative sequential pattern. Let 𝒆 = (𝑒𝑖)𝑖∈ [𝑚] ∈ [𝑛]𝑚 be a soft-embedding of pattern 𝒑 in sequence
𝒔. Then, the deﬁnition matches the one for strict-embedding if 𝑝𝑖 is positive. If 𝑝𝑖 is negative then
∀ 𝑗 ∈ [𝑒𝑖−1 + 1, 𝑒𝑖+1 − 1], 𝑝𝑖 (cid:42)𝐷 𝑠 𝑗 , i.e. ∀ 𝑗 ∈ [𝑒𝑖−1 + 1, 𝑒𝑖+1 − 1], ∀𝛼 ∈ 𝑝𝑖 , 𝛼 ∉ 𝑠 𝑗 and then
∀𝛼 ∈ 𝑝𝑖 , ∀ 𝑗 ∈ [𝑒𝑖−1 + 1, 𝑒𝑖+1 − 1], 𝛼 ∉ 𝑠 𝑗 . Thus, it implies that ∀𝛼 ∈ 𝑝𝑖 , 𝛼 ∉ (cid:208) 𝑗∈ [𝑒𝑖−1+1,𝑒𝑖+1−1] 𝑠 𝑗 ,
i.e. by deﬁnition, 𝑝𝑖 (cid:42)𝐷 (cid:208) 𝑗∈ [𝑒𝑖−1+1,𝑒𝑖+1−1] 𝑠 𝑗 .

The exact same reasoning is done in the reverse way to prove the equivalence.

Proof (Proof of Proposition 3 (Anti-monotonicity of NSP)) Let 𝒑 = (cid:104) 𝑝1 ¬𝑞1 𝑝2 ¬𝑞2 · · · 𝑝𝑘−1
𝑘(cid:48) (cid:105) be two NSP s.t. 𝒑 (cid:67) 𝒑(cid:48). By deﬁni-
𝑝(cid:48)
¬𝑞𝑘−1 𝑝𝑘 (cid:105) and 𝒑(cid:48) = (cid:104) 𝑝(cid:48)
𝑘(cid:48)−1
tion we have that 𝑘 ≤ 𝑘(cid:48).

2 · · · 𝑝(cid:48)

2 ¬𝑞(cid:48)
𝑝(cid:48)

1 ¬𝑞(cid:48)
1

¬𝑞(cid:48)

𝑘(cid:48)−1

To prove the anti-monotonicity, we prove that any embedding (𝑒𝑖)𝑖∈ [𝑘 ] of 𝒑(cid:48) in a sequence 𝒔 generates

an embedding of 𝒑 in 𝒔.

Let 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛 (cid:105) be a sequence s.t. 𝒑(cid:48) (cid:22) 𝒔, i.e. there exists an embedding (𝑒𝑖)𝑖∈ [𝑘(cid:48) ] :

– ∀𝑖, 𝑒𝑖+1 > 𝑒𝑖,
– ∀𝑖, 𝑝(cid:48)
𝑖 ⊆ 𝑠𝑒𝑖 ,
– ∀ 𝑗 ∈ [𝑒𝑖 + 1, 𝑒𝑖+1 − 1], 𝑞(cid:48)

𝑖 (cid:42)𝐷 𝑠𝑒 𝑗
By deﬁnitions of (cid:67) and embedding,

(i) ∀𝑖 ∈ [𝑘 ], 𝑝𝑖 ⊆ 𝑝(cid:48)
(ii) ∀𝑖 ∈ [𝑘 − 1], ∀ 𝑗 ∈ [𝑒𝑖 + 1, 𝑒𝑖+1 − 1], 𝑞(cid:48)

𝑖 ⊆ 𝑠𝑒𝑖 , (𝑒𝑖 exists because 𝑘 ≤ 𝑘(cid:48))

anti-monotone and 𝑞𝑖 ⊆ 𝑞(cid:48)𝑖)

This means that (𝑒𝑖)𝑖∈ [𝑘 ] is an embedding of 𝒑 in 𝒔.

𝑗 (cid:42)𝐷 𝑠𝑒𝑖 , and thus 𝑞 𝑗 (cid:42)𝐷 𝑠𝑒𝑖 (because (cid:42)𝐷 is

Proof (Proof of Proposition 4 (Anti-monotonicity of the support of Constrained NSP)) The proof of this
property is similar to the proof of Proposition 3.

Let 𝒑 = (cid:104) 𝑝1 ¬𝑞1 𝑝2 ¬𝑞2 · · · 𝑝𝑘−1 ¬𝑞𝑘−1 𝑝𝑘 (cid:105) and 𝒑(cid:48) = (cid:104) 𝑝(cid:48)

𝑝(cid:48)
𝑘(cid:48) (cid:105)
be two NSP s.t. 𝒑 (cid:67) 𝒑(cid:48). And let 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛 (cid:105) be a sequence s.t. 𝒑(cid:48) (cid:22) 𝒔, i.e. there exists an embedding
(𝑒𝑖)𝑖∈ [𝑘(cid:48) ] :

2 · · · 𝑝(cid:48)

𝑝(cid:48)
2 ¬𝑞(cid:48)

1 ¬𝑞(cid:48)
1

¬𝑞(cid:48)

𝑘(cid:48)−1

𝑘(cid:48)−1

– ∀𝑖, 𝑒𝑖+1 > 𝑒𝑖 (embedding), 𝑒𝑖+1 − 𝑒𝑖 ≤ 𝜃 (maxgap) and 𝑒𝑘(cid:48) − 𝑒1 ≤ 𝜏 (maxspan),
– ∀𝑖, 𝑝(cid:48)
– ∀ 𝑗 ∈ [𝑒𝑖 + 1, 𝑒𝑖+1 − 1], 𝑞(cid:48)

𝑖 ⊆ 𝑠𝑒𝑖 ,

𝑖 (cid:42)𝐷 𝑠𝑒 𝑗

To prove that 𝒑 (cid:22) 𝒔, we prove that (𝑒𝑖)𝑖∈ [𝑘 ] is an embedding of 𝒑 in 𝒔.
Let us ﬁrst consider that 𝑘 = 𝑘(cid:48), then by deﬁnitions of (cid:67) and the embedding,

(i) ∀𝑖 ∈ [𝑘 ], 𝑝𝑖 ⊆ 𝑝(cid:48)
𝑖 ⊆ 𝑠𝑒𝑖 ,
(ii) ∀𝑖 ∈ [𝑘 − 1], ∀ 𝑗 ∈ [𝑒𝑖 + 1, 𝑒𝑒+1 − 1], 𝑞(cid:48)

𝑗 (cid:42)𝐷 𝑠𝑒𝑖 , and thus 𝑞 𝑗 (cid:42)𝐷 𝑠𝑒𝑖 (because of anti-

monotonicity of (cid:42)𝐷 and 𝑞𝑖 ⊆ 𝑞(cid:48)𝑖)
In addition, we know that maxgap and maxspan constraints are satisﬁed by the embedding, i.e.

(iv) ∀𝑖 ∈ [𝑘 ], 𝑒𝑖+1 − 𝑒𝑖 ≤ 𝜃
(v) 𝑒𝑘 − 𝑒1 = 𝑒𝑘(cid:48) − 𝑒1 ≤ 𝜏

Let us now consider that 𝑘(cid:48) > 𝑘, (𝑖), (𝑖𝑖) and (𝑖𝑖𝑖) still holds, and we know that if 𝑒𝑘 < 𝑒𝑘(cid:48)

(embedding property), then 𝑒𝑘 − 𝑒𝑖 < 𝜃.

This means that (𝑒𝑖)𝑖∈ [𝑘 ] is an embedding of 𝒑 in 𝒔 that satisﬁes gap constraints.

Proof (Proof of Proposition 5 (Complete and correct algorithm)) The correction of the algorithm is given
by lines 2-3 of Algorithm 1. A pattern is outputted only if it is frequent (line 2).

We now prove the completeness of the algorithm. First of all, we have to prove that any pattern can
be reached using a path of elementary transformations ((cid:32)∈ {(cid:32)𝑛, (cid:32)𝑠 , (cid:32)𝑐 }). Let 𝒑(cid:48) = (cid:104) 𝑝(cid:48)
𝑚 (cid:105) be
a pattern with a total amount of 𝑛 items, 𝑛 > 0, then it is possible to deﬁne 𝒑 such that 𝒑 (cid:32) 𝒑(cid:48) where
(cid:32)∈ {(cid:32)𝑛, (cid:32)𝑠 , (cid:32)𝑐 }, and 𝒑 will have exactly 𝑛 − 1 items:

1 · · · 𝑝(cid:48)

– if the last itemset of 𝒑(cid:48) is such that | 𝑝(cid:48)

1 · · · 𝑝(cid:48)
the same preﬁx as 𝒑(cid:48) and an additional itemset, 𝑝𝑚 such that | 𝑝𝑚 | = | 𝑝(cid:48)
𝒑 (cid:32)𝑐 𝒑(cid:48)

𝑚 | > 1 we deﬁne 𝒑 = (cid:104) 𝑝(cid:48)

𝑚−1
𝑚 | − 1 and 𝑝𝑚 ⊂ 𝑝(cid:48)

𝑝𝑚 (cid:105) as the pattern with
𝑚: then

NegPSpan: eﬃcient extraction of negative sequential patterns

39

– if the last itemset of 𝒑(cid:48) is such that | 𝑝(cid:48)

𝑚 | = 1 and 𝑝(cid:48)

𝑚−1

is positive then we deﬁne 𝒑 = (cid:104) 𝑝(cid:48)

1 · · · 𝑝(cid:48)

𝑚−2

– if the last itemset of 𝒑(cid:48) is such that | 𝑝(cid:48)

𝑚 | = 1 and 𝑝(cid:48)

1 · · · 𝑝𝑚−1 𝑝(cid:48)

𝑚 (cid:105) where 𝑝𝑚−1 is such that | 𝑝𝑚−1 | = | 𝑝(cid:48)

𝑚−1

is negative (non-empty) then we deﬁne
: then

𝑚−1 | − 1 and 𝑝𝑚−1 ⊂ 𝑝(cid:48)

𝑚−1

𝑚−1 (cid:105): then 𝒑 (cid:32)𝑠 𝒑(cid:48)
𝑝(cid:48)

𝒑 = (cid:104) 𝑝(cid:48)
𝒑 (cid:32)𝑛 𝒑(cid:48)

Applying this rules recursively, for any pattern 𝒑 there is a path from the empty sequence to 𝒑: ∅ (cid:32)∗ 𝒑.
Also, exactly one of the three extensions can be used at any step, meaning that these path is unique. This
prove that our algorithm is not redundant.

Second, the pruning strategy is correct, so, no frequent pattern will be missed. This comes from the

anti-monotonicity property.

Let 𝒑 and 𝒑(cid:48) be two patterns such that 𝒑 (cid:32) 𝒑(cid:48) where (cid:32)∈ {(cid:32)𝑛, (cid:32)𝑠 , (cid:32)𝑐 }, then is is quite obvious
that 𝒑 (cid:67) 𝒑(cid:48). Let us now consider that 𝒑 (cid:32)∗ 𝒑(cid:48) from 𝒑 to 𝒑(cid:48) then, by transitivity of (cid:67), we also have that
𝒑 (cid:67) 𝒑(cid:48). And then by anti-monotonicity of the support, we have that 𝑠𝑢 𝑝 𝑝 ( 𝒑) ≥ 𝑠𝑢 𝑝 𝑝 ( 𝒑(cid:48)).

Let us now proceed by absurd and consider that 𝒑(cid:48) is a pattern with support 𝑠𝑢 𝑝 𝑝 ( 𝒑(cid:48)) ≥ 𝜎 that was
not found by the algorithm. This means that for all paths7 ∅ (cid:32)∗ 𝒑(cid:48) there exists some pattern 𝒑 such that
∅ (cid:32)∗ 𝒑 (cid:32)∗ 𝒑(cid:48) with 𝑠𝑢 𝑝 𝑝 ( 𝒑) < 𝜎. 𝒑 is the pattern that has been used to prune the search for this path
to 𝒑(cid:48). This is not possible considering that 𝒑 (cid:32)∗ 𝒑(cid:48) and thus 𝑠𝑢 𝑝 𝑝 ( 𝒑) ≥ 𝑠𝑢 𝑝 𝑝 ( 𝒑(cid:48)) ≥ 𝜎.

B NegPSpan extracts a superset of eNSP

Proposition 6 Soft-embedding =⇒ strict-embedding for patterns consisting of items.

Proof Let 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛 (cid:105) be a sequence and 𝒑 = (cid:104) 𝑝1 · · · 𝑝𝑚 (cid:105) be a NSP s.t. | 𝑝𝑖 | = 1 for all 𝑖 ∈ [𝑛] and
𝒑 occurs in 𝒔 according to the soft-embedding semantic.

There exists 𝜖 = (𝑒𝑖)𝑖∈ [𝑚] ∈ [𝑛]𝑚 s.t. for all 𝑖 ∈ [𝑛], 𝑝𝑖 is positive implies 𝑝𝑖 ∈ 𝑠𝑒𝑖 and 𝑝𝑖 is
negative implies that for all 𝑗 ∈ [𝑒𝑖−1 +1, 𝑒𝑖+1 −1], 𝑝𝑖 ∉ 𝑠 𝑗 (items only) then 𝑝𝑖 ∉ (cid:208) 𝑗∈ [𝑒𝑖−1+1,𝑒𝑖+1−1] 𝑠 𝑗
i.e. 𝑝𝑖 (cid:42)∗ (cid:208) 𝑗∈ [𝑒𝑖−1+1,𝑒𝑖+1−1] 𝑠 𝑗 (whatever (cid:42)𝐺 or (cid:42)𝐷). As a consequence 𝜖 is a strict-embedding of 𝑝.
Proposition 7 Let D be a dataset containing sequences made of items and 𝒑 = (cid:104) 𝑝1 · · · 𝑝𝑚 (cid:105) be a
sequential pattern extracted by eNSP. Then, without embedding constraints 𝒑 is extracted by NegPSpan
with the same minimum support.

Proof If 𝒑 is extracted by eNSP, then its positive partner is frequent in the dataset D. As a consequence,
each 𝑝𝑖, 𝑖 ∈ [𝑚] is a singleton itemset.

According to the search space of NegPSpan deﬁned by (cid:67) if 𝒑 is frequent then it will be reached
by the depth-ﬁrst search. Then it is suﬃcient to prove that for any sequence 𝒔 = (cid:104)𝑠1 · · · 𝑠𝑛 (cid:105) ∈ D such
that 𝒑 occurs in 𝒔 according to eNSP semantics (strict-embedding, strong absence), then 𝒑 also occurs in
𝒔 according to the NegPSpan semantics (soft-embedding, weak absence). Consequently, considering the
same minimum support threshold, 𝒑 is frequent according to NegPSpan. Proposition 6 gives this result.

Then we conclude that NegPSpan extracts more patterns than eNSP on sequences of items. In fact,

NegPSpan can extract patterns with negative itemsets larger than 2.

eNSP extracts patterns that are not extracted by NegPSpan on sequences of itemsets. Practically,
NegPSpan uses a size limit for negative itemsets 𝜈 ≥ 1. eNSP extracts patterns whose positive partners are
frequent. The positive partner, extracted by PreﬁxSpan may hold itemsets larger than 𝜈, and if the pattern
with negated itemset is also frequent, then this pattern is extracted by eNSP, but not by NegPSpan.

C Additional experiments

C.1 Inﬂuence of vocabulary size

Figure 6 shows computation time and memory consumption with respect to vocabulary size: eNSP is run
with diﬀerent values of 𝜍 , the minimal frequency of the positive partner of negative patterns (100%, 80%

7 Note that we proved that this path is actually unique.

40

Thomas Guyet, René Quiniou

Fig. 6 Comparison of eNSP and NegPSpan computation time (left) and memory consumption (right) wrt
vocabulary size. The dashed line shows the limit for timeout executions.

and 20% of the minimal frequency threshold) and NegPSpan is run with a maxgap of 10 or without. The
timeout is set to 5 minutes.

Similarly to experiments of the previous section, with no constraint, NegPSpan is less time-eﬃcient
than eNSP but it becomes more time-eﬃcient with a gap constraint whatever the vocabulary size. Memory
consumption curves show that NegPSpan requires signiﬁcantly less memory than eNSP.

Figure 6 clearly shows that the smaller the vocabulary is, the more frequent patterns there are, and
thus the more memory is required and time is high. Indeed, the smaller vocabulary (with a given sequence
length), the higher the probability to extract some sequential pattern. This is the case for positive patterns
as well as for negative patterns considering that the positive part of the negative pattern may more likely
occur in a raw (and may necessarily satisfy the negative constraints). Then, generated datasets have more
positive and negative patterns to extract.

More especially, there are more positive patterns to extract and thus the memory required by eNSP
increases because it requires to store all positive patterns (with a support above 𝜍 ). We can see that when the
vocabulary size decreases, the memory required by eNSP increases very quickly (faster than exponential
growth), while NegPSpan requires almost the same amount of memory for any vocabulary size.

The use of a maxgap constraint (𝜏 = 10) makes NegPSpan several orders of magnitude more time-
eﬃcient for small vocabulary size. With very small vocabulary size (< 20), the number of negative patterns
extracted by NegPSpan explodes and the execution time exceeds the timeout of 5 minutes (300𝑠). For
greater vocabulary size, the diﬀerences between algorithms disappear. NegPSpan (𝜏 = ∞) is more eﬃcient
than eNSP for big vocabulary size because only few frequent negative patterns should be extracted, but there
are many positive patterns: in this case eNSP has to evaluate the support of potential negative sequential
patterns on the basis of the positive patterns while NegPSpan stops the exploration as soon as an unfrequent
negative pattern is found. Thus, eNSP with 𝜍 = 0.8𝜎, which explores more positive patterns than eNSP
with 𝜍 = 𝜎, is less time-eﬃcient.

C.2 Inﬂuence of average sequence length

Figure 7 shows the computation time and memory consumption with respect to average length of sequences
with a minimal support 𝜎 = 20%. eNSP is run with diﬀerent values for 𝜍 , the minimal frequency of the
positive partner of negative patterns (100%, 80% and 20% of the minimal frequency threshold) and
NegPSpan is run with a maxgap 𝜏 = 10 or without maxgap constraint. The timeout is set to 5 𝑚𝑖𝑛.

Computation times and memory consumptions are exponential with respect to the average sequence
length. Curves diﬀer by their factors of exponential growth. In the remainder of this section 𝛼 represents
the exponential growth.

Figure 7 on the left compares the computation times. The exponential growth of NegPSpan without
maxgap (𝛼 ≈ 10−11) is high and the timeout is reached for datasets with an average sequence length of about
30 itemsets. eNSP is one order of magnitude more time-eﬃcient and can analyze dataset with an average
sequence length about 45 itemsets. But, parameter 𝜍 does not changes the computation time signiﬁcantly.
Indeed, the exponential growths are close to each other (𝛼𝜍 =𝜎 ≈ 5.08 × 10−12, 𝛼𝜍 =0.8𝜎 ≈ 2.42 × 10−12,
𝛼𝜍 =0.5𝜎 ≈ 2.72 × 10−12). In contrast, the use of the maxgap constraint (𝜏 = 10) changes signiﬁcantly

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll11004080120160Vocabulary sizeTime (s)●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●1e+041e+054080120160Vocabulary sizeMemory (bytes)lNegPSpan ( t = ¥ )NegPSpan ( t =10 )eNSP ( V = 0.8 s )eNSP ( V = s )NegPSpan: eﬃcient extraction of negative sequential patterns

41

Fig. 7 Comparison of computing time (left) and memory consumption (right) between eNSP and Neg-
PSpan wrt average length of sequences. The dashed line shows the limit for timeout executions.

the diﬃculty of the task: the exponential growth is signiﬁcantly lower (𝛼 ≈ 10−53) and the timeout is
not reached even for sequence containing about 120 itemsets. This result was expected considering that
the maxgap constraint avoids to explore the full sequence to evaluate the support of the pattern. Using
NegPSpan with maxgap constraints is thus very time-eﬃcient to mine negative patterns in long sequences.
Figure 7 on the right compares the memory consumption of the two algorithms. The curves look very
similar to the computation time curves, but it is important to note that the memory consumption requirement
increases signiﬁcantly slower with NegPSpan than with eNSP, whatever the use of maxgap constraint. The
explanation is similar to previous benchmark results: the depth-ﬁrst search strategy does not store patterns
while eNSP does to evaluate the support of negative patterns.

C.3 Computational performances on case study datasets

This appendix presents the computational performances of eNSP and NegPSpan on two datasets of the
case studies (see Section 6): instacart and care pathway analysis.

C.3.1 Instacart data

Figure 8 shows the computation times, the memory requirements and the number of NSPs extracted by
both algorithms on the Instacart dataset (see Section 6.1). On this dataset, we can ﬁrst note that the
number of patterns extracted by NegPSpan is about two orders of magnitude larger. As a consequence, the
computation time is higher even with strong gap constraints: NegPSpan takes about 1, 000𝑠 with 𝜏 = 2
while eNSP takes always less than 500𝑠 to extract 1% NSPs (whatever 𝜍 ). These results can be explained
by the dataset features. With a large vocabulary the support of patterns decreases rapidly when the pattern
length increases. This means that eNSP prunes a lot of patterns while extracting the positive partners. This
explains that few patterns are extracted. In contrast with this fast pruning strategy, NegPSpan explores lots
of potential negative extensions because most of them could be frequent due to the relative low frequency
of each item. eNSP seems more eﬃcient on this dataset, but we recall that it failed to explore large datasets,
not for time reason, but for heavy memory requirements. Figure 8 middle, shows this limitation well: the
memory requirement is several orders larger for eNSP than for NegPSpan and it increases exponentially
when 𝜍 decreases.

C.3.2 Care pathway analysis

Figure 9 gives a comparison of the computation performances (time and memory usage) between eNSP and
NegPSpan with respect to the minimal frequency threshold (𝜎 ∈ [0.08, 0.3]) on the care pathway dataset
(see Section 6.2). Each algorithm is run with diﬀerent settings: the maxgap constraint 𝜏 ∈ {3, 5, 8} for
NegPSpan and the minimal support of positive partners 𝜍 ∈ {0.4𝜎, 0.8𝜎, 𝜎 } for eNSP. The maximal
pattern length is set to 𝑙 = 5. The results obtained on real data conﬁrm the results obtained in Section 5 on

lllllllllllllllllllllllllllllllllllllll1100255075100125Sequence lengthTime (s)●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●1e+041e+051e+06255075100125Sequence lengthMemory (bytes)lNegPSpan ( t = ¥ )NegPSpan ( t =10 )eNSP ( V =0.5 s )eNSP ( V =0.8 s )eNSP ( V = s )42

Thomas Guyet, René Quiniou

Fig. 8 Computation times (on top), memory requirements (in the middle) and numbers of NSP (at the
bottom) extracted from the Instacart dataset with respect to the gap constraint 𝜏 for NegPSpan (on the left)
and with respect to frequent positive ratio ( 𝜍 ) for eNSP (on the right).

Fig. 9 Time, memory usage and number of extracted patterns for NSP and NegPSpan with respect to the
minimal frequency support, and diﬀerent algorithm settings. (care pathways dataset, see Section 6.2)

synthetic data. On the one hand, NegPSpan requires several orders of magnitude less memory than eNSP.
eNSP does not terminate with lowest 𝜎 and 𝜍 values. Its memory requirement exceeds the computer
capacity (8Go). On the other hand, this heavy memory requirement has consequences on computation
times and NegPSpan is several orders of magnitude more time-eﬃcient than eNSP whatever the setting.
We observe that the computation time increases exponentially when the frequency threshold (𝜎) decreases,
and, the lower maxgap, the lower the computation time. This is mainly due to the number of extracted
patterns that grows also exponentially when the frequency threshold decreases.

llll1e+031e+041e+052468tTime (s)llll10030010000.40.50.60.7VTime (s)●●●●200002468τMemory (bytes)●●●●1e+053e+051e+060.40.50.60.7ςMemory (bytes)llll1e+031e+041e+051e+061e+072468t#Patternsllll3e+031e+043e+041e+050.40.50.60.7V#Patternsls =0.5%s =1%lllll1010010000.150.200.250.30Frequency ThresholdTime (s)lllll1e+051e+061e+070.150.200.250.30Frequency ThresholdMemory (bytes)NegPSpaneNSPlt =3t =5t =8V = 0.4 sV = 0.8 sV = sNegPSpan: eﬃcient extraction of negative sequential patterns

43

Table 9 NSP extracted by NegPSpan but not by eNSP

Pattern

support
NegPSpan

(cid:104)6𝐷𝐼 𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 3𝑆𝐸 𝑅) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 2𝑉 𝐼 𝐸 , 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝑅𝐸𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 3𝑆𝐸 𝑅, 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)7𝐴𝐶𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(3𝑆𝐸 𝑅, 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(2𝑉 𝐼 𝐸 , 3𝑆𝐸 𝑅) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(2𝑉 𝐼 𝐸 , 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 2𝑉 𝐼 𝐸) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 4𝐹 𝐴𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 8𝐷𝑂𝑆) ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 8𝐷𝑂𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)(2𝑉 𝐼 𝐸 , 8𝐷𝑂𝑆) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 3𝑆𝐸 𝑅, 8𝐷𝑂𝑆) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 3𝑆𝐸 𝑅), ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝐶𝑇 𝑅 2𝑉 𝐼 𝐸 8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)7𝐴𝐶𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(3𝑆𝐸 𝑅, 8𝐷𝑂𝑆) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 2𝑉 𝐼 𝐸) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)

5,352
2,268
1,946
5,265
1,893
1,947
3,060
2,058
3,106
2,294
2,123
6,597
2,048
3,029
1,900
2,258
1,912
1,901
3,032
2,266

D List of extracted patterns for the CRM dataset

This Appendix provides the complete list of negative sequential patterns involving 3SER or 2VIE negative
items extracted by eNSP or NegPSpan.

44

Thomas Guyet, René Quiniou

Table 10 NSP extracted by eNSP but not by NegPSpan

Pattern

(cid:104)8𝐷𝑂𝑆 ¬(2𝑉 𝐼 𝐸 , 3𝑆𝐸 𝑅) 1𝑅𝐸𝑆 (cid:105)
(cid:104)5𝑅𝐸𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸 𝑆 (cid:105)
(cid:104)5𝑅𝐸𝐶 5𝑅𝐸𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸(cid:48) (cid:48)3𝑆𝐸 𝑅 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 5𝑅𝐸𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)3𝑆𝐸 𝑅 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 2𝑉 𝐼 𝐸 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸 𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 4𝐹 𝐴𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝐶𝑇 𝑅 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 2𝑉 𝐼 𝐸 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)5𝑅𝐸𝐶 5𝑅𝐸𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸 𝑆 (cid:105)
(cid:104)5𝑅𝐸𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)6𝐷𝐼 𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝑅𝐸𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 8𝐷𝑂𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝐶𝑇 𝑅 ¬2𝑉 𝐼 𝐸 1𝑅𝐸 𝑆 (cid:105)
(cid:104)3𝑆𝐸 𝑅 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 8𝐷𝑂𝑆) ¬2𝑉 𝐼 𝐸 1𝑅𝐸 𝑆 (cid:105)

Table 11 NSP extracted by both eNSP and NegPSpan

Pattern

(cid:104)3𝑆𝐸 𝑅 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝑅𝐸𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 8𝐷𝑂𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)5𝑅𝐸𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 ¬3𝑆𝐸 𝑅 1𝑅𝐸 𝑆 (cid:105)
(cid:104)2𝑉 𝐼 𝐸 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 8𝐷𝑂𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(2𝑉 𝐼 𝐸 , 3𝑆𝐸 𝑅) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 4𝐹 𝐴𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)4𝐹 𝐴𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝐶𝑇 𝑅 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)5𝑅𝐸𝐶 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)6𝐷𝐼 𝑆 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)8𝐷𝑂𝑆 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)
(cid:104)3𝑆𝐸 𝑅 ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)(1𝐶𝑇 𝑅, 8𝐷𝑂𝑆) ¬2𝑉 𝐼 𝐸 1𝑅𝐸𝑆 (cid:105)
(cid:104)1𝐶𝑇 𝑅 ¬3𝑆𝐸 𝑅 1𝑅𝐸𝑆 (cid:105)

