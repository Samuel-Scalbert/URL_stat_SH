Sliced-Wasserstein Flows: Nonparametric Generative
Modeling via Optimal Transport and Diffusions
Antoine Liutkus, Umut Ş Imşekli, Szymon Majewski, Alain Durmus,

Fabian-Robert Stöter

To cite this version:

Antoine Liutkus, Umut Ş Imşekli, Szymon Majewski, Alain Durmus, Fabian-Robert Stöter. Sliced-
Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions. ICML
2019 - 36th International Conference on Machine Learning, Jun 2019, Long Beach, United States.
pp.4104-4113. ￿hal-02191302￿

HAL Id: hal-02191302

https://inria.hal.science/hal-02191302

Submitted on 23 Jul 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal
Transport and Diffusions

Antoine Liutkus 1 Umut S¸ ims¸ekli 2 Szymon Majewski 3 Alain Durmus 4 Fabian-Robert St¨oter 1

Abstract

By building upon the recent theory that estab-
lished the connection between implicit generative
modeling (IGM) and optimal transport, in this
study, we propose a novel parameter-free algo-
rithm for learning the underlying distributions of
complicated datasets and sampling from them.
The proposed algorithm is based on a functional
optimization problem, which aims at ﬁnding a
measure that is close to the data distribution as
much as possible and also expressive enough for
generative modeling purposes. We formulate the
problem as a gradient ﬂow in the space of proba-
bility measures. The connections between gradi-
ent ﬂows and stochastic differential equations let
us develop a computationally efﬁcient algorithm
for solving the optimization problem. We provide
formal theoretical analysis where we prove ﬁnite-
time error guarantees for the proposed algorithm.
To the best of our knowledge, the proposed algo-
rithm is the ﬁrst nonparametric IGM algorithm
with explicit theoretical guarantees. Our experi-
mental results support our theory and show that
our algorithm is able to successfully capture the
structure of different types of data distributions.

1. Introduction

Implicit generative modeling (IGM) (Diggle & Gratton,
1984; Mohamed & Lakshminarayanan, 2016) has become
very popular recently and has proven successful in various
ﬁelds; variational auto-encoders (VAE) (Kingma & Welling,

1Inria and LIRMM, Univ.

of Montpellier, France
2LTCI, T´el´ecom Paristech, Universit´e Paris-Saclay, Paris,
3Institute of Mathematics, Polish Academy of Sci-
France
ences, Warsaw, Poland 4CNRS, ENS Paris-Saclay,Universit
Paris-Saclay, Cachan, France.
Correspondence to: An-
toine Liutkus <antoine.liutkus@inria.fr>, Umut S¸ ims¸ekli
<umut.simsekli@telecom-paristech.fr>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

2013) and generative adversarial networks (GAN) (Goodfel-
low et al., 2014) being its two well-known examples. The
goal in IGM can be brieﬂy described as learning the un-
derlying probability measure of a given dataset, denoted as
ν ∈ P(Ω), where P is the space of probability measures on
the measurable space (Ω, A), Ω ⊂ Rd is a domain and A is
the associated Borel σ-ﬁeld.

Given a set of data points {y1, . . . , yP } that are assumed to
be independent and identically distributed (i.i.d.) samples
drawn from ν, the implicit generative framework models
them as the output of a measurable map, i.e. y = T (x), with
T : Ωµ (cid:55)→ Ω. Here, the inputs x are generated from a known
and easy to sample source measure µ on Ωµ (e.g. Gaussian
or uniform measures), and the outputs T (x) should match
the unknown target measure ν on Ω.

Learning generative networks have witnessed several
groundbreaking contributions in recent years. Motivated
by this fact, there has been an interest in illuminating the
theoretical foundations of VAEs and GANs (Bousquet et al.,
2017; Liu et al., 2017). It has been shown that these implicit
models have close connections with the theory of Optimal
Transport (OT) (Villani, 2008). As it turns out, OT brings
new light on the generative modeling problem: there have
been several extensions of VAEs (Tolstikhin et al., 2017;
Kolouri et al., 2018) and GANs (Arjovsky et al., 2017; Gul-
rajani et al., 2017; Guo et al., 2017; Lei et al., 2017), which
exploit the links between OT and IGM.

OT studies whether it is possible to transform samples from
a source distribution µ to a target distribution ν. From this
perspective, an ideal generative model is simply a transport
map from µ to ν. This can be written by using some ‘push-
forward operators’: we seek a mapping T that ‘pushes µ
onto ν’, and is formally deﬁned as ν(A) = µ(T −1(A)) for
all Borel sets A ⊂ A. If this relation holds, we denote the
push-forward operator T#, such that T#µ = ν. Provided
mild conditions on these distributions hold (notably µ is non-
atomic (Villani, 2008)), existence of such a transport map is
guaranteed; however, it remains a challenge to construct it
in practice.

One common point between VAE and GAN is to adopt
an approximate strategy and consider transport maps that

Sliced-Wasserstein Flows

belong to a parametric family Tφ with φ ∈ Φ. Then,
they aim at ﬁnding the best parameter φ(cid:63) that would give
Tφ(cid:63)#µ ≈ ν. This is typically achieved by attempting
to minimize the following optimization problem: φ(cid:63) =
arg minφ∈Φ W2(Tφ#µ, ν), where W2 denotes the Wasser-
stein distance that will be properly deﬁned in Section 2. It
has been shown that (Genevay et al., 2017) OT-based GANs
(Arjovsky et al., 2017) and VAEs (Tolstikhin et al., 2017)
both use this formulation with different parameterizations
and different equivalent deﬁnitions of W2. However, their
resulting algorithms still lack theoretical understanding.

In this study, we follow a completely different approach
for IGM, where we aim at developing an algorithm with
explicit theoretical guarantees for estimating a transport
map between source µ and target ν. The generated transport
map will be nonparametric (in the sense that it does not
belong to some family of functions, like a neural network),
and it will be iteratively augmented: always increasing the
quality of the ﬁt along iterations. Formally, we take Tt as
the constructed transport map at time t ∈ [0, ∞), and deﬁne
µt = Tt#µ as the corresponding output distribution. Our
objective is to build the maps so that µt will converge to
the solution of a functional optimization problem, deﬁned
through a gradient ﬂow in the Wasserstein space. Informally,
we will consider a gradient ﬂow that has the following form:

∂tµt = −∇W2

(cid:110)

Cost(µt, ν) + Reg(µt)

(cid:111)

, µ0 = µ, (1)

where the functional Cost computes a discrepancy between
µt and ν, Reg denotes a regularization functional, and ∇W2
denotes a notion of gradient with respect to a probability
measure in the W2 metric for probability measures1. If this
ﬂow can be simulated, one would hope for µt = (Tt)#µ
to converge to the minimum of the functional optimization
problem: minµ(Cost(µ, ν) + Reg(µ)) (Ambrosio et al.,
2008; Santambrogio, 2017).

We construct a gradient ﬂow where we choose the Cost
functional as the sliced Wasserstein distance (SW2) (Rabin
et al., 2012; Bonneel et al., 2015) and the Reg functional as
the negative entropy. The SW2 distance is equivalent to the
W2 distance (Bonnotte, 2013) and has important computa-
tional implications since it can be expressed as an average
of (one-dimensional) projected optimal transportation costs
whose analytical expressions are available.

We ﬁrst show that, with the choice of SW2 and the negative-
entropy functionals as the overall objective, we obtain a
valid gradient ﬂow that has a solution path (µt)t, and the
probability density functions of this path solve a particular

1This gradient ﬂow is similar to the usual Euclidean gradient
ﬂows, i.e. ∂txt = −∇(f (xt) + r(xt)), where f is typically the
data-dependent cost function and r is a regularization term. The
(explicit) Euler discretization of this ﬂow results in the well-known
gradient descent algorithm for solving minx(f (x) + r(x)).

partial differential equation, which has close connections
with stochastic differential equations. Even though gradient
ﬂows in Wasserstein spaces cannot be solved in general,
by exploiting this connection, we are able to develop a
practical algorithm that provides approximate solutions to
the gradient ﬂow and is algorithmically similar to stochastic
gradient Markov Chain Monte Carlo (MCMC) methods2
(Welling & Teh, 2011; Ma et al., 2015; Durmus et al., 2016;
S¸ ims¸ekli, 2017; S¸ ims¸ekli et al., 2018). We provide ﬁnite-
time error guarantees for the proposed algorithm and show
explicit dependence of the error to the algorithm parameters.

To the best of our knowledge, the proposed algorithm is the
ﬁrst nonparametric IGM algorithm that has explicit theoreti-
cal guarantees. In addition to its nice theoretical properties,
the proposed algorithm has also signiﬁcant practical impor-
tance: it has low computational requirements and can be
easily run on an everyday laptop CPU.Our experiments on
both synthetic and real datasets support our theory and illus-
trate the advantages of the algorithm in several scenarios.

2. Technical Background

2.1. Wasserstein distance, optimal transport maps and

Kantorovich potentials

For two probability measures µ, ν ∈ P2(Ω), P2(Ω) =
{µ ∈ P(Ω) : (cid:82)
Ω (cid:107)x(cid:107)2 µ(dx) < +∞}, the 2-Wasserstein
distance is deﬁned as follows:

W2(µ, ν) (cid:44)

(cid:110)

inf
γ∈C(µ,ν)

(cid:90)

Ω×Ω

(cid:107)x − y(cid:107)2γ(dx, dy)

(cid:111)1/2

,

(2)

where C(µ, ν) is called the set of transportation plans and
deﬁned as the set of probability measures γ on Ω×Ω satisfy-
ing for all A ∈ A, γ(A×Ω) = µ(A) and γ(Ω×A) = ν(A),
i.e. the marginals of γ coincide with µ and ν. From now on,
we will assume that Ω is a compact subset of Rd.

In the case where Ω is ﬁnite, computing the Wasserstein
distance between two probability measures turns out to be a
linear program with linear constraints, and has therefore a
dual formulation. Since Ω is a Polish space (i.e. a complete
and separable metric space), this dual formulation can be
generalized as follows (Villani, 2008)[Theorem 5.10]:

W2(µ, ν) = sup

ψ∈L1(µ)

(cid:110)(cid:90)

Ω

ψ(x)µ(dx) +

ψc(x)ν(dx)

(cid:111)1/2

(cid:90)

Ω

(3)

where L1(µ) denotes the class of functions that are abso-
lutely integrable under µ and ψc denotes the c-conjugate
of ψ and is deﬁned as follows: ψc(y) (cid:44) {inf x∈Ω (cid:107)x −

2We note that, despite the algorithmic similarities, the proposed

algorithm is not a Bayesian posterior sampling algorithm.

Sliced-Wasserstein Flows

y(cid:107)2 − ψ(x)}. The functions ψ that realize the supremum
in (3) are called the Kantorovich potentials between µ and
ν. Provided that µ satisﬁes a mild condition, we have the
following uniqueness result.

Theorem 1 ((Santambrogio, 2014)[Theorem 1.4]). Assume
that µ ∈ P2(Ω) is absolutely continuous with respect to
the Lebesgue measure. Then, there exists a unique optimal
transport plan γ(cid:63) that realizes the inﬁmum in (2) and it
is of the form (Id × T )#µ, for a measurable function T :
Ω → Ω. Furthermore, there exists at least a Kantorovich
potential ψ whose gradient ∇ψ is uniquely determined µ-
almost everywhere. The function T and the potential ψ are
linked by T (x) = x − ∇ψ(x).

The measurable function T : Ω → Ω is referred to as the
optimal transport map from µ to ν. This result implies that
there exists a solution for transporting samples from µ to
samples from ν and this solution is optimal in the sense that
it minimizes the (cid:96)2 displacement. However, identifying this
solution is highly non-trivial. In the discrete case, effective
solutions have been proposed (Cuturi, 2013). However,
for continuous and high-dimensional probability measures,
constructing an actual transport plan remains a challenge.
Even if recent contributions (Genevay et al., 2016) have
made it possible to rapidly compute W2, they do so without
constructing the optimal map T , which is our objective here.

2.2. Wasserstein spaces and gradient ﬂows

By (Ambrosio et al., 2008)[Proposition 7.1.5], W2 is a dis-
tance over P(Ω). In addition, if Ω ⊂ Rd is compact, the
topology associated with W2 is equivalent to the weak con-
vergence of probability measures and (P(Ω), W2)3 is com-
pact. The metric space (P2(Ω), W2) is called the Wasser-
stein space.

In this study, we are interested in functional optimiza-
tion problems in (P2(Ω), W2), such as minµ∈P2(Ω) F(µ),
where F is the functional that we would like to minimize.
Similar to Euclidean spaces, one way to formulate this opti-
mization problem is to construct a gradient ﬂow of the form
∂tµt = −∇W2 F(µt) (Benamou & Brenier, 2000; Lavenant
et al., 2018), where ∇W2 denotes a notion of gradient in
(P2(Ω), W2). If such a ﬂow can be constructed, one can uti-
lize it both for practical algorithms and theoretical analysis.

Gradient ﬂows ∂tµt = ∇W2 F(µt) with respect to a func-
tional F in (P2(Ω), W2) have strong connections with par-
tial differential equations (PDE) that are of the form of a con-
tinuity equation (Santambrogio, 2017). Indeed, it is shown
than under appropriate conditions on F (see e.g.(Ambrosio
et al., 2008)), (µt)t is a solution of the gradient ﬂow if and
only if it admits a density ρt with respect to the Lebesgue
measure for all t ≥ 0, and solves the continuity equation

3Note that in that case, P2(Ω) = P(Ω)

given by: ∂tρt + div(vρt) = 0, where v denotes a vector
ﬁeld and div denotes the divergence operator. Then, for a
given gradient ﬂow in (P2(Ω), W2), we are interested in the
evolution of the densities ρt, i.e. the PDEs which they solve.
Such PDEs are of our particular interest since they have a
key role for building practical algorithms.

2.3. Sliced-Wasserstein distance

ν

ν

µ , F −1

µ (τ ) − F −1

i.e. µ, ν ∈ P2(R), W2
In the one-dimensional case,
has an analytical form, given as follows: W2(µ, ν) =
(cid:82) 1
0 |F −1
(τ )|2 dτ , where Fµ and Fν denote the
cumulative distribution functions (CDF) of µ and ν, respec-
tively, and F −1
denote the inverse CDFs, also called
quantile functions (QF). In this case, the optimal transport
map from µ to ν has a closed-form formula as well, given
as follows: T (x) = (F −1
◦ Fµ)(x) (Villani, 2008). The
optimal map T is also known as the increasing arrangement,
which maps each quantile of µ to the same quantile of ν,
e.g. minimum to minimum, median to median, maximum to
maximum (Villani, 2008). Due to Theorem 1, the derivative
of the corresponding Kantorovich potential is given as:

ν

ψ(cid:48)(x) (cid:44) ∂xψ(x) = x − (F −1

ν

◦ Fµ)(x).

In the multidimensional case d > 1, building a transport
map is much more difﬁcult. The nice properties of the
one-dimensional Wasserstein distance motivate the usage
of sliced-Wasserstein distance (SW2) for practical appli-
cations. Before formally deﬁning SW2, let us ﬁrst deﬁne
the orthogonal projection θ∗(x) (cid:44) (cid:104)θ, x(cid:105) for any direction
θ ∈ Sd−1 and x ∈ Rd, where (cid:104)·, ·(cid:105) denotes the Euclidean
inner-product and Sd−1 ⊂ Rd denotes the d-dimensional
unit sphere. Then, the SW2 distance is formally deﬁned as
follows:

SW2(µ, ν) (cid:44)

(cid:90)

Sd−1

W2(θ∗

#µ, θ∗

#ν) dθ,

(4)

where dθ represents the uniform probability measure on
Sd−1. As shown in (Bonnotte, 2013), SW2 is indeed a
distance metric and induces the same topology as W2 for
compact domains.

#µ, θ∗

#µ and θ∗

The SW2 distance has important practical implications:
provided that the projected distributions θ∗
#ν
can be computed, then for any θ ∈ Sd−1, the distance
W2(θ∗
#ν), as well as its optimal transport map and
the corresponding Kantorovich potential can be analyt-
ically computed (since the projected measures are one-
dimensional). Therefore, one can easily approximate (4)
by using a simple Monte Carlo scheme that draws uniform
random samples from Sd−1 and replaces the integral in (4)
with a ﬁnite-sample average. Thanks to its computational
beneﬁts, SW2 was very recently considered for OT-based
VAEs and GANs (Deshpande et al., 2018; Wu et al., 2018;

Sliced-Wasserstein Flows

Kolouri et al., 2018), appearing as a stable alternative to the
adversarial methods.

3. Regularized Sliced-Wasserstein Flows for

Generative Modeling

3.1. Construction of the gradient ﬂow

In this paper, we propose the following functional minimiza-
tion problem on P2(Ω) for implicit generative modeling:

(cid:110)

λ (µ) (cid:44) 1
F ν
2

min
µ

SW 2

2 (µ, ν) + λH(µ)

(cid:111)
,

(5)

where λ > 0 is a regularization parameter and H denotes the
negative entropy deﬁned by H(µ) (cid:44) (cid:82)
Ω ρ(x) log ρ(x)dx if
µ has density ρ with respect to the Lebesgue measure and
H(µ) = +∞ otherwise. Note that the case λ = 0 has
been already proposed and studied in (Bonnotte, 2013) in
a more general OT context. Here, in order to introduce the
necessary noise inherent to generative model, we suggest
to penalize the slice-Wasserstein distance using H. In other
words, the main idea is to ﬁnd a measure µ(cid:63) that is close
to ν as much as possible and also has a certain amount of
entropy to make sure that it is sufﬁciently expressive for gen-
erative modeling purposes. The importance of the entropy
regularization becomes prominent in practical applications
where we have ﬁnitely many data samples that are assumed
to be drawn from ν. In such a circumstance, the regular-
ization would prevent µ(cid:63) to collapse on the data points and
therefore avoid ‘over-ﬁtting’ to the data distribution. Note
that this regularization is fundamentally different from the
one used in Sinkhorn distances (Genevay et al., 2018).

In our ﬁrst result, we show that there exists a ﬂow (µt)t≥0 in
(P(B(0, r)), W2) which decreases along F ν
λ , where B(0, a)
denotes the closed unit ball centered at 0 and radius a. This
ﬂow will be referred to as a generalized minimizing move-
ment scheme (see Deﬁnition 1 in the supplementary docu-
ment). In addition, the ﬂow (µt)t≥0 admits a density ρt with
respect to the Lebesgue measure for all t > 0 and (ρt)t≥0
is solution of a non-linear PDE (in the weak sense).

√

Theorem 2. Let ν be a probability measure on B(0, 1) with
a strictly positive smooth density. Choose a regularization
d, where d is the data
constant λ > 0 and radius r >
dimension. Assume that µ0 ∈ P(B(0, r)) is absolutely con-
tinuous with respect to the Lebesgue measure with density
ρ0 ∈ L∞(B(0, r)). There exists a generalized minimizing
movement scheme (µt)t≥0 associated to (5) and if ρt stands
for the density of µt for all t ≥ 0, then (ρt)t satisﬁes the
following continuity equation:

in a weak sense. Here, ∆ denotes the Laplacian opera-
tor, div the divergence operator, and ψt,θ denotes the Kan-
torovich potential between θ∗

#µt and θ∗

#ν.

The precise statement of this Theorem, related results and
its proof are postponed to the supplementary document.
For its proof, we use the technique introduced in (Jordan
et al., 1998): we ﬁrst prove the existence of a generalized
minimizing movement scheme by showing that the solution
curve (µt)t is a limit of the solution of a time-discretized
problem. Then we prove that the curve (ρt)t solves the PDE
given in (6).

3.2. Connection with stochastic differential equations

As a consequence of the entropy regularization, we obtain
the Laplacian operator ∆ in the PDE given in (6). We there-
fore observe that the overall PDE is a Fokker-Planck-type
equation (Bogachev et al., 2015) that has a well-known prob-
abilistic counterpart, which can be expressed as a stochastic
differential equation (SDE). More precisely, let us consider
a stochastic process (Xt)t, that is the solution of the follow-
ing SDE starting at X0 ∼ µ0:

dXt = v(Xt, µt)dt +

√

2λdWt,

(8)

where (Wt)t denotes a standard Brownian motion. Then,
the probability distribution of Xt at time t solves the PDE
given in (6) (Bogachev et al., 2015). This informally means
that, if we could simulate (8), then the distribution of Xt
would converge to the solution of (5), therefore, we could
use the sample paths (Xt)t as samples drawn from (µt)t.
However, in practice this is not possible due to two reasons:
(i) the drift vt cannot be computed analytically since it
depends on the probability distribution of Xt, (ii) the SDE
(8) is a continuous-time process, it needs to be discretized.

We now focus on the ﬁrst issue. We observe that the SDE
(8) is similar to McKean-Vlasov SDEs (Veretennikov, 2006;
Mishura & Veretennikov, 2016), a family of SDEs whose
drift depends on the distribution of Xt. By using this connec-
tion, we can borrow tools from the relevant SDE literature
(Malrieu, 2003; Cattiaux et al., 2008) for developing an
approximate simulation method for (8).

Our approach is based on deﬁning a particle system that
serves as an approximation to the original SDE (8). The
particle system can be written as a collection of SDEs, given
as follows (Bossy & Talay, 1997):

dX i

t = v(X i

t , µN

t )dt +

√

2λdW i
t ,

i = 1, . . . , N,

(9)

∂ρt
∂t

= − div(vtρt) + λ∆ρt,

(cid:90)

Sd−1

ψ(cid:48)

t,θ((cid:104)x, θ(cid:105))θdθ

vt(x) (cid:44) v(x, µt) = −

(6)

(7)

where i denotes the particle index, N ∈ N+ denotes the
total number of particles, and µN
j=1 δX j
de-
notes the empirical distribution of the particles {X j
t }N
j=1.
This particle system is particularly interesting, since (i) one

t = (1/N ) (cid:80)N

t

Sliced-Wasserstein Flows

√

typically has limN→∞ µN
t = µt with a rate of convergence
N ) for all t (Malrieu, 2003; Cattiaux et al.,
of order O(1/
2008), and (ii) each of the particle systems in (9) can be sim-
ulated by using an Euler-Maruyama discretization scheme.
We note that the existing theoretical results in (Veretennikov,
2006; Mishura & Veretennikov, 2016) do not directly ap-
ply to our case due to the non-standard form of our drift.
However, we conjecture that a similar result holds for our
problem as well. Such a result would be proven by using
the techniques given in (Zhang et al., 2018); however, it is
out of the scope of this study.

3.3. Approximate Euler-Maruyama discretization

In order to be able to simulate the particle SDEs (9) in
practice, we propose an approximate Euler-Maruyama dis-
cretization for each particle SDE. The algorithm iteratively
applies the following update equation: (∀i ∈ {1, . . . , N })

¯X i
0

i.i.d.∼ µ0, ¯X i

k+1 = ¯X i

k + hˆvk( ¯X i

k) +

√

2λhZ i

k+1, (10)

where k ∈ N+ denotes the iteration number, Z i
k is a stan-
dard Gaussian random vector in Rd, h denotes the step-
size, and ˆvk is a short-hand notation for a computation-
ally tractable estimator of the original drift v(·, ¯µN
kh), with
kh = (1/N ) (cid:80)N
¯µN
being the empirical distribution of
{ ¯X j
k}N
is how to compute this function ˆv.

j=1. A question of fundamental practical importance

j=1 δ ¯X j

k

We propose to approximate the integral in (7) via a simple
Monte Carlo estimate. This is done by ﬁrst drawing Nθ uni-
form i.i.d. samples from the sphere Sd−1, {θn}Nθ
n=1. Then,
at each iteration k, we compute:

ˆvk(x) (cid:44) −(1/Nθ)

(cid:88)Nθ

n=1

ψ(cid:48)

k,θn

((cid:104)θn, x(cid:105))θn,

(11)

where for any θ, ψ(cid:48)
k,θ is the derivative of the Kantorovich
potential (cf. Section 2) that is applied to the OT problem
from θ∗

# ¯µN

kh to θ∗
#ν: i.e.
k,θ(z) = (cid:2)z − (F −1
ψ(cid:48)

#ν ◦ Fθ∗
θ∗

# ¯µN
kh

)(z)(cid:3).

(12)

For any particular θ ∈ Sd−1, the QF, F −1
#ν for the projection
θ∗
of the target distribution ν on θ can be easily computed from
the data. This is done by ﬁrst computing the projections
(cid:104)θ, yi(cid:105) for all data points yi, and then computing the empir-
ical quantile function for this set of P scalars. Similarly,
, the CDF of the particles at iteration k, is easy to
Fθ∗
compute: we ﬁrst project all particles ¯X i
k to get (cid:104)θ, ¯X i
k(cid:105),
and then compute the empirical CDF of this set of N scalar
values.

# ¯µN
kh

Algorithm 1: Sliced-Wasserstein Flow (SWF)

i.i.d.∼ µ0,

i=1, µ0, N , Nθ, h, λ

:D ≡ {yi}P
K}N
i=1

input
output :{ ¯X i
// Initialize the particles
¯X i
0
// Generate random directions
θn ∼ Uniform(Sd−1),
// Quantiles of projected target
for θ ∈ {θn}Nθ

n=1 do

i = 1, . . . , N

n = 1, . . . , Nθ

F −1
θ∗

#ν = QF{(cid:104)θ, yi(cid:105)}P

i=1

// Iterations
for k = 0, . . . K − 1 do

for θ ∈ {θn}Nθ

n=1 do

# ¯µN
kh

// CDF of projected particles
Fθ∗

= CDF{(cid:104)θ, ¯X i

k(cid:105)}N
i=1
// Update the particles
√
¯X i

k − hˆvk( ¯X i

k+1 = ¯X i

k) +

2λhZ i

k+1
i = 1, . . . , N

computed Q ∈ N+ empirical quantiles. Another source of
approximation here comes from the fact that the target ν
will in practice be a collection of Dirac measures on the
observations yi. Since it is currently common to have a very
large dataset, we believe this approximation to be accurate
in practice for the target. Finally, yet another source of ap-
proximation comes from the error induced by using a ﬁnite
number of θn instead of a sum over Sd−1 in (12).

Even though the error induced by these approximation
schemes can be incorporated into our current analysis frame-
work, we choose to neglect it for now, because (i) all of these
one-dimensional computations can be done very accurately
and (ii) the quantization of the empirical CDF and QF can
be modeled as additive Gaussian noise that enters our dis-
cretization scheme (10) (Van der Vaart, 1998). Therefore,
we will assume that ˆvk is an unbiased estimator of v, i.e.
E[ˆv(x, µ)] = v(x, µ), for any x and µ, where the expecta-
tion is taken over θn.

The overall algorithm is illustrated in Algorithm 1. It is re-
markable that the updates of the particles only involves the
learning data {yi} through the CDFs of its projections on the
many θn ∈ Sd−1. This has a fundamental consequence of
high practical interest: these CDF may be computed before-
hand in a massively distributed manner that is independent
of the sliced Wasserstein ﬂow. This aspect is reminiscent
of the compressive learning methodology (Gribonval et al.,
2017), except we exploit quantiles of random projections
here, instead of random generalized moments as done there.

In both cases, the true CDF and quantile functions are ap-
proximated as a linear interpolation between a set of the

Besides, we can obtain further reductions in the computing
time if the CDF, Fθ∗
#ν for the target is computed on random

Sliced-Wasserstein Flows

mini-batches of the data, instead of the whole dataset of size
P . This simpliﬁed procedure might also have some interest-
ing consequences in privacy-preserving settings: since we
can vary the number of projection directions Nθ for each
data point yi, we may guarantee that yi cannot be recovered
via these projections, by picking fewer than necessary for
reconstruction using, e.g. compressed sensing (Donoho &
Tanner, 2009).

3.4. Finite-time analysis for the inﬁnite particle regime

In this section we will analyze the behavior of the proposed
algorithm in the asymptotic regime where the number of
particles N → ∞. Within this regime, we will assume that
the original SDE (8) can be directly simulated by using an
approximate Euler-Maruyama scheme, deﬁned starting at
¯X0

i.i.d.∼ µ0 as follows:
¯Xk+1 = ¯Xk + hˆv( ¯X i

√

2λhZk+1,

k, ¯µkh) +
(13)
where ¯µkh denotes the law of ¯Xk with step size h and {Zk}k
denotes a collection of standard Gaussian random variables.
Apart from its theoretical signiﬁcance, this scheme is also
practically relevant, since one would expect that it captures
the behavior of the particle method (10) with large number
of particles.

In practice, we would like to approximate the measure se-
quence (µt)t as accurate as possible, where µt denotes the
law of Xt. Therefore, we are interested in analyzing the
distance (cid:107)¯µKh − µT (cid:107)TV, where K denotes the total number
of iterations, T = Kh is called the horizon, and (cid:107)µ − ν(cid:107)TV
denotes the total variation distance between two probability
measures µ and ν: (cid:107)µ − ν(cid:107)TV (cid:44) supA∈B(Ω) |µ(A) − ν(A)|.
In order to analyze this distance, we exploit the algorith-
mic similarities between (13) and the stochastic gradient
Langevin dynamics (SGLD) algorithm (Welling & Teh,
2011), which is a Bayesian posterior sampling method hav-
ing a completely different goal, and is obtained as a dis-
cretization of an SDE whose drift has a much simpler form.
We then bound the distance by extending the recent results
on SGLD (Raginsky et al., 2017) to time- and measure-
dependent drifts, that are of our interest in the paper.

We now present our second main theoretical result. We
present all our assumptions and the explicit forms of the
constants in the supplementary document.
Theorem 3. Assume that the conditions given in the supple-
mentary document hold. Then, the following bound holds
for T = Kh:

(cid:107)¯µKh − µT (cid:107)2

TV ≤ δλ

(cid:40)

L2K
2λ

(cid:16) C1h3
3

+ 3λdh2(cid:17)

+

C2δKh
4λ

(cid:41)

,

(14)

for some C1, C2, L > 0, δ ∈ (0, 1), and δλ > 1.

Here, the constants C1, C2, L are related to the regularity
and smoothness of the functions v and ˆv; δ is directly propor-
tional to the variance of ˆv, and δλ is inversely proportional
to λ. The theorem shows that if we choose h small enough,
we can have a non-asymptotic error guarantee, which is
formally shown in the following corollary.

Corollary 1. Assume that the conditions of Theorem 3 hold.
Then for all ε > 0, K ∈ N+, setting

h = (3/C1) ∧

(cid:18) 2ε2λ
δλL2T

(cid:19)1/2

(1 + 3λd)−1

,

(15)

we have

(cid:107)¯µKh − µT (cid:107)TV ≤ ε +

(cid:19)1/2

(cid:18) C2δλδT
4λ

(16)

for T = Kh.

This corollary shows that for a large horizon T , the approxi-
mate drift ˆv should have a small variance in order to obtain
accurate estimations. This result is similar to (Raginsky
et al., 2017) and (Nguyen et al., 2019): for small ε the vari-
ance of the approximate drift should be small as well. On
the other hand, we observe that the error decreases as λ
increases. This behavior is expected since for large λ, the
Brownian term in (8) dominates the drift, which makes the
simulation easier.

We note that these results establish the explicit dependency
of the error with respect to the algorithm parameters (e.g.
step-size, gradient noise) for a ﬁxed number of iterations,
rather than explaining the asymptotic behavior of the algo-
rithm when K goes to inﬁnity.

4. Experiments

In this section, we evaluate the SWF algorithm on a syn-
thetic and a real data setting. Our primary goal is to validate
our theory and illustrate the behavior of our non-standard
approach, rather than to obtain the state-of-the-art results in
IGM. In all our experiments, the initial distribution µ0 is se-
lected as the standard Gaussian distribution on Rd, we take
Q = 100 quantiles and N = 5000 particles, which proved
sufﬁcient to approximate the quantile functions accurately.

4.1. Gaussian Mixture Model

We perform the ﬁrst set of experiments on synthetic data
where we consider a standard Gaussian mixture model
(GMM) with 10 components and random parameters. Cen-
troids are taken as sufﬁciently distant from each other to
make the problem more challenging. We generate P =
50000 data samples in each experiment.

Sliced-Wasserstein Flows

Target

k = 2

k = 3

k = 5

k = 10

λ = 0.1

λ = 0.2

k = 20

k = 50

λ = 0.5

λ = 1

Figure 1. SWF on toy 2D data. Left: Target distribution (shaded contour plot) and distribution of particles (lines) during SWF. (bottom)
SW cost over iterations during training (left) and test (right) stages. Right: Inﬂuence of the regularization parameter λ.

Figure 2. First, we learn an autoencoder (AE). Then, we use SWF
to transport random vectors to the distribution of the bottleneck
features of the training set. The trained decoder is used for visual-
ization.

In our ﬁrst experiment, we set d = 2 for visualization pur-
poses and illustrate the general behavior of the algorithm.
Figure 1 shows the evolution of the particles through the
iterations. Here, we set Nθ = 30, h = 1 and λ = 10−4.
We ﬁrst observe that the SW cost between the empirical
distributions of training data and particles is steadily de-
creasing along the SW ﬂow. Furthermore, we see that the
QFs, F −1
that are computed with the initial set of par-
# ¯µN
θ∗
kh
ticles (the training stage) can be perfectly re-used for new
unseen particles in a subsequent test stage, yielding similar
— yet slightly higher — SW cost.

In our second experiment on Figure 1, we investigate the
effect of the level of the regularization λ. The distribution of
the particles becomes more spread with increasing λ. This
is due to the increment of the entropy, as expected.

4.2. Experiments on real data

In the second set of experiments, we test the SWF algorithm
on two real datasets. (i) The traditional MNIST dataset
that contains 70K binary images corresponding to different
digits. (ii) The popular CelebA dataset (Liu et al., 2015), that

Figure 3. Samples generated after 200 iterations of SWF to match
the distribution of bottleneck features for the training dataset. Vi-
sualization is done with the pre-trained decoder.

contains 202K color-scale images. This dataset is advocated
as more challenging than MNIST. Images were interpolated
as 32 × 32 for MNIST, and 64 × 64 for CelebA.

In experiments reported in the supplementary document,
we found out that directly applying SWF to such high-
dimensional data yielded noisy results, possibly due to the
insufﬁcient sampling of Sd−1. To reduce the dimensional-
ity, we trained a standard convolutional autoencoder (AE)
on the training set of both datasets (see Figure 2 and the
supplementary document), and the target distribution ν con-
sidered becomes the distribution of the resulting bottleneck
features, with dimension d. Particles can be visualized with
the pre-trained decoder. Our goal is to show that SWF per-
mits to directly sample from the distribution of bottleneck
features, as an alternative to enforcing this distribution to

0123456789101520253035404550iteration−60−40−20020SW loss (dB)tasktraintestENCODERtarget distribution 𝜈original datasetbottleneck featuresreconstructed datasetsource distribution μSWFfrom μ to 𝜈 generated samplesgenerated featuresAE TRAININGSYNTHESISDECODERDECODERSliced-Wasserstein Flows

Figure 6. Applying a pre-trained SWF on new samples located
in-between the ones used for training. Visualization is done with
the pre-trained decoder.

dataset, namely GAN (Goodfellow et al., 2014), Wasser-
stein GAN (W-GAN) (Arjovsky et al., 2017) and the Sliced-
Wasserstein Generator (SWG) (Deshpande et al., 2018).
The visual comparison suggests that the samples generated
by SWF are of slightly better quality than those, although
research must still be undertaken to scale up to high dimen-
sions without an AE.

We also provide the outcome of the pre-trained SWF with
samples that are regularly spaced in between those used
for training. The result is shown in Figure 4.2. This plot
suggests that SWF is a way to interpolate non-parametrically
in between latent spaces of regular AE.

5. Conclusion and Future Directions

In this study, we proposed SWF, an efﬁcient, nonparamet-
ric IGM algorithm. SWF is based on formulating IGM as
a functional optimization problem in Wasserstein spaces,
where the aim is to ﬁnd a probability measure that is close
to the data distribution as much as possible while maintain-
ing the expressiveness at a certain level. SWF lies in the
intersection of OT, gradient ﬂows, and SDEs, which allowed
us to convert the IGM problem to an SDE simulation prob-
lem. We provided ﬁnite-time bounds for the inﬁnite-particle
regime and established explicit links between the algorithm
parameters and the overall error. We conducted several ex-
periments, where we showed that the results support our
theory: SWF is able to generate samples from non-trivial
distributions with low computational requirements.

The SWF algorithm opens up interesting future directions:
(i) extension to differentially private settings (Dwork &
Roth, 2014) by exploiting the fact that it only requires ran-
dom projections of the data, (ii) showing the convergence
scheme of the particle system (9) to the original SDE (8),
(iii) providing bounds directly for the particle scheme (10).

3

5

8

10

15

20

30

50

100

200

Figure 4. Initial random particles (left), particles through iterations
(middle, from 1 to 200 iterations) and closest sample from the
training dataset (right), for both MNIST and CelebA.

match some prior, as in VAE. In the following, we set λ = 0,
Nθ = 40000, d = 32 for MNIST and d = 64 for CelebA.

Assessing the validity of IGM algorithms is generally done
by visualizing the generated samples. Figure 3 shows some
particles after 500 iterations of SWF. We can observe they
are considerably accurate. Interestingly, the generated sam-
ples gradually take the form of either digits or faces along
the iterations, as seen on Figure 4. In this ﬁgure, we also dis-
play the closest sample from the original database to check
we are not just reproducing training data.

For a visual comparison, we provide the results presented in
(Deshpande et al., 2018) in Figure 5. These results are ob-
tained by running different IGM approaches on the MNIST

Figure 5. Performance of GAN (left), W-GAN (middle), SWG
(right) on MNIST. (The ﬁgure is directly taken from (Deshpande
et al., 2018).)

77.588.599.510 8.36 9.39 10.22 11.2log(n)log(E[˜W22(ˆPd,ˆP d)])MNISTTFDCelebALSUNFigure3.LimitedsampleestimateoftheslicedWassersteindistanceasafunctionofthesamplesize.051015200.40.81.612.94 10 3Traininginthousandsofiterations Trainingloss=E[˜W22(ˆPd,ˆPf)]Samplesize12825651210241282565121024Figure4.TrainingwithdifferentsamplesizesonMNIST.ThedashedlinesdenoteE[˜W22(ˆPd,ˆP d)].andshowinFig.3,howE[˜W22(ˆPd,ˆP d)]decreaseswiththenumberofsamplesusedforestimation.Toobtainthisquan-titywetaketwosetsofnsamples,eachfromthedatadistri-butionPd.WethencomputetheslicedWassersteindistancebetweenthosesetsinthemannerdescribedinAlg.1.WeobservethatE[˜W22(ˆPd,ˆP d)]decreasesroughlyviaO(n 1).UsingCorollary1,thisimpliesthat˜W22(Pd,P f)decreasesinO(n 1)fortheoptimalsolutionP f.Totestthequalityofthislossestimate,wetrainafullyconnecteddeepnetbasedgeneratorontheslicedWassersteindistancewithdifferentsamplesizesfortheMNISTdataset.Eachconﬁgurationwastrained5timeswithrandomlysetseeds,andtheaverageswitherrorbarsarepresentedinFig.4.Duringtraining,ateveryiteration,gradientsarecomputedusing10,000randomprojections.WeemphasizethesmallGANWGANSWGConvConv+BNFCFC+BNFigure5.MNISTsamplesafter40ktrainingiterationsfordiffer-entgeneratorconﬁgurations.Batchsize=250,Learningrate=0.0005,Adamoptimizererrorbarswhichhighlightthestabilityoftheproposedap-proach.Thegeneratorisabletoproducegoodimagesinallfourcases.Thisshowsthat,inpractice,asetofasfewas128sam-plesisgoodenoughforsimpledistributions.ThegeneratorisabletobeatE[˜W22(ˆPd,ˆP d)](dashedblackline)ontheloss,indicatingthatithasprobablyconvergedinallcases.Asthenumberofsamplesincreases,weseethisboundgettingtighter.4.2.StabilityofTrainingTodemonstratethestabilityoftheproposedapproach,fourdifferentgeneratorarchitecturesaretrainedwithourmethodaswellasthetwoaforementionedbaselinesusingexactlythesamesetofhyperparameters.Onegeneratoriscomposedoffullyconnectedlayerswhiletheotheriscom-posedofconvolutionalanddeconvolutionallayers.Foreachgeneratorweassessitsperformancewhenusingandwhennotusingbatchnormalization[12].ThearchitecturesaredescribedinmoredetailinAppendixD.Forthisexperiment,onlytheGANandWassersteinGANuseadiscriminator,whileourapproachreliesonrandomprojectionsinstead.Furthernotethatthesearchitecturesarearbitrarilychosen,andthiscomparisonisonlyintendedtoshowhowthetrain-ingstabilitycomparesacrossdifferentmethods,aswellashowtheslicedWassersteinlosscorrelateswiththegeneratedsamples.Thisisnottocomparethebestpossiblesamplesfromdifferenttrainingmethods.Samplesobtainedfromtheresultinggeneratorarevisu-Sliced-Wasserstein Flows

Acknowledgments

This work is partly supported by the French National
Research Agency (ANR) as a part of the FBIMATRIX
(ANR-16-CE23-0014) and KAMoulox (ANR-15-CE38-
0003-01) projects. Szymon Majewski is partially sup-
ported by Polish National Science Center grant number
2016/23/B/ST1/00454.

References

Ambrosio, L., Gigli, N., and Savar´e, G. Gradient ﬂows: in
metric spaces and in the space of probability measures.
Springer Science & Business Media, 2008.

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-
erative adversarial networks. In International Conference
on Machine Learning, pp. 214–223, 2017.

Benamou, J.-D. and Brenier, Y. A computational ﬂuid me-
chanics solution to the Monge-Kantorovich mass transfer
problem. Numerische Mathematik, 84(3):375–393, 2000.

Bogachev, V. I., Krylov, N. V., R¨ockner, M., and Shaposh-
nikov, S. V. Fokker-Planck-Kolmogorov Equations, vol-
ume 207. American Mathematical Soc., 2015.

Bonneel, N., Rabin, J., Peyr´e, G., and Pﬁster, H. Sliced and
Radon Wasserstein barycenters of measures. Journal of
Mathematical Imaging and Vision, 51(1):22–45, 2015.

Bonnotte, N. Unidimensional and evolution methods for
optimal transportation. PhD thesis, Paris 11, 2013.

Bossy, M. and Talay, D. A stochastic particle method for the
McKean-Vlasov and the Burgers equation. Mathematics
of Computation of the American Mathematical Society,
66(217):157–192, 1997.

Dalalyan, A. S. Theoretical guarantees for approximate
sampling from smooth and log-concave densities. Jour-
nal of the Royal Statistical Society: Series B (Statistical
Methodology), 79(3):651–676, 2017.

Deshpande, I., Zhang, Z., and Schwing, A. Generative
modeling using the sliced wasserstein distance. arXiv
preprint arXiv:1803.11188, 2018.

Diggle, P. J. and Gratton, R. J. Monte carlo methods of
inference for implicit statistical models. Journal of the
Royal Statistical Society. Series B (Methodological), pp.
193–227, 1984.

Donoho, D. and Tanner, J. Observed universality of phase
transitions in high-dimensional geometry, with implica-
tions for modern data analysis and signal processing.
Philosophical Transactions of the Royal Society of Lon-
don A: Mathematical, Physical and Engineering Sciences,
367(1906):4273–4293, 2009.

Durmus, A., S¸ ims¸ekli, U., Moulines, E., Badeau, R., and
Richard, G. Stochastic gradient Richardson-Romberg
Markov Chain Monte Carlo. In NIPS, 2016.

Dwork, C. and Roth, A. The algorithmic foundations of
differential privacy. Foundations and Trends R(cid:13) in Theo-
retical Computer Science, 9(3–4):211–407, 2014.

Genevay, A., Cuturi, M., Peyr´e, G., and Bach, F. Stochastic
In Ad-
optimization for large-scale optimal transport.
vances in Neural Information Processing Systems, pp.
3440–3448, 2016.

Genevay, A., Peyr´e, G., and Cuturi, M. Gan and vae
from an optimal transport point of view. arXiv preprint
arXiv:1706.01807, 2017.

Bousquet, O., Gelly, S., Tolstikhin, I., Simon-Gabriel, C.-J.,
and Schoelkopf, B. From optimal transport to gener-
the vegan cookbook. arXiv preprint
ative modeling:
arXiv:1705.07642, 2017.

Genevay, A., Peyr´e, G., and Cuturi, M. Learning genera-
tive models with Sinkhorn divergences. In International
Conference on Artiﬁcial Intelligence and Statistics, pp.
1608–1617, 2018.

Cattiaux, P., Guillin, A., and Malrieu, F. Probabilistic ap-
proach for granular media equations in the non uniformly
convex case. Prob. Theor. Rel. Fields, 140(1-2):19–40,
2008.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in neural
information processing systems, pp. 2672–2680, 2014.

S¸ ims¸ekli, U., Yildiz, C., Nguyen, T. H., Cemgil, A. T.,
and Richard, G. Asynchronous stochastic quasi-Newton
MCMC for non-convex optimization. In ICML, pp. 4674–
4683, 2018.

Cuturi, M. Sinkhorn distances: Lightspeed computation
of optimal transport. In Advances in neural information
processing systems, pp. 2292–2300, 2013.

Gribonval, R., Blanchard, G., Keriven, N., and Traonmilin,
Y. Compressive statistical learning with random feature
moments. arXiv preprint arXiv:1706.07180, 2017.

Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and
Courville, A. C. Improved training of Wasserstein GANs.
In Advances in Neural Information Processing Systems,
pp. 5769–5779, 2017.

Sliced-Wasserstein Flows

Guo, X., Hong, J., Lin, T., and Yang, N. Relaxed
Wasserstein with applications to GANs. arXiv preprint
arXiv:1705.07164, 2017.

Jordan, R., Kinderlehrer, D., and Otto, F. The variational
formulation of the Fokker–Planck equation. SIAM journal
on mathematical analysis, 29(1):1–17, 1998.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Kingma, D. P. and Welling, M. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013.

Kolouri, S., Martin, C. E., and Rohde, G. K. Sliced-
wasserstein autoencoder: An embarrassingly simple gen-
erative model. arXiv preprint arXiv:1804.01947, 2018.

Lavenant, H., Claici, S., Chien, E., and Solomon, J. Dynam-
ical optimal transport on discrete surfaces. In SIGGRAPH
Asia 2018 Technical Papers, pp. 250. ACM, 2018.

Lei, N., Su, K., Cui, L., Yau, S.-T., and Gu, D. X. A
geometric view of optimal transportation and generative
model. arXiv preprint arXiv:1710.05488, 2017.

Liu, S., Bousquet, O., and Chaudhuri, K. Approxima-
tion and convergence properties of generative adversarial
learning. In Advances in Neural Information Processing
Systems, pp. 5551–5559, 2017.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face
attributes in the wild. In Proceedings of International
Conference on Computer Vision (ICCV), 2015.

Ma, Y. A., Chen, T., and Fox, E. A complete recipe for
stochastic gradient MCMC. In Advances in Neural Infor-
mation Processing Systems, pp. 2899–2907, 2015.

Malrieu, F. Convergence to equilibrium for granular media
equations and their Euler schemes. Ann. Appl. Probab.,
13(2):540–560, 2003.

Mishura, Y. S. and Veretennikov, A. Y. Existence and
uniqueness theorems for solutions of McKean–Vlasov
stochastic equations. arXiv preprint arXiv:1603.02212,
2016.

Mohamed, S. and Lakshminarayanan, B.

ing in implicit generative models.
arXiv:1610.03483, 2016.

Learn-
arXiv preprint

A. M., and Bronstein, M. M. (eds.), Scale Space and
Variational Methods in Computer Vision, pp. 435–446,
Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
ISBN 978-3-642-24785-9.

Raginsky, M., Rakhlin, A., and Telgarsky, M. Non-convex
learning via stochastic gradient Langevin dynamics: a
nonasymptotic analysis. In Proceedings of the 2017 Con-
ference on Learning Theory, volume 65, pp. 1674–1703,
2017.

Samangouei, P., Kabkab, M., and Chellappa, R. Defense-
GAN: Protecting classiﬁers against adversarial attacks
using generative models. In International Conference on
Learning Representations, 2018.

Santambrogio, F. Introduction to optimal transport theory.
In Pajot, H., Ollivier, Y., and Villani, C. (eds.), Opti-
mal Transportation: Theory and Applications, chapter 1.
Cambridge University Press, 2014.

Santambrogio, F. {Euclidean, metric, and Wasserstein}
gradient ﬂows: an overview. Bulletin of Mathematical
Sciences, 7(1):87–154, 2017.

S¸ ims¸ekli, U. Fractional Langevin Monte Carlo: Explor-
ing L´evy Driven Stochastic Differential Equations for
Markov Chain Monte Carlo. In International Conference
on Machine Learning, 2017.

Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf,
arXiv preprint

B. Wasserstein auto-encoders.
arXiv:1711.01558, 2017.

Van der Vaart, A. W. Asymptotic statistics, volume 3. Cam-

bridge university press, 1998.

Veretennikov, A. Y. On ergodic measures for McKean-
Vlasov stochastic equations. In Monte Carlo and Quasi-
Monte Carlo Methods 2004, pp. 471–486. Springer, 2006.

Villani, C. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic
gradient Langevin dynamics. In International Conference
on Machine Learning, pp. 681–688, 2011.

Wu, J., Huang, Z., Li, W., and Gool, L. V. Sliced wasserstein
generative models. arXiv preprint arXiv:1706.02631,
abs/1706.02631, 2018.

Nguyen, T. H., S¸ ims¸ekli, U., , and Richard, G. Non-
asymptotic analysis of fractional Langevin Monte Carlo
for non-convex optimization. In ICML, 2019.

Zhang, J., Zhang, R., and Chen, C. Stochastic particle-
optimization sampling and the non-asymptotic conver-
gence theory. arXiv preprint arXiv:1809.01293, 2018.

Rabin, J., Peyr´e, G., Delon, J., and Bernot, M. Wasser-
stein barycenter and its application to texture mixing. In
Bruckstein, A. M., ter Haar Romeny, B. M., Bronstein,

Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal
Transport and Diffusions
SUPPLEMENTARY DOCUMENT

Antoine Liutkus 1 Umut S¸ ims¸ekli 2 Szymon Majewski 3 Alain Durmus 4 Fabian-Robert St¨oter 1

1. Proof of Theorem 2

We ﬁrst need to generalize (Bonnotte, 2013)[Lemma 5.4.3] to distribution ρ ∈ L∞(B(0, r)), r > 0.
Theorem S4. Let ν be a probability measure on B(0, 1) with a strictly positive smooth density. Fix a time step h > 0,
regularization constant λ > 0 and a radius r >
d. For any probability measure µ0 on B(0, r) with density ρ0 ∈
L∞(B(0, r)), there is a probability measure µ on B(0, r) minimizing:

√

G(µ) = F ν

λ (µ) +

1
2h

W 2

2 (µ, µ0),

where F ν

λ is given by (5). Moreover the optimal µ has a density ρ on B(0, r) and:

||ρ||L∞ ≤ (1 + h/

√

d)d||ρ0||L∞.

(S1)

Proof. The set of measures supported on B(0, r) is compact in the topology given by W2 metric. Furthermore by (Ambrosio
et al., 2008)[Lemma 9.4.3] H is lower semicontinuous on (P(B(0, r)), W2). Since by (Bonnotte, 2013)[Proposition 5.1.2,
Proposition 5.1.3], SW2 is a distance on P(B(0, r)), dominated by d−1/2W2, we have:

|SW2(π0, ν) − SW2(π1, ν)| ≤ SW2(π0, π1) ≤

1
√
d

W2(π0, π1).

The above means that SW2(·, ν) is continuous with respect to topology given by W2, which implies that SW 2
2 (·, ν) is
continuous in this topology as well. Therefore G : P(B(0, r)) → (−∞, +∞] is a lower semicontinuous function on the
compact set (P(B(0, r)), W2). Hence there exists a minimum µ of G on P(B(0, r)). Furthermore, since H(π) = +∞ for
measures π that do not admit a density with respect to Lebesgue measure, the measure µ must admit a density ρ.

If ρ0 is smooth and positive on B(0, r), the inequality S1 is true by (Bonnotte, 2013)[Lemma 5.4.3.] When ρ0 is just in
L∞(B(0, r)), we proceed by smoothing. For t ∈ (0, 1], let ρt be a function obtained by convolution of ρ0 with a Gaussian
kernel (t, x, y) (cid:55)→ (2π)d/2 exp((cid:107)x − y(cid:107)2 /2), restricting the result to B(0, r) and normalizing to obtain a probability density.
Then (ρt)t are smooth positive densities, and it is easy to see that limt→0 ||ρt||L∞ ≤ ||ρ0||L∞ . Furthermore, if we denote
by µt the measure on B(0, r) with density ρt, then µt converge weakly to µ0. For t ∈ (0, 1] let ˆµt be the minimum of
F ν

2 (·, µt), and let ˆρt be the density of ˆµt. Using (Bonnotte, 2013)[Lemma 5.4.3.] we get

λ (·) + 1

2h W 2

||ˆρt||L∞ ≤ (1 + h

√

d)d||ρt||L∞ .

so ˆρt lies in a ball of ﬁnite radius in L∞. Using compactness of P(B(0, r)) in weak topology and compactness of closed
ball in L∞(B(0, r)) in weak star topology, we can choose a subsequence ˆµtk , ˆρtk , limk→+∞ tk = 0, that converges along
that subsequence to limits ˆµ, ˆρ. Obviously ˆρ is the density of ˆµ, since for any continuous function f on B(0, r) we have:

(cid:90)

(cid:90)

ˆρf dx = lim
k→∞

ρtk f dx = lim
k→∞

(cid:90)

f dµtk =

(cid:90)

f dµ.

Furthermore, since ˆρ is the weak star limit of a bounded subsequence, we have:

||ˆρ||L∞ ≤ lim sup
k→∞

(1 + h

√

d)d||ρtk ||L∞ ≤ (1 + h

√

d)d||ρ0||L∞ .

Sliced-Wasserstein Flows

To ﬁnish, we just need to prove that ˆµ is a minimum of G. We remind our reader, that we already established existence of
some minimum µ (that might be different from ˆµ). Since ˆµtk converges weakly to ˆµ in P(B(0, r)), it implies convergence
in W2 as well since B(0, r) is compact. Similarly µtk converges to µ0 in W2. Using the lower semicontinuity of G we now
have:

F ν

λ (ˆµ) +

W 2

2 (ˆµ, µ0) ≤ lim inf
k→∞

1
2h

(cid:18)

F ν

λ (ˆµtk ) +

W 2

2 (ˆµtk , µ0)

(cid:19)

1
2h

1
2h
1
2h

F ν

λ (µ) +

W 2

2 (µ, µtk )

≤ lim inf
k→∞
1
2h
= F ν

W 2

+

2 (ˆµtk , µ0) −
1
2h

W 2

λ (µ) +

2 (µ, µ0),

W 2

2 (ˆµtk , µtk )

where the second inequality comes from the fact, that ˆµtk minimizes F ν
previously established facts, it follows that ˆµ is a minimum of G with density satisfying S1.

λ (·) + 1

2h W 2

2 (·, µtk ). From the above inequality and

Deﬁnition 1. Minimizing movement scheme Let r > 0 and F : R+ × P(B(0, r)) × P(B(0, r)) → R be a functional. Let
µ0 ∈ P(B(0, r)) be a starting point. For h > 0 a piecewise constant trajectory µh : [0, ∞) → P(B(0, r)) for F starting at
µ0 is a function such that:

• µh(0) = µ0.

• µh is constant on each interval [nh, (n + 1)h), so µh(t) = µh(nh) with n = (cid:98)t/h(cid:99).

• µh((n + 1)h) minimizes the functional ζ (cid:55)→ F(h, ζ, µh(nh)), for all n ∈ N.

We say ˆµ is a minimizing movement scheme for F starting at µ0, if there exists a family of piecewise constant trajectory
(µh)h>0 for F such that ˆµ is a pointwise limit of µh as h goes to 0, i.e. for all t ∈ R+, limh→0 µh(t) = µ(t) in P(B(0, r)).
We say that ˜µ is a generalized minimizing movement for F starting at µ0, if there exists a family of piecewise constant
trajectory (µh)h>0 for F and a sequence (hn)n, limn→∞ hn = 0, such that µhn converges pointwise to ˜µ.
Theorem S5. Let ν be a probability measure on B(0, 1) with a strictly positive smooth density. Fix a regularization constant
d. Given an absolutely continuous measure µ0 ∈ P(B(0, r)) with density ρ0 ∈ L∞(B(0, r)), there
λ > 0 and radius r >
is a generalized minimizing movement scheme (µt)t in P(B(0, r)) starting from µ0 for the functional deﬁned by

√

F ν(h, µ+, µ−) = F ν

λ (µ+) +

1
2h

W 2

2 (µ+, µ−).

(S2)

Moreover for any time t > 0, the probability measure µt = µ(t) has density ρt with respect to the Lebesgue measure and:

||ρt||L∞ ≤ edt

√

d||ρ0||L∞ .

(S3)

Proof. We start by noting, that by S4 for any h > 0 there exists a piecewise constant trajectory µh for S2 starting at µ0.
Furthermore for t ≥ 0 measure µh
t , and:
√

t = µh(t) has density ρh

||ρh

t ||L∞ ≤ ed

d(t+h)||ρ0||L∞ .

(S4)

Let us choose T > 0. We denote ρh(t, x) = ρh
t (x). For h ≤ 1, the functions ρh lie in a ball in L∞([0, T ] × B(0, r)), so
from Banach-Alaoglu theorem there is a sequence hn converging to 0, such that ρhn converges in weak-star topology in
L∞([0, T ] × B(0, r)) to a certain limit ρ. Since ρ has to be nonnegative except for a set of measure zero, we assume ρ is
nonnegative. We denote ρt(x) = ρ(t, x). We will prove that for almost all t, ρt is a probability density and µhn
converges
t
in W2 to a measure µt with density ρt.

First of all, for almost all t ∈ [0, T ], ρt is a probability density, since for any Borel set A ⊆ [0, T ] the indicator of set
A × B(0, r) is integrable, and hence by deﬁnition of the weak-star topology:

(cid:90)

(cid:90)

A

B(0,r)

ρt(x)dxdt = lim
n→∞

(cid:90)

(cid:90)

A

B(0,r)

ρhn
t (x)dxdt,

Sliced-Wasserstein Flows

and so we have to have (cid:82) ρt(x)dx = 1 for almost all t ∈ [0, T ]. Nonnegativity of ρt follows from nonnegativity of ρ.
We will now prove, that for almost all t ∈ [0, T ] the measures µhn
t
take δ < min(T − t, t) and ζ ∈ C1(B(0, r)). We have:

converge to a measure with density ρt. Let t ∈ (0, T ),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

ζdµhn

t −

(cid:90)

B(0,r)

ζdµhm
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

ζdµhn

t −

1
2δ

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90) t+δ

(cid:90)

t−δ

B(0,r)

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90) t+δ

B(0,r)

(cid:90)

ζdµhn

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2δ

t−δ

B(0,r)

ζdµhm

t −

1
2δ

(cid:90) t+δ

(cid:90)

t−δ

B(0,r)

ζdµhm

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

+

ζdµhm

s ds −

1
2δ

(cid:90) t+δ

(cid:90)

t−δ

B(0,r)

ζdµhn

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

.

(S5)

t have densities ρhn
t

Because µhn
right hand side converges to zero, as n, m → ∞. Next, we get a bound on the other two terms.
First, if we denote by γ the optimal transport plan between µhn
t

s , we have:

and µhn

and both ρhn, ρhm converge to ρ in weak-star topology, the last element of the sum on the

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

ζdµhn

t −

(cid:90)

B(0,r)

ζdµhn
s

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

≤

B(0,r)×B(0,r)

|ζ(x) − ζ(y)|2 dγ(x, y) ≤ ||∇ζ||2

∞W 2

2 (µhn
t

, µhn

s ).

(S6)

In addition, for nt = (cid:98)t/hn(cid:99) and ns = (cid:98)s/hn(cid:99) we have µhn

t = µhn

and µhn

s = µhn

nshn

. For all k ≥ 0 we have:

W 2

2 (µhn
khn

, µhn

(k+1)hn

) ≤ 2hn(F ν

) − F ν

λ (µhn

(k+1)hn

).

(S7)

nthn
λ (µhn
khn

Using this result and (S6) and assuming without loss of generality nt ≤ ns, from the Cauchy-Schwartz inequality we get:

W 2

2 (µhn
t

, µhn

s ) ≤

(cid:32)ns−1
(cid:88)

k=nt

W2(µhn
khn

, µhn

(k+1)hn

(cid:33)2
)

≤ |nt − ns|

ns1
(cid:88)

k=nt

W 2

2 (µhn
khn

, µhn

(k+1)hn

)

≤ 2hn|nt − ns|(F ν

λ (µhn

nthn

) − F ν

λ (µhn

nshn

)) ≤ 2C(|t − s| + hn),

(S8)

where we used for the last inequality, denoting C = F ν
(S7) and minP(B(0,r)) F ν
and S6 we get:

λ is ﬁnite since F ν

))n is non-increasing by
λ is lower semi-continuous. Finally, using Jensen’s inequality, the above bound

λ (µ0) − minP(B(0,r)) F ν

λ , that (F ν

λ (µhn
khn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

ζdµhn

t −

1
2δ

(cid:90) t+δ

(cid:90)

t−δ

B(0,r)

2

ζdµhn

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

≤

≤

(cid:90)

1
2δ

(cid:90) t+δ

t−δ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C||∇ζ||2
∞
δ

B(0,r)
(cid:90) t+δ

≤ 2C||∇ζ||2

t−δ
∞(hn + δ).

ζdµhn

t −

(cid:90)

B(0,r)

ζdµhn
s

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ds

(|t − s| + hn)ds

B(0,r) ζdµhn

Together with (S5), when taking δ = hn, this result means that (cid:82)
other hand, since ρhn converges to ρ in weak-star topology on L∞, the limit of (cid:82)
for almost all t ∈ (0, T ). This means that for almost all t ∈ [0, T ] sequence µhn
t
Let S ∈ [0, T ] be the set of times such that for t ∈ S sequence µhn
converges to µt. As we established almost all points
t
from [0, T ] belong to S. Let t ∈ [0, T ] \ S. Then, there exists a sequence of times tk ∈ S converging to t, such that µtk
converge to some limit µt. We have:
W2(µhn
t

is a Cauchy sequence for all t ∈ (0, T ). On the
B(0,r) ζdµhn
B(0,r) ζ(x)ρt(x)dx
converges to a measure µt with density ρt.

, µtk ) + W2(µtk , µt).

, µt) ≤ W2(µhn
t

t has to be (cid:82)

) + W2(µhn
tk

, µhn
tk

t

From which we have for all k ≥ 1:

Sliced-Wasserstein Flows

lim sup
n→∞

W2(µhn
t

, µt) ≤ W2(µtk , µt) + lim sup
n→∞

W2(µhn
t

, µhn
tk

),

and using (S8), we get µhn
so we can choose a subsequence of ρhn
t

t → µt. Furthermore, the measure µt has to have density, since ρhn
t

lie in a ball in L∞(B(0, r)),
converging in weak-star topology to a certain limit ˆρt, which is the density of µt.

n be a sequence converging to 0, such that µh1

We use now the diagonal argument to get convergence for all t > 0. Let (Tk)∞
inﬁnity. Let h1
above, we can choose a subsequence h2
construct subsequences hk
n, and in the end take hn = hn
t > 0, and µt has a density satisfying the bound from the statement of the theorem.

k=1 be a sequence of times increasing to
converge to µt for all t ∈ [0, T1]. Using the same arguments as
converges to a limit µt for all t ∈ [0, T2]. Inductively, we
converges to µt for all

n. For this subsequence we have that µhn
t

n, such that µh2

n of h1

n

n

t

t

Finally, note that (S5) follows from (S4).

Theorem S6. Let (µt)t≥0 be a generalized minimizing movement scheme given by Theorem S5 with initial distribution µ0
with density ρ0 ∈ L(B(0, r)). We denote by ρt the density of µt for all t ≥ 0. Then ρt satisﬁes the continuity equation:

∂ρt
∂t

+ div(vtρt) + λ∆ρt = 0 ,

vt(x) = −

(cid:90)

Sd−1

ψ(cid:48)

t,θ((cid:104)x, θ(cid:105))θdθ,

in a weak sense, that is for all ξ ∈ C∞

c ([0, ∞) × B(0, r)) we have:

(cid:90) ∞

(cid:90)

0

B(0,r)

(cid:20) ∂ξ
∂t

(cid:21)
(t, x) − vt∇ξ(t, x) − λ∆ξ(t, x)

ρt(x)dxdt = −

(cid:90)

B(0,r)

ξ(0, x)ρ0(x)dx.

Proof. Our proof is based on the proof of (Bonnotte, 2013)[Theorem 5.6.1]. We proceed in ﬁve steps.

(1) Let hn → 0 be a sequence given by Theorem S5, such that µhn
t
µhn have densities ρhn that converge to ρ in Lr, for r ≥ 1, and in weak-star topology in L∞. Let ξ ∈ C∞
We denote ξn

k (x) = ξ(khn, x). Using part 1 of the proof of (Bonnotte, 2013)[Theorem 5.6.1], we obtain:

converges to µt pointwise. Furthermore we know that
c ([0, ∞) × B(0, r)).

(cid:90)

B(0,r)

ξ(0, x)ρ0(x)dx +

(cid:90) ∞

(cid:90)

0

B(0,r)

∂ξ
∂t

(t, x)ρt(x)dxdt

= lim
n→∞

−hn

∞
(cid:88)

(cid:90)

k=1

B(0,r)

ξn
k (x)

ρhn
khn

(x) − ρhn

(k−1)hn

(x)

hn

dx.

(S9)

(2) Again, this part is the same as part 2 of the proof of (Bonnotte, 2013)[Theorem 5.6.1]. For any θ ∈ Sd−1 we denote by
#ν, and by ψhn
ψt,θ the unique Kantorovich potential from θ∗
#µt to θ∗
#ν.
Then, by the same reasoning as part 2 of the proof of (Bonnotte, 2013)[Theorem 5.6.1], we get:

t,θ the unique Kantorovich potential from θ∗

#µhn

to θ∗

t

(cid:90) ∞

(cid:90)

(cid:90)

0

B(0,r)

Sd−1

(ψt,θ)(cid:48)((cid:104)θ, x(cid:105))(cid:104)θ, ∇ξ(x, t)(cid:105)dθdµt(x)dt

= lim
n→∞

hn

∞
(cid:88)

(cid:90)

(cid:90)

k=1

B(0,r)

Sd−1

ψhn
khn,θ(θ∗)(cid:104)θ, ∇ξn

k (cid:105)dθdµhn
khn

.

(S10)

(3) Since ξ is compactly supported and smooth, ∆ξ is Lipschitz, and so for any t ≥ 0 if we take k = (cid:98)t/hn(cid:99) we get
|∆ξn

k (x) − ∆ξ(t, x)| ≤ Chn for some constant C. Let T > 0 be such that ξ(t, x) = 0 for t > T . We have:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

k=1

(cid:90)

hn

B(0,r)

∆ξn

k (x)ρhn
khn

(x)dx −

(cid:90) +∞

(cid:90)

0

B(0,r)

∆ξ(t, x)ρhn

(cid:12)
(cid:12)
(cid:12)
t (x)dxdt
(cid:12)
(cid:12)

≤ CT hn.

Sliced-Wasserstein Flows

On the other hand, we know, that ρhn converges to ρ in weak star topology on L∞([0, T ] × B(0, r)), and ∆ξ is bounded, so:

lim
n→+∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) +∞

(cid:90)

0

B(0,r)

∆ξ(t, x)ρhn

t (x)dxdt −

(cid:90) +∞

(cid:90)

0

B(0,r)

∆ξ(t, x)ρt(x)dxdt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0.

Combining those two results give:

lim
n→∞

hn

∞
(cid:88)

(cid:90)

k=1

B(0,r)

∆ξn

k (x)ρhn
khn

(x)dx =

(cid:90) +∞

(cid:90)

0

B(0,r)

∆ξ(t, x)ρt(x)dxdt.

(S11)

k denote the unique Kantorovich potential from µhn
khn

(4) Let φhn
and 5.1.7], as well as (Jordan et al., 1998)[Equation (38)] with Ψ = 0, and optimality of µhn
khn

to µhn

(k−1)hn

, we get:

. Using (Bonnotte, 2013)[Propositions 1.5.7

1
hn

(cid:90)

B(0,r)

(cid:104)∇φhn

k (x), ∇ξn

k (x)(cid:105)dµhn
khn

(cid:90)

(cid:90)

(x) −

B(0,r)

Sd−1

(ψhn
khn

)(cid:48)(θ∗)(cid:104)θ, ∇ξn

k (x)(cid:105)dθdµhn
khn

(x)

(cid:90)

− λ

B(0,r)

∆ξn

k (x)dµhn
khn

(x),

(S12)

which is the derivative of F ν

λ (·) + 1
2hn

W 2

2 (·, µ(k−1)hn ) in the direction given by vector ﬁeld ∇ξn

k is zero.

Let γ be the optimal transport between µhn
khn

and µhn

(k−1)hn

. Then:

(cid:90)

B(0,r)

ξn
k (x)

ρhn
khn

(x) − ρhn

(k−1)hn

(x)

hn

dx =

1
hn

(cid:90)

B(0,r)

(cid:104)∇φhn

k (x), ∇ξn

k (x)(cid:105)dµhn
khn

(x) =

1
hn

1
hn

(cid:90)

B(0,r)

(cid:90)

B(0,r)

(ξn

k (y) − ξn

k (x))dγ(x, y).

(cid:104)∇ξn

k (x), y − x(cid:105)dγ(x, y).

(S13)

(S14)

Since ξ is C∞
(cid:104)∇ξ(x), y − x(cid:105)| ≤ C|x − y|2, and hence:

c , it has Lipschitz gradient. Let C be twice the Lipschitz constant of ∇ξ. Then we have |ξ(y) − ξ(x) −

(cid:90)

B(0,r)

|ξn

k (y) − ξn

k (x) − (cid:104)∇ξn

k (x), y − x(cid:105)|dγ(x, y) ≤ CW 2

2 (µhn

(k−1)hn

, µhn
khn

).

(S15)

Combining (S13), (S14) and (S15), we get:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

k=1

(cid:90)

hn

B(0,r)

ξn
k (x)

ρhn
khn

(k−1)hn

− ρhn
hn

dx +

∞
(cid:88)

k=1

(cid:90)

hn

B(0,r)

As some F ν

λ have a ﬁnite minimum on P(B(0, r)), we have:

(cid:104)∇φhn

k , ∇ξn

k (cid:105)dµhn
khn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ C

∞
(cid:88)

k=1

W 2

2 (µhn

(k−1)hn

, µhn
khn

).

(S16)

∞
(cid:88)

k=1

W 2

2 (µhn

(k−1)hn

, µhn
khn

) ≤ 2hn

≤ 2hn

∞
(cid:88)

k=1
(cid:32)

F ν

λ (µhn

(k−1)hn

) − F ν

λ (µhn
khn

)

F ν

λ (µ0) − min

P(B(0,r))

(cid:33)

F ν
λ

.

(S17)

and so the sum on the right hand side of the equation goes to zero as n goes to inﬁnity.

Sliced-Wasserstein Flows

From (S16), (S17) and (S12) we conclude:

lim
n→∞

−hn

∞
(cid:88)

k=1

ξn
k (x)

ρhn
khn

(k−1)hn

− ρhn
hn

dx =

(cid:32)

lim
n→∞

hn

∞
(cid:88)

(cid:90)

(cid:90)

k=1

B(0,r)

Sd−1

ψhn
khn,θ(θ∗)(cid:104)θ, ∇ξn

k (cid:105)dθdµhn
khn

+ hn

∞
(cid:88)

(cid:90)

k=1

B(0,r)

(cid:33)

∆ξn

k (x)ρhn
khn

(x)dx

,

(S18)

where both limits exist, since the difference of left hand side and right hand side of the equation goes to zero, while the left
hand side converges to a ﬁnite value by (S9).

(5) Combining (S9), (S10), (S11) and (S18) we get the result.

2. Proof of Theorem 3

Before proceeding to the proof, let us ﬁrst deﬁne the following Euler-Maruyama scheme which will be useful for our
analysis:

ˆXk+1 = ˆXk + hˆv( ˆXk, µkh) +

√

2λhZn+1,

(S19)

where µt denotes the probability distribution of Xt with (Xt)t being the solution of the original SDE (8). Now, consider the
probability distribution of ˆXk as ˆµkh. Starting from the discrete-time process ( ˆXk)k∈N+, we ﬁrst deﬁne a continuous-time
process (Yt)t≥0 that linearly interpolates ( ˆXk)k∈N+, given as follows:

dYt = ˜vt(Y )dt +

√

2λdWt,

(S20)

where ˜vt(Y ) (cid:44) − (cid:80)∞
time process (Ut)t≥0 that linearly interpolates ( ¯Xk)k∈N+, deﬁned by (13), given as follows:

k=0 ˆvkh(Ykh)1[kh,(k+1)h)(t) and 1 denotes the indicator function. Similarly, we deﬁne a continuous-

dUt = ¯vt(U )dt +

√

2λdWt,

(S21)

where ¯vt(U ) (cid:44) − (cid:80)∞
distributions of (Xt)t∈[0,T ], (Yt)t∈[0,T ] and (Ut)t∈[0,T ] as πT

k=0 ˆv(Ukh, ¯µkh)1[kh,(k+1)h)(t) and ¯µkh denotes the probability distribution of ¯Xk. Let us denote the

X , πT

Y and πT

U respectively with T = Kh.

We consider the following assumptions:
HS1. For all λ > 0, the SDE (8) has a unique strong solution denoted by (Xt)t≥0 for any starting point x ∈ Rd.
HS2. There exits L < ∞ such that

where vt(x) = v(x, µt) and

(cid:107)vt(x) − vt(cid:48)(x(cid:48))(cid:107) ≤ L((cid:107)x − x(cid:48)(cid:107) + |t − t(cid:48)|),

(cid:107)ˆv(x, µ) − ˆv(x(cid:48), µ(cid:48))(cid:107) ≤ L((cid:107)x − x(cid:48)(cid:107) + (cid:107)µ − µ(cid:48)(cid:107)TV).

HS3. For all t ≥ 0, vt is dissipative, i.e. for all x ∈ Rd,

(cid:104)x, vt(x)(cid:105) ≥ m(cid:107)x(cid:107)2 − b,

(S22)

(S23)

(S24)

for some m, b > 0.
HS4. The estimator of the drift satisﬁes the following conditions: E[ˆvt] = vt for all t ≥ 0, and for all t ≥ 0, x ∈ Rd,

E[(cid:107)ˆv(x, µt) − v(x, µt)(cid:107)2] ≤ 2δ(L2(cid:107)x(cid:107)2 + B2),

(S25)

for some δ ∈ (0, 1).

Sliced-Wasserstein Flows

HS5. For all t ≥ 0: |Ψt(0)| ≤ A and (cid:107)vt(0)(cid:107) ≤ B, for A, B ≥ 0, where Ψt = (cid:82)

Sd−1 ψt((cid:104)θ, ·(cid:105))dθ.

We start by upper-bounding (cid:107)ˆµKh − µT (cid:107)TV.
Lemma S1. Assume that the conditions HS2 to S5 hold. Then, the following bound holds:

(cid:107)ˆµKh − µT (cid:107)2

TV ≤ (cid:107)πT

Y − πT

X (cid:107)2

TV ≤

L2K
4λ

(cid:16) C1h3
3

+ 3λdh2(cid:17)

+

C2δKh
8λ

,

(S26)

where C1 (cid:44) 12(L2C0 + B2) + 1, C2 (cid:44) 2(L2C0 + B2), C0 (cid:44) Ce + 2(1 ∨ 1
of µ0.

m )(b + 2B2 + dλ), and Ce denotes the entropy

Proof. We use the proof technique presented in (Dalalyan, 2017; Raginsky et al., 2017). It is easy to verify that for all
k ∈ N+, we have Ykh = ˆXk.

By Girsanov’s theorem to express the Kullback-Leibler (KL) divergence between these two distributions, given as follows:

KL(πT

X ||πT

Y ) =

=

=

1
4λ

1
4λ

1
4λ

(cid:90) Kh

E[(cid:107)vt(Yt) + ˜vt(Y )(cid:107)2] dt

0
K−1
(cid:88)

k=0

K−1
(cid:88)

(cid:90) (k+1)h

kh

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vt(Yt) + ˜vt(Y )(cid:107)2] dt

E[(cid:107)vt(Yt) − ˆvkh(Ykh)(cid:107)2] dt.

(S27)

(S28)

(S29)

By using vt(Yt) − ˆvkh(Ykh) = (vt(Yt) − vkh(Ykh)) + (vkh(Ykh) − ˆvkh(Ykh)), we obtain

KL(πT

X ||πT

Y ) ≤

1
2λ

K−1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vt(Yt) − vkh(Ykh)(cid:107)2] dt

+

1
2λ

K−1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vkh(Ykh) − ˆvkh(Ykh)(cid:107)2] dt

(S30)

≤

L2
λ

K−1
(cid:88)

(cid:90) (k+1)h

k=0

kh

(cid:0)E[(cid:107)Yt − Ykh(cid:107)2] + (t − kh)2(cid:1) dt

+

1
2λ

K−1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vkh(Ykh) − ˆvkh(Ykh)(cid:107)2] dt.

(S31)

The last inequality is due to the Lipschitz condition HS2.
Now, let us focus on the term E[(cid:107)Yt − Ykh(cid:107)2]. By using (S20), we obtain:

Yt − Ykh = −(t − kh)ˆvkh(Ykh) + (cid:112)2λ(t − kh)Z,
where Z denotes a standard normal random variable. By adding and subtracting the term −(t − kh)vkh(Ykh), we have:
Yt − Ykh = −(t − kh)vkh(Ykh) + (t − kh)(vkh(Ykh) − ˆvkh(Ykh)) + (cid:112)2λ(t − kh)Z.

(S32)

(S33)

Taking the square and then the expectation of both sides yields:

E[(cid:107)Yt − Ykh(cid:107)2] ≤3(t − kh)2E[(cid:107)vkh(Ykh)(cid:107)2] + 3(t − kh)2E[(cid:107)vkh(Ykh) − ˆvkh(Ykh)(cid:107)2]

+ 6λ(t − kh)d.

(S34)

As a consequence of HS2 and HS5, we have (cid:107)vt(x)(cid:107) ≤ L(cid:107)x(cid:107) + B for all t ≥ 0, x ∈ Rd. Combining this inequality with H
S4, we obtain:

E[(cid:107)Yt − Ykh(cid:107)2] ≤6(t − kh)2(L2E[(cid:107)Ykh(cid:107)2] + B2) + 6(t − kh)2(L2E[(cid:107)Ykh(cid:107)2] + B2)

+ 6λ(t − kh)d

=12(t − kh)2(L2E[(cid:107)Ykh(cid:107)2] + B2) + 6λ(t − kh)d.

(S35)

(S36)

Sliced-Wasserstein Flows

By Lemma 3.2 of (Raginsky et al., 2017)4, we have E[(cid:107)Ykh(cid:107)2] ≤ C0 (cid:44) Ce + 2(1 ∨ 1
the entropy of µ0. Using this result in the above equation yields:

m )(b + 2B2 + dλ), where Ce denotes

E[(cid:107)Yt − Ykh(cid:107)2] ≤12(t − kh)2(L2C0 + B2) + 6λ(t − kh)d.

(S37)

We now focus on the term E[(cid:107)vkh(Ykh) − ˆvkh(Ykh)(cid:107)2] in (S31). Similarly to the previous term, we can upper-bound this
term as follows:

E[(cid:107)vkh(Ykh) − ˆvkh(Ykh)(cid:107)2] ≤2δ(L2E[(cid:107)Ykh(cid:107)2] + B2)

≤2δ(L2C0 + B2).

By using (S37) and (S39) in (S31), we obtain:

K−1
(cid:88)

(cid:90) (k+1)h

k=0

kh

(cid:0)12(t − kh)2(L2C0 + B2) + 6λ(t − kh)d + (t − kh)2(cid:1)dt

KL(πT

X ||πT

Y ) ≤

L2
λ

+

K−1
(cid:88)

(cid:90) (k+1)h

2δ(L2C0 + B2) dt

1
2λ

k=0
(cid:16) C1h3
3

kh

+

(cid:17)

6λdh2
2

+

C2δKh
2λ

,

=

L2K
λ

where C1 = 12(L2C0 + B2) + 1 and C2 = 2(L2C0 + B2).

Finally, by using the data processing and Pinsker inequalities, we obtain:

(cid:107)ˆµKh − µT (cid:107)2

TV ≤ (cid:107)πT

X − πT

Y (cid:107)2

TV ≤

=

1
4
L2K
4λ

KL(πT

X ||πT
Y )
(cid:16) C1h3
+ 3λdh2(cid:17)
3

+

C2δKh
8λ

.

This concludes the proof.

Now, we bound the term (cid:107)¯µKh − ˆµKh(cid:107)TV.
Lemma S2. Assume that HS2 holds. Then the following bound holds:

(cid:107)πT

U − πT

Y (cid:107)2

TV ≤

L2Kh
16λ

(cid:107)πT

X − πT

U (cid:107)2

TV.

Proof. We use that same approach than in Lemma S1. By Girsanov’s theorem once again, we have

(S38)

(S39)

(S40)

(S41)

(S42)

(S43)

(S44)

KL(πT

Y ||πT

U ) =

1
4λ

K−1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)ˆv(Ukh, µkh) − ˆv(Ukh, ¯µkh)(cid:107)2] dt,

(S45)

where πT

U denotes the distributions of (Ut)t∈[0,T ] with T = Kh. By using HS2, we have:

KL(πT

Y ||πT

U ) ≤

L2h
4λ

K−1
(cid:88)

k=0

(cid:107)µkh − ¯µkh(cid:107)2
TV

≤

L2Kh
4λ

(cid:107)πT

X − πT

U (cid:107)2

TV.

(S46)

(S47)

By applying the data processing and Pinsker inequalities, we obtain the desired result.

4Note that Lemma 3.2 of (Raginsky et al., 2017) considers the case where the drift is not time- or measure-dependent. However, with

HS3 it is easy to show that the same result holds for our case as well.

2.1. Proof of Theorem 3

Sliced-Wasserstein Flows

Here, we precise the statement of Theorem 3.
Theorem S7. Assume that the assumptions in Lemma S1 and Lemma S2 hold. Then for λ > KL2h
holds:

8

, the following bound

(cid:107)¯µKh − µT (cid:107)2

TV ≤ δλ

(cid:40)

L2K
2λ

(cid:16) C1h3
3

+ 3λdh2(cid:17)

+

(cid:41)
,

C2δKh
4λ

where δλ = (1 − KL2h

8λ )−1.

Proof. We have the following decomposition: (with T = Kh)

(cid:107)πT

X − πT

U (cid:107)2

TV ≤ 2(cid:107)πT
L2K
2λ

≤

(cid:16)

≤

1 −

Y − πT

Y (cid:107)2
X − πT
(cid:16) C1h3
3
KL2h
8λ

TV + 2(cid:107)πT
+ 3λdh2(cid:17)
(cid:17)−1(cid:40)

L2K
2λ

+

U (cid:107)2
TV
C2δKh
4λ
(cid:16) C1h3
3

+

L2Kh
8λ
+ 3λdh2(cid:17)

+

(cid:107)πT

X − πT
U (cid:107)2
TV
(cid:41)
.

C2δKh
4λ

(S48)

(S49)

(S50)

(S51)

The second line follows from Lemma S1 and Lemma S2. Last line follows from the assumption that λ is large enough. This
completes the proof.

3. Proof of Corollary 1

Proof. Considering the bound given in Theorem 3, the choice h implies that

δλL2K
2λ

(cid:16) C1h3
3

+ 3λdh2(cid:17)

≤ ε2.

(S52)

This ﬁnalizes the proof.

4. Additional Experimental Results

4.1. The Sliced Wasserstein Flow

The whole code for the Sliced Wasserstein Flow was implemented in Python, for use with Pytorch5. The code was written
so as to run efﬁciently on GPU, and is available on the publicly available repository related to this paper6.

In practice, the SWF involves relatively simple operations, the most important being:

• For each random θ ∈ {θn}n=1...Nθ , compute its inner product with all items from a dataset and obtain the empirical

quantiles for these projections.

• At each step k of the SWF, for each projection z = (cid:10)θ, ¯X i

(cid:11), apply two piece-wise linear functions, corresponding to

k

the scalar optimal transport ψ(cid:48)

k,θ(z).

Even if such steps are conceptually simple, the quantile and required linear interpolation functions were not available on
GPU for any framework we could ﬁgure out at the time of writing this paper. Hence, we implemented them ourselves for
use with Pytorch, and the interested reader will ﬁnd the details in the Github repository dedicated to this paper.

Given these operations, putting a SWF implementation together is straightforward. The code provided allows not only to
apply it on any dataset, but also provides routines to have the computation of these sketches running in the background in a
parallel manner.

5http://www.pytorch.org.
6https://github.com/aliutkus/swf.

Sliced-Wasserstein Flows

Figure S1. The evolution of SWF through 15000 iterations, when the original high-dimensional data is kept instead of working on reduced
bottleneck features as done in the main document. Showing results on the MNIST and FashionMNIST datasets. For a visual comparison
for FashionMNIST, we refer the reader to (Samangouei et al., 2018).

4.2. The need for dimension reduction through autoencoders

In this study, we used an autoencoder trained on the dataset as a dimension reduction technique, so that the SWF is applied
to transport particles in a latent space of dimension d ≈ 50, instead of the original d > 1000 of image data.

The curious reader may wonder why SWF is not applied directly to this original space, and what performances should be
expected there. We have done this experiment, and we found out that SWF has much trouble rapidly converging to satisfying
samples. In ﬁgure S1, we show the progressive evolution of particles undergoing SWF when the target is directly taken as
the uncompressed dataset.

In this experiment, the strategy was to change the projections θ at each iteration, so that we ended up with a set of projections
being {θn,k}k=1...K
instead of the ﬁxed set of Nθ we now consider in the main document (for this, we picked Nθ = 200).
n=1...Nθ
This strategy is motivated by the complete failure we observed whenever we picked such ﬁxed projections throughout
iterations, even for a relatively large number as Nθ = 16000.

As may be seen on Figure S1, the particles deﬁnitely converge to samples from the desired datasets, and this is encouraging.
However, we feel that the extreme number of iterations required to achieve such convergence comes from the fact that theory
needs an integral over the d−dimensional sphere at each step of the SWF, which is clearly an issue whenever d gets too
large. Although our solution of picking new samples from the sphere at each iteration alleviated this issue to some extent,
the curse of dimensionality prevents us from doing much better with just thousands of random projections at a time.

Sliced-Wasserstein Flows

Figure S2. Approximately computed SW2 between the output ¯µN
dimensions d for the bottleneck features (and the corresponding pre-trained AE).

k and data distribution ν in the MNIST experiment for different

This being said, we are conﬁdent that good performance would be obtained if millions of random projections could
be considered for transporting such high dimensional data because i/ theory suggests it and ii/ we observed excellent
performance on reduced dimensions.

However, we, unfortunately, did not have the computing power it takes for such large scale experiments and this is what
motivated us in the ﬁrst place to introduce some dimension-reduction technique through AE.

4.3. Structure of our autoencoders for reducing data dimension

As mentioned in the text, we used autoencoders to reduce the dimensionality of the transport problem. The structure of these
networks is the following:

• Encoder Four 2d convolution layers with (num chan out, kernel size, stride, padding) being (3, 3, 1, 1), (32, 2, 2, 0),
(32, 3, 1, 1), (32, 3, 1, 1), each one followed by a ReLU activation. At the output, a linear layer gets the desired
bottleneck size.

• Decoder A linear layer gets from the bottleneck features to a vector of dimension 8192, which is reshaped as
(32, 16, 16). Then, three convolution layers are applied, all with 32 output channels and (kernel size, stride, panning)
being respectively (3, 1, 1), (3, 1, 1), (2, 2, 0). A 2d convolution layer is then applied with an output number of channels
being that of the data (1 for black and white, 3 for color), and a (kernel size, stride, panning) as (3, 1, 1). In any case,
all layers are followed by a ReLU activation, and a sigmoid activation is applied a the very output.

Once these networks deﬁned, these autoencoders are trained in a very simple manner by minimizing the binary cross entropy
between input and output over the training set of the considered dataset (here MNIST, CelebA or FashionMNIST). This
training was achieved with the Adam algorithm (Kingma & Ba, 2014) with learning rate 1e − 3.

No additional training trick was involved as in Variational Autoencoder (Kingma & Welling, 2013) to make sure the
distribution of the bottleneck features matches some prior. The core advantage of the proposed method in this respect is
indeed to turn any previously learned AE as a generative model, by automatically and non-parametrically transporting
particles drawn from an arbitrary prior distribution µ to the observed empirical distribution ν of the bottleneck features over
the training set.

4.4. Convergence plots of SWF

In the same experimental setting as in the main document, we also illustrate the behavior of the algorithm for varying
dimensionality d for the bottleneck-features. To monitor the convergence of SWF as predicted by theory, we display the

02004006008001000Number of Iterations (k)102101100101102Sliced-Wasserstein Lossd8032644816Sliced-Wasserstein Flows

Figure S3. The evolution of SWF through 200 iterations on the MNIST dataset. Plots are for 1, 11, 21, 31, 41, 51, 101 and 201 iterations

approximately computed SW2 distance between the distribution of the particles and the data distribution. Even though
minimizing this distance is not the real objective of our method, arguably, it is still a good proxy for understanding the
convergence behavior.

Figure S2 illustrates the results. We observe that, for all choices of d, we see a steady and smooth decrease in the cost for all
runs, which is in line with our theory. The absolute value of the cost for varying dimensions remains hard to interpret at this
stage of our investigations.

5. Additional samples

5.1. Evolution throughout iterations

In Figures S3 and S4 below, we provide the evolution of the SWF algorithm on the Fashion MNIST and the MNIST datasets
in higher resolution, for an AE with d = 48 bottleneck features.

5.2. Training samples, interpolation and extrapolation

In Figures S5 and S6 below, we provide other examples of outcome from SWF, both for the MNIST and the FashionMNIST
datasets, still with d = 48 bottleneck features.

The most noticeable fact we may see on these ﬁgures is that while the actual particles which went through SWF, as well
as linear combinations of them, all yield very satisfying results, this is however not the case for particles that are drawn
randomly and then brought through a pre-learned SWF.

Once again, we interpret this fact through the curse of dimensionality: while we saw in our toy GMM example that using a
pre-trained SWF was totally working for small dimensions, it is already not so for d = 48 and only 3000 training samples.

Sliced-Wasserstein Flows

Figure S4. The evolution of SWF through 200 iterations on the FashionMNIST dataset. Plots are for 1, 11, 21, 31 (upper row) and 41, 51,
101, 201 (lower row) iterations

Sliced-Wasserstein Flows

(a) particles undergoing SWF

(b) After SWF is done: applying learned
map on linear combinations of train parti-
cles

(c) After SWF is done: applying learned
map on random inputs.

Figure S5. SWF on MNIST: training samples, interpolation in learned mapping, extrapolation.

This noticed, we highlight that this generalization weakness of SWF for high dimensions is not really an issue, since it is
always possible to i/ run SWF with more training samples if generalization is required ii/ re-run the algorithm for a set of
new particles. Remember indeed that this does not require passing through the data again, since the distribution of the data
projections needs to be done only once.

Sliced-Wasserstein Flows

(a) particles undergoing SWF

(b) After SWF is done: applying learned
map on linear combinations of train parti-
cles

(c) After SWF is done: applying learned
map on random inputs.

Figure S6. SWF on FashionMNIST: training samples, interpolation in learned mapping, extrapolation.

