Graph-based keyword search in heterogeneous data
sources
Angelos Christos Anadiotis, Mhd Yamen Haddad, Ioana Manolescu

To cite this version:

Angelos Christos Anadiotis, Mhd Yamen Haddad, Ioana Manolescu. Graph-based keyword search in
heterogeneous data sources. BDA 2020 - 36ème Conférence sur la Gestion de Données – Principes,
Technologies et Applications, Oct 2020, Online, France. ￿hal-02934277￿

HAL Id: hal-02934277

https://inria.hal.science/hal-02934277

Submitted on 9 Sep 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Graph-based keyword search in heterogeneous data sources

Angelos Christos Anadiotis1, Mhd Yamen Haddad2, Ioana Manolescu2
1Ecole Polytechnique and Institut Polytechnique de Paris, 2Inria and Institut Polytechnique de Paris,
{name.surname} @ 1polytechnique.edu, 2inria.fr

ABSTRACT
Data journalism is the field of investigative journalism which fo-
cuses on digital data by treating them as first-class citizens. Follow-
ing the trends in human activity, which leaves strong digital traces,
data journalism becomes increasingly important. However, as the
number and the diversity of data sources increase, heterogeneous
data models with different structure, or even no structure at all,
need to be considered in query answering.

Inspired by our collaboration with Le Monde, a leading French
newspaper, we designed a novel query algorithm for exploiting
such heterogeneous corpora through keyword search. We model
our underlying data as graphs and, given a set of search terms,
our algorithm finds links between them within and across the
heterogeneous datasets included in the graph. We draw inspiration
from prior work on keyword search in structured and unstructured
data, which we extend with the data heterogeneity dimension,
which makes the keyword search problem computationally harder.
We implement our algorithm and we evaluate its performance using
synthetic and real-world datasets.

1 INTRODUCTION
Data analysis is increasingly important for several organizations
today, as it creates value by drawing meaningful insights from the
data. As we are moving towards large data lakes installations where
huge amounts of data are stored, the opportunities for important
discoveries are growing; unfortunately, on par with the useless
information. Moreover, the data to be processed is often stored
in different formats, ranging from fully and semi-structured, to
completely unstructured, like free text. Accordingly, the challenges
in processing all this data that is available today, reside in both
expressing and answering queries.

Research in heterogeneous data processing has proposed several
approaches in addressing the above challenges. On the one side,
massively parallel processing systems like Spark [33], Hive [28]
and Pig [26] provide connectors for heterogeneous data sources
and allow the execution of data analysis tasks on top of them, us-
ing either a platform-specific API or a query language like SQL.
Polystore-based approaches [3, 9, 12] focus more on the data model
and the query planning and optimization on top of heterogeneous
data stores. Finally, the so-called just-in-time (JIT) data virtualiza-
tion approach generates the query engine at runtime based on the
data format [20, 21]. All these works consider that users, typically
data scientists, already know what they are looking for, and they
express it either using a powerful query language or a rich API.

However, today the data analysis paradigm has shifted and a
central point is to find parts of the data which feature interesting
patterns. The patterns may not be known at query time; instead,
users may have to discover them through a process of trial and er-
ror. A popular query paradigm in such a context is keyword search.

A staple of Information Retrieval in data with little or no struc-
ture, keyword search has been applied also on relational, XML
or graph data, when users are unsure of the structure and would
like the system to identify possible connections. In this work, we
model a set of heterogeneous data sources as a graph, and focus on
answering queries asking for connections among the nodes
of the graph which are of interest to the users. This work is
inspired from our collaboration with Les Décodeurs, Le Monde’s
fact-checking team1, within the ContentCheck collaborative re-
search project2. Our study is novel with respect to the state of the
art (Section 6) as we are the first to consider that an answer may
span over multiple datasets of different data models, with very dif-
ferent or even absent internal structure, e.g., text data. For instance,
a national company registry is typically relational, contracts or
political speeches are text, social media content typically comes as
JSON documents, and open data is often encoded in RDF graphs.
Integrated graph preserving all original nodes In the data jour-
nalism context mentioned above, it is important to be able to show
where each piece of information in an answer came from, and how the
connections were created. This is a form of provenance, and can also
be seen as result explanation. Therefore, the queried graph needs
to preserve the identity of each node from the original sources. At
the same time, to enable interesting connections, we: (i) extract
several kinds of meaningful entities from all the data sources of all
kinds; (ii) interconnect data sources that comprise the same entity,
or very similar ones, through so-called sameAs. Both extraction and
similarity produce results with some confidence, a value between 0
and 1, thus, some edges in our graph have can be seen as uncertain
(but quite likely).
No help from a score function An important dimension of key-
word search problems is scoring, i.e., how do we evaluate the inter-
estingness of a given connection (or query result). This is important
for two reasons. First, in many scenarios, the number of results is
extremely large, users can only look at a small number of results,
say k. Second, some answer score measures have properties that
can help limit the search, by allowing to determine that some of
the answers not explored yet would not make it into the top k. Un-
fortunately, while desirable from an algorithmic perspective (since
they simplify the problem), such assumptions on the cost model
are not always realistic from a user perspective, as we learned by
exchanging with journalists; we detail this in Section 3.
Bidirectional search All edges in our graph are directed, e.g., from
the subject to the object in an RDF graph, from the parent to the
child in a hierarchical document etc., and, in keeping with our
goal of integral source preservation, we store the edge direction
in the graph. However, we allow answer trees to traverse edges
in any direction, since heterogeneous data sources may model the

1http://www.lemonde.fr/les-decodeurs/
2https://team.inria.fr/cedar/contentcheck/

BDA 2020, October 2020, Paris, France

A. C. Anadiotis, M. Y. Haddad and I. Manolescu

same information either, say, of the form Alice

wrote
−−−−−→ Paper1 or
hasAuthor
Paper1
−−−−−−−−→ Alice; since users are unfamiliar with the data,
they should not be penalized for not having “guessed” correctly
the edge directions. This is in contrast with many prior works (see
Section 6) which define answers as a tree where from the root, a
node matching each keyword is reached by traversing edges in their
original direction only. For instance, assume the graph comprises
wrote
−−−−−→ p1. With a restricted notion of answers,
a1
the query {a1 a2} has no answer; in contrast, in our approach, the
answer connecting them through p1 is easily found. Bidirectional
search gives a functional advantage, but makes the search more
challenging: in a graph of |E| edges, the search space is multiplied
by 2 |E |.

wrote
−−−−−→ p1 and a2

The contributions made in this work are as follows:

• We formalize the problem of bidirectional keyword search
on graphs as described above, built from a combination of
data sources.

• With respect to scoring, we introduce a general score func-
tion that can be extended and customized to reflect all
interesting properties of a given answer. We show that
this generality, together with the possibility of confidence
lower than 1.0 on some edges, does not enable search to
take advantage of simplifying assumptions made in prior
work.

• We propose a complete (if exhaustive) algorithm for solving
the keyword search problem in this context, as well as some
original pruning criteria arising specifically in the context
of our graphs. Given the usually huge search space size, a
practical use of this algorithm is to run it until a time-out
and retain the best answers found.

• We have implemented our algorithm and present a set of

experiments validating its practical interest.

A previous version of our system had been demonstrated in [5].
Since then, we have completely re-engineered the graph construc-
tion (this is described in the companion paper [4]), deepened our
analysis of the query problem, and proposed a new algorithm, de-
scribed in the present work; this also differs from (and improves
over) our previous technical report [7].

2 OUTLINE AND PROBLEM STATEMENT
In this section, we formalize our keyword search problem over
a graph that we build by integrating data from various datasets,
organized in different data models.

2.1 Integrated graph
We consider a set M of data models: relational (including SQL
databases, CSV files etc.), RDF, JSON, HTML, XML, and text. A
dataset D is an instance of one of these data models3.

From a set D = {D1, D2, . . . , Dn } of datasets, we create an in-
tegrated graph G = (N , E), where N is the set of nodes and E the
set of edges. For instance, consider the dataset collection shown in
Figure 1. Starting from the top left, in clockwise order, it shows: a

3Our graph can also integrate other kinds of files, in particular PDF documents and
spreadsheet files, by converting them to one or several instances of the above data
models; as this is orthogonal wrt this paper, we delegate those details to [4].

table with assets of public officials, a JSON listing of France elected
officials, an article from the newspaper Libération with entities
highlighted, and a subset of the DBPedia RDF knowledge base.

Figure 2 shows the graph produced from the datasets in Figure 1.

There are several observations to be made on this graph:

(i) The graph comprises four dataset nodes (the ones filled with

yellow), one for each data source.

(ii) All the internal structure present in the input datasets is pre-
served in the graph: each RDF node became a node in the integrated
graph, and each triple became an edge. A node is created for each
map, array, and value in the JSON document. A node is created
from each tuple, and from each attribute in the relational databases.
Finally, a single node is created from the whole text document,
which has no internal structure. When a text consists of more than
one phrase, we segment it as a sequence of phrases, each of which is
a node (child of the dataset node) to avoid overly large nodes that
are hard to interpret by users.

(iii) Entity nodes (rounded-corners blue boxes) are extracted
using Information Extraction (IE) techniques. Thus, in the example,
nodes labeled “P. Balkany”, “I. Balkany” are recognized as People,
“Levallois-Perret” and “Centrafrique” are recognized as Locations,
while “Areva” is an Organization. An extracted entity is added to
the graph as a child of the node (leaf in an XML, HTML, JSON or
text document; attribute value from a relational dataset; or RDF
literal) from which it has been extracted.

(iv) Equivalence edges (solid red edges in Figure 1) connect nodes
found in different datasets which are considered to refer to the same
real-world entity. For instance, the three occurrences of “P. Balkany”
are pairwise connected by edges with a confidence of 1.0. The confi-
dence of the edges derived directly from the datasets, as explained
above, is 1.0; we do not show it in the figure to avoid clutter. We
say nodes connected by equivalence edges are equivalent.

(v) Similarity edges (dotted, curved red edge between “Central
African Republic” and “Centrafrique” in Figure 1) connect nodes
which are considered strongly similar but not equivalent. In our
example, the two nodes have a similarity of 0.85, which is attached
to the edge as confidence.

2

For efficiency, when k nodes are equivalent, we do not consider
all the k (k −1)
edges; instead, one of the nodes (the first to be added
to the graph - any other choice could be made) is designated the
representative of all of them, and we store associated with each
node, the ID of its representative.

The purpose of the equivalence and similarity edges is to inter-
connect nodes within and across the datasets; entity extraction
prepares the ground for the same, since it creates nodes that may
co-occur across data sources, e.g., entities mentioned in separate
texts, such as “P. Balkany” in the figure. This increases the value
and usefulness of the graph, since it allows to find connections
which cannot be established based on any dataset taken separately.
For instance, consider the question: «What connections exist be-
tween “I. Balkany”, “Africa”, and “real estate”?» This can be asked
as a three-keyword query {“I. Balkany”, “Africa", “Estate”}, for
which an answer (a tree composed of graph edges) is shown as
a light green highlight in Figure 1; the three nodes matching the
respective keywords are shown in bold. This answer interconnects
all four data sources.

Graph-based keyword search in heterogeneous data sources

BDA 2020, October 2020, Paris, France

Figure 1: Sample dataset collection D.

Figure 2: Integrated graph corresponding to the datasets of Figure 1.

We formalize this keyword search query problem below.

2.2 Search problem
Given our graph G = (N , E), we denote by L the set of all the
labels of G nodes, plus the special constant ϵ denoting the empty
label. We denote by λ(·) a function assigning to each node and edge
a label, which may be empty. As illustrated in Figure 2, internal
nodes, which correspond, e.g., to a relational tuple, or to a JSON
map or array, have an empty label.

Let W be the set of keywords, obtained by stemming the label
set L; a search query is a set of keywords Q = {w1, ..., wm }, where
wi ∈ W . We define an answer tree (AT, in short) as a set t of G

edges which (i) together, form a tree (each node is reachable from
any other through exactly one path), (ii) for each wi , contain at least
one node whose label matches wi . Here, the edges are considered
b
c
undirected, that is: n1
−→ n4 is a sample AT, such
←− n3
that for all wi ∈ Q, there is a node ni ∈ t such that wi ∈ λ(ni ).

a
−→ n2

We treat the edges of G as undirected when defining the AT in
order to allow more query results, on a graph built out of hetero-
geneous content whose structure is not well-known to users. For
instance, consider a query consisting of the keywords k1, k4 such
that k1 ∈ λ(n1) and k4 ∈ λ(n4) on the four-nodes sample AT intro-
duced above. If our ATs were restricted to the original direction

BDA 2020, October 2020, Paris, France

A. C. Anadiotis, M. Y. Haddad and I. Manolescu

of G edges, the query would have no answer; ignoring the edge
directions, it has one. One could easily extend the definition and
the whole discussion in order to allow matches to also occur on
edges (just enlarge L to also include the stemmed edge labels).
Further, we are interested in minimal answer trees, that is:

(1) Removing an edge from the tree should make it lack one

or more of the query keywords wi .

(2) If a query keyword wi matches the label of more than one
nodes in the answer tree, then all these matching nodes
must be equivalent.

Condition (2) is specific to the graph we consider, originating in
several data sources connected by equivalence or similarity edges. In
classical graph keyword search problems, each query keyword is
matched exactly once in an answer (otherwise, the tree is considered
non-minimal). In contrast, our answer trees may need to traverse
equivalence edges, and if wi is matched by one node connected by
such an edge, it is also matched by the other. For instance, consider
the three-keyword query “Gyucy Balkany Levallois” in Figure 2: the
keyword Balkany is matched by the two nodes labeled “P. Balkany”
which are part of the answer.

As a counter-example to condition (2), consider the query “Balkany

Centrafrique” in Figure 2, assuming the keyword Centrafrique is
also matched in the label “Central African Republic”4. Consider
the tree that connects a “P. Balkany” node with “Centrafrique”,
and also traverses the edge between “Centrafrique” and “Central
African Republic”: this tree is not minimal, thus it is not an answer.
The intuition for rejecting it is that “Centrafrique” and “Central
African Republic” may or may not be the same thing (we have a
similarity, not an equivalence edge), therefore the query keyword
“Centrafrique” is matched by two potentially different things in this
answer, making it hard to interpret.

A direct consequence of minimality is that in an answer, each

and every leaf matches a query keyword.

Several minimal answer trees may exist in G for a given query.
We consider available a scoring function which assigns a higher
value to more interesting answer trees (see Section 3). Thus, our
problem can be stated as follows:

Problem statement Given the graph G built out of the
datasets D and a query Q, return the k highest-score minimal
answer trees.

An AT may potentially span over the whole graph, (also) because
it can traverse G edges in any direction; this makes the problem
challenging.
Discussion: degraded answers. In some cases, a query may have
no answer (as defined above) on a given graph, yet if one is willing
to drop the second condition concerning nodes matching the same
query keyword, an answer tree could be found. For instance, con-

l
−→ b1

m
−−→ b2

n
−→ c1, such that b1 is not
sider a graph of the form a1
equivalent to b2, and the query {a, b, c}, such that the keyword a
matches the node a1, b matches b1 and b2 and c matches c1. Given
our definition of answers above, this query has no answer, because
b matches the two nodes b1 and b2.

4This may be the case using a more advanced indexing system that includes some
natural language understanding, term dictionaries etc.

If we removed condition (2), we could accept such an answer,
which we call degraded, since it is harder to interpret for users
(lacking one clearly identified node for each keyword). One could
then generalize our problem statement into: (i) solve the problem
stated above, and (ii) only if there are no answers, find the top-
k degraded answers (if they exist). We do not pursue degraded
answer search further in this paper, and focus instead on finding
those defined above.

2.3 Search space and complexity
The problem that we study is related to the (Group) Steiner Tree
Problem, which we recall below.

Given a graph G with weights (costs) on edges, and a set of
m nodes n1, . . . , nm , the Steiner Tree Problem (STP) [14] consists
of finding the smallest-cost tree in G that connects all the nodes
together. We could answer our queries by solving one STP problem
for each combination of nodes matching the keywords w1, . . . , wm .
However, there are several obstacles left: ((cid:5)) STP is a known NP-hard
problem in the size of G, denoted |G |; ((cid:66)) as we consider that each
edge can be taken in the direct or reverse direction, this amounts to
“doubling” every edge in G. Thus, our search space is 2|G | larger
than the one of the STP, or that considered in similar works,
discussed in Section 6. This is daunting even for small graphs of a
few hundred edges; ((cid:67)) we need the k smallest-cost trees, not just
one; (◦) each keyword may match several nodes, not just one.

The closely related Group STP (GSTP, in short) [14] is: given m
sets of nodes from G, find the minimum-cost subtree connecting one
node from each of these subtrees. GSTP does not raise the problem
(◦), but still has all the others.

In conclusion, the complexity of the problem we consider is
extremely high. Therefore, solving it fully is unfeasible for large
and/or high-connectivity graphs. Instead, our approach is:

• Attempt to find all answers from the smallest (fewest edges)
to the largest. Enumerating small trees first is both a prac-
tical decision (we use them to build larger ones) and fits
the intuition that we shouldn’t miss small answers that a
human could have found manually. However, as we will
explain, we still “opportunistically” build some trees before
exhausting the enumeration of smaller ones, whenever this
is likely to lead faster to answers. The strategy for choos-
ing to move towards bigger instead of smaller tress leaves
rooms for optimizations on the search order.

• Stop at a given time-out or when m answers have been found,

for some m ≥ k;

• Return the k top-scoring answers found.

3 SCORING ANSWER TREES
We now discuss how to evaluate the quality of an answer. Sec-
tion 3.1 introduces the general notion of score on which we base
our approach. Section 3.2 describes one particular metric we attach
to edges in order to instantiate this score, finally Section 3.3 details
the actual score function we used.

3.1 Generic score function
We have configured our problem setting to allow any scoring func-
tion, which enables the use of different scoring schemes fitting the

Graph-based keyword search in heterogeneous data sources

BDA 2020, October 2020, Paris, France

requirements of different users. As a consequence, this approach
allows us to study the interaction of the scoring function with
different properties of the graph. For instance, we are currently
investigating the possibility to learn what makes an answer inter-
esting for a user, so that we may return customized answers to each
user.

Given an answer tree t to a query Q, we consider a score function

consisting of (at least) the following two components:

• The matching score ms(t), which reflects the quality of the
answer tree, that is, how well its leaves match the query
terms.

• The connection score cs(t), which reflects the quality of the
tree connecting the edges. Any formula can be used here,
considering the number of edges, the confidence or any
other property attached to edges, or a query-independent
property of the nodes, such as their PageRank or between-
ness centrality score etc.

The score of t for Q, denoted s(t), is computed as a combina-
tion of the two independent components ms(t) and cs(t). Popular
combinations functions (a weighted sums, or product etc.) are mo-
notonous in both components, however, our framework does not
require it. Finally, both ms(t) and cs(t) can be tuned based on a
given user’s preferences, to personalize the score, or make them
evolve in time through user feedback etc.

3.2 Edge specificity
We now describe a metric on edges, which we used (through the
connection score cs(t)) to favor edges that are “rare” for both nodes
they connect. This metric was inspired by our experiments with
real-world data sources, and it helped return interesting answer
trees in our experience.

For a given node n and label l, let N l

edges entering n, and N l
The specificity of an edge e = n1

→n

be the number of l-labeled
n→ the number of l-labeled edges exiting n.
l
−→ n2 is defined as:
→n2 ).

s(e) = 2/(N l

n1→ + N l
s(e) is 1.0 for edges that are “unique” for both their source and
their target, and decreases when the edge does not “stand out”
among the edges of these two nodes. For instance, the city council
of Levallois-Perret comprises only one mayor (and one individual
cannot be mayor of two cities in France, because he has to inhabit
the city where he runs for office). Thus, the edge from the city
council to P. Balkany has a specificity of 2/(1.0 + 1.0) = 1.0. In
contrast, there are 54 countries in Africa (we show only two), and
each country is in exactly one continent; thus, the specificity of the
dbo:partOf edges in the DBPedia fragment, going from the node
named Morocco (or the one named Central African Republic) to
the node named Africa is 2/(1 + 54) (cid:39) .036.

Specificity computation. When registering the first dataset
D1, computing the specificity of its edges is trivial. However, when
registering subsequent datasets D2, D3 etc., if some node, say n2 ∈
D2 is found to be equivalent to a node n1 ∈ D1, all the D1 edges
adjacent to n1 and the D2 edges adjacent to n2 should be reflected
in the specificity of each of these edges. Thus, in particular, the
specificity of D1 edges needs to be recomputed when a node in a
source added after D1 is equivalent to one of its nodes.

. . .

. . .

es1

n1

b

c

l

l

l

. . .

x

. . .

. . .

. . .

l

l

. . .

d

n2

Figure 3: Illustration for specificity (re)computation. The

l
specificity of the edge x
−→ n1, s(e) is initially computed out
of the blue edges; when n2 joins the equivalence set es1, it is
recomputed to also reflect the violet edges.

A naïve approach would be: when the edges of D2 are traversed
(when we add this dataset to the graph), re-traverse the edges of n1
in D1 in order to (re)compute their specificity. However, that would
be quite inefficient.

→•, respectively N e

Instead, below, we describe an efficient incremental algorithm to
compute specificity. We introduce two notations. For any edge e,
we denote N e
◦→, the two numbers out of which
the specificity of e has been most recently computed5. Specifically,
→• counts l-labeled edges incoming to the target of e, while N e
N e
◦→
counts l-labeled edges outgoing the source of e. In Figure 3, if e is
◦→ = 1, thus
the edge x
s(e) = 2/4 = .5.

→• = 3 (blue edges) and N e

l
−→ n1, then N e

Let n1 ∈ D1 be a node, es1 be the set of all nodes equivalent to
n1, and n2 ∈ D2 be a node in a dataset we currently register, and
which has just been found to be equivalent to n1, also.

Further, let l be a label of an edge incoming or outgoing (any)
node from es1, and/or n2. We denote by N l
→n )
n→); they are the num-
and similarly by N l
bers of l-labeled outgoing (resp., incoming) l-labeled edges of any
node in es1. When n2 joins the equivalence set es1 of n1 (see Fig-
ure 3):

es1→ the sum (cid:205)

→es1
n ∈es1 (N l

n ∈es1 (N l

the sum (cid:205)

→n2

→es1

(1) If N l

(cid:44) 0 and N l

(cid:44) 0, the specificity of every l-
labeled edge e incoming either a node in es1 or the node n2
must be recomputed.
Let e be such an incoming edge labeled l. When n2 is
→• +
added to the set es1, the specificity of e becomes 2/((N e
→n2 ) + N e
◦→), to reflect that n2 brings more incoming
N l
l-labeled edges. This amounts to 2/(3 + 2 + 1) = .33 in Fig-
ure 3: the violet edges have joined the blue ones. Following
this adjustment, the numbers out of which e’s specificity
has been most recently computed are modified as follows:
, thus 3 + 2 = 5 in Figure 3;
N e
N e
(2) If N l

→• + N l
→• becomes N e
◦→ remains unchanged.
→n2

(cid:44) 0, the specificity of every l-
labeled edge e incoming n2 does not change when n2 joins
the equivalence set es1.

= 0 and N l

→es1

→n2

(3) If N l

→es1

(cid:44) 0 and N l

= 0, the newly added node n2
does not change the edges adjacent to the nodes of es1, nor
their specificity values.

→n2

es1→ (cid:44) 0 and N l

The last two cases, when N l
es1→ = 0 and N l
N l
The above method only needs, for a given node n2 newly added
to the graph, and label l, the number of edges adjacent to n2 in its
dataset, and the number of l edges adjacent to a node equivalent

n2→ (cid:44) 0, are handled in a similar manner.

n2→ (cid:44) 0, respectively,

5This can be either during the first specificity computation of e, or during a recompu-
tation, as discussed below.

BDA 2020, October 2020, Paris, France

A. C. Anadiotis, M. Y. Haddad and I. Manolescu

to n2. Unlike the naïve specificity computation method, it does not
need to actually traverse these edges previously registered edges,
making it more efficient.

Concretely, for each edge e ∈ E, we store three attributes: N e
→•, N e
◦→ and s, the last-computed specificity, and we update N e

→•,
◦→

N e
as explained above.

3.3 Concrete score function
In our experiments, we used the following score function.

For an answer t to the query Q, we compute the matching score
ms(t) as the average, over all query keywords wi , of the similarity
between the t node matching wi and the keyword wi itself; we used
the edit distance.

We compute the connection score cs(t) based on edge confidence,
on one hand, and edge specificity on the other. We multiply the
confidence values, since we consider that uncertainty (confidence
< 1) multiplies; and we also multiply the specificities of all edges in
t, to discourage many low-specificity edges. Specifically, our score
is computed as:
score(t, Q) = α · ms(t, Q) + β · (cid:206)
e ∈E s(e)
where α, β are parameters of the system such that 0 ≤ α, β < 1 and
α + β ≤ 1.

e ∈E c(e) + (1 − α − β) · (cid:206)

3.4 Orthogonality between the score and the

algorithm

Before we describe the search algorithm, we make a few more
remarks on the connection between the score function and the
search algorithm.

We start by considering the classical Steiner Tree and Group
Steiner Tree Problems (Section 2.3). These assume that the tree cost
is monotonous, that is: for any query Q and all trees T , ˆT where
T is a subtree of ˆT it follows that the cost of T is higher (in our
terminology, its score is lower) than the cost of ˆT . This is naturally
satisfied if the cost is the addition of edge weights. In contrast, the
score, in its general form (Section 3.1), and in particular our concrete
one (Section 3.3), is not monotonous, as illustrated in Figure 4,
where on each edge, c is the confidence and s is the specificity.
Denoting T the four-edge tree rooted in n1, the connection score
cs(T ) = β +(1−α −β)·(.5)4, while cs(T (cid:48)) = β ·.5+(1−α −β)(.5)4 ·.25.
If we assume α = β = 1
3 , then cs(T ) = 1
3 (1 + (.5)4) (cid:39) .35 while
cs(T (cid:48)) = 1
3 · (.5 + (.5)5) (cid:39) .17, which is clearly smaller. Assuming T (cid:48)
has the same matching score as T , the global score of T (cid:48) is smaller
than that of T , contradicting the monotonicity assumption.

Another property sometimes assumed by score functions is the
called optimal substructure, that is: the best solution for a prob-
lem of size p is part of the best solution for a problem of size p + 1
that is an extension of p, for some problem size p. When this holds,
the problem can be efficiently solved in a dynamic programming
fashion. However, STP does not enjoy this property: the smallest-
cost tree connecting two nodes n1, n2 is not necessarily part of
the smallest-cost tree that connects n1, n2, n3 (and the same holds
for GSTP). Some existing algorithms also assume a variant of the
optimal substructure property (see Section 6). In contrast, our score
function (both in its general and its concrete form) does not ensure
such favorable properties. This is why the search algorithm we

describe next has to find as many answers as possible, as quickly
as possible.

4 ANSWERING KEYWORD QUERIES
We now present our approach for computing query answers, based
on the integrated graph.

4.1 Grow and Merge
Our first algorithm uses some concepts from the prior literature [10,
18] while exploring many more trees. Specifically, it starts from the
sets of nodes N1, . . . , Nm where the nodes in Ni all match the query
keyword wi ; each node ni, j ∈ Ni forms a one-node partial tree. For
instance, in Figure 2, one-node trees are built from the nodes with
boldface text, labeled “Africa”, “Real Estate” and “I. Balkany”. Two
transformations can be applied to form increasingly larger trees,
working toward query answers:

• Grow(t, e), where t is a tree, e is an edge adjacent to the root
of t, and e does not close a loop with a node in t, creates
a new tree t (cid:48) having all the edges of t plus e; the root of
the new tree is the other end of the edge e. For instance,
starting from the node labeled “Africa”, a Grow can add
the edge labeled dbo:name.

• Merge(t1, t2), where t1, t2 are trees with the same root,
whose other nodes are disjoint, and matching disjoint sets
of keywords, creates a tree t (cid:48)(cid:48) with the same root and with
all edges from t1 and t2. Intuitively, Grow moves away
from the keywords, to explore the graph; Merge fuses two
trees into one that matches more keywords than both t1
and t2.

In a single-dataset context, Grow and Merge have the following
properties. (дm1) Grow alone is complete (guaranteed to find all
answers) for k = 1, 2 only; for higher k, Grow and Merge together
are complete. (дm2) Using Merge steps helps find answers faster
than using just Grow [18]: partial trees, each starting from a leaf
that matches a keyword, are merged into an answer as soon as they
have reached the same root. (дm3) An answer can be found through
multiple combinations of Grow and Merge. For instance, con-
sider a linear graph n1 → n2 → . . . np and the two-keyword query
{a1, ap } where ai matches the label of ni . The answer is obviously
the full graph. It can be found: starting from n1 and applying p − 1
Grow steps; starting from np and applying p −1 Grow steps; and in
p − 2 ways of the form Merge(Grow(Grow. . . ), Grow(Grow. . . )),
each merging in an intermediary node n2, . . . , np−1. These are all
the same according to our definition of an answer (Section 2.2),
which does not distinguish a root in an answer tree; this follows
users’ need to know how things are connected, and for which the
tree root is irrelevant.

4.2 Adapting to multi-datasets graphs
The changes we brought for our harder problem (bidirectional edges
and multiple interconnected datasets) are as follows.

1. Bidirectional growth. We allow Grow to traverse an edge both
going from the source to the target, and going from the target to the
source. For instance, the type edge from “Real Estate” to <tuple1> is

Graph-based keyword search in heterogeneous data sources

BDA 2020, October 2020, Paris, France

n2

n3

s = .5, c = 1
s = .5, c = 1

n1

s = .5, c = 1
s = .5, c = 1

n4

n5

s = .25, c = .5

s = 1, c = 1

n6

n7

Figure 4: Example (non-monotonicity of the tree score). T is the four-edges tree rooted in n1.

traversed target-to-source, whereas the location edge from <tuple1>
to “Real Estate” is traversed source-to-target.

2. Many-dataset answers. As defined in a single-dataset scenario,
Grow and Merge do not allow to connect multiple datasets. To
make that possible, we need to enable one, another, or both to also
traverse similarity and equivalence edges (shown in solid or dotted
red lines in Figure 2. We decide to simply extend Grow to allow it
to traverse not just data edges, but also similarity edges between
nodes of the same or different datasets. We handle equivalence edges
as follows:

2.a Naïve solution: Grow-to-equivalent. The simplest idea is
to allow Grow to also add an equivalence edge to the root of a tree.
However, this can be very inefficient. Consider three equivalent
nodes m, m(cid:48) and m(cid:48)(cid:48), e.g., the three “P. Balkany” nodes in Figure 2:
a Grow step could add one equivalence edge, the next Grow could
add another on top of it etc. More generally, for a group of p equiva-
lent nodes, from a tree rooted in one of these nodes, 2p trees would
be created just by Grow. In our French journalistic datasets, some
entities, e.g. “France”, are very frequent, leading to high p; explor-
ing 2p subtrees every time we reach a “France” node is extremely
expensive6.

2.b Grow-to-representative To avoid this, we devise a third algo-
rithmic step, called Grow-to-representative (Grow2Rep), as follows.
Let t be a partial tree developed during the search, rooted in a node
n, such that the representative of n (recall Section 2.1) is a node
nr ep (cid:44) n. Grow2Rep creates a new tree by adding to t the edge
≡
−→ nr ep ; this new tree is rooted in nr ep . If n is part of a group
n
of p equivalent nodes, only one Grow2Rep step is possible from t,
to the unique representative of n; Grow2Rep does not apply again
on Grow2Rep(t), because the root of this tree is nr ep , which is its
own representative.

Together, Grow, Grow2Rep and Merge enable finding answers

that span multiple data sources, as follows:

• Grow allows exploring data edges within a dataset, and

similarity edges within or across datasets;

• Grow2Rep goes from a node to its representative when
they differ; the representative may be in a different dataset;
• Merge merges trees with a same root: when that root is
the representative of a group of p equivalent nodes, this al-
lows connecting partial trees, including Grow2Rep results,
containing nodes from different datasets. Thus, Merge can
build trees spanning multiple datasets.

6Note that similarity edges do not raise the same problem, because in our graph we
only have such edges if the similarity between two nodes is above a certain threshold
τ . Thus, if a node n1 is at least τ -similar to n2, and n2 is at least τ -similar to n3, n1
may be at least τ -similar to n3, or not. This leads to much smaller groups of similar
nodes, than the groups of equivalent nodes we encountered.

One potential performance problem remains. Consider again p
equivalent nodes n1, . . . , np ; assume without loss of generality that
their representative is n1. Assume that during the search, a tree ti
is created rooted in each of these p nodes. Grow2Rep applies to
all but the first of these trees, creating the trees t (cid:48)
, all
rooted in n1. Now, Merge can merge any pair of them, and can then
repeatedly apply to merge three, then four such trees etc., as they
all have the same root n1. The exponential explosion of Grow trees,
avoided by introducing Grow2Rep, is still present due to Merge!
We solve this problem as follows. Observe that in an answer, a

3, . . . , t (cid:48)
p

2, t (cid:48)

≡
−→ n2

≡
path of two or more equivalence edges of the form n1
−→ n3
such that a node internal to the path, e.g. n2, has no other adjacent
edge, even if allowed by our definition, is redundant. Intuitively, such
a node brings nothing to the answer, since its neighbors, e.g., n1 and
n3, could have been connected directly by a single equivalence edge,
thanks to the transitivity of equivalence. We call non-redundant an
answer that does not feature any such path, and decide to search
for non-redundant answers only.

The following properties hold on non-redundant answers:

Property 1. There exists a graph G and a k-keyword query Q
such that a non-redundant answer contains k −1 adjacent equivalence
edges (edges that, together, form a single connected subtree).

Figure 5: Sample answer trees for algorithm discussion.

We prove this by exhibiting such an instance. Let G be a graph
of 2k nodes shown in Figure 5 (a), such that all the xi are equiv-
alent, and consider the k-keyword query Q = {a1, . . . , ak } (each
keyword matches exactly the respective ai node). An answer needs
to traverse all the k edges from ai to xi , and then connect the nodes
xi , . . . , xk

; we need k − 1 equivalence edges for this.

Next, we show:

Property 2. Let t be a non-redundant answer to a query Q of k
keywords. A group of adjacent equivalence edges contained in t has
at most k − 1 edges.

We prove this by induction over k. For k = 1, each answer has 1

node and 0 edge (trivial case).

Now, consider this true for k and let us prove it for k + 1. Assume
by contradiction that a non-redundant answer tQ to a query Q of
k + 1 keywords comprises k + 1 adjacent equivalence edges. Let
Q (cid:48) be the query having only the first k keywords of Q, and t (cid:48) be a
subtree of t that is a non-redundant answer to Q (cid:48):

BDA 2020, October 2020, Paris, France

A. C. Anadiotis, M. Y. Haddad and I. Manolescu

• t (cid:48) exists, because t connects all Q keywords, thus also the

• t (cid:48) is non-redundant, because its edges are also in the (non-

Q (cid:48) keywords;

redundant) t.

By the induction hypothesis, t (cid:48) has at most k − 1 adjacent equiva-
lence edges. This means that there are two adjacent equivalent edges
in t \ t (cid:48).

(1) If these edges, together, lead to two distinct leaves of t, then
t has two leaves not in t (cid:48). This is not possible, because by
definition of an answer, t has k + 1 leaves (each matching
a keyword) and similarly t (cid:48) has k leaves.

(2) It follows, then, that the two edges lead to a single leaf of t,
therefore the edges form a redundant path. This contradicts
the non-redundancy of t, and concludes our proof.

Property 2 gives us an important way to control the exponential
development of trees due to p equivalent nodes. Grow, Grow2Rep
and Merge, together, can generate trees with up to k (instead of k−1)
adjacent equivalence edges. This happens because Grow2Rep may
“force” the search to visit the representative of a set of k equivalent
nodes (see Figure 5(b), assuming x1 is the representative of all the
equivalent xi s, and the query {a2, . . . , ak }). The resulting answer
may be redundant, if the representative has no other adjacent edges
in the answer other than equivalence edges. In such cases, in a post-
processing step, we remove from the answer the representative
and its equivalence edges, then reconnect the respective equivalent
nodes using k − 1 equivalence edges. This guarantees obtaining a
non-redundant tree, such as the one in Figure 5(c).

4.3 The GAM algorithm
We now have the basic exploration steps we need: Grow, Grow2Rep
and Merge. In this section, we explain how we use them in our
integrated keyword search algorithm.

We decide to apply in sequence: one Grow or Grow2Rep (see
below), leading to a new tree t, immediately followed by all the
Merge operations possible on t. Thus, we call our algorithm Grow
and Aggressive Merge (GAM, in short). We merge aggressively
in order to detect as quickly as possible when some of our trees,
merged at the root, form an answer.

Given that every node of a currently explored answer tree can be
connected with several edges, we need to decide which Grow (or
Grow2Rep) to apply at a certain point. For that, we use a priority
queue U in which we add (tree, edge) entries: for Grow, with the
notation above, we add the (t, e) pair, while for Grow2Rep, we add
t together with the equivalence edge leading to the representative
of t’s root. In both cases, when a (t, e) pair is extracted from U ,
we just extend t with the edge e (adjacent to its root), leading to a
new tree tG , whose root is the other end of the edge e. Then we
aggressively merge tG with all compatible trees explored so far,
finally we read from the graph the (data, similarity or equivalence)
edges adjacent to tG ’s root and add to U more (tree, edge) pairs to
be considered further during the search. The algorithm then picks
the highest-priority pair in U and reiterates; it stops when U is
empty, at a timeout, or when a maximum number of answers are
found (whichever comes first).

The last parameter impacting the exploration order is the priority
used in U : at any point, U gives the highest-priority (t, e) pair, which
determines the operations performed next.

(1) Trees matching many query keywords are preferable, to go

toward complete query answers;

(2) At the same number of matched keywords, smaller trees

are preferable in order not to miss small answers;

(3) Finally, among (t1, e1), (t2, e2) with the same number of
nodes and matched keywords, we prefer the pair with the
higher specificity edge.

Algorithm details Beyond the priority queue U described above,
the algorithm also uses a memory of all the trees explored, called E.
It also organizes all the (non-answer) trees into a map K in which
they can be accessed by the subset of query keywords that they
match. The algorithm is shown in pseudocode in Figure 6, following
the notations introduced in the above discussion.

While not shown in Figure 6 to avoid clutter, the algorithm only
develops minimal trees (thus, it only finds minimal answers). This is
guaranteed:

• When creating Grow and Grow2Rep opportunities (steps
3 and 4d): we check not only that the newly added does
not close a cycle, but also that the matches present in the
new tree satisfy our minimality condition (Section 2.2).
• Similarly, when selecting potential Mergecandidates (step

4(c)iiiA).

5 EXPERIMENTAL EVALUATION
We implemented our approach in the ConnectionLens proto-
type, available online at https://gitlab.inria.fr/cedar/connectionlens,
which we used to experimentally evaluate the performance of our
algorithms. This section presents the results that we obtained by
using synthetic graphs, which are similar to the real-world datasets
that we have obtained. First, we describe the hardware and soft-
ware setup that we used to run our experiments, and then we give
our findings for various combinations of amount of keywords and
graph sizes.

5.1 Hardware and software setup
We conducted our experiments on a server equipped with 2x10-core
Intel Xeon E5-2640 CPUs clocked at 2.40GHz, and 128GB DRAM.
The graph is constructed following the approach described in [4]
and we used Postgres 9.6.5 to store and query the graph for nodes,
edges and labels. The search algorithms are implemented in a Java
application which communicates with the database over JDBC,
whereas it also maintains an in-memory cache. Every time that the
search algorithm needs information about a node, it first looks into
the cache, and if the requested information is not there, it is directly
retrieved from the database and then stored in the cache. To avoid
any effects of the cache replacement algorithm, in our experiments
we set the cache to be large enough to include all the information
that has been retrieved from the database.
Synthetic datasets For controlled experiments, we generated dif-
ferent types of (RDF) graphs. The first type is a line graph, which the
simplest model that we can use. In the line graph, every node is con-
nected with two others, having one edge for each node, except two
nodes which are connected with only one. By using the line graph,

Graph-based keyword search in heterogeneous data sources

BDA 2020, October 2020, Paris, France

Procedure process(tree t)

• if t is not already in E
• then

– add t to E
– if t has matches for all the query keywords
– then post-process t if needed; output the result as an answer

• else insert t into K

Algorithm GAMSearch(query Q = {w1, w2, . . . , wk })

(1) For each wi , 1 ≤ i ≤ k
• For each node nj
i

(2) Initial merge∗: try to merge every pair of trees from E, and process any resulting answer tree.
(3) Initialize U (empty so far):

matching wi , let t j
i

be the 1-node tree consisting of nj
i

; process(t j
i

)

(a) Create Grow opportunities: Insert into U the pair (t, e), for each t ∈ E and e a data or similarity edge adjacent to t’s root.
(b) Create Grow2Rep opportunities: Insert into U the pair (t, n → nr ep ) for each t ∈ E whose root is n, such that the representative

of n is nr ep (cid:44) n.

(4) While (U is not empty)

(a) Pop out of U the highest-priority pair (t, e).
(b) Apply the corresponding Grow or Grow2Rep, resulting in a new tree t (cid:48)(cid:48); process(t (cid:48)(cid:48)).
(c) If t (cid:48)(cid:48) was not already in E, agressively Merge:

(i) Let NT be a set of new trees obtained from the Merge (initially ∅).
(ii) Let p1 be the keyword set of t (cid:48)(cid:48)
(iii) For each keyword subset p2 that is a key within K, and such that p1 ∩ p2 = ∅

(A) For each tree t i that corresponds to p2, try to merge t (cid:48)(cid:48) with t i . Process any possible result; if it is new (not in E

previously), add it to NT .

(d) Re-plenish U (add more entries in it). This is performed as in step 3 but based on NT (not on E).

Figure 6: Outline of GAM algorithm

we clearly show the performance of Grow and Merge operations
with respect to the size of the graph. The second type is a chain
graph, which is the same as the line graph, but instead of one edge
connecting every pair of nodes, we have two. We use this type to
show the performance of the algorithm as we double the amount of
edges of the line graph and we give more options to the Grow and
the Merge algorithms. The third type is the star graph, where we
have several line graphs connected through a strongly connected
cluster of nodes with a representative. We use this type to show
the performance of Grow2Rep, by placing the query keywords
on different line graphs. The fourth type is a random graph based
on the Barabasi-Albert (BA, in short) model [1], which generates
scale-free networks with only a few nodes (referred to as hubs)
of the graph having much higher degree than the rest. The graph
in this model is created in a two-staged process. During the first
stage, a network of some nodes is created. Then, during the second
stage, new nodes are inserted in the graph and they are connected
to nodes created during the first stage. At the second stage, we can
control how many connections every node will have to the ones
created at the first stage. By setting that every node created at the
second stage to be connected with exactly one node created at the
first stage, we have observed that we can construct graphs which
are similar to the real-world ones, and therefore we tune our model
accordingly.

Real-world dataset The real-world dataset that we used is
based on data that we have obtained from journalists with whom

)
s

m

(

e
m

i
t
n
o
i
t
u
c
e
x
E

7,000
6,000
5,000
4,000
3,000
2,000
1,000
0

0

100 200 300 400 500 600 700 800 900 1,000

Number of nodes in the graph

Figure 7: Line graph execution time

we collaborate. Our dataset combines information on French pol-
itics, which we obtained by crawling the web pages of a French
newspaper, as we explain in the corresponding Section.

For the microbenchmarks, we report the time needed for our
system to return the first answer, as well as the time for all answers.
For the macrobenchmarks, we only report the time to return all
the answers, as we do not have full control over the graphs and,
hence, it is hard to draw meaningful conclusions and explain them
relying on the whole graphs. Finally, we set an upper bound to the
overall execution time at 120 seconds, which is applied to all the
experiments that we performed.

BDA 2020, October 2020, Paris, France

A. C. Anadiotis, M. Y. Haddad and I. Manolescu

5.2 Querying synthetic datasets
Figure 7 shows the execution time of our algorithm when executing
a query with two keywords on a line graph, as we vary the number
of nodes of the graph. We place the keywords on the two “ends”
of the graph to show the impact of the distance on the execution
time. The performance of our algorithm is naturally affected by the
size of the graph, as it generates 2 ∗ N answer trees, where N is the
number of nodes. Given that this is a line graph, there is only one
answer, which is the whole graph, and, therefore, the time to find
the first answer is also the overall execution time.

Figure 8 shows the execution of our algorithm on a chain graph.
Specifically, Figure 8a shows the time elapsed until the first answer
is found, whereas Figure 8b shows the overall execution time. The
execution times reported in Figure 8a are almost the same, as the
size of the graph increases slowly. On the other hand, the overall
execution times increase at a much higher (exponential) rate, as
shown in Figure 8b, where the y axis has a logarithmic scale. The
reason is that every pair of nodes is connected with two edges,
which increases the amount of answers exponentially with the
amount of nodes in the graph.

Similar to the chain graph, in Figure 9 we report the execution
time until our algorithm finds the first and all answers (left and
right hand side, respectively). Given that we use keywords which
are placed in two different lines connected through the center of
the graph, the algorithm has to use Grow2Rep, whereas in the
previous cases it only had to use Grow and Merge. The number
of branches, depicted on the x axis of Figure 9, corresponds to the
number of line graphs connected in the star. Each line graph has
10 nodes and we place the query keywords at the extremities of
two different line graphs. Given that our algorithm will have to
check all possible answers, it follows that the number of merges is
exponential to the number of branches, that is O(2K ), where K is
the number of branches. This behaviour is clearly shown in both
parts of Figure 9, where on the y axis (in logarithmic scale) we show
the times to find the first, and, respectively, all answers. Above 12
branches, the timeout of 120 seconds that we have set is hit and,
thus, search is terminated, as shown in Figure 9b.

Figure 10 depicts the performance of our algorithm when con-
sidering the Barabasi-Albert graph model. In this experiment, we
keep the graph with 2000 nodes fixed and we vary the position of
two keywords, by choosing nodes which have a distance, as given
in the x axis; note the logarithmic y axis. Due to the fact that the
graph is randomly generated within the BA model, we note some
irregularity in the time to the first solution, which however grows at
a moderate pace as the distance between the keyword node grows.
The overall relation between the time to the first solution and the
total time confirms that the search space is very large but that most
of the exploration is not needed, since the first solution is found
quite fast.

5.3 Querying a real-world dataset
This Section includes the results that we obtained by running our
algorithm on real-world data. Our dataset is a corpus of 462 HTML
articles (about 6MB) crawled from the French online newspaper
Mediapart with the search keywords “gilets jaunes" (yellow vests,
a protest movement in France over the last year). We built a graph

using these articles which consists of 90626 edges and 65868 nodes,
out of which 1525 correpond to people, 1240 to locations and 1050
to organizations. We query the graph using queries of one, two and
three different keywords.

We report our findings in Table 1. The results are not given as a
basis comparison, rather than as a proof of concept. Nevertheless,
there are several interesting observations to be made. First, the
amount of answers for every query is generally larger than 1. We
allow several results, as the end users (in our case, the investigative
journalists) need to see different connections to reach to potentially
interesting conclusions. Second, there are queries where several
answers are found, and the execution is interrupted due to the
threshold. We allow the user to set the threshold, based on the
results returned every time. Third, the answers returned to the user
are significantly less than the answer trees discovered, showing the
impact of minimality as a requirement for returning an answer.

6 RELATED WORK AND CONCLUSIONS
Keyword search (KS, in short) is the method of choice for searching
in unstructured (typically text) data, and it is also the best search
method for novice users, as witnessed by the enormous success
of keyword-based search engines. As databases grew larger and
more complex, KS has been proposed as a method for searching
also in structured data [31], when users are not perfectly famil-
iar with the data, or to get answers enabled by different tuple
connection networks. For relational data, in [19] and subsequent
works, tuples are represented as nodes, and two tuples are inter-
connected only through primary key-foreign key pairs. The graphs
that result are thus quite uniform, e.g., they consist of “Company
nodes”, “Employee nodes” etc. The same model was considered
in [8, 27, 29, 30, 32]; [27] also establishes links based on similarity
(or equality) of constants appearing in different relational attributes.
As explained in Section 2.3, our problem is (much) harder since
our trees can traverse edges in both directions, and paths can be
(much) longer than those based on PK-FK alone. [30] proposes to
incorporate user feedback through active learning to improve the
quality of answers in a relational data integration setting. We are
working to devise such a learning-to-rank approach for our graphs,
also.

KS has also been studied in XML documents [17, 25]. Here,
an answer is defined as a subtree of the original document, whose
leaves match the query keywords. This problem is much easier than
the one we face, since: (i) an XML document is a tree, guaranteeing
just one connection between any two nodes; in contrast, there
can be any number of such connections in our graphs; (ii) the
maximum size of an answer to a k-keywords query is k · h where h,
the height of an XML tree, is almost always quite small, e.g., 20 is
considered “quite high”; in contrast, with our bi-directional search,
the bound is k · D where D is the diameter of our graph - which
can be enormously larger.

Our Grow and Merge steps are borrowed from [10, 18], which
address KS for graphs, assuming optimal-substructure which does
not hold for us, and single-direction edge traversal. For RDF graphs [13,
22] traverse edges in their direction only; moreover, [22] also make
strong assumptions on the graph, e.g., that all non-leaf nodes have
types, and that there are a small number of types (regular graph).

Graph-based keyword search in heterogeneous data sources

BDA 2020, October 2020, Paris, France

40

35

30

25

20

105

104

103

102

)
s

m

(

e
m

i
t
n
o
i
t
u
c
e
x
E

)
s

m

(

e
m

i
t
n
o
i
t
u
c
e
x
E

)
s

m

(

e
m

i
t
n
o
i
t
u
c
e
x
E

106

105

104

103

102

2

3

4

5

6

7

8

9

10

2

3

4

5

6

7

8

9

10

Number of nodes in the graph

Number of nodes in the graph

(a) Time to find the first answer

(b) Time to find all answers

Figure 8: Chain graph execution time

106

105

104

103

102

)
s

m

(

e
m

i
t
n
o
i
t
u
c
e
x
E

2

3

4

5

6

7

8

9

10 11 12 13 14

2

3

4

5

6

7

8

9

10 11 12 13 14

Number of branches in the graph

Number of branches in the graph

(a) Time to find the first answer

(b) Time to find all answers

Figure 9: Star graph execution time

Query keyword(s)

Answers Answer trees Time to 1st (ms) Total time (ms)

Macron
Trump
Melenchon
Christophe, Dettinger
Etienne, Chouard, Rodrigues
Thierry–Paul, Valette, Drouet
Melenchon, Aubry
Castaner, flashball
Drouet, Levavasseur
Dupont–Aignan, Chalencon
Estrosi, Castaner
Alexis, Corbiere, Ruffin
Macron, Nunez
Hamon, Drouet
Drouet, Ludosky
Salvini, Ludosky
Salvini, Chouard
Corbiere, Drouet
Cauchy, Drouet
Benalla, Nunez

118
10
8
1105
1
0
9
17
18
21
16
11
13
5
27
17
16
13
22
15

179
26
31
136
144
N/A
38
61
145
53
205
57
1511
71
43
111
76
129
96
199
Table 1: Results with real-world dataset

0
0
0
319611
194
300813
284
1724
518
1850
2203
3782
4107
421
486
1156
3205
2341
516
1027

390
36
39
123932
146
120001
929
545
309
393
529
1022
1561
145
145
375
710
673
260
347

In [6], the authors investigate a different kind of answers to key-
word search, the so-called r -clique graphs, which they solve with
the help of specific indexes.

Keyword search across heterogeneous datasets has been pre-
viously studied in [11, 23]. However, in these works, each answer
comes from a single dataset, that is, they never consider answers

BDA 2020, October 2020, Paris, France

A. C. Anadiotis, M. Y. Haddad and I. Manolescu

104

103

102

)
s

m

(

e
m

i
t
n
o
i
t
u
c
e
x
E

First solution
All solutions

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

Distance between keyword nodes

Figure 10: Barabasi-Albert graph execution time

spanning over and combining multiple datasets, such as the one
shown in Figure 2.

In the literature, (G)STP has been addressed under various sim-
plifications that do not hold in our context. For instance: the qual-
ity of a solution exponentially decreases with the tree size, thus
search can stop when all trees are under a certain threshold [2];
edges are considered in a single direction [13, 22, 32]; the cost
function has the suboptimal-structure property [10, 24] etc. These
assumptions reduce the computational cost; in contrast, to leave
our options open as to the best score function, we worked to build
a feasible solution for the general problem we study. Some works
have focused on finding bounded (G)STP approximations, i.e.,
(G)STP trees solutions whose cost is at most f times higher than
the optimal cost, e.g., [15, 16]. Beyond the differences between our
problem and (G)STP, due notably to the fact that our score is much
more general (Section 3), non-expert users find it hard to set f .

Beyond the differences we mentioned above, most of which
concern our bidirectional search, and the lack of favorable cost
hypothesis, our work is the first to study querying of graphs orig-
inating from integrating several data sources, while at the same
time preserving the identity of each node from the original document;
this is a requirement for integrating, and simultaneously preserv-
ing, datasets of journalistic interest. In a companion paper [4] we
present our latest algorithms for creating such graphs, relying also
on information extraction, data matching, and named entity disam-
biguation; earlier versions were outlined in [5, 7].

Acknowledgements The authors would like to thank: Helena Gal-
hardas and Julien Leblay who contributed to previous versions on
this work [5, 7] and Tayeb Merabti for his support in the devel-
opment and maintenance of the ConnectionLens system [4]. This
work was partially supported by the H2020 research program under
grant agreement nr. 800192, and by the ANR AI Chair SourcesSay.

REFERENCES
[1] Albert-László Barabási and Réka Albert. 1999. Emergence of Scaling in Random
Networks. Science 286, 5439 (1999). DOI:http://dx.doi.org/10.1126/science.286.
5439.509

[2] Raphaël Bonaque, Bogdan Cautis, François Goasdoué, and Ioana Manolescu.

2016. Social, Structured and Semantic Search. In EDBT.

[3] Francesca Bugiotti, Damian Bursztyn, Alin Deutsch, Ioana Ileana, and Ioana

Manolescu. 2015. Invisible Glue: Scalable Self-Tunning Multi-Stores. In CIDR.

[4] Oana Bˇalˇalˇau, Catarina Conceia¸o, Helena Galhardas, Ioana Manolescu, Tayeb
Merabti, Jingmao You, and Youssr Youssef. 2020. Graph integration of structured,
semistructured and unstructured data for data journalism. BDA. (2020). https:
//hal.inria.fr/hal-02904797

[5] Camille Chanial, Rédouane Dziri, Helena Galhardas,

Julien Leblay,
Minh Huong Le Nguyen, and Ioana Manolescu. 2018. ConnectionLens: Finding
Connections Across Heterogeneous Data Sources (demonstration). VLDB (2018).
[6] Yu-Rong Cheng, Ye Yuan, Jia-Yu Li, Lei Chen, and Guo-Ren Wang. 2016. Keyword
Query over Error-Tolerant Knowledge Bases. Journal of Computer Science and
Technology 31 (2016). Issue 4.

[7] Felipe Cordeiro, Helena Galhardas, Julien Leblay, Ioana Manolescu, and Tayeb
Merabti. 2020. Keyword Search in Heterogeneous Data Sources. (2020). https:
//hal.inria.fr/hal-02559688 Technical report.

[8] Pericles de Oliveira, Altigran Soares da Silva, and Edleno Silva de Moura. 2015.
Ranking Candidate Networks of relations to improve keyword search over rela-
tional databases. In IEEE.

[9] David J. DeWitt, Alan Halverson, Rimma Nehme, Srinath Shankar, Josep Aguilar-
Saborit, Artin Avanes, Miro Flasza, and Jim Gramling. 2013. Split Query Process-
ing in Polybase. In SIGMOD. DOI:http://dx.doi.org/10.1145/2463676.2463709

[10] B. Ding, J. X. Yu, S. Wang, L. Qin, X. Zhang, and X. Lin. 2007. Finding top-k

min-cost connected trees in databases. In ICDE.

[11] Xin Dong and Alon Halevy. 2007. Indexing Dataspaces. In SIGMOD.
[12]

Jennie Duggan, Aaron J. Elmore, Michael Stonebraker, Magda Balazinska, Bill
Howe, Jeremy Kepner, Sam Madden, David Maier, Tim Mattson, and Stan Zdonik.
2015. The BigDAWG Polystore System. SIGMOD Rec. 44, 2 (2015). DOI:http:
//dx.doi.org/10.1145/2814710.2814713

[13] Shady Elbassuoni and Roi Blanco. 2011. Keyword Search over RDF Graphs. In

CIKM.

[14] Michael R. Garey and David S. Johnson. 1990. Computers and Intractability: A
Guide to the Theory of NP-Completeness. W. H. Freeman & Co. New York.
[15] N. Garg, G. Konjevod, and R. Ravi. 1998. A polylogarithmic approximation

algorithm for the group Steiner tree problem. In SIAM.

[16] Andrey Gubichev and Thomas Neumann. 2012. Fast approximation of Steiner

trees in large graphs. In CIKM.

[17] Lin Guo, Feng Shao, Chavdar Botev, and Jayavel Shanmugasundaram. 2003.
XRANK: Ranked keyword search over XML documents. In Proceedings of the
2003 ACM SIGMOD international conference on Management of data. 16–27.
[18] Hao He, Haixun Wang, Jun Yang, and Philip S. Yu. 2007. BLINKS: ranked keyword

searches on graphs. In SIGMOD.

[19] Vagelis Hristidis and Yannis Papakonstantinou. 2002. DISCOVER: Keyword

Search in Relational Databases. In VLDB.

[20] Manos Karpathiotakis, Ioannis Alagiannis, and Anastasia Ailamaki. 2016. Fast
Queries over Heterogeneous Data through Engine Customization. PVLDB 9, 12
(2016). DOI:http://dx.doi.org/10.14778/2994509.2994516

[21] Manos Karpathiotakis, Ioannis Alagiannis, Thomas Heinis, Miguel Branco, and
Anastasia Ailamaki. 2015. Just-In-Time Data Virtualization: Lightweight Data
Management with ViDa. In CIDR.

[22] Wangchao Le, Feifei Li, Anastasios Kementsietsidis, and Songyun Duan. 2014.
Scalable Keyword Search on Large RDF Data. IEEE Trans. Knowl. Data Eng. 26,
11 (2014).

[23] Guoliang Li, Beng Chin Ooi, Jianhua Feng, Jianyong Wang, and Lizhu Zhou.
2008. EASE: An Effective 3-in-1 Keyword Search Method for Unstructured,
Semi-structured and Structured Data. In SIGMOD.

[24] Rong-Hua Li, Lu Qin, Jeffrey Xu Yu, and Rui Mao. 2016. Efficient and Progressive
Group Steiner Tree Search. In SIGMOD, Fatma Özcan, Georgia Koutrika, and
Sam Madden (Eds.).

[25] Ziyang Liu and Yi Chen. 2007. Identifying meaningful return information for
XML keyword search. In Proceedings of the 2007 ACM SIGMOD international
conference on Management of data. 329–340.

[26] Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew
Tomkins. 2008. Pig Latin: A Not-so-Foreign Language for Data Processing. In
SIGMOD (SIGMOD ’08). DOI:http://dx.doi.org/10.1145/1376616.1376726
[27] Mayssam Sayyadian, Hieu LeKhac, AnHai Doan, and Luis Gravano. 2007. Effi-

cient Keyword Search Across Heterogeneous Relational Databases. In ICDE.

[28] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka,
Suresh Anthony, Hao Liu, Pete Wyckoff, and Raghotham Murthy. 2009. Hive: A
Warehousing Solution over a Map-Reduce Framework. PVLDB 2, 2 (2009).
[29] Quang Hieu Vu, Beng Chin Ooi, Dimitris Papadias, and Anthony K. H. Tung.
2008. A graph method for keyword-based selection of the top-K databases. In
SIGMOD.

[31]

[30] Zhepeng Yan, Nan Zheng, Zachary G. Ives, Partha Pratim Talukdar, and Cong
Yu. 2015. Active learning in keyword search-based data integration. VLDB J. 24,
5 (2015).
Jeffrey Xu Yu, Lu Qin, and Lijun Chang. 2009. Keyword Search in Databases. DOI:
http://dx.doi.org/10.2200/S00231ED1V01Y200912DTM001
Jeffrey Xu Yu, Lu Qin, and Lijun Chang. 2010. Keyword Search in Relational
Databases: A Survey. IEEE Data Eng. Bull. 33, 1 (2010).

[32]

[33] Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J.
Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016.
Apache Spark: A Unified Engine for Big Data Processing. CACM 59, 11 (2016).

