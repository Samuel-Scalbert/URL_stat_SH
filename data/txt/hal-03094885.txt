A Performance-Explainability Framework to Benchmark
Machine Learning Methods: Application to Multivariate
Time Series Classifiers
Kevin Fauvel, Véronique Masson, Elisa Fromont

To cite this version:

Kevin Fauvel, Véronique Masson, Elisa Fromont. A Performance-Explainability Framework to Bench-
mark Machine Learning Methods: Application to Multivariate Time Series Classifiers. IJCAI-PRICAI
2020 - Workshop on Explainable Artificial Intelligence (XAI), Jan 2021, Yokohama, Japan. pp.1-8.
￿hal-03094885v2￿

HAL Id: hal-03094885

https://hal.science/hal-03094885v2

Submitted on 20 Dec 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

A Performance-Explainability Framework
to Benchmark Machine Learning Methods:
Application to Multivariate Time Series Classiﬁers

Kevin Fauvel , V´eronique Masson and ´Elisa Fromont
Univ Rennes, Inria, CNRS, IRISA, France
kevin.fauvel@inria.fr, {veronique.masson, elisa.fromont}@irisa.fr

Abstract
Our research aims to propose a new performance-
explainability analytical framework to assess and
benchmark machine learning methods. The frame-
work details a set of characteristics that systematize
the performance-explainability assessment of exist-
ing machine learning methods. In order to illustrate
the use of the framework, we apply it to benchmark
the current state-of-the-art multivariate time series
classiﬁers.

1 Introduction
There has been an increasing trend in recent years to leverage
machine learning methods to automate decision-making pro-
cesses. However, for many applications, the adoption of such
methods cannot rely solely on their prediction performance.
For example, the European Union’s General Data Protection
Regulation, which became enforceable on 25 May 2018, in-
troduces a right to explanation for all individuals so that they
can obtain “meaningful explanations of the logic involved”
when automated decision-making has “legal effects” on indi-
viduals or similarly “signiﬁcantly affecting” them1. There-
fore, in addition to their prediction performance, machine
learning methods have to be assessed on how they can supply
their decisions with explanations.

The performance of a machine learning method can be as-
sessed by the extent to which it correctly predicts unseen in-
stances. A metric like the accuracy score commonly mea-
sures the performance of a classiﬁcation model. However,
there is no standard approach to assess explainability. First,
there is no mathematical deﬁnition of explainability. A def-
inition proposed by [Miller, 2019] states that the higher the
explainability of a machine learning algorithm, the easier it
is for someone to comprehend why certain decisions or pre-
dictions have been made. Second, there are several methods
belonging to different categories (explainability-by-design,
post-hoc model-speciﬁc explainability and post-hoc model-
agnostic explainability) [Du et al., 2020], which provide their
own form of explanations.

The requirements for explainable machine learning meth-
ods are dependent upon the application and to whom the ex-
planations are intended for [Tomsett et al., 2018; Bohlen-

1https://ec.europa.eu/info/law/law-topic/data-protection en

der and K´ohl, 2019]. In order to match these requirements
and conduct experiments to validate the usefulness of the ex-
planations by the end-users, there is a need to have a com-
prehensive assessment of the explainability of the existing
methods. Doshi-Velez and Kim [2017] claim that creating a
shared language is essential for the evaluation and compar-
ison of machine learning methods, which is currently chal-
lenging without a set of explanation characteristics. As far
as we have seen, there is no existing framework which de-
ﬁnes a set of explanation characteristics that systematize the
assessment of the explainability of existing machine learning
methods.

Hence, in this paper, we propose a new framework to assess
and benchmark the performance-explainability characteris-
tics of machine learning methods. The framework hypoth-
esizes a set of explanation characteristics, and as emphasized
in [Wolf, 2019], focuses on what people might need to under-
stand about machine learning methods in order to act in con-
cert with the model outputs. The framework does not claim to
be exhaustive and excludes application-speciﬁc implementa-
tion constraints like time, memory usage and privacy. It could
be a basis for the development of a comprehensive assessment
of the machine learning methods with regards to their perfor-
mance and explainability and for the design of new machine
learning methods. Due to space constraint, we limit the illus-
tration of the use of the framework to one category of machine
learning methods and we choose the Multivariate Time Series
(MTS) classiﬁers. Multivariate data which integrates tempo-
ral evolution has received signiﬁcant interests over the past
decade, driven by automatic and high-resolution monitoring
applications (e.g. healthcare [Li et al., 2018], mobility [Jiang
et al., 2019], natural disasters [Fauvel et al., 2020a]). More-
over, the available explainability solutions to support the cur-
rent state-of-the-art MTS classiﬁers remain limited, so this
category of methods appears meaningful to assess for us.

The contributions of this paper are the following:
• We present a new performance-explainability analytical
framework to assess and benchmark machine learning
methods;

• We detail a set of characteristics that systematize the
performance-explainability assessment of existing ma-
chine learning methods;

• We illustrate the use of the framework by benchmarking

the current state-of-the-art MTS classiﬁers.

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

2 Related Work
In this section, we ﬁrst position this paper in the related work
and introduce the different categories of explainability meth-
ods as a background to the notions that will be discussed in
the framework. Then, we present the state-of-the-art machine
learning methods that will be used to illustrate the framework,
i.e. MTS classiﬁers.

2.1 Explainability
Multiple taxonomies of explainability methods have been
derived from different frameworks [Guidotti et al., 2018;
Ventocilla et al., 2018; Du et al., 2020]. However, none of
them deﬁnes a set of explanation characteristics that system-
atize the assessment of the explainability of existing machine
[Guidotti et al., 2018] provides a clas-
learning methods.
siﬁcation of the main problems addressed in the literature
with respect to the notion of explanation and the type of ma-
[Ventocilla et al., 2018] proposes
chine learning systems.
a high-level taxonomy of interpretable and interactive ma-
chine learning composed of six elements (Dataset, Optimizer,
Model, Predictions, Evaluator and Goodness). And, [Du et
al., 2020] categorizes existing explainability methods of ma-
chine learning models into either by design or post-hoc ex-
plainability. As our framework aims to cover all types of
methods, we do not present the frameworks focusing on a
particular type of explainability methods (e.g. [Lundberg and
Lee, 2017; Ancona et al., 2018; Henin and M´etayer, 2019]).
A ﬁve-step method to understand the requirements for
explainable AI systems has been published in [Hall et al.,
2019]. The ﬁve steps are: explainee role deﬁnition, expla-
nation characteristics identiﬁcation, requirements collection,
existing methods assessment and requirements/existing meth-
ods mapping. Our framework can be positioned as a fur-
ther development of the fourth step of the method by de-
tailing a set of explanations characteristics that systematize
the assessment of existing methods. Our framework does not
include application-speciﬁc implementation constraints like
time, memory usage and privacy.

As a background to the notions that will be discussed
in the framework, we introduce the three commonly recog-
nized categories (explainability-by-design, post-hoc model-
speciﬁc explainability and post-hoc model-agnostic explain-
ability) [Du et al., 2020] to which all of the explainability
methods are belonging to. First, some machine learning mod-
els provide explainability-by-design. These self-explanatory
models incorporate explainability directly to their structures.
This category includes, for example, decision trees, rule-
based models and linear models. Next, post-hoc model-
speciﬁc explainability methods are speciﬁcally designed to
extract explanations for a particular model. These meth-
ods usually derive explanations by examining internal model
structures and parameters. For example, a method has been
designed to measure the contribution of each feature in ran-
dom forests [Palczewska et al., 2013]; and another one has
been designed to identify the regions of input data that are
important for predictions in convolutional neural networks
using the class-speciﬁc gradient information [Selvaraju et al.,
2019]. Finally, post-hoc model-agnostic explainability meth-
ods provide explanations from any machine learning model.

These methods treat the model as a black-box and does not
inspect internal model parameters. For example, the permu-
tation feature importance method [Altmann et al., 2010] and
the methods using an explainable surrogate model [Lakkaraju
et al., 2017; Lundberg and Lee, 2017; Ribeiro et al., 2018;
Guidotti et al., 2019] belong to this category.

The explainability methods presented reﬂect the diver-
sity of explanations generated to support model predictions,
therefore the need for a framework in order to benchmark the
machine learning methods explainability. The next section
present the MTS classiﬁers that will be used to illustrate the
framework.

2.2 Multivariate Time Series Classiﬁers
The state-of-the-art MTS classiﬁers consist of a diverse range
of methods which can be categorized into three families:
similarity-based, feature-based and deep learning methods.

Similarity-based methods make use of similarity mea-
sures to compare two MTS. Dynamic Time Warping (DTW)
has been shown to be the best similarity measure to use
along the k-Nearest Neighbors (k-NN) [Seto et al., 2015].
There are two versions of kNN-DTW for MTS: dependent
(DTWD) and independent (DTWI ). Neither dominates over
the other [Shokoohi-Yekta et al., 2017] from an accuracy per-
spective but DTWI allows the analysis of distance differences
at feature level.

Next,

include

feature-based methods

shapelets
(gRSF [Karlsson et al., 2016], UFS [Wistuba et al.,
2015]) and bag-of-words (LPS [Baydogan and Runger,
2016], mv-ARF [Tuncel and Baydogan, 2018], SMTS [Bay-
dogan and Runger, 2014], WEASEL+MUSE [Sch¨afer and
Leser, 2017]) models. WEASEL+MUSE shows better
results compared to gRSF, LPS, mv-ARF, SMTS and UFS
on average (20 MTS datasets). WEASEL+MUSE generates
a bag-of-words representation by applying various sliding
windows with different sizes on each discretized dimension
(Symbolic Fourier Approximation)
to capture features
(unigrams, bigrams, dimension identiﬁcation). Following a
feature selection with chi-square test, it classiﬁes the MTS
based on a logistic regression classiﬁer.

Then, deep learning methods use Long-Short Term Mem-
ory (LSTM) and/or Convolutional Neural Networks (CNN).
According to the results published, the current state-of-the-
art model (MLSTM-FCN) is proposed in [Karim et al., 2019]
and consists of a LSTM layer and a stacked CNN layer along
with Squeeze-and-Excitation blocks to generate latent fea-
tures.

the best-in-class

Therefore, we choose to benchmark the performance-
explainability of
for each similarity-
based, feature-based and deep learning category (DTWI ,
WEASEL+MUSE and MLSTM-FCN classiﬁers). The next
section introduces the performance-explainability frame-
work, which is illustrated with the benchmark of the best-
in-class MTS classiﬁers in section 4.

3 Performance-Explainability Framework
The framework aims to respond to the different questions an
end-user may ask to take an informed decision based on the
predictions made by a machine learning model: What is the

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

is the level of performance of

level of performance of the model? Is the model compre-
hensible? Is it possible to get an explanation for a partic-
ular instance? Which kind of information does the explana-
tion provide? Can we trust the explanations? What is the
target user category of the explanations? The performance-
explainability framework that we propose is composed of
the following components, which will also be translated into
terms speciﬁc to our application (MTS classiﬁers) whenever
relevant:
Performance What
the
model? The ﬁrst component of the framework character-
izes the performance of a machine learning model. Dif-
ferent methods (e.g. holdout, k-fold cross-validation) and
metrics (e.g.
accuracy, F-measure, Area Under the ROC
Curve) exist to evaluate the performance of a machine learn-
ing model [Witten et al., 2016]. However, there is no consen-
sus on an evaluation procedure to assess the performance of a
machine learning model. Recent work suggests that the deﬁ-
nition of such an evaluation procedure necessitates the devel-
opment of a measurement theory for machine learning [Flach,
2019]. Many of the problems stem from a limited apprecia-
tion of the importance of the scale on which the evaluation
measures are expressed.

Then, in current practices, the choice of a metric to eval-
uate the performance of a machine learning model depends
on the application. According to the application, a metric
aligned with the goal of the experiments is selected, which
prevents the performance comparison of machine learning
models across applications.

Therefore, the performance component in the framework is
deﬁned as a ﬁrst step towards a standard procedure to assess
the performance of machine learning models. It corresponds
to the relative performance of a model on a particular applica-
tion. More speciﬁcally, it indicates the relative performance
of the models as compared to the state-of-the-art model on
a particular application and an evaluation setting. This deﬁ-
nition allows the categorization of the models’ performance
on an application and an evaluation setting. In the case of
different applications with a similar machine learning task,
the performance component can give the list of models which
outperformed current state-of-the-art models on their respec-
tive application. Thus, it points to certain models that could
be interesting to evaluate on a new application, without pro-
viding guarantee that these models would perform the same
on this new application. We propose an assessment of the
performance in three categories:
• Best: best performance.

It corresponds to the per-
formance of the ﬁrst ranked model on the applica-
tion following an evaluation setting (models, evaluation
method, datasets);

• Similar: performance similar to that of the state-of-the-
art models. Based on the same evaluation setting, it cor-
responds to all the models which do not show a statisti-
cally signiﬁcant performance difference with the second
ranked model. For example, the statistical comparison
of multiple classiﬁers on multiple datasets is usually pre-
sented on a critical difference diagram [Demˇsar, 2006];

• Below: performance below that of the state-of-the-art

It corresponds to the performance of the re-

models.
maining models with the same evaluation setting.
Model Comprehensibility Is the model comprehensible?
The model comprehensibility corresponds to the ability for
the user to understand how the model works and produces
certain predictions. Comprehensibility is tightly linked to
the model complexity; yet, there is no consensus on model
complexity assessment [Guidotti et al., 2018]. Currently,
two categories of models are commonly recognized: “white-
box” models, i.e. easy-to-understand models, and “black-
box” models, i.e. complicated-to-understand models [Lip-
ton, 2016]. For example, many rule-based models and deci-
sion trees are regarded as “white-box” models while ensem-
ble methods and deep learning models are “black-box” mod-
els. Not all rule-based models or decision trees are “white-
box” models. Cognitive limitations of humans place restric-
tions on the complexity of the approximations that are un-
derstandable to humans. For example, a decision tree with a
hundred levels cannot be considered as an easy-to-understand
model [Lakkaraju et al., 2017].

Nevertheless, the distinction between “white-box” models
and “black-box” models is clear among the machine learning
methods of this paper. The state-of-the-art MTS classiﬁers
are all “black-box” except one which is an easy-to-understand
similarity-based approach. Therefore, due to space limitation,
we propose a ﬁrst assessment of the comprehensibility in two
categories and we plan to further elaborate this component in
future work:

• Black-Box: “black-box” model, i.e.

complicated-to-

understand models;

• White-Box: “white-box” model, i.e. easy-to-understand

models.

Is it possible to get an
Granularity of the Explanations
explanation for a particular instance? The granularity indi-
cates the level of possible explanations. Two levels are gener-
ally distinguished: global and local [Du et al., 2020]. Global
explainability means that explanations concern the overall be-
havior of the model across the full dataset, while local ex-
plainability informs the user about a particular prediction.
Some methods can provide either global or local-only ex-
plainability while other methods can provide both (e.g. de-
cision trees). Therefore, we propose an assessment of the
granularity in three categories:

• Global: global explainability;
• Local: local explainability;
• Global & Local: both global and local explainability.
Information Type Which kind of information does the ex-
planation provide? The information type informs the user
about the kind of information communicated. The most valu-
able information is close to the language of human reasoning,
with causal and counterfactual rules [Pearl and Mackenzie,
2018]. Causal rules can tell the user that certain observed
variables are the causes of speciﬁc model predictions. How-
ever, machine learning usually leverages statistical associa-
tions in the data and do not convey information about the
causal relationships among the observed variables and the un-
observed confounding variables. The usual statistical asso-

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

ciations discovered by machine learning methods highly de-
pend on the machine learning task. Therefore, we ﬁrst give a
generic high-level deﬁnition of the information type and then
we detail and illustrate it for the application case of this pa-
per (MTS classiﬁcation). We propose a generic assessment
of the information type in 3 categories from the least to the
most informative:

• Importance: the explanations reveal the relative impor-
tance of each dataset variable on predictions. The impor-
tance indicates the statistical contribution of each vari-
able to the underlying model when making decisions;

• Patterns:

the explanations provide the small conjunc-
tions of symbols with a predeﬁned semantic (patterns)
associated with the predictions;

• Causal:

the most informative category corresponds to

explanations under the form of causal rules;

In this paper, the issue of Multivariate Time Series (MTS)
classiﬁcation is addressed. A MTS M = {x1, ..., xd} ∈
Rd∗l is an ordered sequence of d ∈ N streams with xi =
(xi,1, ..., xi,l), where l is the length of the time series and d
is the number of multivariate dimensions. Thus, considering
the MTS data type, the information can be structured around
the features, i.e.
the observed variables, and the time. We
propose to decompose the 3 categories presented into 8 cate-
gories. In addition, we will illustrate each of these categories
with an application in the medical ﬁeld. Figure 1 shows the
ﬁrst MTS of the UEA Atrial Fibrilation [Bagnall et al., 2018]
test set that belongs to the class Non-Terminating Atrial Fibri-
lation. This MTS is composed of two dimensions (two chan-
nels ECG) with a length of 640 (5 second period with 128
samples per second). It is worth noting that the explanations
provided to illustrate each category are assumptive rather than
validated, they are given as illustrative in nature.

Figure 1: The ﬁrst MTS sample of the UEA Atrial Fibrilation test
set. It belongs to the class Non-Terminating Atrial Fibrilation and is
composed of two channels ECG on a 5 second period (128 samples
per second).

• Features (Importance): the explanations reveal the rel-
ative importance of the features on predictions. For ex-
ample, in order to support a model output from the MTS
of the Figure 1, the explanations could tell the user that
the channel 2 has a greater importance on the prediction
than the channel 1;

• Features + Time (Importance): the explanations provide
the relative importance of the features and timestamps
on predictions. For example, in order to support a model
output from the MTS of the Figure 1, the explanations
could tell the user that the channel 2 has a greater im-
portance on the prediction than the channel 1 and that
the timestamps are in increasing order of importance on
the prediction;

• Features + Time + Values (Importance): in addition to
the relative importance of the features and timestamps
on predictions, the explanations indicate the discrimina-
tive values of a feature for each class. For example in
Figure 1, the explanations could give the same expla-
nations as the previous category, plus, it could tell the
user that the timestamps with the highest importance are
associated with high values (values above 0.15) on the
channel 2;

• Uni Itemsets (Patterns):

the explanations provide pat-
terns under the form of groups of values, also called
itemsets, which occur per feature and are associated with
the prediction. For example, in order to support a model
output from the MTS of the Figure 1, the explanations
could tell the user that the following itemsets are asso-
ciated with the prediction: {channel 1: extremely high
low value (below -0.05)}
value (above 1); channel 1:
and {channel 2: high value (above 0.15); channel 2: ex-
tremely low value (below -0.1)}. The ﬁrst itemset can be
read as: the prediction is associated with the occurence
on the channel 1 of an extremely high value being above
1 and a low value being below -0.05 at another moment,
without information on which one appears ﬁrst;

• Multi Itemsets (Patterns): the explanations provide pat-
terns under the form of multidimensional itemsets, i.e.
groups of values composed of different features, which
are associated with the prediction. For example, in order
to support a model output from the MTS of the Figure 1,
the explanations could tell the user that the following
itemset is associated with the prediction: {channel 1:
extremely high value (above 1); channel 2: high value
(above 0.15)};

• Uni Sequences (Patterns): the explanations provide pat-
terns under the form of ordered groups of values, also
called sequences, which occur per feature and are as-
sociated with the prediction. For example, in order to
support a model output from the MTS of the Figure 1,
the explanations could tell the user that the following se-
quences are associated with the prediction: <channel 1:
extremely high value (above 1); channel 1: low value
(below -0.05)> and <channel 2: high values (above
0.15) with an increase during 1 second>. The ﬁrst se-
quence can be read as: the prediction is associated with
the occurrence on the channel 1 of an extremely high
value being above 1 followed by a low value being be-
low -0.05;

• Multi Sequences (Patterns):

the explanations provide
patterns under the form of multidimensional sequences,
i.e. ordered groups of values composed of different fea-
tures, which are associated with the prediction. For ex-
ample, in order to support a model output from the MTS

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

Table 1: Summary of framework results of the state-of-the-art MTS classiﬁers.

Similarity-Based
DTWI

Feature-Based
WEASEL+MUSE with SHAP

Deep Learning
MLSTM-FCN with SHAP

Performance
Comprehensibility
Granularity
Information
Faithfulness
User

Below1
White-Box
Local
Features+Time
Perfect
Domain Expert

Similar1
Black-Box
Both Global & Local
Features+Time
Imperfect
Domain Expert

Best1
Black-Box
Both Global & Local
Features+Time
Imperfect
Domain Expert

1 Predeﬁned train/test splits and an arithmetic mean of the accuracies on 35 public datasets [Karim et al., 2019]. As presented
in section 2.2, the models evaluated in the benchmark are: DTWD, DTWI , gRSF, LPS, MLSTM-FCN, mv-ARF, SMTS,
UFS and WEASEL+MUSE.

of the Figure 1, the explanations could tell the user that
the following sequence is associated with the prediction:
<channel 1: extremely high value (above 1); channel
2: high values (above 0.15) with an increase during 1
second>;

• Causal:

the last category corresponds to explanations
under the form of causal rules. For example, in order to
support a model output from the MTS of the Figure 1,
the explanations could tell the user that the following
rule applies: if (channel 1: extremely high value (above
1)) & (channel 2: high values (above 0.15) with an in-
crease during 1 second), then the MTS belongs to the
class Non-Terminating Atrial Fibrilation.

Faithfulness Can we trust the explanations? The faithful-
ness corresponds to the level of trust an end-user can have in
the explanations of model predictions, i.e. the level of related-
ness of the explanations to what the model actually computes.
An explanation extracted directly from the original model is
faithful by deﬁnition. Some post-hoc explanation methods
propose to approximate the behavior of the original “black-
box” model with an explainable surrogate model. The expla-
nations from the surrogate models cannot be perfectly faithful
with respect to the original model [Rudin, 2019]. The ﬁdelity
criteria is used to quantify the faithfulness by the extent to
which the surrogate model imitates the prediction score of
the original model [Guidotti et al., 2018].

In this paper, two MTS classiﬁers use an explainable sur-
rogate model among the three state-of-the-art methods pre-
sented in section 4. However, there is no need to distinguish
between the degree of ﬁdelity of the surrogate models for the
purpose of the comparison in this paper. Therefore, due to
space limitation, we propose a ﬁrst assessment of the faith-
fulness in two categories and we plan to further elaborate this
component in future work:

• Imperfect: imperfect faithfulness (use of an explainable

surrogate model);

• Perfect: perfect faithfulness.

User category What is the target user category of the expla-
nations? The user category indicates the audience to whom
the explanations are accessible. The user’s experience will
affect what kind of cognitive chunks they have, that is, how
they organize individual elements of information into collec-
tions [Neath and Surprenant, 2003]. Thus, it could be in-
teresting to categorize the user types and associate with the
model to whom the explanations will be accessible to. The
broader the audience, the better are the explanations. There-
fore, we propose an assessment in three categories:

• Machine Learning Expert;
• Domain Expert: domain experts (e.g. professionals, re-

searchers);

• Broad Audience: non-domain experts (e.g. policy mak-

ers).

In order to compare the methods visually using the pro-
posed framework, the different aspects can be represented on
a parallel coordinates plot. A parallel coordinate plot allows
a 2-dimensional visualization of a high dimensional dataset
and is suited for the categorical data of this framework. The
next section presents an example of parallel coordinates plots
comparing the state-of-the-art MTS classiﬁers.

4 Application to Multivariate Time Series

Classiﬁers

This section shows how the framework presented in the pre-
vious section can be used to assess and benchmark the state-
of-the-art MTS classiﬁers. As introduced in section 2.2, the
state-of-the-art MTS classiﬁers are: DTWI , MLSTM-FCN
and WEASEL+MUSE. The results of the assessment are
summarized in Table 1, illustrated in Figure 2 and detailed
in the following paragraphs.

The ﬁrst MTS classiﬁer belongs to the similarity-based cat-
egory and is the one-nearest neighbor MTS classiﬁer with
DTW distance (DTWI ). DTWI classiﬁes MTS based on the
label of the nearest sample and a similarity calculated as the
cumulative distances of all dimensions independently mea-
sured under DTW. For each MTS, the explanation supporting
the classiﬁcation is the ranking of features and timestamps
in decreasing order of their DTW distance with the near-
est MTS. Based on predeﬁned train/test splits and an arith-
metic mean of the accuracies, DTWI underperforms the cur-
rent state-of-the-art MTS classiﬁers on the 35 public datasets
(Performance: Below). The results from [Karim et al., 2019]
shows that DTWI has a statistically signiﬁcant lower perfor-
mance than MLSTM-FCN and WEASEL+MUSE. Further-
more, DTWI supports its predictions with limited informa-
tion (Information: Features+Time) that needs to be analyzed
by a domain expert to ensure that it is relevant for the appli-
cation (User: Domain Expert). However, DTWI is an easy-
to-understand model (Comprehensibility: White-Box) which
provides faithful explanations (Faithfulness: Perfect) for each
MTS (Granularity: Local).

Then, we can analyze MLSTM-FCN and WEASEL+
MUSE together. First, based on predeﬁned train/test splits
and an arithmetic mean of the accuracies, MLSTM-FCN
exhibits the best performance on the 35 public datasets

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

Figure 2: Parallel coordinates plot of the state-of-the-art MTS classiﬁers. Performance evaluation method: predeﬁned train/test splits and
an arithmetic mean of the accuracies on 35 public datasets [Karim et al., 2019]. As presented in section 2.2, the models evaluated in the
benchmark are: DTWD, DTWI , gRSF, LPS, MLSTM-FCN, mv-ARF, SMTS, UFS and WEASEL+MUSE.

(Performance: Best) [Karim et al., 2019], followed by
WEASEL+MUSE (Performance: Similar). Second, both
MLSTM-FCN and WEASEL+MUSE are “black-box” clas-
siﬁers without being explainable-by-design or having a post-
hoc model-speciﬁc explainability method. Thus, the explain-
ability characteristics of these models depend on the choice of
the post-hoc model-agnostic explainability method. We have
selected SHapley Additive exPlanations (SHAP) [Lundberg
and Lee, 2017], a state-of-the-art post-hoc model-agnostic
explainability method offering explanations at all granular-
ity levels. SHAP method measures how much each vari-
able (Features+Time) impacts predictions and comes up with
a ranking of the variables which could be exploited by
domain experts. The combination of MLSTM-FCN and
WEASEL+MUSE with SHAP enables them to outperform
DTWI while reaching explanations with a similar level of
information (Information: Features+Time, DTWI : Fea-
tures+Time), in the meantime remaining accessible to the
same user category (User: Domain Expert, DTWI : Domain
Expert). However, as opposed to DTWI , SHAP relies on a
surrogate model which cannot provide perfectly faithful ex-
planations (Faithfulness: Imperfect, DTWI : Perfect).

Therefore, based on the performance-explainability frame-
work introduced, if a “white-box” model and perfect faith-
fulness are not required, it would be preferable to choose
MLSTM-FCN with SHAP instead of the other state-of-the-art
MTS classiﬁers on average on the 35 public datasets. In ad-
dition to its better level of performance, MLSTM-FCN with
SHAP provides the same level of information and at all gran-
ularity levels.

However, the imperfect faithfulness of the explanations
could prevent the use of MLSTM-FCN with a surrogate
explainable model on numerous applications.
In addition,
the level of information provided to support the predictions
remains limited (Information: Features+Time). Therefore,
based on the assessment of the current state-of-the-art MTS
classiﬁers with the framework proposed, it would be valu-
able for instance to design some new high-performing MTS

classiﬁers which provide faithful and more informative ex-
planations. For example, it could be interesting to work in
the direction proposed in [Fauvel et al., 2020b]. It presents
a new MTS classiﬁer (XEM) which reconciles performance
(Performance: Best) and faithfulness while providing the
time window used to classify the whole MTS (Information:
Uni Sequences). XEM is based on a new hybrid ensemble
method that combines an explicit approach to handle the bias-
variance trade-off and an implicit approach to individualize
classiﬁer errors on different parts of the training data [Fau-
vel et al., 2019]. Nevertheless, the explanations provided by
XEM are only available per MTS (Granularity: Local) and
the level of information could be further improved. As sug-
gested by the authors, it could be interesting to analyze the
time windows identiﬁed for each class to determine if they
contain some common multidimensional sequences (Infor-
mation: Multi Sequences, Granularity: Both Global & Lo-
cal). These patterns could also broaden the audience as they
would summarize the key information in the discriminative
time windows.

5 Conclusion

We have presented a new performance-explainability analyt-
ical framework to assess and benchmark the machine learn-
ing methods. The framework details a set of characteristics
that systematize the performance-explainability assessment
of machine learning methods. In addition, it can be employed
to identify ways to improve current machine learning meth-
ods and to design new ones. Finally, we have illustrated the
use of the framework by benchmarking the current state-of-
the-art MTS classiﬁers. With regards to future work, we plan
to further elaborate the deﬁnition of the different components
of the framework (especially the Model Comprehensibility,
the Information Type and the Faithfulness) and evaluate the
relevance of integrating new components. Then, we plan to
apply the framework extensively to assess the different types
of existing machine learning methods.

PerformanceBelowSimilarBestComprehensibilityBlack-BoxWhite-BoxGranularityGlobalLocalGlobal & LocalInformationFeaturesFaithfulnessImperfectPerfectUserMachine Learning ExpertDomain ExpertBroad AudienceFeatures+TimeFeatures+Time+ValuesUni ItemsetsMulti ItemsetsUni SequencesMulti SequencesCausalWEASEL+MUSE withSHAPMLSTM-FCN withSHAPDTWIProceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

Acknowledgments
This work was supported by the French National Research
Agency under the Investments for the Future Program (ANR-
16-CONV-0004) and the Inria Project Lab “Hybrid Ap-
proaches for Interpretable AI” (HyAIAI).

References
[Altmann et al., 2010] A. Altmann, L. Tolosi, O. Sander, and
T. Lengauer. Permutation Importance: A Corrected Fea-
ture Importance Measure. Bioinformatics, 26(10):1340–
1347, 2010.

[Ancona et al., 2018] M. Ancona, E. Ceolini, C.

¨Oztireli,
and M. Gross. Towards Better Understanding of Gradient-
Based Attribution Methods for Deep Neural Networks. In
Proceedings of the 6th International Conference on Learn-
ing Representations, 2018.

[Bagnall et al., 2018] A. Bagnall, J. Lines, and E. Keogh.
The UEA UCR Time Series Classiﬁcation Archive. 2018.
[Baydogan and Runger, 2014] M. Baydogan and G. Runger.
Learning a Symbolic Representation for Multivariate Time
Series Classiﬁcation. Data Mining and Knowledge Dis-
covery, 29(2):400–422, 2014.

[Baydogan and Runger, 2016] M. Baydogan and G. Runger.
Time Series Representation and Similarity Based on Lo-
cal Autopatterns. Data Mining and Knowledge Discovery,
30(2):476–509, 2016.

[Bohlender and K´ohl, 2019] D. Bohlender and M. K´ohl. To-
wards a Characterization of Explainable Systems. ArXiv,
2019.

[Demˇsar, 2006] J. Demˇsar. Statistical Comparisons of Clas-
siﬁers over Multiple Data Sets. Journal of Machine Learn-
ing Research, 7:1–30, 2006.

[Doshi-Velez and Kim, 2017] F. Doshi-Velez and B. Kim.
Towards a Rigorous Science of Interpretable Machine
Learning. ArXiv, 2017.

[Du et al., 2020] M. Du, N. Liu, and X. Hu. Techniques for
Interpretable Machine Learning. Communications of the
ACM, 2020.

[Fauvel et al., 2019] K. Fauvel, V. Masson,

´E. Fromont,
P. Faverdin, and A. Termier. Towards Sustainable Dairy
Management - A Machine Learning Enhanced Method
In Proceedings of the 25th ACM
for Estrus Detection.
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, 2019.

[Fauvel et al., 2020a] K. Fauvel, D. Balouek-Thomert,
D. Melgar, P. Silva, A. Simonet, G. Antoniu, A. Costan,
V. Masson, M. Parashar, I. Rodero, and A. Termier. A
Distributed Multi-Sensor Machine Learning Approach to
In Proceedings of the 34th
Earthquake Early Warning.
AAAI Conference on Artiﬁcial Intelligence, 2020.

[Fauvel et al., 2020b] K. Fauvel,

´E. Fromont, V. Masson,
P. Faverdin, and A. Termier. XEM: An Explainable-by-
Design Ensemble Method for Multivariate Time Series
Classiﬁcation. ArXiv, 2020.

[Flach, 2019] P. Flach. Performance Evaluation in Machine
Learning: The Good, the Bad, the Ugly, and the Way For-
In Proceedings of the 33rd AAAI Conference on
ward.
Artiﬁcial Intelligence, 2019.

[Guidotti et al., 2018] R. Guidotti, A. Monreale, S. Ruggieri,
F. Turini, F. Giannotti, and D. Pedreschi. A Survey of
Methods for Explaining Black Box Models. ACM Com-
puting Survey, 2018.

[Guidotti et al., 2019] R. Guidotti, A. Monreale, F. Gian-
notti, D. Pedreschi, S. Ruggieri, and F. Turini. Factual
and Counterfactual Explanations for Black Box Decision
Making. IEEE Intelligent Systems, 34(6):14–23, 2019.
[Hall et al., 2019] M. Hall, D. Harbone, R. Tomsett,
V. Galetic, S. Quintana-Amate, A. Nottle, and A. Preece.
A Systematic Method to Understand Requirements for Ex-
plainable AI (XAI) Systems. In Proceedings of the IJCAI
Workshop on Explainable Artiﬁcial Intelligence, 2019.
[Henin and M´etayer, 2019] C. Henin and D. Le M´etayer. To-
wards a Generic Framework for Black-Box Explanation
Methods. In Proceedings of the IJCAI Workshop on Ex-
plainable Artiﬁcial Intelligence, 2019.

[Jiang et al., 2019] R Jiang, X. Song, D. Huang, X. Song,
T. Xia, Z. Cai, Z. Wang, K. Kim, and R. Shibasaki. Deep-
UrbanEvent: A System for Predicting Citywide Crowd
Dynamics at Big Events. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, 2019.

[Karim et al., 2019] F. Karim, S. Majumdar, H. Darabi, and
S. Harford. Multivariate LSTM-FCNs for Time Series
Classiﬁcation. Neural Networks, 116:237–245, 2019.

[Karlsson et al., 2016] I. Karlsson, P. Papapetrou,

and
H. Bostr¨om. Generalized Random Shapelet Forests.
Data Mining and Knowledge Discovery, 30(5):1053–
1085, 2016.

[Lakkaraju et al., 2017] H. Lakkaraju, E. Kamar, R. Caru-
ana, and J. Leskovec.
Interpretable and Explorable Ap-
proximations of Black Box Models. In Proceedings of the
KDD Workshop on Fairness, Accountability, and Trans-
parency in Machine Learning, 2017.

[Li et al., 2018] Jia Li, Y. Rong, H. Meng, Z. Lu, T. Kwok,
and H. Cheng. TATC: Predicting Alzheimer’s Disease
In Proceedings of the 24th ACM
with Actigraphy Data.
SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, 2018.

[Lipton, 2016] Z. Lipton.

The Mythos of Model Inter-
pretability. In Proceedings of the ICML Workshop on Hu-
man Interpretability in Machine Learning, 2016.

[Lundberg and Lee, 2017] S. Lundberg and S. Lee. A Uni-
ﬁed Approach to Interpreting Model Predictions. In Pro-
ceedings of the 31st International Conference on Neural
Information Processing Systems, 2017.

[Miller, 2019] T. Miller. Explanation in Artiﬁcial Intelli-
gence: Insights from the Social Sciences. Artiﬁcial In-
telligence, 267:1–38, 2019.

Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)

[Wolf, 2019] C. Wolf. Explainability Scenarios: Towards
Scenario-Based XAI Design. In Proceedings of the 24th
International Conference on Intelligent User Interfaces,
2019.

[Neath and Surprenant, 2003] I. Neath and A. Surprenant.
Human Memory: An Introduction to Research, Data, and
Theory. Thomson/Wadsworth, 2003.
[Palczewska et al., 2013] A. Palczewska,

J. Palczewski,
R. Robinson, and D. Neagu. Interpreting Random Forest
Models Using a Feature Contribution Method. In Proceed-
ings of the 14th IEEE International Conference on Infor-
mation Reuse Integration, 2013.

[Pearl and Mackenzie, 2018] J. Pearl and D. Mackenzie. The
Book of Why: The New Science of Cause and Effect. Basic
Books, 2018.

[Ribeiro et al., 2018] M. Ribeiro, S. Singh, and C. Guestrin.
Anchors: High-Precision Model-Agnostic Explanations.
In Proceedings of the 32nd AAAI Conference on Artiﬁcial
Intelligence, 2018.

[Rudin, 2019] C. Rudin. Stop Explaining Black Box Ma-
chine Learning Models for High Stakes Decisions and
Use Interpretable Models Instead. Nature Machine Intelli-
gence, 1:206–215, 2019.

[Sch¨afer and Leser, 2017] P. Sch¨afer and U. Leser. Mul-
tivariate Time Series Classiﬁcation with WEASEL +
MUSE. ArXiv, 2017.

[Selvaraju et al., 2019] R. Selvaraju, A. Das, R. Vedantam,
M. Cogswell, D. Parikh, and D. Batra. Grad-CAM: Visual
Explanations from Deep Networks via Gradient-Based
International Journal of Computer Vision,
Localization.
128:336–359, 2019.

[Seto et al., 2015] S. Seto, W. Zhang, and Y. Zhou. Multi-
variate Time Series Classiﬁcation Using Dynamic Time
Warping Template Selection for Human Activity Recog-
nition. In Proceedings of the IEEE Symposium Series on
Computational Intelligence, 2015.

[Shokoohi-Yekta et al., 2017] M. Shokoohi-Yekta, B. Hu,
H. Jin, J. Wang, and E. Keogh. Generalizing DTW to the
Multi-Dimensional Case Requires an Adaptive Approach.
Data Mining and Knowledge Discovery, 31:1–31, 2017.
[Tomsett et al., 2018] R. Tomsett, D. Braines, D. Harborne,
A. Preece, and S. Chakraborty.
Interpretable to Whom?
A Role-Based Model for Analyzing Interpretable Machine
Learning Systems. In Proceedings of the ICML Workshop
on Human Interpretability in Machine Learning, 2018.
[Tuncel and Baydogan, 2018] K. Tuncel and M. Baydogan.
Autoregressive Forests for Multivariate Time Series Mod-
eling. Pattern Recognition, 73:202–215, 2018.
T.

Helldin,
Towards a
Interpretable and Interactive Machine
In Proceedings of the IJCAI Workshop on

M. Riveiro, J. Bae, and N. Lavesson.
Taxonomy for
Learning.
Explainable Artiﬁcial Intelligence, 2018.

[Ventocilla et al., 2018] E.

Ventocilla,

[Wistuba et al., 2015] M. Wistuba,

and
L. Schmidt-Thieme. Ultra-Fast Shapelets for Time Series
Classiﬁcation. ArXiv, 2015.

J. Grabocka,

[Witten et al., 2016] I. Witten, E. Frank, M. Hall, and C. Pal.
Data Mining: Practical Machine Learning Tools and
Techniques. Morgan Kaufmann Series. 2016.

