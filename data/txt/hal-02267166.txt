Admissible generalizations of examples as rules (2019)
Philippe Besnard, Thomas Guyet, Véronique Masson

To cite this version:

Philippe Besnard, Thomas Guyet, Véronique Masson. Admissible generalizations of examples as rules
(2019). 31st International Conference on Tools with Artificial Intelligence (ICTAI 2019), IEEE, Nov
2019, Portland, OR, United States. ￿10.1109/ICTAI.2019.00211￿. ￿hal-02267166￿

HAL Id: hal-02267166

https://inria.hal.science/hal-02267166

Submitted on 19 Aug 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Admissible Generalizations of Examples as Rules

Philippe Besnard
CNRS – IRIT
France

Thomas Guyet
Agrocampus Ouest – IRISA
France

Véronique Masson
Université de Rennes 1 – IRISA
France

Abstract— Rule learning is a data analysis task that consists
in extracting rules that generalize examples. This is achieved by
a plethora of algorithms. Some generalizations make more sense
for the data scientists, called here admissible generalizations.
The purpose of this article is to show formal properties of
admissible generalizations. A formalization for generalization of
examples is proposed allowing the expression of rule admissibility.
Some admissible generalizations are captured by preclosure and
capping operators. Also, we are interested in selecting supersets
of examples that induce such operators. We then deﬁne classes
of selection functions. This formalization is more particularly
developed for examples with numerical attributes. Classes of such
functions are associated with notions of generalization and they
are used to comment some results of the CN2 algorithm [5].

I. INTRODUCTION

Generalizing a given set of examples is essential in many
machine learning techniques such as rule learning or pattern
mining. Particularly, rule learning is a data mining task that
consists in generating disjunctive sets of rules from a dataset
of examples labeled by a class identiﬁer. We are focusing on
propositional rule induction [8] where rules have the form
“IF Conditions THEN class-label”. Rule learning consists in
ﬁnding individual rules [19], each rule generalizes a subset of
the dataset examples. Many algorithms achieve rule induction,
from CN2 [5], Ripper [6] to recent subgroup discovery algo-
rithms [3]. Usually each rule is evaluated by different measures
using the number of positive and negative examples cov-
ered, i.e. generalized, by the rule. Numerous interestingness
measures [11] on rules have been proposed. Some heuristic
measures guide the machine learning algorithms and some of
them are used in a post-processing step to select ﬁnal rules.

Rule learning algorithms received recent attention in the
machine learning community due to their interpretability.
Explainability and interpretability of machine learning results
is a hot topic [7]. The logical structure of a rule can be easily
interpreted by users not familiar with machine learning or data
mining concepts. We feel that generalization of examples by
machine learning algorithms impacts interpretability, particu-
larly when this generalization is counter-intuitive.

Table I is an illustration of a dataset of house rental ads.
Each row is a house rental ad and each column is an attribute.
With a minimal coverage size of 2, CN2 extracts the following
three rules predicting a value for the class-attribute C:
: A5 = Downtown → C = expensive
: A2 < 2.50 ∧ A4 = Toulouse → C = low-priced
: A1 > 36.00 ∧ A3 = D → C = cheap

• πCN 2
1
• πCN 2
2
• πCN 2
3

§ Corresponding author: Véronique Masson (veronique.masson@irisa.fr)

TABLE I
DATASET OF HOUSE RENTAL ADS. BLANK CELLS ARE MISSING VALUES.

(C)
Price Area #Rooms Energy Town District Exposure

(A3)

(A5)

(A2)

(A1)

(A4)

(A6)

cheap

1 low-priced 45
75
2
3 expensive
65
4 low-priced 32
5 mid-priced 65
6 expensive 100
40
7

cheap

2
4
3
2
2
5
2

D
D

D
D
C
D

Toulouse Minimes
Toulouse Rangueil
Toulouse Downtown
Toulouse
Rennes
Rennes Downtown
Betton

SE
SW

S

Generalization of a metric attribute leads to the difﬁcult
question of deﬁning boundary values. The πCN 2
rule uses a
value for A2 attribute (i.e., 2.5) which is not in the original
dataset. The choice of this boundary is motivated by statistical
reasons: with an hypothesis of uniform distribution of numer-
ical attribute, it minimizes the generalization error. One can
notice that it is less intuitive (although equivalent) than the
rule: A2≤ 2 ∧ A4 = Toulouse → C = low-priced.

2

This small example illustrates that existing rule learning
algorithms underestimate the effects that
their underlying
hypotheses about generalization can have on the value of
extracted rules for a data scientist – some rules sometimes
fail to capture an intuitive generalization of the examples.

We are wondering whether it is possible to highlight some
general principles of intuitive generalization that would help to
analyze or to qualify rules or rulesets extracted by rule learning
algorithms. This means that we are interested in analyzing
consequences of choices made by rule learning algorithm
when generalizing examples.

There are different approaches to reach such an objective:
scoring interestingness or quality measures [1] or analyzing
results on the light of subjective criteria [20] (see related works
for more details).

The purpose of this paper is to propose a topological
formalization for generalization of examples which favours an
analysis on the admissibility of the generalization by enabling
to express different notions of admissibility. One objective is
to make it possible to compare the outputs of rule learning
algorithms with the theoretically admissible rules in order to
shed light on some poorly interpretable outputs.

Importantly, our work is also original as it pays special
attention to the values (mostly as boundaries) occurring in
rules whereas work in the literature on improving rules or
rulesets usually focus on the structure of rules or of rulesets,
see e.g. [13], [4].

(C)
Price

D
D

(A1)
(A3)
(A2)
Area #Rooms Energy
2
4
3
2
2
5
2

D
D
C
D

1

cheap

low-priced 45
75
2
65
3 expensive
low-priced 32
4
5 mid-priced 65
6 expensive 100
40
7

cheap

(A4)
Town

(A5)
District
Toulouse Minimes
Toulouse Rangueil
Toulouse Downtown
Toulouse
Rennes
Rennes Downtown
Betton

(A6)
Exposure

SE
SW

S

S

S(cid:48)

S(cid:48)(cid:48)

φ

φ

φ

φ

...

f

f

f

ˆπ

ˆπ(cid:48)

ˆπ(cid:48)(cid:48)

...

f

S(cid:48)···

ˆπ(cid:48)···

Fig. 1. Abstract modeling of the LearnOneRule process (see text for details). Grey cells illustrate selected rows and columns that may generate πCN 2

2

.

The contributions of this work are:
• we propose an abstract formalization of the rule learning
process introducing the generalization of examples as
a choice of a generalization rule π = (cid:98)S (for a set of
examples S) among supersets of S,

• we introduce the notion of admissibility, we derive two
alternative versions of admissibility from Kuratowski’s
axioms and we give sufﬁcient conditions on choice func-
tions to induce admissible generalizations,

• we instantiate admissibility in the speciﬁc case of metric
attributes and we analyze some CN2 results in the light
of our framework.

Note that we do not have an immediate practical objective:
our purpose is not to design a new rule learning algorithm
but to set a general framework that may help to shed light on
some aspects of existing, or future, rule learning algorithms.
In particular, we illustrate that a well-known algorithm such
as CN2 makes some counterintuitive choices of boundaries in
rules with metric attributes.

II. GENERALIZATION OF DATA AS A RULE

This work focuses on the LearnOneRule step of the rule
learning process [19], [8]. The LearnOneRule process is
viewed as a two-step process, depicted in Figure 1:

1) Some subsets of data are selected. In Figure 1, φ selects

possible subsets of data.

2) Each subset of data (both subset of columns and rows
of the dataset) is assumed to be generalized by a single
rule.

A subset of data is generalized by a rule. For a data subset
(A, S) (some examples restricted to some attributes) of the
dataset, the idea of the “generalization” function f from (A, S)
is to generate the rule π. Thus, a data subset is generalized by
a single rule.

This work investigates the f function, i.e., how to generate
a rule from a subset of data. We do not tackle the question of
determining φ, i.e., how to select data subsets. It is assumed
that rules are generated from all possible selected subsets.

TABLE II
RANGES FOR THE ATTRIBUTES OF THE DATASET FROM TABLE I.

Range

Attr.
A0 {cheap, low-priced, mid-priced, expensive}
A1
A2
A3
A4
A5
A6

[1, 500]
{1, 2, 3, 4, 5, 6}
{A, B, C, D, E}
{Toulouse, Rennes, Betton}
{Downtown, Rangueil, Minimes}
{S, N, W, E, SE, SW, N E, N W }

Structure
total order
metric
metric
total order

partial order

A. Data and rules

Data consist of tuples of size n (for n attributes A1, · · · , An).
It is assumed that each tuple is assigned a class value. The
range of an attribute Ai, denoted Rng Ai, may, or may not,
be ordered. For Table I, range of attributes and their structure
are given in Table II. Values of attributes are of various types:
ﬂoats (e.g., A1), integers (A2), discrete values either structured
(e.g., A6 can be partially ordered), or unstructured (e.g., A4).

A rule learning algorithm elicits rules of the form:

Aπ(1)(x) ∈ v1 ∧ · · · ∧ Aπ(k)(x) ∈ vk → C(x) ∈ v0

(∗)

where 1 ≤ k ≤ n, vi ⊆ Rng Aπ(i) for i = 1..k, v0 ⊆ Rng C
and {π(1), · · · , π(k)} ⊆ {1, · · · , n}.

Such a rule expresses that for an item x, if the value of
each attribute Aπ(i) is one within vi then the class value of x
is one within v0.

The value vi is a subset of the range of the attribute Aπ(i)
(or class C). So, vi can be a singleton subset {u} of the range
Rng Aπ(i) of the attribute i.e. Aπ(i)(x) ∈ vi is Aπ(i)(x) = u.
Or, vi can be a ﬁnite subset {u1, · · · , uip } of Rng Aπ(i) hence
Aπ(i)(x) ∈ vi is just the disjunctive condition Aπ(i)(x) = u1
or Aπ(i)(x) = u2 or . . . or Aπ(i)(x) = uip . Disjunctive
conclusions are unusual in rule learning but they could be
desired and they generalize the approach. Lastly, vi can be
an arbitrary subset of the range Rng Aπ(i). Structure over
Rng Aπ(i) can be exploited, e.g. Aπ(i)(x) ≥ r is captured by
setting vi to the interval [r, M ] (if M is the greatest element).

B. General form of rules

We can identify a rule with a sequence of values for some
attributes among A1, · · · , An as well as C thus resulting in
the general form for a rule π

π = Aπ(1)(x) ∈ vπ
with 1 ≤ kπ ≤ n, vπ
and {π(1), · · · , π(kπ)} ⊆ {1, · · · , n}.

1 ∧ · · · ∧ Aπ(kπ)(x) ∈ vπ
kπ
i ⊆ Rng Aπ(i) for i = 1..kπ, vπ

→ C(x) ∈ vπ
0

(†)

0 ⊆ Rng C

For the sake of simplicity, such a rule can be expressed as
a member of 2Rng C × 2Rng A1 × · · · × 2Rng An , i.e., a vector
1 vπ

2 · · · vπ
n)

(cid:126)π = (vπ

0 vπ

(‡)

where for i = 1..n, vπ

i = Rng Ai if Ai (cid:54)∈ {Aπ(1), · · · , Aπ(kπ)}.

A tuple (x1, · · · , xn) which is assigned the class value c
0 and all

is said to be covered by the rule (cid:126)π above if c ∈ vπ
xi ∈ vπ

i (for i ∈ {π(1), · · · , π(kπ)}).

Notation: In the sequel, Si denotes the set of values that

the attribute Ai takes in the subset S of the data, i.e.,

Si

def= {xi | (x0, x1, · · · , xn) ∈ S}

C. Admissible rule generation

This work focuses on ﬁnding one rule, generalizing a subset
of the dataset, which makes sense for the data scientist.
Learning one rule aims at extending the set of values actually
taken by the examples. Theoretically, any extra value, not
covered by a counter-example, would work. However, some
extended sets make more sense than others: we call this notion
rule admissibility.

We do not consider how to ﬁnd such a subset of the dataset
but, given such a subset, we investigate the question of what
is an admissible generalization of these examples for a user.
Please note that we don’t provide a deﬁnition for admissibility.
What we provide is a framework to express different notions of
“admissibility”. Indeed, it depends upon application and users.
The way we propose to analyze rule learning algorithms is to
confront practical results with these different notions.

There is a sense in which a data subset determines a single

rule. It is the view that the only rule generated by S is

A1(x) ∈ (cid:99)S1 ∧ · · · ∧ An(x) ∈ (cid:99)Sn → C(x) ∈ (cid:99)S0

(§)

where (cid:98)X is the smallest rule admissible superset of X (for
X ⊆ Rng Ai or X ⊆ Rng C). Please note that this requires
the assumption that attributes are independent for the purpose
of rule admissibility.

That S is a data subset under φ (see Figure 1) means that
if S is to amount to a rule π then each tuple in S is covered
by π. Therefore, such a rule (§) is to be of the kind

Aπ(1)(x) ∈ v1 ∧ · · · ∧ Aπ(k)(x) ∈ vk → C(x) ∈ v0

(∗∗)

where 1 ≤ k ≤ n, Sπ(i) ⊆ vi for i = 1, · · · , k and S0 ⊆ v0
(as usual, {π(1), · · · , π(k)} ⊆ {1, · · · , n}).

What vector (v0 v1 v2 · · · vn) can count as a rule for the
purpose of capturing S? Since (∗∗) is meant to capture S, we
are looking for a vector (cid:126)π ≥ (S0 S1 S2 · · · Sn) (i.e., Si ⊆ vπ
i
for i = 0, · · · , n) where every vπ
i

is rule admissible.

Technically, the least1 such π is the case that vi = Si for
i = 0, · · · , n. As a rule, it does not ﬁt. If S is to be viewed
as a rule, the intuition is that a tuple close enough to some
member(s) of S is expected to behave similarly to this member
(or those members).

D. Properties of generalization as choice

The intuition we point out in the latter remark suggests that
generalizing a set of values to a superset thereof amounts to
applying a closure-like2 operator (cid:98)· . For any attribute Ai and
data subset S, generalizing Si is identiﬁed with mapping Si to
(cid:98)Si, with properties taken from the list of Kuratowski’s axioms:

(cid:98)∅ = ∅
S ⊆ (cid:98)S ⊆ Rng Ai
(cid:98)(cid:98)S = (cid:98)S
(cid:100)S ∪ S(cid:48) = (cid:98)S ∪ (cid:98)S(cid:48)

(pre-closure)

Actually, we downgrade Kuratowski’s axioms as follows

(the Appendix reminds the deﬁnitions of related operators)

(cid:98)S ⊆ (cid:98)S(cid:48) whenever S ⊆ S(cid:48)
(cid:98)S = (cid:98)S(cid:48) whenever S ⊆ S(cid:48) ⊆ (cid:98)S
(cid:100)S ∪ S(cid:48) ⊆ (cid:98)S whenever S(cid:48) ⊆ (cid:98)S

(closure)
(cumulation)
(capping)

We thus arrive at

two classes of weaker operators that
are worth exploring further: preclosure operators and capping
operators, resp. realizing interpolation from single points and
interpolation from pairs of points (with the view that rule
generation encompasses interpolation of some kind).

This notion of rule admissibility is to be captured by a
selection function, f , to ﬁt the general view of Figure 1. Such
a function (from a special class) determines an appropriate
superset of Si given some subsets of the powerset of Rng Ai.
The intuition here is that rule admissible subsets of the range
Rng Ai of an attribute Ai can be characterized as choices
from the powerset of Rng Ai. Depending on what principles
underly the actual choice, a different kind of closure embodies
rule generation through the rule generalization principle.

The next theorems (with Rng Ai generalized to a set Z)
specify two classes of selection functions that induce a closure-
like operator over a powerset: preclosure and capping.

Theorem 1 (Selection functions inducing a preclosure operator):

Let Z be a set such that f : 22Z
→ 2Z is a function satisfying
the three conditions below for all X ⊆ 2Z such that X is
upward closed and all Y ⊆ 2Z:

1. f (2Z) = ∅
2. f (X ) ∈ X
3. f (X ∩ Y) = f (X ) ∪ f (Y) whenever (cid:83) min(X ∩ Y) =

(cid:83) min X ∪ (cid:83) min Y

The mapping (cid:101)· : 2Z → 2Z such that

(cid:101)X def= f ({Y | X ⊆ Y ⊆ Z})

is a preclosure operator on Z.

1 (cid:126)π ≤ (cid:126)π(cid:48) iff vπ
for i = 0, · · · , n.
2 Closure-like operators are topological operators, cf Appendix.

i ⊆ vπ(cid:48)

i

Theorem 2 (Selection functions inducing a capping operator):

Let Z be a set, f : 22Z
→ 2Z be a function obeying the next
two conditions for all X ⊆ 2Z s.t. (cid:84) X ∈ X and all Y ⊆ 2Z:
1. f (X ) ∈ X ,
2.
The mapping (cid:101)· : 2Z → 2Z such that

if Y ⊆ X and ∃H ∈ Y, H ⊆ f (X ) then f (Y) ⊆ f (X ).

(cid:101)X def= f ({Y | X ⊆ Y ⊆ Z})

is a capping operator on Z.

III. GENERATION OF RULES WITH METRIC ATTRIBUTES

Back to the idea of rule generation as interpolation, we are
considering the simplest case of rules with a collection of
items that all take the value u for metric attribute Ai and that
all are in class c. A rule for this case is

Ai(x) = u → C(x) = c

i.e. c = fi(u) for some function fi. This rule is restrictive as
it only applies for items that take exactly the value u for Ai.
As items take a set of speciﬁc values {u1, u2, . . . , un}, our
idea is to generate rules that generalize them to an interval
of values [v, w]. The main issue is to determine classes of
selection functions that yield intuitive intervals of values.

The ﬁrst approach is to propose a neighborhood principle.
Since c = fi(u), it seems rather reasonable to still expect the
class to be c for all values close enough to u. Assuming a
notion of neighborhood, a rule exemplifying this would be

Ai(x) ∈ [u − r, u + r] → C(x) = c.

This is developed in the next section where a class of selection
functions is given that all induce a preclosure operator.

A drawback of the neighborhood approach is its predeﬁned
radius, r, which does not take into account the actual values
distribution when it comes to ﬁnding intervals. Another section
proposes a second approach that deals with interpolation from
pairs (u, v) of values for an attribute Ai. This captures the
idea that an interval of values is made of elements that are
close enough to each other. We show that this principle can
be captured through a capping operator.

A. Neighborhoods

As an application, consider neighborhoods for real-valued
data (i.e., Rng Ai ⊆ IR). For a datum u ∈ IR, we look at a
generalization for u in the form of the neighborhood centered
at u of radius r, for a given r > 0. For r > 0 ∈ IR, let
nr : 2IR → 2IR be the function:

nr(X) def=

(cid:40) ∅
(cid:83)
u∈X

if X = ∅
[u − r, u + r] otherwise

It happens that nr is a preclosure operator, i.e., as presented
in the Appendix, nr is a mapping c : 2U → 2U such that:

c(∅) = ∅
X ⊆ c(X) ⊆ U
c(X ∪ Y ) = c(X) ∪ c(Y )

(null ﬁxpoint)
(extension)
(preservation of binary unions)

Besides this independent evidence, we get the same conclusion
from Theorem 1, giving an actual selection function f . Now,
a useful abbreviation is ↑ {X} def= {Y | X ⊆ Y ⊆ Z}.

For a subset S of IR, deﬁne f (↑ {S}) by:
(cid:40) ∅
(cid:83)
x∈S

if S = ∅
[x − r, x + r] otherwise

f (↑ {S}) def=

Extend f to all of 22Z

by taking
(cid:91)

f (X ) def=

f (↑ {S}).

S ∈ min X
(Since X denotes a collection of subsets of Z, min X denotes
those sets in X that have no proper subset in X .)
Then, f satisﬁes conditions 1-3 of Theorem 1.

B. Intervals

We look now at generalization from Si as interpolation of
the kind: If u ∈ Si and v ∈ Si such that the distance between
u and v is smaller than some threshold then generalize u and
v to all values (in the range of Ai) between u and v. We
again follow the idea that generalizing Si amounts to applying
some kind of closure operator (cid:98)· , giving (cid:98)Si. We start with
considering closure operators (see the Appendix), i.e., for all
S ⊆ Rng Ai and S(cid:48) ⊆ Rng Ai, the following holds:

S ⊆ (cid:98)S ⊆ Rng Ai,
(cid:98)(cid:98)S = (cid:98)S,
(cid:98)S ⊆ (cid:98)S(cid:48) whenever S ⊆ S(cid:48).
Interestingly, for a data subset S, in order to determine the
rule π induced by S, applying (cid:98)(cid:98)S = (cid:98)S means that if Si is rule
admissible then it is enough to set vπ
i = Si and no further
adjustment over π is needed regarding the attribute Ai (further
adjustements are likely for other attributes).
C. Example of (cid:98)· not being a closure operator

intervals over IN. For instance,

Imagine a principle that generalizes values (from IN) to
let attribute A2 be
small
distance to townhall. Let the class attribute C be level of rent
(understood as ranging over cheap, low-priced, . . . ). From the
minimalistic S consisting of items 1 and 2 below:

A1

. . .

. . .

item 1

item 2

Distance
to townhall
3 km

7 km

. . .

. . .

. . .

Level
of rent
cheap

cheap

then such a principle could make S = {item 1, item 2} to
generate a rule π with vπ
2 = [3, 7] (i.e., 3 km to 7 km).
Intuitively, the rule would express that ﬂats for rent within
3 to 7 km from the town hall are most affordable. However,
from S(cid:48) consisting of items 1 to 4 as follows:

A1

. . .

. . .

. . .

. . .

item i(cid:48)
1
item i(cid:48)
2
item i(cid:48)
3
item i(cid:48)
4

Distance
to townhall
3 km

7 km

1 km

9 km

. . .

. . .

. . .

. . .

. . .

Level
of rent
cheap

cheap

cheap

cheap

the same principle can make S(cid:48) = {i(cid:48)
with vπ(cid:48)
because (isotony) fails: S2 ⊆ S(cid:48)

4} to give a rule
2 = [1, 3] ∪ [7, 9]. This is a counterexample to closure
2 but vπ

2 (cid:54)⊆ vπ(cid:48)
2 .

1, . . . , i(cid:48)

π(cid:48) says that rents of ﬂats in the vicinity of the town hall are
low and so are rents of ﬂats in a 7 to 9 km ring from the town
hall but no example conﬁrm this for ﬂats in the range 3 to 7 km.
We can regard [1,3] ∪ [7,9] as more admissible than the large [1,9].

1 to i(cid:48)

then the same principle can make S(cid:48) (items i(cid:48)
20) to give
a rule with vπ(cid:48)
2 = [4.1, 5] ∪ [5.5, 6.9]. Again, the idea is that
the gap between two consecutive values is to be turned into
an interval unless the gap is much greater than most of the
other gaps in the series: the gap from 5.0 to 5.5 has length .5
but all other gaps here (from 4.1 to 4.2, . . . , from 6.8 to 6.9)
have length at most .2.

D. Example of (cid:98)· not being a cumulation operator

Since (cid:98)·

fails to be a closure operator, the next possibility
is a cumulation operator (every closure operator is

is that (cid:98)·
a cumulation operator but the converse is untrue).

Again, think of some principle that generalizes values (from
IN) to small intervals over IN. Here is a brief example (we use
decimals of km to abbreviate hundreds of meters), with S
consisting of items i1 to i9 below:

A1

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

item 1

item 2

item 3

item 4

item 5

item 6

item 7

item 8

item 9

Distance
to townhall
4.1 km

4.4 km

4.8 km

5 km

5.5 km

5.8 km

6.1 km

6.6 km

6.9 km

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

Level
of rent
cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

Then, such a principle could make S (the above 9 items) to
generate a rule with vπ
2 = [4.1, 6.9]. Indeed, gaps between
any two consecutive values among these nine values are of
somewhat similar length and are turned into intervals. Now,
from S(cid:48) consisting of items i(cid:48)

20 below:

1 to i(cid:48)
Distance
to townhall
4.1 km

4.2 km

4.3 km

4.4 km

4.6 km

4.7 km

4.8 km

6.6 km

6.9 km

4.1 km

4.1 km

4.4 km

4.8 km

5 km

5.5 km

5.8 km

6.1 km

6.6 km

6.8 km

6.9 km

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

Level
of rent
cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

cheap

A1

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

10

item i(cid:48)
1
item i(cid:48)
2
item i(cid:48)
3
item i(cid:48)
4
item i(cid:48)
5
item i(cid:48)
6
item i(cid:48)
7
item i(cid:48)
8
item i(cid:48)
9
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)
item i(cid:48)

14

15

16

17

18

19

20

11

12

13

but vπ

2

2 ⊆ vπ

2 (cid:54)⊆ vπ(cid:48)

(still, vπ(cid:48)

It is a counterexample to cumulation because S2 ⊆ S(cid:48)
2 ).
All this suggests some kind of preservation principle:
If new items conﬁrming a rule are added, gener-
alization should not make the rule to be further
generalized.

2 ⊆ vπ
2

It seems that such an approach to generalizing a set of values
to a superset thereof amounts to applying a capping operator.
The next section shows that such a view can be identiﬁed with
using a selection function (from the special class speciﬁed in
Theorem 2) to determine the appropriate superset eliciting the
rule.

E. Capping operator: selection via power means

Let the special case that Z is totally ordered and f selects,
among all supersets of S = (cid:104)x1, . . . , xm(cid:105), 3 the union of the
intervals over Z that have both endpoints in S and length
(denoted by l) bounded by a threshold value ∆(S) as follows
(cid:26)[xj, xj+1] if l([xj, xj+1]) ≤ ∆(S)
else (including j = m)
[xj, xj]

ϕS(xj) def=

(1)

where ∆ is a function on the increasing sequences over Z, i.e.
S (cid:55)→ ∆(S) for every increasing sequence S.

For all ﬁnite S ⊆ Z, deﬁne
f (↑ {S}) def=

(cid:91)

x∈S

ϕS(x)

(2)

where the ϕS : S → 2Z functions can be required to satisfy,
for all x ∈ S and all ﬁnite S(cid:48) ⊆ Z, the following constraints

x ∈ ϕS(x),

(i)
(ii) S ⊆ S(cid:48) ⊆ (cid:83) ϕS(S) ⇒ (cid:83) ϕS(cid:48)(S(cid:48)) ⊆ (cid:83) ϕS(S).

Extend f to all of 22Z

by taking

f (X ) def=

(cid:92)

X whenever X (cid:54)= ↑ {S} for all S ⊆ Z.

(3)

Proposition 1: If ϕ satisﬁes conditions (i) and (ii) then f

as deﬁned by (2)-(3) enjoys conditions 1.-2. of Theorem 2.

We focus on intervals with endpoints in IR (hence S ⊆ IR)

and length the absolute difference between both endpoints.

Proposition 2: Let ϕ be as in (1) with ∆ such that for all
ﬁnite S and S(cid:48), if S ⊆ S(cid:48) ⊆ (cid:83) ϕS(S) then ∆(S(cid:48)) ≤ ∆(S).
If ∆ is real-valued, if Z is IR and if l([x, y]) = |y − x| then
ϕ satisﬁes (i)-(ii).

An almost direct application of Theorem 2 then implies
that functions deﬁned as in (1) induce capping operators. The

3 Here, S is identiﬁed with its enumeration in increasing order since this

simpliﬁes the formulation in (1).

following instances for ∆ functions give selection functions,
as per (1)–(3), generating admissible rules:

• Geometric mean: ∆(S) =

(xi+1 − xi)

(cid:115)
• Higher power means: ∆(S) = p

1
m−1

p
(xi − xi−1)

m
(cid:80)
i=2

(cid:18)m−1
(cid:81)
i=1

(cid:19) 1

m−1

IV. RELATED WORK

Rule learning algorithms are a class of machine learning
algorithms mainly developed in the 90’s [22] that drew recent
interest due to the interpretable nature of its outputs [8].

Prior works have proposed foundations for rule learning
and many algorithms exist. A major reference is [8] that
broadly presents the concepts used in rule learning algorithms.
It mainly focuses on practical aspects that enable the reader to
understand a broad range of algorithms. Our framework aims
at turning the rule learning formalization to a more conceptual
level. We investigate in particular the LearnOneRule step of
the rule learning process. A key issue in the LearnOneRule
algorithm is how to evaluate and compare different rules [8]
using several measures such as precision, information gain,
correlation, m-estimate, etc. The basic principle underlying
these measures is a simultaneous optimization of consistency
and coverage. This optimization addresses the way of choosing
a subset of examples covered by a rule but does not give
any information on the interest of a generalization from a
data scientist viewpoint. Our framework allows to address
admissibility of generalizations and thus to deﬁne classes of
generalizations able to catch empirical concepts of neighbor-
hood for example.

This notion of admissibility contributes to a formalization
of the interpretativeness of the outputs of rule learning al-
gorithms. Similar questions have been addressed in previous
works. To the best of our knowledge, none of them addressed
the problem of the choice of the values in the rules. They take
into consideration the structure of the rules (e.g. their size) or
the rule set [4], [2], [21], [17], [3]. Instances of the GUHA
method to mining association rule [12] fall under our approach
if conclusions of such rules are to play the role of classes.

In [1], the proposed framework is based on a score for rule
quality measures. It does not use quality measures that select
intuitive rules, but the most accurate ones. [15] addressed the
intuitiveness of rules through the effects of cognitive biases.
They notice that a number of biases can be triggered by the
lack of understanding of attributes or their values appearing
in rules. In [9], the authors suggest that “longer explanations
may be more convincing than shorter ones” (see [20], too) and
evaluate this criterion using a crowd-sourcing study based on
about 3.000 judgments.

Hence, one of our contributions is to relate generalization of
examples to closure-like operators. Relationship with Formal
Concept Analysis [10] then comes to mind. In [14], the authors
investigate the problem of mining numerical data with Formal
Concept Analysis. This amounts to a way to generate some
subsets of data. As we have shown that the operators at work

Fig. 2. Data distributions for classes blue (in blue) and green (in green).
In the upper histogram, the data distributions are simulated using uniform
distributions. In the lower histogram, the data distributions are simulated using
a mixture of two normal distributions per class.

in generalizing by intervals are weaker than closure operators,
no equivalence is expected. Even the idea that a subset (A, S)
of the dataset is always a subset of a concept fails in general.
For instance, let φ capture the idea of “contraries”, in which
case a selected subset of size two consists in two examples
e1 and e2 such that Ai(e1) (cid:54)= Ai(e2) for all Ai in A hence
σ({e1, e2}) = ∅ which entails that no superset of {e1, e2} can
be a concept with a non-empty set of attributes. In contrast,
there exist selection functions that provide a subset of Rng Ai
as a generalization for {e1, e2}.

V. ILLUSTRATION WITH CN2 RULES

We illustrate some behaviours of CN2 [5] when facing artiﬁ-
cial data distributions, to show that our proposed formalisation
offers a framework for analyzing rule learning algorithms. We
use a simulated dataset with a single numerical attribute, v, and
two classes. The form of the generated rules is v ∈ [l, u] ⇒ C
where [l, u] is an interval and C ∈ {blue, green}.

Remember, the abstract modeling of a rule learning process
has two steps: selection of a subset of data and generalisation
of this subset of data by a rule. This article is focused on
the generalisation step. The simple case studies below make
the assumption that each class of the dataset corresponds to a
subset of examples to generalize. Then, our analysis assumes
two subsets of data (and thus two rules): the subset of data
labeled as blue and the ones labeled as green. These two
subsets are for illustration purposes only, we make no claim
that they are more sensible than other alternative subsets.

A. CN2 splits potentially interesting intervals

Here, we illustrate the fact that, despite the relative continu-
ity of the attribute, the CN2 algorithm splits attribute intervals
of a rule in some speciﬁc cases of overlapping values.

Figure 2 illustrates two data distributions for which we
run the CN2 algorithm. The distribution of each class is
dense from lower to higher value, i.e., the gaps between two
consecutive examples of a class are bounded. It is desirable
to have exactly one rule per class generalizing all examples.
This would be the case if rule generalization were to follow
the principles of neighborhoods or intervals applied on data
subsets made of examples belonging to the same class. Yet,
the CN2 algorithm splits the interval in several sub-intervals
to improve its selection criteria based on accuracy.

The extracted rules in case of uniform distributions (top of
Figure 2), resp., for normal distributions (bottom of Figure 2)

0510152025024681012141618105051015202505101520253035are in the leftmost list, resp., in the rightmost list below:

• v ∈ [−∞, 0.96] ⇒ blue
• v ∈ [−∞, 10.03] ⇒ blue
• v ∈ [12.73, 14.83] ⇒ blue • v ∈ [0.97, 2.57] ⇒ blue
• v ∈ [10.65, 12.81] ⇒ green • v ∈ [3.09, 10.04] ⇒ blue
• v ∈ [3.50, 7.18] ⇒ green
• v ∈ [15.01, ∞] ⇒ green
• v ∈ [11.55, 13.14] ⇒ green
• v ∈ [13.15, ∞] ⇒ green

In the case of the uniform distributions, the intervals of rules
with different decision classes may overlap. CN2 thus allows
for intervals occurring in different rules to overlap.

B. Impact of example density on boundaries choice

Figure 3 illustrates the case of two (single-attribute) datasets
whose class distributions are similar: uniform distribution with
the same bounds, but different intensities. Example-classes are
balanced in the ﬁrst dataset but not in the second.

Two datasets centered on 0 (each bar is one example). The gap
Fig. 4.
between consecutive examples is 1 unit at the top and 10 units at the bottom.

the same. Our experiments show that the gap length has no
consequences on the rules. The very same rule is generated
splitting examples by comparing their value with 0. We can
conclude from this example that CN2 does not behave in the
way described by preclosure operators.

The two preceding experiments seem to show that rules
are generated only from the extreme values of examples sets
without considering the actual example distribution. Such a
behaviour appears to be more constrained than the one of a
capping operator. It amounts to a ∆ function that would not be
decreasing. But these examples are speciﬁc cases of datasets
with well-separated classes. In case of overlapping range of
values (see Figure 2), the rule choices that have been made
depend on example distributions.

Fig. 3. Balanced (on top) and unbalanced (on bottom) distributions of two
classes examples. Classes are separated by a ﬁxed distance.

VI. CONCLUSION AND PERSPECTIVES

The idea here is to study the impact of multiple instances
on the choice of boundaries by CN2. As per the function
inducing a capping operator, ∆ has to be non-decreasing
over conﬁrming examples. So, having more examples leads to
change the boundaries of the rule that generalizes examples.
But in both cases, CN2 ﬁnds the very same bound 3.49:
v ∈ [−∞, 3.49] ⇒ blue and v ∈ [3.49, ∞] ⇒ green.

This illustrates the more general situation that an approach
insensitive to density of examples over the choice of bound-
aries amounts to a cumulation operator:

Theorem 3: Let Z be a set such that f : 22Z

→ 2Z is a
function satisfying the two conditions below for all nonempty
X ⊆ 2Z and Y ⊆ 2Z:

1. f (X ) ∈ X ,
2. f (X ∪ Y) ∈ X ⇒ f (X ∪ Y) = f (X ).

The mapping ¯· : 2Z → 2Z such that

X def= f ({Y | X ⊆ Y ⊆ Z})

is a cumulation operator on Z.

C. Impact of example sparsity on boundaries choice

Figure 4 illustrates two datasets whose example distributions
differ by the gaps between consecutive examples of the same
class. In the ﬁrst case, these gaps are small (average gap of 1
unit) w.r.t. the gap between the two classes (3 units). In such
a case, we expect to generate a rule that gather all examples
in a single interval. It is actually what happens with CN2. In
the second case, gaps between consecutive examples are larger
(average gap of 10 units), but the behaviour of CN2 remains

Evaluation and comparison of rules in the rule learning
task are currently based on optimization of consistency and
coverage measures. In this article, we look at the notion of
rule admissibility as the interest of a generalization from a
data scientist viewpoint. We deﬁne a framework providing
a formal approach to the generalization of examples in the
attribute-value rule learning task and some foundations for the
admissible generalizations of examples as rules. Our notion of
admissibility is presented as a choice of one generalization
among all supersets of examples. Distinguished notions of
choice are shown to capture a closure-like operator (preclosure
or capping). In the case of metric attributes, we offer actual
selection functions that induce such operators.

These selection functions show how our framework supports
the analysis of rule-learning evaluation. We have generated
synthetic datasets to analyze the behaviour of CN2 in view of
notions arising from our framework. Thus, we point out some
counter-intuitive behaviours of this algorithm.

Some novelty in our work lies with it focussing on the val-

ues occuring in rules, instead of, e.g., the structure of rules.

Since rule learning may involve non-numerical attributes, a
short term perspective is to also give selection functions for
attributes with weaker structure than enjoyed by numerical
attributes (metric structure). Finally,
this article does not
address the issue of selection of data subsets. Future work is to
focus on this part of the process to propose a more complete
formal model for attribute-value rule learning.

APPENDIX
Given a set U, a Kuratowki closure operator [16] is a map-
ping c : 2U → 2U such that for all X ⊆ 2U and all Y ⊆ 2U

01234567024681012012345670246810121416151050510150.00.20.40.60.81.0100500501000.00.20.40.60.81.0c(∅) = ∅
X ⊆ c(X) ⊆ U
c(X) = c(c(X))
c(X ∪ Y ) = c(X) ∪ c(Y )

(null ﬁxpoint)
(extension)
(idempotence)
(preservation of binary unions)

A ﬁrst direction to weaken Kuratowki closure operators is to
drop (idempotence), resulting in preclosure operators

c(∅) = ∅
X ⊆ c(X) ⊆ U
c(X ∪ Y ) = c(X) ∪ c(Y )

(null ﬁxpoint)
(extension)
(preservation of binary unions)

Another direction amounts to dropping (null ﬁxpoint) and
replacing (preservation of binary unions) by a weaker axiom
with all this giving abstract closure operators

X ⊆ c(X) ⊆ U
c(X) = c(c(X))
X ⊆ Y ⇒ c(X) ⊆ c(Y )

(extension)
(idempotence)
(isotony)

These, in turn, can be weakened (various axioms weaker than
(isotony) are detailed in [18]) to cumulation operators

X ⊆ c(X) ⊆ U
X ⊆ Y ⊆ c(X) ⇒ c(X) = c(Y )

(extension)
(cumulation)

which can themselves be weakened to capping operators4

X ⊆ c(X) ⊆ U
Y ⊆ c(X) ⇒ c(X ∪ Y ) ⊆ c(X)

(extension)
(capping)

For cumulation and capping operators, (idempotence) holds as
it is actually a consequence of the other two axioms.

SKETCH OF PROOFS

Proof:

[Theorem 1] (Preservation of binary unions) For
X = ↑ {X} and Y = ↑ {Y }, the proviso for condition 3. is
(cid:83) min(↑ {X} ∩ ↑ {Y }) = (cid:83) min ↑ {X} ∪ (cid:83) min ↑ {Y }.
However, ↑ {X} ∩ ↑ {Y } = ↑ {X ∪ Y } hence the proviso
becomes (cid:83) min(↑ {X ∪ Y }) = (cid:83) min ↑ {X} ∪ (cid:83) min ↑ {Y }
i.e. X ∪ Y = X ∪ Y (as min(↑ {W } = {W }). Condition 3.
gives f (↑ {X} ∩ ↑ {Y }) = f (↑ {X}) ∪ f (↑ {Y }). Applying
↑ {X} ∩ ↑ {Y } = ↑ {X ∪ Y } once again, f (↑ {X ∪ Y }) =
f (↑ {X}) ∪ f (↑ {Y }). Equivalently, (cid:103)X ∪ Y = (cid:101)X ∪ (cid:101)Y .

Proof:

[Theorem 2] (Capping) Assume Y ⊆ (cid:101)X i.e.
Y ⊆ f (↑ {X}). 1. can be applied to give X ⊆ f (↑ {X}).
Therefore, X∪Y ⊆ f (↑ {X}). In view of X∪Y ∈ ↑ {X ∪ Y },
this gives ∃H ∈ ↑ {X ∪ Y } such that H ⊆ f (↑ {X}). Also,
↑ {X ∪ Y } ⊆ ↑ {X} because ↑ is antitone. Applying now 2.,
f (↑ {X ∪ Y }) ⊆ f (↑ {X}) ensues, i.e., (cid:103)X ∪ Y ⊆ (cid:101)X.

REFERENCES

[1] José L. Balcázar and Francis Dogbey. Evaluation of association rule
quality measures through feature extraction.
In Allan Tucker, Frank
Höppner, Arno Siebes, and Stephen Swift, editors, Proceedings of the
12th International Symposium on Advances in Intelligent Data Analysis
XII (IDA’2013), volume 8207 of Information Systems and Applications,
pages 68–79, London, UK, October 2013. Springer.

[2] Fernando Benites and Elena Sapozhnikova. Hierarchical interestingness
measures for association rules with generalization on both antecedent
and consequent sides. Pattern Recognition Letters, 65:197–203, 2015.

4Capping originates in logic where it is called Restricted Cut as it captures
the principle that intermediate conclusions can be freely removed from the
premises with no loss among conclusions.

[3] Mario Boley, Bryan R. Goldsmith, Luca M. Ghiringhelli, and Jilles
Vreeken.
Identifying consistent statements about numerical data with
dispersion-corrected subgroup discovery. Data Mining and Knowledge
Discovery, 31(5):1391–1418, September 2017.

[4] Alberto Cano, Amelia Zafra, and Sebastián Ventura. An interpretable
Information Sciences, 240:1–20,

classiﬁcation rule mining algorithm.
2013.

[5] Peter Clark and Tim Niblett. The CN2 induction algorithm. Machine

Learning, 3(4):261–283, 1989.

[6] William W Cohen. Fast effective rule induction. In Armand Prieditis
and Stuart J. Russell, editors, Proceedings of the 12th International
Conference on Machine Learning (ICML’1995), pages 115–123, Tahoe
City, CA, USA, July 1995. Morgan Kaufmann.

[7] Hugo Jair Escalante, Sergio Escalera, Isabelle Guyon, Xavier Baró,
Ya˘gmur Güçlütürk, Umut Güçlü, and Marcel A. J. van Gerven, editors.
Explainable and Interpretable Models in Computer Vision and Machine
Learning. The Springer Series on Challenges in Machine Learning.
Springer, 2018.

[8] Johannes Fürnkranz, Dragan Gamberger, and Nada Lavraˇc. Foundations

of Rule Learning. Springer Science & Business Media, 2012.

[9] Johannes Fürnkranz, Tomás Kliegr, and Heiko Paulheim. On cog-
nitive preferences and the plausability of rule-based models. CoRR,
abs/1803.01316, 2019.

[10] Bernhard Ganter and Rudolf Wille. Formal Concept Analysis: Mathe-
matical Foundations. Springer Science & Business Media, 2012.

[11] Liqiang Geng and Howard J. Hamilton.

Interestingness measures for

data mining: A survey. ACM Computing Surveys, 38(3), 2006.

[12] Petr Hájek, Martin Holena, and Jan Rauch. The GUHA method and its
meaning for data mining. Computer System Science, 76(1):34–48, 2010.
[13] Jon Hills, Anthony J. Bagnall, Beatriz de la Iglesia, and Graeme
Richards. BruteSuppression: a size reduction method for apriori rule
sets. Intelligent Information Systems, 40(3):431–454, 2013.

[14] Mehdi Kaytoue, Sergei O. Kuznetsov, and Amedeo Napoli. Revisiting
numerical pattern mining with formal concept analysis.
In Toby
Walsh, editor, Proceedings of the 22nd International Joint Conference
on Artiﬁcial Intelligence (IJCAI’2011), pages 1342–1347, Barcelona,
Catalonia, Spain, July 2011. IJCAI/AAAI Press.

[15] Tomás Kliegr, Stepán Bahník, and Johannes Fürnkranz. A review
of possible effects of cognitive biases on interpretation of rule-based
machine learning models. CoRR, abs/1804.02969, 2018.

[16] Kazimierz Kuratowski. Topology, volume I. Academic Press, 1966.
[17] Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec.

Inter-
pretable decision sets: A joint framework for description and prediction.
In Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.
Aggarwal, Dou Shen, and Rajeev Rastogi, editors, Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (KDD’2016), pages 1675–1684, San Francisco, CA,
USA, August 2016. ACM.

[18] David Makinson. General patterns in nonmonotonic reasoning. In Dov
M. Gabbay, Chris J. Hogger, and John Alan Robinson, editors, Hand-
book of Logic in Artiﬁcial Intelligence and Logic Programming, volume
III (Donald Nute, volume co-ordinator). Clarendon Press, Oxford, 1994.
[19] Tom M Mitchell. Generalization as search. Artiﬁcial Intelligence,

18:203–226, 1982.

[20] Julius Stecher, Frederik Janssen, and Johannes Fürnkranz. Shorter rules
are better, aren’t they? In Toon Calders, Michelangelo Ceci, and Donato
Malerba, editors, Proceedings of
the 19th International Conference
on Discovery Science (DS’2016), volume 9956 of Lecture Notes in
Computer Science, pages 279–294, Bari, Italy, October 2016. Springer.
[21] Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica
Klampﬂ, and Perry MacNeille. Bayesian rule sets for interpretable
classiﬁcation. In Francesco Bonchi, Josep Domingo-Ferrer, Ricardo A.
Baeza-Yates, Zhi-Hua Zhou, and Xindong Wu, editors, 16th IEEE
International Conference on Data Mining (ICDM’2016), pages 1269–
1274, Barcelona, Spain, December 2016. IEEE.

[22] Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal. Data
Mining: Practical Machine Learning Tools and Techniques. Morgan
Kaufmann, 4th edition, 2016.

