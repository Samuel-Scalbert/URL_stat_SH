Deep Active Learning from Multispectral Data Through
Cross-Modality Prediction Inconsistency
Heng Zhang, Elisa Fromont, Sébastien Lefevre, Bruno Avignon

To cite this version:

Heng Zhang, Elisa Fromont, Sébastien Lefevre, Bruno Avignon. Deep Active Learning from Multi-
spectral Data Through Cross-Modality Prediction Inconsistency. ICIP 2021 - 28th IEEE International
Conference on Image Processing, Sep 2021, Anchorage, United States. pp.1-5. ￿hal-03236409￿

HAL Id: hal-03236409

https://hal.science/hal-03236409

Submitted on 26 May 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

DEEP ACTIVE LEARNING FROM MULTISPECTRAL DATA THROUGH
CROSS-MODALITY PREDICTION INCONSISTENCY

Heng ZHANG1,3

Elisa FROMONT1,4

S´ebastien LEFEVRE2

Bruno AVIGNON3

1Univ Rennes, IRISA

2Univ Bretagne Sud, IRISA

3ATERMES Company

4 IUF, Inria

ABSTRACT

Data from multiple sensors provide independent and com-
plementary information, which may improve the robustness
and reliability of scene analysis applications. While there
exist many large-scale labelled benchmarks acquired by a
single sensor, collecting labelled multi-sensor data is more
expensive and time-consuming.
In this work, we explore
the construction of an accurate multispectral (here, visible
& thermal cameras) scene analysis system with minimal
annotation efforts via an active learning strategy based on
the cross-modality prediction inconsistency. Experiments on
multiple multispectral datasets and vision tasks demonstrate
the effectiveness of our method. In particular, with only 10%
of labelled data on KAIST multispectral pedestrian detection
dataset, we obtain comparable performance as other fully
supervised State-of-the-Art methods.

Index Terms— Active learning, multispectral pedestrian

detection, semantic segmentation, multiple sensor fusion

1. INTRODUCTION

The development of deep learning in computer vision greatly
enhances the ability of scene analysis and empowers many
intelligent vision systems. For example, object detection
and semantic segmentation methods have been applied to
autonomous driving and automated video surveillance. How-
ever, most of these methods are based on RGB images, and
their performance may be compromised in many real life
situations (such as nighttime or shaded areas).
In order to
solve these difﬁcult cases, multispectral systems have been
introduced, in two types of camera sensors (e.g. RGB and
thermal) are combined to provide complementary informa-
tion under various illumination conditions. RGB cameras
extract colour and texture visual details while the thermal
ones provide heat maps (based on temperature) of the scenes.
In Fig. 1, we show some image pairs from visible &
thermal cameras of identical scenes and their corresponding
monospectral pedestrian detection results. In this ﬁgure, the
image acquisition and the pedestrian detection from the two
modalities are completely independent. We split these multi-
spectral image pairs into two categories: pairs with consistent
detections (on the left side of Fig. 1) and inconsistent de-

Fig. 1. Exemplary multispectral image pairs and their corre-
sponding mono-spectral pedestrian detection results.

tections (on the right side). From these image pairs, we can
observe that the detection results from the two modalities are
similar in most cases, which indicates the redundancy for a
multispectral system; whereas at least one modality is wrong
when the detections are contradictory, which demonstrates
the complementarity of multispectral systems.

While there exist many large-scale benchmarks acquired
by a single sensor, collecting labelled multi-sensor data is
more expensive and time-consuming. E.g., acquiring well-
aligned multispectral image pairs requires speciﬁc equipment,
and few open datasets acquired with a similar equipment can
be used as supplementary data. We suggest relying on the
redundancy and complementarity of different sensors for the
adaptive selection of multispectral samples to be annotated.
Our proposed active criterion is based on the cross-modality
prediction inconsistency, deﬁned by the mutual information
between predictions from different modalities. To the best of
our knowledge, this is the ﬁrst work in deep active learning
within the context of multispectral scene analysis (including
object detection and semantic segmentation).

In Section 2 we review some representative work on mul-
tispectral scene analysis and active learning; Section 3 intro-
duces implementation details of our approach; In Section 4,
we evaluate our method on three different public multispec-
tral datasets [1, 2, 3]; Section 5 concludes the paper.

Consistent detectionsInconsistent detections2. RELATED WORK

2.1. Multispectral pedestrian detection

[4] demonstrated the ﬁrst application of deep learning-based
approach to multispectral pedestrian detection, where a late
fusion architecture is adopted for information fusion. Since
then, multiple studies [5, 6] explore the optimal network ar-
chitecture for multispectral feature fusion. It turns out that
the half-way feature fusion outperforms early or late fusion.
Moreover, [7, 8] apply attention mechanisms to learn an au-
tomatic re-weighting of visible and thermal features in the
fusion module; [9, 10] utilize illumination information as a
guidance for the adaptive fusion of both features; [11, 12] al-
leviate the inconsistency between visible and thermal features
to facilitate the optimization of a dual-modality network.

2.2. Multispectral semantic segmentation

MFNet [3] employs two identical backbone networks for vis-
ible and thermal feature extraction and a short-cut block to
concatenate the extracted features. Based on that, RTFNet
[13] integrates residual layers into this network architecture
to further boost the performance. FuseNet [14] adopts a sim-
ilar double feature extraction network for RGB-D semantic
segmentation. In this paper, we replace its RGB-D input im-
ages by multispectral images for comparison.

2.3. Active learning

Labelled data are critical for today’s supervised deep learning
applications. Active learning, which aims to relieve human
labelling efforts, is thus particularly appealing. The active
learning protocol usually starts by pre-training a model on a
small subset of the labelled dataset Dl. Then, several active
learning cycles are repeated. Fig. 2(a) illustrates a typical ac-
tive learning cycle. The model inference is performed on the
unlabelled dataset Du to select the most informative samples
(i.e., multispectral image pairs in our work). These selected
samples are then sent to an external oracle for annotation and
appended to the labelled dataset Dl, where the model is con-
sequently ﬁne-tuned on. The most important component of
an active learning cycle is the scoring function which ranks
the informativeness of unlabelled samples.

Most studies on deep active learning in computer vision
are based on mono-modal RGB images, including the most
recent ones in deep active learning for object detection [15,
16] and semantic segmentation [17]. Conversely to these ex-
isting works that score the informativeness of a single image,
we aim to score a pair of multispectral images according to
their relationships. Our work can be seen as complemen-
tary to existing methods, and coupling intra-modality (as done
with existing methods) and inter-modality (as proposed here)
informativeness scoring could lead to further improvements
than what is presented in this paper.

(a)

(b)

Fig. 2. Active learning loop diagram (a) and cross-modality
prediction inconsistency visualization (b).

Fig. 3. Overview of the proposed model for deep active mul-
tispectral scene analysis. The blue and green mono-modal
branches are used for data informativeness ranking while the
purple one provides the ﬁnal detection results.

3. APPROACH

3.1. Overview

An overview of our network architecture is given in Fig. 3.
It takes a spatially-aligned multispectral image pair as input,
then visible and thermal features are extracted independently
via the modality-speciﬁc feature extraction networks. After-
wards, three prediction branches are used: one based on vis-
ible features, one based on thermal features, and the last one
based on fused features. These three prediction branches are
jointly optimized during the model training phase. Note that
in Fig. 3 the prediction networks are used for a pedestrian de-
tection task but can be adapted to other vision tasks such as
general object detection or semantic segmentation.

3.2. Cross-modality prediction inconsistency

At the selection stage of each active learning cycle, we mea-
sure the relevance of labelling a particular image pair by rank-
ing the aforementioned cross-modality prediction inconsis-
tency, i.e., we compare predictions from visible and thermal
cameras, then select for labelling the image pairs with the
highest prediction difference. More speciﬁcally, for each pre-

fine-tuneLabelleddatasetselectionUnlabelleddatasetappendOracle (e.g., humanannotation)inferenceNeuralnetwork0.00.51.0Visible prediction0.00.51.0Thermal prediction0.00.20.40.60.8Unlabelled multispectralimage pairsFusion prediction branchVisible featureextraction branchThermal featureextraction branchVisible predictionbranchThermal prediction  branchMutualInformation RankingFusionSelect multispectralimage pairs for labellingdiction p, its inconsistency is deﬁned as:

I = H (p) −

1
2

(cid:88)

H (pm)

m∈{v,t}

Where pv and pt denote the prediction from visible and
thermal prediction branches; p is the average of both predic-
tions; H is the 2-set entropy function calculated as:

in 4,128 multispectral pairs for training. The usual mean Av-
erage Precision (mAP) metric is applied for evaluation.
TOKYO Dataset [3].
This dataset provides labelled mul-
tispectral image pairs for semantic segmentation within the
ADAS context. It contains nine hand-labelled classes. It in-
cludes 2,338 multispectral pairs in total. Visible and thermal
images are again well aligned. The mean Intersection over
Union (mIoU) metric is adopted for evaluation.

H (p) = −p log p − (1 − p) log (1 − p)

4.2. Implementation details

For a better understanding of this inconsistency calcula-
tion, we plot in Fig. 2(b) the visualization of the inconsistency
score with different visible (x-axis) and thermal (y-axis) pre-
diction scores. It can be observed that this inconsistency score
varies from 0 (very consistent) to 1 (very different).

3.3. Scale-balanced inconsistency aggregation

After obtaining the inconsistency for one prediction (i.e. clas-
siﬁcation of an anchor box in object detection or classiﬁca-
tion of a pixel in semantic segmentation), we adopt the scale-
balanced strategy for full-images inconsistency aggregation.
This is justiﬁed because recent deep learning approaches ap-
ply feature pyramid for multi-scale prediction thus, if we di-
rectly average all predictions for a given image pair, the in-
consistency estimation will be dominated by the scale with
the most predictions (i.e., the largest feature map in a feature
pyramid). Therefore, we ﬁrst separately average the incon-
sistency for each pyramid scale, then average the averaged
inconsistency across all scales.

4. EXPERIMENTS

4.1. Datasets

KAIST Dataset [1].
This well-known multispectral
dataset is built for the pedestrian detection task.
In order
to tackle the misalignment problem between visible-thermal
image pairs, [18] proposes the “paired” annotations by sepa-
rately relabelling pedestrians for each modality. We remove
unpaired images according to the matching of visible and
thermal annotations, thus keeping 11,695 images for training.
For a fair comparison with other State-of-the-Art methods,
we evaluate our model with the Miss Rate metric (lower
is better) under the “reasonable” setting, i.e., a test set that
does not contain heavily/partially occluded pedestrians or
pedestrians smaller than 55 pixels.
FLIR Dataset [2].
This thermal dataset is released for
general object detection from thermal images within the Ad-
vanced Driver Assistance Systems (ADAS) context. Three
categories are involved: car, pedestrian and bicycle. [11] pro-
poses the multispectral version of FLIR dataset 1 by manually
aligning corresponding colour-thermal image pairs, resulting

1This aligned dataset can be downloaded here: http://shorturl.at/ahAY4

Network architecture. We adopt VGG16 [19] as the fea-
ture extraction network, GAFF [8] as the multispectral fea-
ture fusion network and SSD [20] as the prediction network
for the object detection tasks. For the semantic segmentation
task, the prediction branch is simply one layer of convolution
whose number of output channels is equal to the number of
classes. In order not to change the aspect ratio of the original
images, input images are resized to 480×384 or 640×512 for
KAIST and FLIR datasets (object detection) and 640 × 480
for TOKYO dataset (semantic segmentation). Random crop-
ping, expanding, ﬂipping are adopted for data augmentation.
Active learning setting.
For each active learning experi-
ment, we ﬁrst randomly initialize a labelled dataset Dl with b
images and pretrain the model on Dl; then we actively select
b images from an unlabelled dataset Du with the most cross-
modality prediction inconsistency I for annotation and add
these newly labelled images into Dl; afterwards we ﬁne-tune
the model with the new Dl; we repeat the previous two steps
until the annotation budget B is exhausted. Since semantic
segmentation annotations are more difﬁcult to acquire, we set
b to 200 and B to 1200 for the object detection tasks, b to 50
and B to 350 for the semantic segmentation task.

4.3. Results

Active vs Random.
Fig. 4 plots the performance evolu-
tions along all learning cycles for KAIST Dataset (subﬁgure
a and b), FLIR Dataset (c and d) and TOKYO Dataset (e and
f). For all multispectral datasets, all tasks, all evaluation met-
rics and all input resolutions, our active strategy (green lines
in the ﬁgure) achieves statistically signiﬁcant better perfor-
mance than the random strategy (red lines).
Active vs SotA. We list in Tables 1, 2 and 3 the compar-
isons between our active learning results and other State-of-
the-Art methods for each multispectral dataset. With a small
quantity of labelled data, our active models achieve com-
parable results with fully supervised SotA methods, which
demonstrates the effectiveness of the proposed method.

4.4. Visualization

We show in Fig. 5 some image pairs selected by our active
method. For each dataset, we plot the separate predictions
from the visible or thermal cameras, and their cross-modality

inconsistency map: our strategy does select some difﬁcult
cases where at least one modality makes mistakes. We be-
lieve that adding these informative examples into the labelled
dataset for ﬁne-tuning is the main reason for improvements.

5. CONCLUSION

In this paper, we start from the observation of the redundancy
and the complementarity of a multispectral system. We build
upon these to suggest relying on the cross-modality prediction
inconsistency as the criterion to select informative image pairs
for labelling within active learning cycles. Extensive exper-
iments on three public multispectral datasets and two scene
analysis tasks demonstrate the effectiveness of the proposed
method. To the best of our knowledge, our work is the ﬁrst
applying deep active learning for multispectral scene analysis.
We hope that our method could help reduce manual labelling
efforts when setting up multispectral or multi-sensor datasets.

Methods

ACF [1]
Halfway Fusion [21]
Fusion RPN+BF [5]
IAF R-CNN [10]
IATDNN+IASS [9]
CIAN [7]
MSDS-RCNN [6]
AR-CNN [18]
MBNet [12]
Ours (full dataset)
Ours (10.26% of data)

Day

Miss Rate (lower, better)
Night
All
47.32% 42.57% 56.17%
25.75% 24.88% 26.59%
18.29% 19.57% 16.27%
15.73% 14.55% 18.26%
14.95% 14.67% 15.72%
14.12% 14.77% 11.13%
11.34% 10.53% 12.94%
8.38%
9.94%
9.34%
7.86%
8.28%
8.13%
6.77%
8.86% 10.01%
7.70%
9.32% 10.13%

Table 1. Miss Rate comparisons on KAIST Dataset.

Methods
CFR [11]
GAFF [8]
Ours (full dataset)
Ours (29.07% of data)

mAP
-

AP75
AP50
-
72.4%
37.3% 72.7% 30.9%
37.0% 72.1% 31.2%
35.1% 71.0% 30.6%

Table 2. mAP comparisons on FLIR Dataset.

Methods

MFNet [3]
FuseNet [14]
RTFNet [13]
Ours (full dataset)
Ours (17.99% of data)

mIoU (higher, better)
Night
Day
All
39.7% 36.1% 36.8%
45.6% 41.0% 43.9%
53.2% 45.8% 54.8%
53.6% 46.8% 53.3%
51.0% 46.6% 48.9%

Table 3. mIoU comparisons on TOKYO Dataset.

(a) KAIST 480x384

(b) KAIST 640x512

(c) FLIR 480x384

(d) FLIR 640x512

(e) TOKYO recall

(f) TOKYO mIoU

Fig. 4. Experimental results of models trained by the pro-
posed active learning strategy (green lines) and random se-
lection strategy (red lines) on KAIST Dataset (a, b), FLIR
Dataset (c, d) and TOKYO Dataset (e, f). Black dotted lines
indicate fully supervised results.

Fig. 5. Examples of selected image pairs for labelling by the
proposed method. Zoom in to see details.

10030050070090011001300Number of images101214161820Miss Rate (%)19.2916.9214.6413.1411.9011.7919.2914.0512.8511.4011.0110.4810.21Random selectionActive learning1.713.425.136.848.5510.26Proportion in dataset (%)10030050070090011001300Number of images81012141618Miss Rate (%)18.6314.6212.6111.4110.489.8818.6312.7611.0710.469.999.328.86Random selectionActive learning1.713.425.136.848.5510.26Proportion in dataset (%)10030050070090011001300Number of images29313335mAP (%)29.932.133.134.134.434.429.931.132.533.233.834.135.1Active learningRandom selection4.849.6914.5319.3824.2229.07Proportion in dataset (%)10030050070090011001300Number of images2931333537mAP (%)28.832.233.233.934.935.131.231.732.232.933.837.0Active learningRandom selection4.849.6914.5319.3824.2229.07Proportion in dataset (%)2575125175225275325375Number of images4043464952555861recall (%)42.448.851.152.053.755.355.745.348.949.852.052.553.560.6Active learningRandom selection2.575.147.7110.2812.8515.4217.99Proportion in dataset (%)2575125175225275325375Number of images4042444648505254mIoU (%)40.644.546.447.649.250.551.042.445.346.247.848.349.253.6Active learningRandom selection2.575.147.7110.2812.8515.4217.99Proportion in dataset (%)TOKYO DatasetKAIST DatasetFLIR DatasetInconsistency mapThermal cameraVisible cameraInconsistency mapThermal cameraVisible cameraVisible cameraThermal cameraInconsistency mapbackgroundcarpersonbikecurvecar stopguardrailcolor_conebump6. REFERENCES

[1] Soonmin Hwang, Jaesik Park, Namil Kim, Yukyung
Choi, and In So Kweon, “Multispectral pedestrian de-
tection: Benchmark dataset and baselines,” in Proceed-
ings of IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2015.

[2] “Free ﬂir
ing,”
adas-dataset-form/.

thermal dataset

algorithm train-
for
https://www.flir.com/oem/adas/

[3] Q. Ha, K. Watanabe, T. Karasawa, Y. Ushiku, and
T. Harada, “Mfnet: Towards real-time semantic seg-
mentation for autonomous vehicles with multi-spectral
scenes,” in 2017 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), 2017.

[4] J¨org Wagner, Volker Fischer, Michael Herman, and
Sven Behnke, “Multispectral pedestrian detection us-
ing deep fusion convolutional neural networks,” in 24th
European Symposium on Artiﬁcial Neural Networks,
(ESANN), 2016.

[5] Daniel K¨onig, Michael Adam, Christian Jarvers, Georg
Layher, Heiko Neumann, and Michael Teutsch, “Fully
convolutional region proposal networks for multispec-
in 2017 IEEE Conference on
tral person detection,”
Computer Vision and Pattern Recognition Workshops,
CVPR Workshops, 2017.

[6] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang,
“Multispectral pedestrian detection via simultaneous de-
tection and segmentation,” in British Machine Vision
Conference (BMVC), 2018.

[7] Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang,
Hong Qiao, Kaizhu Huang, and Amir Hussain, “Cross-
modality interactive attention network for multispectral
pedestrian detection,” Information Fusion, 2019.

[8] Heng Zhang, Elisa Fromont, S´ebastien Lef`evre, and
Bruno Avignon, “Guided attentive feature fusion for
multispectral pedestrian detection,” in Proceedings of
the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV), January 2021.

[9] Dayan Guan, Yanpeng Cao, Jiangxin Yang, Yanlong
Cao, and Michael Ying Yang, “Fusion of multispectral
data through illumination-aware deep neural networks
for pedestrian detection,” Information Fusion, 2019.

[10] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang,
“Illumination-aware faster R-CNN for robust multispec-
tral pedestrian detection,” Pattern Recognition, 2019.

[11] Heng Zhang, Elisa Fromont, S´ebastien Lef`evre, and
Bruno Avignon, “Multispectral Fusion for Object De-
tection with Cyclic Fuse-and-Reﬁne Blocks,” in ICIP
2020 - IEEE International Conference on Image Pro-
cessing, 2020.

[12] Kailai Zhou, Linsen Chen, and Xun Cao, “Improving
multispectral pedestrian detection by addressing modal-
ity imbalance problems,” in European Conference on
Computer Vision (ECCV), 2020.

[13] Yuxiang Sun, Weixun Zuo, and Ming Liu, “RTFNet:
RGB-Thermal Fusion Network for Semantic Segmenta-
tion of Urban Scenes,” IEEE Robotics and Automation
Letters, vol. 4, no. 3, pp. 2576–2583, July 2019.

[14] C. Hazirbas, L. Ma, C. Domokos, and D. Cremers,
“Fusenet: incorporating depth into semantic segmenta-
tion via fusion-based cnn architecture,” in Asian Con-
ference on Computer Vision, November 2016.

[15] Chieh-Chi Kao, Teng-Yok Lee, Pradeep Sen, and Ming-
Yu Liu, “Localization-aware active learning for object
detection,” in Asian Conference on Computer Vision.
Springer, 2018, pp. 506–522.

[16] Hamed H Aghdam, Abel Gonzalez-Garcia, Joost van de
Weijer, and Antonio M L´opez, “Active learning for deep
detection neural networks,” in Proceedings of the IEEE
International Conference on Computer Vision, 2019.

[17] Arantxa Casanova, Pedro O. Pinheiro, Negar Ros-
tamzadeh, and Christopher J. Pal, “Reinforced active
learning for image segmentation,” in International Con-
ference on Learning Representations, 2020.

[18] Lu Zhang, Xiangyu Zhu, Xiangyu Chen, Xu Yang, Zhen
Lei, and Zhiyong Liu, “Weakly aligned cross-modal
learning for multispectral pedestrian detection,” in In-
ternational Conference on Computer Vision, 2019.

[19] K. Simonyan and A. Zisserman, “Very deep convolu-
tional networks for large-scale image recognition,” in
International Conference on Learning Representations,
2015.

[20] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexan-
der C. Berg,
“SSD: single shot multibox detector,”
in European Conference on Computer Vision (ECCV),
2016.

[21] Jingjing Liu, Shaoting Zhang, Shu Wang, and Dim-
itris N. Metaxas, “Multispectral deep neural networks
for pedestrian detection,” in Proceedings of the British
Machine Vision Conference 2016, BMVC 2016, York,
UK, September 19-22, 2016, 2016.

