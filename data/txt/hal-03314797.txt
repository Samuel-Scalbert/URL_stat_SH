”Don’t discuss”: Investigating Semantic and
Argumentative Features for Supervised Propagandist
Message Detection and Classification
Vorakit Vorakitphan, Elena Cabrio, Serena Villata

To cite this version:

Investigating Semantic and
Vorakit Vorakitphan, Elena Cabrio, Serena Villata. ”Don’t discuss”:
Argumentative Features for Supervised Propagandist Message Detection and Classification. RANLP
2021 - Recent Advances in Natural Language Processing, Sep 2021, Varna / Virtual, Bulgaria. ￿hal-
03314797￿

HAL Id: hal-03314797

https://hal.science/hal-03314797

Submitted on 5 Aug 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

“Don’t discuss”: Investigating Semantic and Argumentative Features for
Supervised Propagandist Message Detection and Classiﬁcation

Vorakit Vorakitphan, Elena Cabrio, Serena Villata
Universit´e Cˆote d’Azur, Inria, CNRS, I3S, France
vorakit.vorakitphan@inria.fr,
elena.cabrio@univ-cotedazur.fr, villata@i3s.unice.fr

Abstract

One of the mechanisms through which dis-
information is spreading online, in particular
through social media, is by employing pro-
paganda techniques. These include speciﬁc
rhetorical and psychological strategies, rang-
ing from leveraging on emotions to exploit-
In this paper, our goal
ing logical fallacies.
is to push forward research on propaganda de-
tection based on text analysis, given the cru-
cial role these methods may play to address
this main societal issue. More precisely, we
propose a supervised approach to classify tex-
tual snippets both as propaganda messages
and according to the precise applied propa-
ganda technique, as well as a detailed linguis-
tic analysis of the features characterising pro-
paganda information in text (e.g., semantic,
sentiment and argumentation features). Ex-
tensive experiments conducted on two avail-
able propagandist resources (i.e., NLP4IF’19
and SemEval’20-Task 11 datasets) show that
the proposed approach,
leveraging different
language models and the investigated linguis-
tic features, achieves very promising results
on propaganda classiﬁcation, both at sentence-
and at fragment-level.

1

Introduction

Propaganda represents an effective, even though of-
ten misleading, communication strategy to promote
a cause or a viewpoint, for instance in the political
context (Lasswell, 1938; Koppang, 2009; Dillard
and Pfau, 2009; Longpre et al., 2019). Different
communication means can be used to disseminate
propaganda, i.e., textual documents, images, videos
and oral speeches. The ability to effectively iden-
tify and manifestly label such kind of misleading
and potentially harmful content is of primarily im-
portance to restrain the spread of such information
to avoid detrimental consequences for the society.

In this paper, we tackle this challenging is-
sue (Da San Martino et al., 2020b) by proposing
a textual propaganda detection model. More pre-
cisely, we address the following research questions:
(i) how to automatically identify propaganda in tex-
tual documents and further classify them into ﬁne-
grained categories?, and (ii) what are the linguistic
distinctive features of propaganda text snippets?
The contribution of this paper consists not only in
proposing a new effective neural architecture to au-
tomatically identify and classify propaganda in text,
but we also present a detailed linguistic analysis of
the features characterising propaganda messages.
Our work focuses on the propaganda detection
and classiﬁcation task, casting it both as a binary
and as a multi-class classiﬁcation task, and we ad-
dress it both at sentence-level and at fragment-level.
We investigate different architectures of recent lan-
guage models (i.e., BERT, RoBERTa), combining
them with a rich set of linguistic features ranging
from sentiment and emotion to argumentation fea-
tures, to rhetorical stylistic ones. The extensive
experiments we conducted on two standard bench-
marks (i.e., the NLP4IF’19 and SemEval’20-Task
11 datasets) show that the proposed architectures
achieve satisfying results, outperforming state-of-
the-art systems on most of the propaganda detec-
tion and classiﬁcation subtasks. An error analy-
sis discusses the main sources of misclassiﬁcation.
Furthermore, we analysed how the most relevant
features for propaganda detection impact the ﬁne-
grained classiﬁcation of the different techniques
employed in propagandist text, revealing the impor-
tance of semantic and argumentation features.

2 Related Work

In the last years, there has been an increasing inter-
est in investigating methods for textual propaganda
detection and classiﬁcation. Among them, (Barr´on-

Cede˜no et al., 2019) present a system to organize
news events according to the level of propagandist
content in the articles, and introduces a new corpus
(QProp) annotated with the propaganda vs. trust-
worthy classes, providing information about the
source of the news articles. (Da San Martino et al.,
2019) present the benchmark of the shared task
NLP4IF’191 on ﬁne-grained propaganda detection.
As a follow up, in 2020 SemEval proposed a shared
task (T11) (Da San Martino et al., 2020a) reducing
the number of propaganda categories with respect
to NLP4IF’19, and proposing a more restrictive
evaluation scheme. To evaluate the proposed ap-
proach, we rely on these two standard benchmarks,
i.e., the NLP4IF’19 and SemEval’20 datasets.

The most recent approaches for propaganda de-
tection are based on language models that mostly
involve transformer-based architectures. The ap-
proach that performed best on the NLP4IF’19
sentence-level classiﬁcation task relies on the
BERT architecture with hyperparameters tun-
ing without activation function (Mapes et al.,
2019). (Yoosuf and Yang, 2019) focused ﬁrst on
the pre-processing steps to provide more informa-
tion regarding the language model along with ex-
isting propaganda techniques, then they employ
the BERT architecture casting the task as a se-
quence labeling problem. The systems that took
part in the SemEval 2020 Challenge - Task 11
represent the most recent approaches to identify
propaganda techniques based on given propagan-
dist spans. The most interesting and successful
approach (Jurkiewicz et al., 2020) proposes ﬁrst to
extend the training data from a free text corpus as a
silver dataset, and second, an ensemble model that
exploits both the gold and silver datasets during the
training steps to achieve the highest scores. Notice
that most of the most performing recent models
heavily rely on transformer-based architectures.

In this paper, we also rely on language model
architectures for the detection and classiﬁcation
of propaganda messages, empowering them with
a rich set of features we identiﬁed as pivotal in
propagandist text from the computational social
science literature. In particular, (Morris, 2012) dis-
cusses how emotional markers and affect at word-
or phrase-level are employed in propaganda text,
whilst (Ahmad et al., 2019) show that the most
effective technique to extract sentiment for the

1https://propaganda.qcri.org/

nlp4if-shared-task/

propaganda detection task is to rely on lexicon-
based tailored dictionaries. Recent studies (Li et al.,
2017) show how to detect degrees of strength from
calmness to exaggeration in press releases. Fi-
nally, (Troiano et al., 2018) focus on feature ex-
traction of text exaggeration and show that main
factors include imageability, unexpectedness, and
the polarity of a sentence.

3 Propaganda detection as a

classiﬁcation task

(Da San Martino et al., 2019) deﬁne the Fine-
Grained Propaganda Detection task as two sub-
i) Sentence-
tasks, with different granularities:
Level Classiﬁcation task (SLC), which asks to pre-
dict whether a sentence contains at least one propa-
ganda technique, and ii) Fragment-Level Classiﬁ-
cation task (FLC), which asks to identify both the
spans and the type of propaganda technique.

In the following example, “In a glaring sign of
just how stupid and petty things have become in
Washington these days, Manchin was invited on Fox
News Tuesday morning to discuss how he was one
of the only Democrats in the chamber for the State
of the Union speech not looking as though Trump
killed his grandma.” the span “stupid and petty”
carries some propagandist bias, and is labeled as
“Loaded Language” , “not looking as though Trump
killed his grandma” is considered as “Exaggeration
and Minimisation”, and “killed his grandma” is
“Loaded Language”. According to the SLC task,
the whole sentence should be classiﬁed as a propa-
ganda message given that it contains at least one
token (e.g., “stupid and petty”) considered as such.
As previously introduced, current systems ad-
dress these tasks relying on word embedding mod-
els (e.g., BERT-embedding) and standard features
(e.g., PoS, name-entity, n-grams), as representa-
tions to feed various RNN architectures (Morio
et al., 2020; Chernyavskiy et al., 2020). Recently,
the language model BERT (Devlin et al., 2019) has
been widely utilized to optimize the performances
of classiﬁcation tasks, but there is still room for
improvement, in particular when applied to pro-
paganda detection (Da San Martino et al., 2020a,
2019). In this work, we experiment with multiple
architectures and language models to classify pro-
pagandist messages on both sentence and fragment-
level. Prior to that, we conduct a detailed investi-
gation of linguistic and argumentation features to
capture propaganda strategies.

4 Feature Analysis

Propaganda strategies generally involve speciﬁc
targets to be stimulated by the message. To better
study such techniques from a computational point
of view, we investigate a set of features that we
assume to play a role in propaganda.

4.1 Persuasion

Speech style. To analyze the writing style of the
messages, we apply the dictionary-based mapping
tool “General Inquirer (v. 1.02 )” (Gilman, 1968).
It relies on a list of lexicons from 26 domains (e.g.,
politician speeches, consumer protests) annotated
according to 182 rating categories and dimensions
(e.g., valence categories and words indicating over-
statement and understatement)2. We apply such
tool on our data and we sum the ratings of each
token to obtain a global score for a sentence.

Lexical complexity. Given that pre-trained lan-
guage models have shown to capture lexical and
semantic complexities of words, we rely on BERT
(base-uncased) (Devlin et al., 2019) to extract lexi-
cal complexity features. We extract a vector of 768
dimensions per each token, then we average w.r.t.
all tokens in a sentence, to obtain one vector of 768
dimensions to represent a sentence.

Concreteness. Propaganda messages tend to
employ words with concrete meaning, that has
more impact in conveying the intention of the mes-
sage than using abstract words (Eliasberg, 1957)
We rely on the concreteness lexicon (Brysbaert
et al., 2013) and we sum the standardized score of
each token in a sentence to obtain the global score.
Subjectivity. We rely on the subjectivity lexicon
from (Wilson et al., 2005). We sum up the scores
over all tokens in a sentence found in the lexicon
as our extracted feature. Each word labeled as
“weaksubj” is set to 0.5, and “strongsubj” to 1.

4.2 Sentiment

Sentiment labels. We use SentiWordNet 3.0 (Bac-
cianella et al., 2010) to obtain word-level sentiment
labels (positive, negative, or neutral). We sum the
sentiment scores of each word in a sentence, pro-
ducing a vector with 3 dimensions (i..e, pos, neg,
neu) for each sentence.

Emotion labels. We extract 8 emotions (i.e.,
afraid, amused, angry, annoyed, don’t care, happy,
inspired, sad) from DepecheMood++ lexicon

2http://www.wjh.harvard.edu/˜inquirer/

homecat.htm

(Araque et al., 2019). For each word that evokes
emotions in a sentence, we produce our features by
summing up each set of emotions evoked by each
token, then ﬁnd the average by emotions. Hence,
we produce 8 emotion scores for a sentence.

VAD labels. In the three-dimensional model of
affect, valence ranges from unhappiness to happi-
ness and expresses the pleasant or unpleasant feel-
ing about something, arousal expresses the level of
affective activation, ranging from sleep to excite-
ment, and dominance reﬂects the level of control of
the emotional state, from submissive to dominant.
We use Warriner lexicon (Warriner et al., 2013) to
match each word in a sentence to its VAD standard-
ized word scores and sum up as our features.

Connotation. Propaganda can convey sentiment
beyond its original meaning. Connotation lexicon
(Feng et al., 2013) provides positive, negative and
neutral labels of each word. We count the frequen-
cies of the three labels evoked in each sentence.

Politeness. Politeness evokes sentiment in read-
ers. We use a lexicon of positive and negative
words from (Danescu-Niculescu-Mizil et al., 2013),
then we count the frequencies of both positive and
negative words found in each sentence.

4.3 Message Simplicity

To keep the message simple and picturable is one of
main purposes of propaganda. We list the features
we considered to extract the simplicity of message.
Exaggeration. We use imageability lexicon
(Tsvetkov et al., 2014) based on picturable vocabu-
lary which mentally leads to an exaggerating state
of mind. We consider the scores of abstraction and
concreteness at each word token. We then sum up
the scores for all the labels found in a sentence.

Length. “The less words used, the better to
understand” can be a concept to easily interpret
the propagandist message. We apply two strate-
gies: i) we count the average char-length, actual
char-length, word length, punctuation frequency,
capital-case frequency per sentence (Ferreira Cruz
ii) we apply length encoding at
et al., 2019);
character-level, plus one additional dimension for
non-alphabetical char count.

Pronouns. Loaded language, name calling and
labelling are the most used techniques in propa-
ganda text (Da San Martino et al., 2019), and they
all make use of pronouns. We create a lexicon
of 123 pronouns in English3 and perform one-hot

3https://www.englishclub.com/vocabulary/pronouns-

encoding of common used pronouns in English.

4.4 Argumentation

We assume that argumentation plays an important
role in propaganda. To extract argumentative fea-
tures representing our data, we train a supervised
classiﬁer for the task of argumentative sentence
classiﬁcation on the persuasive essays dataset (Stab
and Gurevych, 2014). First, we cast it as a binary
classiﬁcation task, merging premises, claims and
major claims into the argumentative label, as op-
posed to the non-argumentative label. Then, for
the argumentation component task, we rearrange
the data to binary labels where the major claims
and claims labels are merged, and they are opposed
to premises. To address these tasks, we build and
ﬁne-tune a BERT classiﬁer. We use a learning rate
of 1e-5 with 80/20 split of the dataset. We run our
classiﬁer 3 times at different random states. The
results for the argumentative sentence classiﬁcation
are (macro-average) F1 0.84, precision 0.86, recall
0.82, while for the component classiﬁcation they
are F1 0.77, precision 0.80, recall 0.75.

To extract argumentative features from the anno-
tations provided by our classiﬁers, we use BERT-
based features. After ﬁne-tuning, we freeze the
hidden states of these ﬁne-tuned BERT models. To
extract the argumentative and components features
from each classiﬁer, we take the [CLS] token of
each sentence from the ﬁne-tuned BERT model.

4.5 Ablation Study

To investigate the impact of the proposed features
(Section 4) for propaganda detection, we perform
ablation tests by testing a supervised classiﬁer re-
lying on BERT + logistic regression. To the pur-
pose, we use the NLP4IF’19 training and test sets
(Da San Martino et al., 2019).

Table 1 reports on the performances obtained
while integrating groups of features to the proposed
model. A logistic regression model is used as a
baseline. Best results are obtained when adding all
the proposed features, but the argumentation ones.
Argumentation features alone perform almost iden-
tical as semantic features, therefore - unexpectedly
- no added value can be demonstrated.

5 Sentence-level classiﬁcation

In the following, we describe the experiments we
carried out to address the propaganda detection task

type.php,
pronouns.htm

https://www.thefreedictionary.com/List-of-

n
o
i
s
a
u
s
r
e
P

t
n
e
m

i
t
n
e
S

y
t
i
c
i
l

p
m

i
s

e
g
a
s
s
e

n
o
i
t
a
t
n
e
m
u
g
r
A

Logistic Regression

M

(cid:68)

(cid:68)

(cid:68)

(cid:68)

(cid:68) (cid:68)
(cid:68)
(cid:68)

F1
0.68
0.62
0.63
(cid:68) 0.67
0.68
0.69
(cid:68) 0.69
0.66
(cid:68) 0.70
(cid:68) (cid:68) 0.66
(cid:68) (cid:68) 0.69
(cid:68) 0.68
(cid:68) (cid:68) (cid:68) 0.70
(cid:68) (cid:68) (cid:68)
0.71
(cid:68) (cid:68) (cid:68) (cid:68) 0.70

(cid:68) (cid:68)
(cid:68)

(cid:68)
(cid:68) (cid:68)

Precision Recall
0.67
0.57
0.62
0.67
0.67
0.68
0.68
0.65
0.69
0.65
0.68
0.68
0.69
0.69
0.69

0.69
0.69
0.66
0.68
0.70
0.71
0.70
0.68
0.71
0.68
0.70
0.69
0.72
0.72
0.71

BERT + Featured LR
F1
0.70
0.71
0.70
0.71
0.71
0.70
0.70
0.71
0.71
0.70
0.71
0.70
0.70
0.72
0.71

Precision Recall
0.70
0.69
0.68
0.70
0.69
0.69
0.70
0.70
0.70
0.69
0.70
0.69
0.69
0.70
0.69

0.71
0.72
0.72
0.72
0.71
0.71
0.71
0.73
0.72
0.72
0.72
0.71
0.71
0.74
0.72

Table 1: Ablation test on binary classiﬁcation setting.

at sentence level, investigating different architec-
tures and leveraging both recent language models
and the features that proved to play a role in propa-
ganda messages. For the evaluation, we used the
two available datasets for propaganda detection:
i) the NLP4IF’19 data set (Da San Martino et al.,
2019) (293 articles for training and 101 for testing);
and ii) the data from SemEval’20 T11 (Da San Mar-
tino et al., 2020a) (371 articles for training and 75
in the development set).

5.1 Prediction Models

In the following, we ﬁrst describe the baseline and
the SOTA models we tested in our experiments, and
then we present the three architectures we propose
(underlined) integrating the propagandist features
previously investigated (Section 4).
BERT. Our baseline model relies on a pre-trained
bidirectional transformer language model to en-
code context speciﬁc sentence tokens (Devlin et al.,
2019) (no ﬁne tuning, default hyperparameters).
Fine-tuned BERT. We ﬁne-tune the BERT model
with a learning rate of 5e-5, and AdamW optimizer.
We set the gradients to zero at every training batch.
Then we use softmax activation to gate the output
with the threshold of 0.5.
Fine-tuned T5.
To ﬁne-tune the text-to-
text transformer (Raffel et al., 2020), we use
T5ForConditionalGeneration approach (equally to
question-answering task) where the input is a sen-
tence (as a question), and the output is an answer
(as a label). We use a learning rate of 3e-4, with
max sequence length of 512.
Linear-Neuron Attention BERT. We replicate

Model

BERT Baseline
SOTA
Fine-tuned BERT
Fine-tuned T5
Linear-Neuron Attention BERT
Multi-granularity BERT
Proposed Architecture w/ Semantic Features
Multi-granularity + Featured BERT
BERT + Featured BiLSTM
BERT + Featured Logistic Regression
Proposed Architecture w/ Semantic Features + Argumentation Features
BERT + Featured Logistic Regression

NLP4IF’19 Test Set

F1
0.52

Precision Recall
0.50

0.53

SemEval’20-T11 Dev. Set
Precision Recall
F1
0.48
0.48

0.48

0.58
0.64
0.63
0.61

0.63
0.65
0.72

0.63
0.64
0.60
0.60

0.65
0.80
0.74

0.53
0.65
0.67
0.62

0.61
0.55
0.70

0.61
0.66
0.66
0.65

0.67
0.65
0.68

0.63
0.65
0.69
0.68

0.71
0.75
0.71

0.71

0.72

0.69

0.68

0.70

0.60
0.66
0.63
0.63

0.64
0.58
0.66

0.67

Table 2: Results on the Sentence-level classiﬁcation (SLC) task (binary task).

the winning approach of the NLP4IF’19 shared-
task (Mapes et al., 2019). It relies on BERT archi-
tecture with some modiﬁcations of hyperparame-
ters (sentence length of 50 tokens, a learning rate
of 1e-5, along with 12 attention heads and 12 trans-
former blocks). It uses the linear neuron without
an activation function, and a threshold of 0.3 for
the ﬁnal prediction.

This

BERT.

Multi-granularity
model
(Da San Martino et al., 2019) relies on BERT
transformer with multi-granularity network on top
that has multi-classiﬁers for different granularity
levels of text (e.g., document, paragraph, sentence,
word, subword, and character-level). We replicate
this model with BertAdam optimizer and ReLU
activation function.

Multi-granularity + Featured BERT. We inte-
grate the proposed features (Section 4) into
(Da San Martino et al., 2019), taking only the last
layer of sentence-level granularity. We feed the
proposed features to a BERT classiﬁer to obtain
logits which then aggregate with the last layer of
sentence-level granularity to produce predictions.

BERT + Featured BiLSTM. We build a pre-
trained BERT transformer architecture, and Bi-
directional Long Short-Term Memory (BiLSTM)
architecture on top of the BERT model to handle
the transformer architecture with our propaganda
features. Firstly, the BERT model is used with
learning rate of 0.001, with AdamW optimizer. We
use the output of BERT that represents the [CLS]
token of each sentence to combine with propaganda
features as our input to the second model, the BiL-
STM. For the BiLSTM model, after we feed our
inputs of both [CLS] tokens combined with propa-
ganda features, we train our BiLSTM model with
hidden size of 256. Our BiLSTM hidden states con-

sist of the last hidden states, and the last cell state
for the BiLSTM layers. We then apply relu gate
function, with a linear dense, then use a dropout
function of 0.1. At the last layer, we use another lin-
ear dense layer to output ﬁnal logits, then we apply
a sigmoid activation function as ﬁnal outputs.
BERT + Featured Logistic Regression. We use
pre-trained BERT transformer architecture to out-
put [CLS] token, then use this output to stack with
another prediction model, i.e., logistic regression.
We build a linear classiﬁer and feed it with propa-
ganda features as a dense layer. We then combine
these logits with [CLS] tokens as the input to logis-
tic regression on top of BERT.

5.2 Results and error analysis

Table 2 reports on the results obtained for the SLC
task (propaganda vs no propaganda). We run each
experiment 5 times and report the macro-average
of all metrics. Our proposed models achieve the
highest F1-score of 0.72 using BERT + Featured
Logistic Regression model (persuasion, sentiment,
and message simplicity features), and the highest
precision-score 0.80 using BERT + Featured BiL-
STM model on NLP4IF’19 dataset, outperforming
the state-of-the-art models. For SemEval’20-T11,
we do not have the scores from the challenge (the
binary task was not proposed), but we compare
the obtained results with the replicated architec-
tures of SOTA models. Our proposed architecture
obtained the best F1-score using BERT+Featured
Logistic Regression. Using semantic features alone
perform slightly better than combining them with
argumentation features.

Table 3 reports on some misclassiﬁed examples
of our best model on NLP4IF’19 dataset. Some
short sentences containing strong intention key-
words (e.g., “hate”, “slave”) have been missclas-

False Positive
People who hate freedom will get unfettered access to the minds
of 2 billion people.
You are a slave to white America.

False Negative
The American people have a right to know, and those that en-
gaged in this type of behavior do not have a right to hide.
Hitler was a very great man.

Table 3: Examples of misclassiﬁed sentences by the BERT + featured logistic regression model (NLP4IF’19)

siﬁed as false positives. As for false negatives,
the underlined fragments are labeled propaganda in
gold standard, but have not been recognized as such
by the classiﬁer (mainly informative statements).

6 Fragment-level Classiﬁcation

In this section, we address the task of fragment-
level classiﬁcation, meaning that both the spans
and the type of propaganda technique should be
identiﬁed in the sentences. Again, to test the pro-
posed methods, we use both NLP4IF’19 and Se-
mEval’20 T11 datasets. However, in the two chal-
lenges, the FLC task was evaluated according to
different strategies, explained in the following.

6.1 Task 1: FLC on NLP4IF’19 dataset

In the NLP4IF’19 dataset, 18 propaganda tech-
niques are annotated. Prediction is expected to
be at token-level. Multiple tokens can belong to
the same span, and annotated with one propaganda
type. Tokens that do not carry any propaganda bias
are annotated as “no propaganda”. To perform to-
kenization we run the tokenizer provided with the
pretrained model of each transformer4.

6.1.1 Prediction Models
Fine-tuned BERT (baseline). Pretrained bert-
base-uncased model and BERT architecture (De-
vlin et al., 2019) with default hyperparameters. Our
implementation is based on huggingface transform-
ers. Settings: learning rate of 5e-5, padded length
of 128, and batch size of 16. We use CrossEntropy-
Loss as a loss function, and softmax activation
function to gate output neurons.
Fine-tuned RoBERTa (baseline). We use
roberta-base model with the same hyperparameters
of loss and activation functions as the ﬁne-tuned
BERT model mentioned above.
State-of-the-art Model. The winning team ap-
plied BERT architecture for token classiﬁcation
(Yoosuf and Yang, 2019) on 20 labels (i.e., 18 pro-
paganda classes, plus “background” as non pro-
paganda, and “auxiliary” for fractions of previous
tokens). They use a BERT language model, then

4huggingface.co/transformers/

apply softmax function, followed by a linear multi-
label classiﬁcation layer to output their predictions.
Transformer + CRF. We use a pre-trained model
base-uncased with a learning rate of 3e-5 for BERT
transformer, and a pre-trained model roberta-base
with a learning rate of 2e-5 for RoBERTa trans-
formers (hyperparameters: dropout of 0.1 with the
max length of 128, batch size of 16 with AdamW
optimizer and CrossEntropy loss function). We use
CRF layer as the ﬁnal layer.

6.1.2 Results and error analysis
Table 4 reports on the obtained performances. Eval-
uation is reported as the average of micro-F1 scores
of 5 run-times (we use the evaluation scripts pro-
vided by (Da San Martino et al., 2019)).

The proposed architecture based on transformers
with CRF output layer at different learning gradi-
ents (epochs) outperforms SOTA model on several
propaganda techniques at different learning gradi-
ent ranging from 5 to 15 epochs. We also tested
other architectures such as Transformer+CRF with
less learning gradients (3 epochs), Transformer ar-
chitecture with semantic and/or argumentation fea-
tures + CRF layer by adding extracted features
from sentence-level (Section 4) to each token of
its sentence to a linear layer before a loss function,
with no major improvements.

In Table 4, we compare the performances of
the proposed models w.r.t. the SOTA (Yoosuf and
Yang, 2019), on the most frequent classes. Table 5
reports examples of misclassiﬁcation related to that
technique. We observe that our proposed model
does not capture well the articles (i.e., it, as, an,
the), but rather focuses on capturing intentional
word tokens (i.e., white, unbelievably, rude, won-
derful, treasonous). As for future work to improve
results on this speciﬁc category, we will investigate
the work of (Habernal et al., 2018) according to
which a dedicated strategy is needed.

6.2 Task 2: FLC on SemEval’20 T11 dataset

In SemEval’20 T11 dataset, 14 propaganda tech-
niques are annotated. We focus here on the task
called Technique-Classiﬁcation task (TC). We cast
it as a sentence-span classiﬁcation problem, where

Baseline
Fine-tuned BERT
Fine-tuned RoBERTa
SOTA (from NLP4IF’19) (Yoosuf and Yang, 2019)
Proposed Architecture
Fine-tuned BERT + CRF (5 epochs)
Fine-tuned BERT + CRF (15 epochs)
Fine-tuned RoBERTa + CRF (5 epochs)
Fine-tuned RoBERTa + CRF (7 epochs)
Fine-tuned RoBERTa + CRF (10 epochs)
Fine-tuned RoBERTa + CRF (12 epochs)
Fine-tuned RoBERTa + CRF (15 epochs)
Fine-tuned RoBERTa + CRF (5 epochs 3x-Oversampled)

r
a
e
F
-
l
a
e
p
p
A

0.09
0.06
0.21

0.27
0.25
0.32
0.40
0.30
0.31
0.35
0.34

e
t
i
h
W
-
k
c
a
l
B

0.04
0.02
0.09

.0
0.02
.0
0.23
0.19
0.17
0.16
0.14

.
r
e
v
O

-
l
a
u
s
a
C

0.03
0.01
.0

0.04
0.04
0.09
0.08
0.09
0.05
0.03
0.07

e
g
a
r
e
v
A

0.03
0.02
0.24

0.13
0.11
0.16
0.14
0.15
0.15
0.16
0.15

t
b
u
o
D

0.07
0.05
0.17

0.08
0.07
0.11
0.13
0.13
0.19
0.16
0.13

NLP4IF’19

.

n
i
M

-
.
g
a
x
E

0.07
0.07
0.16

0.20
0.28
0.35
0.37
0.31
0.31
0.35
0.30

g
n
i
v
a
W
-
g
a
l
F

0.17
0.10
0.44

0.59
0.61
0.37
0.46
0.53
0.47
0.49
0.52

.

L
-
d
e
d
a
o
L

0.08
0.09
0.33

0.26
0.25
0.42
0.37
0.35
0.33
0.33
0.34

g
n
i
l
l
a
c
e
m
a
N

0.07
0.07
0.40

0.28
0.22
0.37
0.31
0.29
0.32
0.27
0.27

.
t
i

H
-
o
i
t
c
u
d
e
R

0.02
0.01
.0

0.08
0.04
.0
0.05
.0
.0
.0
0.05

n
o
i
t
i
t
e
p
e
R

0.01
0.01
0.01

0.01
0.04
.0
.0
0.01
0.03
0.01
0.02

s
n
a
g
o
l
S

0.04
0.02
0.13

0.10
0.13
0.06
0.17
0.25
0.14
0.24
0.33

Table 4: Experimental results on fragment-level classiﬁcation on NLP4IF’19 test set. All scores are reported in
micro-F1 (as in the original challenge). Scores in bold are the ones outperforming SOTA model.

Technique: NameCalling Labeling
(a) We look forward to continuing to defend the White House’s
lawful actions.
(b) This is a man who follows a demonic religion, Islam, and
supports textcolorredcop killers.
(c) It was not widely recognized as an anti-Jewish organization
during its early years (its early literature, though, focused on
“the white man” as “the white devil”).

Table 5: Misclassiﬁed NameCalling Labeling. False
Negative (in red), False Positive (in blue), the correctly
classiﬁed propaganda spans (underlined).

we combine logits of tokenized elements from
the sentence and the span, to learn the prediction.
Moreover, we add the semantic and argumentation
features to enhance the performance.

As pre-processing, both the tokenized sentence
and the span are used to feed the transformer (Hug-
gingface tokenizers) as follows: i) we input a sen-
tence to the tokenizer where max length is set to
128 with padding; ii) we input the span provided
by the propaganda span-template published by the
workshop, and we set max length value of 20 with
padding. If a sentence does not contain propaganda
spans, it is labeled as a “none-propaganda”.

6.2.1 Prediction Models

Baseline. For all the tested architectures (BERT
and RoBERTa), we use the same type of trans-
former model to produce logits (L) regarding the
sentence-level and span-level individually. For
BERT model, we use pre-trained model bert-base-
uncased, learning rate of 5e-5, and α of 0.1. For
RoBERTa, we take roberta-base pre-trained model
with learning rate of 2e-5 with α of 0.5. All trans-
former models apply Adam optimizer, dropout 0.1,

and CrossEntropy as a loss function per sentence
(losssentence) and span (lossspan).

We arrange these alignment of L to calculate the
average loss as joint loss (lossjoint loss) from each
loss element. Here we introduce a lossjoint loss
function before back-propagation:

N loss

lossjoint loss = α × (losssentence+lossspan)

where
N loss stands for a number of loss elements that are
taken into the model.
State-of-the-art Model. The winning team (Ju-
rkiewicz et al., 2020) applies RoBERTa (roberta-
large) with pre-trained model. The training set
is increased with silver annotation based on gold
annotation, and then another RoBERTa model is
stacked on top to output the predictions.
Proposed Architecture. We propose another
set of elements to feed the transformer by in-
troducing the semantic and argumentation fea-
tures into BiLSTM layer to produce L of pro-
posed features, then we apply CrossEntropy as a
loss function of our BiLSTM as lossproposed features
then perform an addition with other loss in the
lossjoint loss =
lossjoint loss function as follows:
α ×
Hyper-
parameters:
256 hidden size, 1 hidden layer,
drop out of 0.1 with ReLU function at the last layer
before the joint loss function.

(losssentence+lossspan+lossproposed features)
N loss

6.2.2 Results and error analysis
As mentioned before, the gold labels of the test
set of SemEval’20 T11 are not available, but it
is possible to submit a system run to the chal-
lenge website and to obtain the evaluation score.
The evaluation system only accepts the exact list

e
g
a
r
e
v
A

y
t
i
r
o
h
t
u
A
o
t

l
a
e
p
p
A

e
c
i
d
u
j
e
r
p
-
r
a
e
f

o
t

l
a
e
p
p
A

SOTA (from SemEval’20 T11) (Jurkiewicz et al., 2020)
Proposed Architecture + Proposed argumentation features
Fine-tuned RoBERTa (3 epochs)
Fine-tuned RoBERTa (5 epochs)
Fine-tuned RoBERTa (10 epochs)
Fine-tuned RoBERTa (15 epochs)
Proposed Architecture + All proposed features
Fine-tuned RoBERTa (3 epochs)
Fine-tuned RoBERTa (5 epochs)
Fine-tuned RoBERTa (10 epochs)
Fine-tuned RoBERTa (15 epochs)

0.64

0.48

0.47

0.53
0.53
0.51
0.51

0.54
0.52
0.51
0.51

0.08
0.14
0.18
0.14

0.16
0.09
0.09
0.15

0.34
0.34
0.33
0.29

0.38
0.35
0.31
0.32

SemEval’20 T11

y
c
a
l
l
a
F
-
e
t
i
h
W
-
k
c
a
l
B

n
o
i
t
a
c
ﬁ
i
l
p
m

i
s
r
e
v
O

-
l
a
u
s
a
C

t
b
u
o
D

n
o
i
t
a
s
i

,

m
i
n
i
M
n
o
i
t
a
r
e
g
g
a
x
E

g
n
i
v
a
W
-
g
a
l
F

e
g
a
u
g
n
a
L

d
e
d
a
o
L

,

g
n
i
l
e
b
a
L
g
n
i
l
l
a
C
e
m
a
N

n
o
i
t
i
t
e
p
e
R

s
n
a
g
o
l
S

0.51

0.23

0.56

0.37

0.70

0.78

0.76

0.59

0.59

0.17
0.26
0.37
0.31

0.29
0.31
0.37
0.40

0.06
0.09
0.22
0.14

0.18
0.21
0.28
0.29

0.52
0.46
0.37
0.42

0.50
0.43
0.36
0.37

0.32
0.35
0.33
0.35

0.33
0.34
0.35
0.31

0.61
0.60
0.58
0.55

0.60
0.61
0.54
0.54

0.72
0.73
0.73
0.73

0.72
0.74
0.73
0.75

0.68
0.72
0.68
0.69

0.65
0.70
0.70
0.66

0.22
0.17
0.17
0.13

0.23
0.21
0.19
0.18

0.12
0.36
0.34
0.35

0.29
0.23
0.38
0.43

.
t
i
h

d
a

,

o
i
t
c
u
d
e
R
n
o
g
a
w
d
n
a
B
0.08

0.14
0.17
0.13
0.12

0.20
0.13
0.17
0.07

s
e
h
c
i
l

C
g
n
i
t
a
n
i
m
r
e
t
-
t
h
g
u
o
h
T
0.39

0.42
0.30
0.17
0.25

0.32
0.33
0.14
0.14

,

.
r
e
H
d
e
R
n
e
M
w
a
r
t
S

,
.

b
a
t
a
h
W
0.28

.0
0.18
0.23
0.21

0.09
0.12
0.19
0.12

Table 6: Results on span classiﬁcation on SemEval’20 T11 test set (micro-F1).

Technique: Repetition

False Negative

All Features
(1) When she arrived at Jean’s door, Guyger entered
a unique door key with an electronic chip into the
keyhole, the afﬁdavit says.
(2) She told the 911 operator as well as responding
ofﬁcers that she thought she was at her apartment
when she shot Jean, according to the afﬁdavit.

Bandwagon,Reductio ad hitlerum

Doubt

Table 7: Misclassiﬁed Repetition spans (in red).

of span-templates of the test set (partial overlap-
ping spans or missing spans are not accepted).
Table 6 reports on the obtained results (through
such evaluation system) on 5 runs as micro-F1.
Scores in bold are the ones for which signiﬁcant
improvement can be observed w.r.t. SOTA model.
RoBERTa with argumentation features can out-
perform results on “Thought-terminating Cliches”.
Moreover, by using all semantic and argumentation
features together, we can obtain some improve-
ments over “Bandwagon,Reductio ad hitlerum”
and “Casual-Oversimpliﬁcation”. Table 7 shows
some examples of missclassiﬁed instances.
In
general, we noticed that using different training
epochs help detecting different propaganda tech-
niques.
In particular, it is observed that some
techniques tend to be learnt best at low training
epochs (i.e., “Bandwagon,Reductio ad hitlerum”,
“Thought-terminating Cliches”), some at high train-
ing epochs (i.e., “Casual-Oversimpliﬁcation”).

7 Concluding remarks

In this paper, we proposed a new neural architecture
combined with state-of-the-art language models
and a rich set of linguistic features for the detection
of propaganda messages in text, and their further
classiﬁcation along with standard propaganda tech-

niques. Despite the boost in accuracy we achieved
on two standard benchmarks for propaganda de-
tection and classiﬁcation (∼ 10% of F1 scores on
sentence-level classiﬁcation and on speciﬁc propa-
ganda techniques on fragment-level classiﬁcation),
this task remains challenging, in particular regard-
ing the ﬁne-grained classiﬁcation of the different
propaganda classes. The state-of-the-art results on
this subtask require further improvement to actually
embed these solutions in real-world systems.

Future work goes in this direction, with the aim
to improve the performance both of the disinfor-
mation detection task and of the classiﬁcation of
propaganda techniques. Moreover, we are currently
investigating the propaganda classes we discussed
in this paper in the context of political debates, with
the aim of building a fallacy detection systems that
relies on the identiﬁcation of propagandist mes-
sages in political speeches.

Acknowledgments

This work is partially supported by the AN-
SWER project PIA FSN2 n.
P159564-
2661789/DOS0060094 between Inria and Qwant.
Moreover, this work has been supported by the
French government, through the 3IA Cˆote d’Azur
Investments in the Future project managed by the
National Research Agency (ANR) with the refer-
ence number ANR- 19-P3IA-0002.

References

Siti Rohaidah Ahmad, Muhammad Zakwan Muham-
mad Rodzi, Nurlaila Syaﬁra Shapiei, Nurhaﬁzah
Moziyana Mohd Yusop, and Suhaila Ismail. 2019.

A review of feature selection and sentiment analy-
sis technique in issues of propaganda. International
Journal of Advanced Computer Science and Appli-
cations, 10(11).

Oscar Araque, Lorenzo Gatti, Jacopo Staiano, and
Marco Guerini. 2019. Depechemood++: a bilingual
emotion lexicon built through simple yet powerful
IEEE Transactions on Affective Com-
techniques.
puting, pages 1–1.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
In Proceedings of the Seventh International
ing.
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta. European Language Re-
sources Association (ELRA).

Alberto Barr´on-Cede˜no, Israa Jaradat, Giovanni Mar-
tino, and Preslav Nakov. 2019. Proppy: Organizing
the news based on their propagandistic content. In-
formation Processing Management, 56.

Marc Brysbaert, Amy Warriner, and Victor Kuperman.
2013. Concreteness ratings for 40 thousand gen-
erally known english word lemmas. Behavior re-
search methods, 46.

Anton Chernyavskiy, Dmitry Ilvovsky, and Preslav
Nakov. 2020. Aschern at SemEval-2020 task 11:
It takes three to tango: RoBERTa, CRF, and trans-
In Proceedings of the Fourteenth
fer learning.
Workshop on Semantic Evaluation, pages 1462–
1468, Barcelona (online). International Committee
for Computational Linguistics.

Giovanni Da San Martino, Alberto Barr´on-Cede˜no,
Henning Wachsmuth, Rostislav Petrov, and Preslav
Nakov. 2020a. SemEval-2020 task 11: Detection of
propaganda techniques in news articles. In Proceed-
ings of the 14th International Workshop on Semantic
Evaluation, SemEval 2020, Barcelona, Spain.

Giovanni Da San Martino, Stefano Cresci, Alberto
Barron-Cedeno, Seunghak Yu, Roberto Di Pietro,
and Preslav Nakov. 2020b. A survey on computa-
tional propaganda detection. In Proceedings of 29th
International Joint Conference on Artiﬁcial Intelli-
gence and the 17th Paciﬁc Rim International Confer-
ence on Artiﬁcial Intelligence (IJCAI-PRICAI2020),
IJCAI-PRICAI2020, Yokohama, Japan.

Giovanni Da San Martino, Seunghak Yu, Alberto
Barr´on-Cede˜no, Rostislav Petrov,
and Preslav
Nakov. 2019. Fine-grained analysis of propaganda
In Proceedings of the 2019 Con-
in news article.
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 5636–5646, Hong Kong, China. As-
sociation for Computational Linguistics.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.

2013. A computational approach to politeness with
application to social factors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
250–259, Soﬁa, Bulgaria. Association for Computa-
tional Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL-HLT.

James Price Dillard and Michael Pfau. 2009. The Per-
suasion Handbook: Developments in Theory and
Practice. Sage Publications, Inc.

Wladimir Gottlieb Eliasberg. 1957. Toward a phi-
Jewish Social Studies,

losophy of propaganda.
19(1/2):51–63.

Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1774–1784, Soﬁa, Bulgaria. Associa-
tion for Computational Linguistics.

Andr´e Ferreira Cruz, Gil Rocha, and Henrique
Lopes Cardoso. 2019. On sentence representations
for propaganda detection: From handcrafted fea-
In Proceedings of the
tures to word embeddings.
Second Workshop on Natural Language Process-
ing for Internet Freedom: Censorship, Disinforma-
tion, and Propaganda, pages 107–112, Hong Kong,
China. Association for Computational Linguistics.

Richard C. Gilman. 1968. The general inquirer: A
computer approach to content analysis. philip j.
stone , dexter c. dunphy , marshall s. smith , daniel m.
ogilvie. American Journal of Sociology, 73(5):634–
635.

Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,
and Benno Stein. 2018. Before name-calling: Dy-
namics and triggers of ad hominem fallacies in web
argumentation. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
386–396, New Orleans, Louisiana. Association for
Computational Linguistics.

Dawid Jurkiewicz, Łukasz Borchmann, Izabela Kos-
mala, and Filip Grali´nski. 2020. ApplicaAI at
SemEval-2020 task 11: On RoBERTa-CRF, span
CLS and whether self-training helps them. In Pro-
ceedings of the Fourteenth Workshop on Semantic
Evaluation, pages 1415–1424, Barcelona (online).
International Committee for Computational Linguis-
tics.

Haavard Koppang. 2009. Social inﬂuence by manipu-
lation: A deﬁnition and case of propaganda. Middle
East Critique, 18:117 – 143.

for Computational Linguistics (Volume 1: Long Pa-
pers), pages 248–258, Baltimore, Maryland. Associ-
ation for Computational Linguistics.

Amy Warriner, Victor Kuperman, and Marc Brysbaert.
2013. Norms of valence, arousal, and dominance for
13,915 english lemmas. Behavior research methods,
45.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Process-
ing, pages 347–354, Vancouver, British Columbia,
Canada. Association for Computational Linguistics.

Shehel Yoosuf and Yin Yang. 2019. Fine-grained pro-
In Pro-
paganda detection with ﬁne-tuned BERT.
ceedings of the Second Workshop on Natural Lan-
guage Processing for Internet Freedom: Censor-
ship, Disinformation, and Propaganda, pages 87–
91, Hong Kong, China. Association for Computa-
tional Linguistics.

Harold Dwight Lasswell. 1938. Propaganda technique

in the world war.

Yingya Li, Jieke Zhang, and Bei Yu. 2017. An NLP
analysis of exaggerated claims in science news. In
Proceedings of the 2017 EMNLP Workshop: Natu-
ral Language Processing meets Journalism, pages
106–111, Copenhagen, Denmark. Association for
Computational Linguistics.

Liane Longpre, Esin Durmus, and Claire Cardie. 2019.
Persuasion of the undecided: Language vs. the lis-
tener. In Proceedings of the 6th Workshop on Argu-
ment Mining, pages 167–176, Florence, Italy. Asso-
ciation for Computational Linguistics.

Norman Mapes, Anna White, Radhika Medury, and
Sumeet Dua. 2019. Divisive language and pro-
paganda detection using multi-head attention trans-
formers with deep learning BERT-based language
models for binary classiﬁcation. In Proceedings of
the Second Workshop on Natural Language Process-
ing for Internet Freedom: Censorship, Disinforma-
tion, and Propaganda, pages 103–106, Hong Kong,
China. Association for Computational Linguistics.

Gaku Morio, Terufumi Morishita, Hiroaki Ozaki, and
Toshinori Miyoshi. 2020. Hitachi at SemEval-2020
task 11: An empirical study of pre-trained trans-
former family for propaganda detection. In Proceed-
ings of the Fourteenth Workshop on Semantic Eval-
uation, pages 1739–1748, Barcelona (online). Inter-
national Committee for Computational Linguistics.

Travis Morris. 2012. Extracting and networking emo-
In 2012 European
tions in extremist propaganda.
Intelligence and Security Informatics Conference,
pages 53–59.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1–67.

Christian Stab and Iryna Gurevych. 2014. Annotating
argument components and relations in persuasive es-
says. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 1501–1510, Dublin,
Ireland. Dublin City University and Association for
Computational Linguistics.

Enrica Troiano, Carlo Strapparava, G¨ozde ¨Ozbal, and
Serra Sinem Tekiro˘glu. 2018. A computational ex-
In Proceedings of the
ploration of exaggeration.
2018 Conference on Empirical Methods in Natu-
ral Language Processing, pages 3296–3304, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,
Eric Nyberg, and Chris Dyer. 2014. Metaphor detec-
tion with cross-lingual model transfer. In Proceed-
ings of the 52nd Annual Meeting of the Association

