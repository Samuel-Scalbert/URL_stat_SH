Variable size segmentation for eﬀicient representation
and querying of non-uniform time series datasets
Lamia Djebour, Reza Akbarinia, Florent Masseglia

To cite this version:

Lamia Djebour, Reza Akbarinia, Florent Masseglia. Variable size segmentation for eﬀicient representa-
tion and querying of non-uniform time series datasets. SAC 2022 - 37th ACM/SIGAPP Symposium on
Applied Computing, Apr 2022, Virtual Event, United States. pp.395-402, ￿10.1145/3477314.3507000￿.
￿lirmm-03806053￿

HAL Id: lirmm-03806053

https://hal-lirmm.ccsd.cnrs.fr/lirmm-03806053

Submitted on 7 Oct 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Variable Size Segmentation for Efficient Representation and
Querying of Non-Uniform Time Series Datasets

Lamia Djebour
INRIA & LIRMM, Univ Montpellier
France
lamia.djebour@inria.fr

Reza Akbarinia
INRIA & LIRMM, Univ Montpellier
France
reza.akbarinia@inria.fr

Florent Masseglia
INRIA & LIRMM, Univ Montpellier
France
florent.masseglia@inria.fr

ABSTRACT
Existing approaches for time series similarity computing are the
core of many data analytics tasks. Given the considered data vol-
umes, or simply the need for fast response times, they often rely on
shorter representations, usually with information loss. This incurs
approximate comparisons where precision is a major issue. We
present and experimentally evaluate ASAX, a new approach for
segmenting time series before their transformation into symbolic
representations. ASAX reduces significantly the information loss
incurred by possible splittings at different steps of the represen-
tation calculation, particularly for datasets with unbalanced (non-
uniform) distributions. We provide theoretical guarantees on the
lower bound of similarity measures, and our experiments illustrate
that our method outperforms the state of the art, with significant
gain in precision for datasets with unbalanced distributions.

CCS CONCEPTS
• Information systems → Data mining; Spatial-temporal sys-
tems; Nearest-neighbor search.

KEYWORDS
Time Series, Representations, Information Retrieval

ACM Reference Format:
Lamia Djebour, Reza Akbarinia, and Florent Masseglia. 2022. Variable Size
Segmentation for Efficient Representation and Querying of Non-Uniform
Time Series Datasets. In Proceedings of ACM SAC Conference (SAC’22). ACM,
New York, NY, USA, Article 4, 8 pages.

1 INTRODUCTION
Many applications in different domains generate time series data
at an increasing rate. That continuous flow of emitted data may
concern personal activities (e.g., through smart-meters or smart-
plugs for electricity or water consumption) or professional activities
(e.g., for monitoring heart activity or through the sensors installed
on plants by farmers). This results in the production of large and
complex data, usually in the form of time series [8, 9, 13, 15–19, 21,
22] that challenge knowledge discovery. Data mining techniques
on such massive sets of time series have drawn a lot of interest
since their application may lead to improvements in a large number
of these activities, relying on fast and accurate similarity search in
time series for performing tasks like , e.g., Classification, Clustering,
and Motifs Discovery [14, 17, 23].

Because of the considered data volumes in such applications,
these tasks can be slow on raw data. This is why approximation

SAC’22, April 25 –April 29, 2022, Brno, Czech Republic
.

(a) SAX segmentation on D, with 6 segments

(b) ASAX segmentation on D, with 6 segments

Figure 1: SAX segmentation Vs. ASAX segmentation

over time series is often regarded as a means to allow fast com-
putation of similarity search. SAX [11] is one of the most popular
representations of time series, allowing dimensionality reduction
on the classic data mining tasks. SAX is efficient because it con-
structs symbolic representations by splitting the time domain into
segments of equal size. This approximation model is effective for
time series having a uniform and balanced distribution over the
time domain. However, we observe that, in the case of time series
having high variation over some time intervals, this "one size fits
all" division into segments of fixed length is not advantageous.

To illustrate the impact of a fixed length division of the series
into segments, let us consider Figure 1. It shows a set D of time
series, taken from ECGFiveDays dataset of UCR Archive [5], where
the time series length is 132. In this dataset, the distribution of
values over time domain is not uniform. We can notice that there is
almost no variation from time point 1 to 44 and from 95 to 132. On
the other hand, the remaining part, from time point 44 to 95, shows
an important variation in the data values. Figure 1a shows the SAX
division on D, with a fixed-size segmentation on the time series.
In this example the segment size is 22, leading to 6 segments in

022446688110132−6−4−20246S1S2S3S4S5S6022446688110132−6−4−20246SAC’22, April 25 –April 29, 2022, Brno, Czech Republic

Lamia Djebour, Reza Akbarinia, and Florent Masseglia

total. If we take any time series X from D and we convert it into its
SAX representation, the first two segments are always represented
by the same symbol, all the values of these two segments being
close to each other. Actually, there is no need to consider these two
distinct segments. And the same applies to the last two segments.
Meanwhile, for segments 3 and 4, all the values of each segment
are represented by a single symbol while the data values present
great variations, causing a significant loss of information on these
segments.

As one can observe, it is not necessary to split the parts that are
constant or where the variation is low since they don’t carry any
relevant information and would therefore better form a single seg-
ment. It is more efficient to divide into several small segments the
parts where variation is important in order to preserve potentially
relevant information as shown in Figure 1b. The splitting of Figure
1b is the actual splitting obtained by our approach with a segment
budget limited to 6. The first two and last two segments better
correspond to the information carried by the series. It would be
rather counter-intuitive to merge segments 1 and 2, while it is the
opposite for Figure 1a. The time intervals where data values show
important differences are split like, e.g., between times point 66 and
88. By proposing such a customized splitting, we aim at improving
the performance of information retrieval algorithms that will rely
on our data representation.
In order to improve the quality of similarity search, and to achieve
adaptive splitting as illustrated above, we propose a new approxima-
tion method for time series that considers the time series shape and
does the splitting by means of segments of variable size on the time
domain. By measuring the entropy of symbolic representations, our
algorithm chooses between different possible splittings at each step
of the representation computation. This approach allows reducing
information loss, and thus increasing the accuracy of time series
representations leading to better precision during retrieval phases,
particularly from non-uniform datasets. In this paper, we make the
following contributions:

• We propose a new representation technique, called ASAX
(Adaptive SAX), that allows obtaining a variable-size seg-
mentation of time series with better precision in retrieval
tasks thanks to its lower information loss. Our representa-
tion is based on entropy measurement for detecting what
time intervals should be split.

• We propose a lower bounding method that allows approxi-
mating the distance between the original time series based
on their representations in ASAX.

• We implemented our approach and conducted empirical ex-
periments using more than 10 real world datasets. The results
suggest that ASAX can obtain significant performance gains
in terms of precision for similarity search compared to SAX.
They illustrate that the more the data distribution in the time
domain is unbalanced, the greater is the precision gain of
ASAX. For example, for the EGCFiveDays dataset that has a
non-uniform distribution in the time domain, the precision
of ASAX is 82% compared to 55% for SAX.

The rest of the paper is organized as follows, we review the related
works in Section 2. In Section 3, we describe the details of ASAX
representation. In Section 4, we present the experimental evaluation

of our approach. We discuss the related work in Section 5, and finally
conclude in Section 6.

2 PROBLEM DEFINITION AND

BACKGROUND

In this section, we first present the background about SAX repre-
sentation, and then define the problem we address.

Table 1: Some frequently used symbols

𝐷

Time series database

𝑋 , 𝑌 , 𝑄 Time series
𝑛 = |𝑋 |
𝑙
𝑤
𝑎
ˆ𝑋
𝑘

The length of time series 𝑋
The segment size
The number of PAA segments
The cardinality (the alphabet size)
The SAX representation of time series 𝑋
the 𝑘 nearest neighbors

A time series 𝑋 is a sequence of values 𝑋 = {𝑥1, ..., 𝑥𝑛 }. We
assume that every time series has a value at every timestamp 𝑡 =
1, 2, ..., 𝑛. The length of 𝑋 is denoted by |𝑋 |.

SAX allows a time series 𝑇 of length 𝑛 to be reduced to a string
of arbitrary length 𝑤. Table 1 lists the notations used in this paper.

2.1 SAX Representation
Given two time series 𝑋 = {𝑥1, ..., 𝑥𝑛 } and 𝑌 = {𝑦1, ..., 𝑦𝑛 }, the
Euclidean distance between 𝑋 and 𝑌 is defined as [6]: 𝐸𝐷 (𝑋, 𝑌 ) =
(cid:113)(cid:205)𝑛

𝑖=1 (𝑥𝑖 − 𝑦𝑖 )2
The Euclidean distance is one of the most straightforward simi-

larity measurement methods used in time series analysis.

The SAX representation is based on the Piecewise Aggregate
Approximation (PAA) representation [11] which allows for dimen-
sionality reduction while providing the important lower bounding
property as we will show later. The idea of PAA is to have a fixed
segment size, and minimize dimensionality by using the mean val-
ues on each segment. Example 1 gives an illustration of PAA.

Example 1. Figure 2b shows the PAA representation of 𝑋 , the
time series of Figure 2a. The representation is composed of 𝑤 = |𝑋 |/𝑙
values, where 𝑙 is the segment size. With PAA, for each segment,
the set of values is replaced with their mean. The length of the final
representation 𝑤 is the number of segments (and, usually, 𝑤 << |𝑋 |).

By transforming the original time series 𝑋 and 𝑌 into PAA repre-
sentations, 𝑋 = {𝑥 1, ..., 𝑥 𝑤 } and 𝑌 = {𝑦1, ..., 𝑦𝑤 }, the lower bound-
ing approximation of the Euclidean distance for these two repre-
𝑖=1 (𝑥𝑖 − 𝑦𝑖 )2
sentations can be obtained by : 𝐷𝑅𝑓 (𝑋, 𝑌 ) =
The SAX representation takes as input the reduced time series
obtained using PAA. It discretizes this representation into a prede-
fined set of symbols, with a given cardinality, where a symbol is a
binary number. The size of the symbols set is called the cardinality.
Example 2 gives an illustration of the SAX representation.

(cid:113)(cid:205)𝑤

(cid:113) 𝑛
𝑤

Example 2. In Figure 2c, we have converted the time series 𝑋 to
SAX representation with 4 segments, and cardinality 4 using the PAA

Variable Size Segmentation for Efficient Representation and Querying of Non-Uniform Time Series Datasets

SAC’22, April 25 –April 29, 2022, Brno, Czech Republic

where the function 𝑑𝑖𝑠𝑡 ( ˆ𝑥𝑖, ˆ𝑦𝑖 ) is the distance between two SAX
symbols ˆ𝑥𝑖 and ˆ𝑥𝑖 . The lower bounding condition is formulated as:
𝑀𝐼 𝑁 𝐷𝐼𝑆𝑇𝑓 ( ˆ𝑋, ˆ𝑌 ) ≤ 𝐸𝐷 (𝑋, 𝑌 )

2.2 Similarity Queries
The problem of similarity queries is one of the main problems in
time series analysis and mining. In information retrieval, finding
the 𝑘 nearest neighbors (k-NN) of a query is a fundamental problem.
Let us define exact and approximate 𝑘 nearest neighbors.

Definition 1. (Exact 𝑘 nearest neighbors) Given a query time
series 𝑄 and a set of time series 𝐷, let 𝑅 = 𝐸𝑥𝑎𝑐𝑡𝑘𝑁 𝑁 (𝑄, 𝐷) be the
set of 𝑘 nearest neighbors of 𝑄 from 𝐷. Let 𝐸𝐷 (𝑋, 𝑌 ) be the Euclidean
distance between two time series 𝑋 and 𝑌 , then the set 𝑅 is defined as
follows:

(𝑅 ⊆ 𝐷) ∧ (|𝑅| = 𝑘) ∧ (∀𝑎 ∈ 𝑅, ∀𝑏 ∈ (𝐷 − 𝑅), 𝐸𝐷 (𝑎, 𝑄) ≤ 𝐸𝐷 (𝑏, 𝑄))

Definition 2. (Approximate 𝑘 nearest neighbors) Given a set
of time series 𝐷, a query time series 𝑄, and 𝜖 > 0. We say that 𝑅 =
𝐴𝑝𝑝𝑘𝑁 𝑁 (𝑄, 𝐷) is the approximate 𝑘 nearest neighbors of 𝑄 from 𝐷,
if 𝐸𝐷 (𝑎, 𝑄) ≤ (1 + 𝜖)𝐸𝐷 (𝑏, 𝑄), where 𝑎 is the 𝑘𝑡ℎ nearest neighbor
from 𝑅 and 𝑏 is the true 𝑘𝑡ℎ nearest neighbor.

2.3 Time Series Approximation
The SAX representation proceeds to an approximation by mini-
mizing the dimensionality: the original time series are divided into
segments of equal size.

This representation does not depend on the time series values,
but on their length. It allows SAX to perform the segmentation
in 𝑂 (𝑛) where 𝑛 is the time series of length. However, for a given
reduction in dimensionality, the modeling error may not be minimal
since the model does not adapt to the information carried by the
series. Our claim is that, by taking into account the information
carried by time series for choosing the segments, we may obtain
significant increase in the precision of kNN queries. This issue
motivated us for proposing an adaptive representation aiming at
minimizing the information loss.

2.4 Problem Statement
Our goal is to propose a variable-size segmentation of the time
domain that minimizes the loss of information in the time series
representation.

The problem we address is stated as follows. Given a database
of time series 𝐷 and a number 𝑤, divide the time domain into 𝑤
segments of variable size such that the representation of the times
series based on that segmentation lowers the error of kNN queries.

3 ADAPTIVE SAX (ASAX)
In this section, we propose ASAX, a variable-size segmentation tech-
nique for the time series representation. To create a segmentation
with minimum information loss, ASAX divides the time domain
based on the representation entropy.

In the rest of this section, we first describe the notion of entropy
for the time series representation. Then, we describe our algorithm
for creating the variable-size segments. Finally, we present our
method for measuring the lower bound distance between time

(a) A time series X of
length 8

(b) A PAA representation of X, with 4 segments

(c) A SAX representation of X, with 4 segments and car-
dinality 4, [00, 00, 01, 11].

Figure 2: A time series 𝑋 is discretized by obtaining a PAA
representation and then using predetermined break-points
to map the PAA coefficients into SAX symbols. Here, the
symbols are given in binary notation, where 00 is the first
symbol, 01 is the second symbol, etc. The time series of Fig-
ure 2a in the representation of Figure 2c is [first, first, second,
fourth] (which becomes [00, 00, 01, 11] in binary).

representation shown in Figure 2b. In this example, we have 4 possible
symbols: 11, 10, 01, 00. For each segment, SAX chooses the symbol
that corresponds to the PAA value of the segment. Thus, the SAX
representation of X is: [00, 00, 01, 11].

The lower bounding approximation of the Euclidean distance
for SAX representation ˆ𝑋 = { ˆ𝑥1, ..., ˆ𝑥𝑤 } and ˆ𝑌 = { ˆ𝑦1, ..., ˆ𝑦𝑤 } of two
time series 𝑋 and 𝑌 is defined as:
𝑀𝐼 𝑁 𝐷𝐼𝑆𝑇𝑓 ( ˆ𝑋, ˆ𝑌 ) =

𝑖=1 (𝑑𝑖𝑠𝑡 ( ˆ𝑥𝑖, ˆ𝑦𝑖 ))2

(cid:113)(cid:205)𝑤

(cid:113) 𝑛
𝑤

02468−2−101202468−2−101202468−2−101200011011SAC’22, April 25 –April 29, 2022, Brno, Czech Republic

Lamia Djebour, Reza Akbarinia, and Florent Masseglia

series in the proposed representation. This lower bounding is useful
for efficient evaluation of kNN queries.

3.1 Entropy
Entropy is a mathematical function which intuitively corresponds
to the amount of information contained or delivered by a source
of information. This source of information can be of various types.
The more the source emits different information the higher is the en-
tropy. If the source always sends the same information, the entropy
is minimal. Formally, entropy is defined as follows.

Definition 3. Given a set 𝑋 of elements, and each element 𝑥 ∈ 𝑋
having a probability 𝑃𝑥 of occurrence, the entropy 𝐻 of the set 𝑋 is
defined as: 𝐻 (𝑋 ) = − (cid:205)

𝑥 ∈𝑋 𝑃𝑥 × log 𝑃𝑥

In our context, we calculate the entropy on a set containing the
different symbolic representations obtained from the transforma-
tion of the original time series of a dataset according to a given
segmentation. The entropy computed on this set allows to measure
the quantity of information contained in the time series represen-
tations. Let us illustrate this using an example.

Example 3. Consider the database D={x,y,z} in Figure 3 where x,
y and z are time series with l=8. Let us create a representation having
two segments (e.g., 0-4, and 4-8), and then compute the entropy of
the representation of the set D. To generate the representation of the
time series x, y and z, they are discretized by obtaining their PAA
representation and then using predetermined break-points to map
the PAA coefficients into the corresponding symbols like the SAX
representation proceeds. We have converted the 3 time series into
symbolic representations with size 2, and cardinality 4. Thus, the
symbolic representations of x, y and z are ˆ𝑥 = [00, 10], ˆ𝑦 = [00, 10]
and ˆ𝑧 = [00, 10], respectively. We notice that the 3 time series have
the same symbolic representation, thus, the set X consists of only this
unique symbolic representation with an occurrence equal to 3., i.e.,
𝑋 = {[00, 10]}. The entropy H(X) of X is computed as follows:

𝐻 (𝑋 ) = −(𝑃 (𝑥 = [00, 10]) × log2 𝑃 (𝑥 = [00, 10]))
where the probability for the word x is 𝑃 (𝑥 = [00, 10]) = 3
3 = 1.
Therefore, we have 𝐻 (𝑋 ) = −(1 log 1) = 0 meaning that in the
representation 𝑋 there is no information allowing to distinguish the
three original time series from each other. This is explained by the fact
that they have the same representation with a fixed-size segmentation.

Figure 3: ASAX segmentation with 2 segments

In the next subsection, we describe our algorithm to create

variable-size segments based on entropy.

3.2 Variable-Size Segmentation Based on

Entropy Measurement

Given a database of time series 𝐷, and a number 𝑤, our goal is to find
the 𝑘 variable size segments that minimize the loss of information
in time series representations.

Intuitively, our algorithm works as follows. First it splits the time
domain into two segments of equal size. Then, it performs 𝑤 − 2
iterations, and in each iteration it finds the segment 𝑠 whose split
makes the minimum loss in entropy, and it splits that segment. By
doing this, in each iteration a new segment is added to the set of
segments. This continues until having 𝑤 segments.

Let us now describe ASAX in more details. The pseudo-code
is shown in Algorithm 1. It first splits the time domain into two
equal parts and creates two segments that are included to the set
𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 (Line 1). Then, it sets the current number of segments,
denoted as 𝑘, to 2 (Line 2).

Afterwards, in a loop, until the number of segments is less than
𝑤 the algorithm proceeds as follows. For each segment 𝑖 (from 1
to 𝑘), 𝑖 is divided into two equal parts, if its size is greater than
𝑚𝑖𝑛𝑆𝑖𝑧𝑒, which is the minimum possible size of a segment, and it’s
default value is 1. Then, a temporary set of segments 𝑡𝑒𝑚𝑝𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑠
is created including the two new segments and all previously cre-
ated segments except 𝑖 (i.e., expect the one that has been divided).
Then, for each time series 𝑡𝑠 in the database 𝐷, the algorithm gen-
erates the symbolic representation of 𝑡𝑠 (denoted as 𝑤𝑜𝑟𝑑) using
the segments included in 𝑡𝑒𝑚𝑝𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑠 with the given cardinality
𝑎 (Line 12), and inserts it to a hash table (Line 13). Note that for
all time series, ASAX uses the same cardinality to map the PAA
coefficients into the corresponding symbols. After having inserted
all the representations of the time series contained in 𝐷 to the hash
table, the entropy of the representations is calculated (Line 14). If
the entropy is higher than the maximum entropy obtained until
now, the algorithm sets 𝑖 as the segment to be split, and keeps the
entropy of the representation. This procedure continues by split-
ting one of the segments at each time, and computing the entropy.
The algorithm selects the one whose entropy is the highest, and
updates the set of the segments by removing the selected segment,
and inserting its splits to the set 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 (Lines 18-20). Then, the
variable 𝑘, which shows the number of current segments, is incre-
mented by one. The algorithm ends if the number of segments is
equal to the required number, i.e., 𝑤.

Example 4. Let us consider the dataset D in Figure 3 which rep-
resents the initialization of the algorithm, i.e., the time domain is
divided into two segments of the same size. The next step is to create
the 3rd segment by splitting one of the two existing segments. Two
different scenarios are possible.

Scenario 1 : The first scenario is shown in Figure 4a where the left
segment is divided into two equal parts. We generate the symbolic
representation of the time series 𝑥, 𝑦, and 𝑧 by using the 3 seg-
ments. Let’s assume the cardinality is 4. Then, ˆ𝑥 = [00, 00, 10],
ˆ𝑦 = [00, 00, 10] and ˆ𝑧 = [00, 00, 10] are the symbolic representa-
tion of x, y and z, respectively. Thus, the set 𝑋1 consists of only
one representation [00,00,10] with an occurrence of 3, i.e., 𝑋1 =
[00, 00, 10]. The entropy is then calculated as: 𝐻 (𝑋1) = −(𝑃 (𝑥 =
[00, 00, 10]) log 𝑃 (𝑥 = [00, 00, 10])) where 𝑃 (𝑥 = [00, 00, 10]) = 3
3 =

02468−2−101200011011xyzVariable Size Segmentation for Efficient Representation and Querying of Non-Uniform Time Series Datasets

SAC’22, April 25 –April 29, 2022, Brno, Czech Republic

Algorithm 1: ASAX variable-size segmentation
Input: 𝐷: time series database; 𝑛: the length of time series;
𝑚𝑖𝑛𝑆𝑖𝑧𝑒: the minimum possible size of a segment; 𝑎:
cardinality of symbols; 𝑤: the required number of
segments

1 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 = {[0, 𝑛

Output: 𝑤 variable-size segments
2 ], [ 𝑛
equal size segments

2 , 𝑛]}; // split time domain into two

2 𝑘 = 2
3 while 𝑘 ≠ 𝑤 do
4

𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑇𝑜𝑆𝑝𝑙𝑖𝑡 = 1
𝑒𝑛𝑡𝑟𝑜𝑝𝑦 = 0
for 𝑖=1 to 𝑘 do

5

6

7

8

9

10

11

12

13

14

15

16

17

𝑡𝑒𝑚𝑝𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑠 = 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠
if 𝑙𝑒𝑛𝑔𝑡ℎ(tempSegments[𝑖]) > 𝑚𝑖𝑛𝑆𝑖𝑧𝑒 then

split segment 𝑖 into two equal parts, and replace
the segment 𝑖 by its corresponding parts in
𝑡𝑒𝑚𝑝𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑠

ℎ𝑎𝑠ℎ𝑡𝑎𝑏𝑙𝑒 = new HashTable
foreach 𝑡𝑠 ∈ 𝐷 do

𝑤𝑜𝑟𝑑 = ASAX(𝑡𝑠, 𝑡𝑒𝑚𝑝𝑆𝑒𝑔𝑚𝑒𝑛𝑡𝑠, 𝑎)
ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒.put(𝑤𝑜𝑟𝑑)

𝑒 = entropy(ℎ𝑎𝑠ℎ𝑇 𝑎𝑏𝑙𝑒)
if 𝑒 > 𝑒𝑛𝑡𝑟𝑜𝑝𝑦 then

𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑇𝑜𝑆𝑝𝑙𝑖𝑡 = 𝑖
𝑒𝑛𝑡𝑟𝑜𝑝𝑦 = 𝑒

19

18

split 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑇𝑜𝑆𝑝𝑙𝑖𝑡 into two equal size segments 𝑠1
and 𝑠2
𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 = 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 - {𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑇𝑜𝑆𝑝𝑙𝑖𝑡 }
𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 = 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠 (cid:208){𝑠1, 𝑠2}
𝑘 = 𝑘+1
21
22 return 𝑠𝑒𝑔𝑚𝑒𝑛𝑡𝑠

20

1 and we have 𝐻 (𝑋1) = −(1 log 1) = 0.

Scenario 2 : This scenario is shown in Figure 4b in which the right
segment is split. As for Scenario 1 we generate the symbolic represen-
tation of time series x, y and z using the 3 segments, and cardinality
of 4. ˆ𝑥 = [00, 01, 10], ˆ𝑦 = [00, 01, 11] and ˆ𝑧 = [00, 01, 11] are the
symbolic representation of x, y and z, respectively. In this scenario the
representation set 𝑋2 consists of [00,01,10] with an occurrence of 1 and
[00,01,11] with an occurrence of 2, i.e., 𝑋 = [00, 01, 10], [00, 01, 10].
The entropy is calculated as:
𝐻 (𝑋2) = −(𝑃 (𝑥 = [00, 01, 10]) log 𝑃 (𝑥 = [00, 01, 10]) +
𝑃 (𝑥 = [00, 01, 11]) log 𝑃 (𝑥 = [00, 01, 11])) where 𝑃 (𝑥 = [00, 01, 10]) =
1
3 and 𝑃 (𝑥 = [00, 01, 11]) = 2
0.918.

3 . Then, 𝐻 (𝑋2) = −( 1

3 log 1

3 log 2

3 + 2

3 ) =

(a) Scenario 1 of ASAX segmentation with 3 segments

(b) Scenario 2 of ASAX segmentation with 3 segments

Figure 4: The two different scenarios of ASAX segmentation
with 3 segments. Scenario 4b is the one chosen because it
optimizes the entropy.

3.3 Lower Bounding of the Similarity Measure
SAX [12] defines a distance measure on the representation of time
series as described in Section 2.1. Given the representation of
two time series, the 𝑀𝐼 𝑁 𝐷𝐼𝑆𝑇𝑓 function allows obtaining a lower
bounding approximation of the Euclidean distance between the
original time series. By the following theorem, we propose a lower
bounding approximation formula for the case of variable size seg-
mentation in ASAX.

Theorem 1. Let 𝑋 and 𝑌 be two time series. Suppose that by using
ASAX we create a variable size segmentation with 𝑤 segments, such
that the size of the 𝑖𝑡ℎ segment is 𝑙𝑖 .
Let 𝑋 and 𝑌 be the PAA representation of variable size of 𝑋 and 𝑌 in
ASAX, 𝐷𝑅𝑣 (𝑋, 𝑌 ) gives a lower bounding approximation of the Eu-
𝑖=1 ((𝑥𝑖 − 𝑦𝑖 )2 × 𝑙𝑖 )
clidean distance between 𝑋 and 𝑌 : 𝐷𝑅𝑣 (𝑋, 𝑌 ) =
Let ˆ𝑋 and ˆ𝑌 be the representations of 𝑋 and 𝑌 in ASAX obtained by
converting 𝑋 and 𝑌 into symbolic representation. Then, 𝑀𝐼 𝑁 𝐷𝐼𝑆𝑇𝑣 ( ˆ𝑋, ˆ𝑌 )
gives a lower bounding approximation of the Euclidean distance be-
tween 𝑋 and 𝑌 : 𝑀𝐼 𝑁 𝐷𝐼𝑆𝑇𝑣 ( ˆ𝑋, ˆ𝑌 ) =

𝑖=1 (𝑑𝑖𝑠𝑡 ( ˆ𝑥𝑖, ˆ𝑦𝑖 )2 × 𝑙𝑖 )
Proof. The proof has been removed due to lack of space.

(cid:113)(cid:205)𝑤

(cid:113)(cid:205)𝑤

After having calculated the entropy for the two scenarios, we see
that 𝐻 (𝑋1) < 𝐻 (𝑋2). We aim to maximize the entropy, therefore we
choose the segmentation generated in Scenario 2 for this iteration of
our algorithm. We continue the next iterations, until the number of
segment reaches w.

3.4 Uniform Distribution of Symbols
SAX breakpoints divide the value domain into regions of different
size where small regions are concentrated on the middle of the value
domain and regions at extreme values are larger. This is illustrated

02468−2−101200011011xyz02468−2−101200011011xyzSAC’22, April 25 –April 29, 2022, Brno, Czech Republic

Lamia Djebour, Reza Akbarinia, and Florent Masseglia

by Figure 5, with three time series from our motivating example in
Figure 1. The breakpoints of SAX with 10 symbols are represented
by horizontal lines, and, logically, they appear close to the center of
the distribution. If we keep such distribution of symbols, then we
would have two issues. First, the extreme values of the series like
those above 2 or below -4 would be assigned the same symbol (their
PAA value on the segment would fall in the same symbol). Second,
the adaptive segmentation would consider that the slight variations
around zero are more important than the ones at extreme values,
ending in irrelevant splits that favor minor information gain. For
this reason, we propose to calculate the breakpoints differently. In
ASAX, the discretization is done based on breakpoints that produce
uniform distributions of symbols. These breakpoints divide the
value domain into regions of equal size. In the case of Figure 5 the
10 symbol regions will be evenly distributed in the range of data
values.

Figure 5: The Gaussian based distribution of symbols in SAX
are not suitable for ASAX since they would favor minor in-
formation gain.

4 EXPERIMENTS
In this section, we report the results of experimental studies on the
proposed ASAX segmentation approach that illustrate its perfor-
mance in improving the accuracy of time series representations in
order to get better precision during information search operations.

4.1 Datasets and Experimental Settings
We compared the ASAX representation with the existing SAX rep-
resentation on datasets selected for their particular (lack of) uni-
formity. Notice that SAX and its extensions in the literature use a
fixed-size segmentation of the time domain. But, ASAX proposes a
variable-size segmentation based on information theory techniques.
The approaches are implemented in Python programming lan-
guage and Numba JIT compiler is used to optimize machine code
at runtime 1. The experimental evaluation was conducted on a
machine using Fedora 31 operating system with 16 Gigabytes of
main memory, an Intel Core i7 1,90 GHz-4,80 GHz processor with
4 cores.

1Our code is available for download here: https://github.com/lamiad/ASAX

We carried out our experiments on several real world datasets
from the UCR Time Series Classification Archive [5]. Table 2 gives
basic information about the datasets: name, type, length of the time
series (number of values). Notice that almost all selected datasets
have non-uniform distributions over time domain (see Figure 6),
else SyntheticControl that has a quasi uniform distribution.

For each approach, we set the default cardinality value to 32 and
the length w of the approximate representations is reduced to 10%
of the original time series length.

Table 2: Datasets basic information

Name
AllGestureWiimoteZ
ECG200
ECG5000
ECGFiveDays
Fungi
GesturePebbleZ1
MedicalImages
SonyAIBORobotSurface1
SyntheticControl

Type
Sensor
ECG
ECG
ECG
HRM
Sensor
Image
Sensor
Simulated

time series Length
500
90
140
130
200
450
90
70
60

In the experiments, we measure the ASAX and SAX precision in
similarity search by applying a k-Nearest Neighbor (k-NN) search,
as detailed in Subsection 4.2. For ASAX, we measure the time cost
of the variable-size segmentation in Subsection 4.3.

4.2 Precision of k-Nearest Neighbor Search
In this part of experiments, we compare the quality of ASAX and
SAX representation on the different datasets described in Table 2 by
measuring the precision of the approximate k-NN search for both
of the two approaches. The precision reported for each dataset rep-
resents the average precision for a set of arbitrary random queries
taken from this dataset. The search precision for each query Q from
a dataset D is calculated as follows :

𝑝 =

|𝐴𝑝𝑝𝑘𝑁 𝑁 (𝑄, 𝐷) ∩ 𝐸𝑥𝑎𝑐𝑡𝑘𝑁 𝑁 (𝑄, 𝐷)|
𝑘

where AppkNN(Q,D) and ExactkNN(Q,D) are the sets of approximate
k nearest neighbors and exact k nearest neighbors of Q from D,
respectively. AppkNN(Q,D) is obtained using 𝐷𝑅𝑓 distance measure
for SAX and 𝐷𝑅𝑣 for the ASAX representation and the set Exac-
tkNN(Q,D) contains the k-NN of Q using the euclidean distance ED.
AppkNN(Q,D) and ExactkNN(Q,D) use a linear search that consists
in computing the distance from the query point Q to every other
point in D, keeping track of the "best so far" result.
The precision results are reported in Figure 6 where each dataset
is plotted with the precision obtained (as percentage) for both ap-
proaches and the datasets are sorted in descending order of preci-
sion gain. The plots show the shape of the different time series of
each dataset and we can notice that the distribution of time series
over the time domain varies from one dataset to another. Let us
take for example the ECGFiveDays dataset presented in Figure 6a
and SyntheticControl shown in Figure 6i. On the first one, we were
able to achieve a precision of 82% for ASAX while it is 55% for
SAX, which is a significant gain in precision. This higher precision

022446688110132−6−4−202xyzVariable Size Segmentation for Efficient Representation and Querying of Non-Uniform Time Series Datasets

SAC’22, April 25 –April 29, 2022, Brno, Czech Republic

(a) ECGFiveDays

(b) MedicalImages

(c) ECG5000

(d) GesturePebbleZ1

(e) AllGestureWiimoteZ

(f) Fungi

(g) ECG200

(h) SonyAIBORobotSurface1

(i) SyntheticControl

Figure 6: The data distribution of the tested datasets, and the precision results for each dataset. p(SAX) and p(ASAX) show the
precision of SAX and ASAX respectively. The datasets are sorted in descending order of precision gain.

for ASAX is due to the variable-size segmentation which created
segments in the parts that undergo a significant variation (from
time point 44 to 95) as discussed in our motivating example, except
that, here, we have 13 segments (allowing ASAX to perform a better
distribution of the segments according to information gain). For
SyntheticControl we can see that the precision of the approximate
k-NN search is the same for both ASAX and SAX approaches which
is 32%. In this dataset, the shape of the time series is balanced over
the time, and the segmentations obtained by ASAX and SAX are
the same, resulting in equivalent precision.

These results suggest the advantage of our approach over the
state-of-the-art when applied to time series with unbalanced distri-
bution.

4.3 Time cost of ASAX segmentation algorithm
Figure 7 reports the time cost of our proposed approach. It gives
the segmentation time of ASAX on the datasets of our experiments.
It does not concern SAX since SAX divides the time domain into
segments of fixed size which does not require any computation

Figure 7: Runtime of ASAX segmentation algorithm for each
dataset

020406080100120−6−4−20246p(SAX)= 55%p(ASAX)= 82%0102030405060708090−202468p(SAX)= 48%p(ASAX)= 68%020406080100120140−6−4−2024p(SAX)= 54%p(ASAX)= 67%050100150200250300350400450−40−2002040p(SAX)= 52%p(ASAX)= 61%0100200300400500−4−20246p(SAX)= 79%p(ASAX)= 87%0255075100125150175200020406080p(SAX)= 92%p(ASAX)= 97%0102030405060708090−2024p(SAX)= 72%p(ASAX)= 77%010203040506070−4−20246p(SAX)= 37%p(ASAX)= 40%0102030405060−2−10123p(SAX)= 32%p(ASAX)= 32%AllGestureWiimoteZECG200ECG5000ECGFiveDaysFungiGesturePebbleZ1MedicalImagesSonyAIBORobotSurface1SyntheticControlDataset10−1100101Runtime (s)SAC’22, April 25 –April 29, 2022, Brno, Czech Republic

Lamia Djebour, Reza Akbarinia, and Florent Masseglia

REFERENCES
[1] Rakesh Agrawal, Christos Faloutsos, and Arun N. Swami. 1993. Efficient Similarity

Search In Sequence Databases. In Proc. of the 4th Int. Conf. on FODO.

[2] A. Camerra, J. Shieh, T. Palpanas, T. Rakthanmanon, and E. J. Keogh. 2014. Beyond
one billion time series: indexing and mining very large time series collections
with i SAX2+. Knowl. Inf. Syst. (2014).

[3] Kin-pong Chan and Ada Wai-Chee Fu. 1999. Efficient Time Series Matching by

Wavelets. In Proc. of the ICDE.

[4] Richard Cole, Dennis Shasha, and Xiaojian Zhao. 2005. Fast Window Correlations

over Uncooperative Time Series. In KDD Conf. 743–749.

[5] Hoang Anh Dau, Eamonn Keogh, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan
Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, Yanping, Bing
Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, Gustavo Batista, and
Hexagon-ML. 2018. The UCR Time Series Classification Archive. https://www.
cs.ucr.edu/~eamonn/time_series_data_2018/.

[6] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. 1994. Fast Subsequence
Matching in Time-series Databases. SigRec 23, 2 (1994), 419–429. https://doi.org/
10.1145/191843.191925

[7] Christos Faloutsos, M. Ranganathan, and Yannis Manolopoulos. 1994. Fast Sub-

sequence Matching in Time-series Databases. In Proc. of the SIGMOD.

[8] Pablo Huijse, Pablo A. Estévez, Pavlos Protopapas, Jose C. Principe, and Pablo
Zegers. 2014. Computational Intelligence Challenges and Applications on Large-
Scale Astronomical Time Series Databases.
IEEE Comp. Int. Mag. 9, 3 (2014),
27–39.

[9] Kunio Kashino, Gavin Smith, and Hiroshi Murase. 1999. Time-series active search

for quick retrieval of audio and video. In ICASSP.

[10] Eamonn J. Keogh, Kaushik Chakrabarti, Michael J. Pazzani, and Sharad Mehrotra.
2001. Dimensionality Reduction for Fast Similarity Search in Large Time Series
Databases. Knowl. Inf. Syst. 3, 3 (2001), 263–286.

[11] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. 2003. A Symbolic Representation of

Time Series, with Implications for Streaming Algorithms. In SIGMOD.

[12] J. Lin, E. Keogh, L. Wei, and S. Lonardi. 2007. Experiencing SAX: A Novel Symbolic

Representation of Time Series. Data Min. Knowl. Discov. (2007).

[13] Michele Linardi and Themis Palpanas. 2018. ULISSE: ULtra Compact Index for

Variable-Length Similarity Search in Data Series. In ICDE.

[14] Michele Linardi, Yan Zhu, Themis Palpanas, and Eamonn J. Keogh. 2018. Matrix
Profile X: VALMOD - Scalable Discovery of Variable-Length Motifs in Data Series.
In SIGMOD.

[15] Michele Linardi, Yan Zhu, Themis Palpanas, and Eamonn J. Keogh. 2018.
VALMOD: A Suite for Easy and Exact Detection of Variable Length Motifs in
Data Series. In SIGMOD.

[16] Themis Palpanas. 2015. Data Series Management: The Road to Big Sequence

Analytics. SIGMOD Record 44, 2 (2015), 47–52.

[17] T. Rakthanmanon, B. Campana, A. Mueen, G. Batista, B. Westover, Q. Zhu, J.
Zakaria, and E. Keogh. 2012. Searching and Mining Trillions of Time Series
Subsequences Under Dynamic Time Warping. In KDD.

[18] Usman Raza, Alessandro Camerra, Amy L. Murphy, Themis Palpanas, and
Gian Pietro Picco. accepted for publication, 2015. Practical Data Prediction for
Real-World Wireless Sensor Networks. IEEE Trans. Knowl. Data Eng. (accepted
for publication, 2015). https://doi.org/10.1109/TKDE.2015.2411594

[19] Dennis Shasha. 1999. Tuning Time Series Queries in Finance: Case Studies and

Recommendations. IEEE Data Eng. Bull. 22, 2 (1999), 40–46.

[20] J. Shieh and E. Keogh. 2008. iSAX: Indexing and Mining Terabyte Sized Time

Series. In KDD Conf. 623–631.

[21] S. Soldi, Volker Beckmann, Wayne H. Baumgartner, Gabriele Ponti, Chris R.
Shrader, P. Lubinski, H. A. Krimm, F. Mattana, and Jack Tueller. 2014. Long-term
variability of AGN at hard X-rays. Astronomy and Astrophysics - A&A 563, A57
(March 2014), 16. https://doi.org/10.1051/0004-6361/201322653

[22] Lexiang Ye and Eamonn J. Keogh. 2009. Time series shapelets: a new primitive

for data mining. In KDD.

[23] Kostas Zoumpatianos and Themis Palpanas. 2018. Data Series Management:

Fulfilling the Need for Big Sequence Analytics. In ICDE.

beforehand. The longest segmentation time is approximately 13
seconds, while the shortest one is around 20 milliseconds. It depends
on both the number of time series in the dataset and their length.

5 RELATED WORK
Several techniques have been yet proposed to reduce the dimension-
ality of time series. Examples of such techniques that can signifi-
cantly decrease the time and space required for similarity search are:
singular value decomposition (SVD) [7], the discrete Fourier trans-
formation (DFT) [1], discrete wavelets transformation (DWT) [3],
piecewise aggregate approximation (PAA) [10], random sketches
[4], and symbolic aggregate approXimation (SAX) [12].

SAX [12] is one of the most popular techniques for time series
representation. It uses a symbolic representation that segments all
time series into equi-length segments and symbolizes the mean
value of each segment. The symbolic representation allows to lower
bound the corresponding distance measures defined on the original
time series. Several extensions of SAX have been yet proposed,
mainly for improving the similarity search performance via index-
ing. For example, iSAX [20] is an indexable version of SAX designed
for indexing large collections of time series.

iSAX 2.0 [2] proposes a new mechanism and also algorithms for
efficient bulk loading and node splitting policy, wich is not sup-
ported by iSAX index. In [2], two extensions of iSAX 2.0, namely
iSAX 2.0 Clustered and iSAX2+, have been proposed. These exten-
sions focus on the efficient handling of the raw time series data
during the bulk loading process, by using a technique that uses main
memory buffers to group and route similar time series together
down the tree, performing the insertion in a lazy manner.

However, the segmentation in SAX and its extensions is not adap-
tive to the data since their division principle of the time domain
is based on fixed-size segments which is not always effective for
unbalanced data distributions. To increase the quality of the approx-
imation for this type of time series we propose our approach ASAX
based on SAX representation and a variable-length segmentation
algorithm. Our approach is complementary to the SAX extensions,
such as iSAX and iSAX 2.0 which have been proposed for improving
the kNN queries response time.

6 CONCLUSION
In this paper, we proposed a new approximation technique, called
ASAX, that considers the time series distribution on the time do-
main and performs variable-size segmentation, by using the en-
tropy of symbolic representations. Our technique allows reducing
information loss and thus increasing the accuracy of time series
representations. We implemented our technique and evaluated its
performance using several real world datasets. The experimental
results suggest that ASAX can obtain significant performance gains
in terms of precision for similarity search compared to SAX. The
results show that the more the data distribution in the time domain
is unbalanced (non-uniform), the greater is the precision gain of
ASAX, e.g., for the EGCFiveDays dataset that has a non-uniform
distribution in the time domain, the precision of ASAX is 82% com-
pared to 55% for SAX.

