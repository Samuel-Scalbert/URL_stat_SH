Privacy-Preserving and Bandwidth-Eﬀicient Federated
Learning: An Application to In-Hospital Mortality
Prediction
Raouf Kerkouche, Gergely Acs, Claude Castelluccia, Pierre Genevès

To cite this version:

Raouf Kerkouche, Gergely Acs, Claude Castelluccia, Pierre Genevès. Privacy-Preserving and
Bandwidth-Eﬀicient Federated Learning: An Application to In-Hospital Mortality Prediction. CHIL
2021 - ACM Conference on Health, Inference, and Learning, Apr 2021, virtual event, France. pp.1-11,
￿10.1145/3450439.3451859￿. ￿hal-03160473￿

HAL Id: hal-03160473

https://inria.hal.science/hal-03160473

Submitted on 5 Mar 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Privacy-Preserving and Bandwidth-Efficient Federated
Learning: An Application to In-Hospital Mortality Prediction

Raouf Kerkouche
raouf .kerkouche@inria.fr
Privatics team, Univ. Grenoble Alpes, Inria

Claude Castelluccia
claude.castelluccia@inria.fr
Privatics team, Univ. Grenoble Alpes, Inria

Gergely Ács
acs@crysys.hu
Crysys Lab, BME-HIT

Pierre Genevès
pierre.geneves@cnrs.fr
Tyrex team, Univ. Grenoble Alpes, CNRS, Inria, Grenoble
INP, LIG

ABSTRACT
Machine Learning, and in particular Federated Machine Learning, 
opens new perspectives in terms of medical research and patient 
care. Although Federated Machine Learning improves over central-
ized Machine Learning in terms of privacy, it does not provide prov-
able privacy guarantees. Furthermore, Federated Machine Learning 
is quite expensive in term of bandwidth consumption as it requires 
participant  nodes  to  regularly  exchange  large  updates.  This  pa-
per proposes a bandwidth-efficient privacy-preserving Federated 
Learning that provides theoretical privacy guarantees based on 
Differential Privacy. We experimentally evaluate our proposal for 
in-hospital mortality prediction using a real dataset, containing 
Electronic Health Records of about one million patients. Our re-
sults suggest that strong and provable patient-level privacy can 
be enforced at the expense of only a moderate loss of prediction 
accuracy.

INTRODUCTION

1 
An Electronic Health Record (EHR) is a digital version of the pa-
tient’s medical information. EHR data open new perspectives, espe-
cially with the development of machine learning. EHR data can be 
used to train predictive models in order to predict patient’s medical 
conditions  and  help  medical  doctors  to  develop  appropriate  care 
[18, 36].

However,  medical  data  is  considered  as  sensitive  information  that 
can  lead  to  some  real  and  serious  damage  to  the  patient  if  any 
leakage  happens.  For  example,  medical  data  can  be  exploited  by 
insurance companies to adapt their insurance fees, by banks to deny 
loans, or by politicians to discredit their opponents. Therefore, the 
privacy  of  such  kind  of  sensitive  data  must  be  guaranteed  and 
privacy-preserving predictive models are needed.

Predictive models are typically built using machine learning al-
gorithms that are trained on centralized datasets. When a model is 
trained on multiple datasets, collected for example by several hospi-
tals, the centralization of all datasets on a single server introduces 
additional, and often unacceptable, privacy risks. To mitigate this 
problem, Federated learning (FL) was proposed as a new learning 
protocol. Federated Learning consists of distributing the learning 
process on the different entities providing data: instead of aggre-
gating the data on a single server, the training is performed locally 
by each participating entities and the models are then shared and 
aggregated  [27,  38].  Although  Federated  Learning  mitigates  the 
privacy risks by design, recent results have shown that some at-
tacks, such as membership and property inference attacks, are still 
possible [29, 33]. Moreover, complete training samples can also be 
reconstructed purely from the captured gradients [43, 44].

Furthermore,  since  participating  entities  must  collaborate  by 
exchanging their model updates, the required bandwidth during 
the training phase is often significant and prohibitive [22].

Contribution.  This paper proposes a bandwidth-efficient privacy-
preserving  Federated  Learning  scheme  that  provides  theoretical 
privacy guarantees. Our proposal guarantees Differential Privacy 
with practical utility even on highly imbalanced training data. This 
is challenging as imbalanced data increases the injected noise re-
quired  by  Differential  Privacy  and  hence  substantially  degrades 
model quality. Our solution relies on the extreme quantization of 
the gradients in order to reduce communication costs as well as on 
downsampling of mini-batches to diminish the noise needed for 
Differential Privacy. We experimentally evaluate the performance 
of our solution for in-hospital mortality prediction using real EHR 
data, containing about one million records of patients. Our results 
suggest that patient-level privacy can be enforced at the expense 
of only a moderate loss of prediction accuracy.

Outline.  We describe the background in Section 2. We introduce 
our privacy-preserving scheme in Section 3. We report on experi-
ments with real-world data in Section 4. Finally we discuss related 
works in Section 5 before concluding in Section 7.

2  BACKGROUND
2.1  Federated Learning (FL-STANDARD)
In  federated  learning  [27,  38],  multiple  parties  (clients)  build  a 
common machine learning model on the union of their training data 
without sharing them with each other. At each round of the training, 
some clients retrieve the global model from the parameter server, 
update  the  global  model  based  on  their  own  training  data,  and 
send back their updated model to the server. The server aggregates 
the updated models of all clients to obtain a global model that is 
re-distributed to some selected parties in the next round.

In particular, a subset K of all 𝑁  clients are randomly selected 
at each round to update the global model, and 𝐶  = |K|/𝑁  denotes 
the fraction of selected clients. At round 𝑡,  a selected client 𝑘  ∈ K 
local gradient descent iterations on the common model 
executes 𝑇gd 
), and obtains 
(𝐷  =  ∪𝑘 ∈K𝐷𝑘 
using its own training data 𝐷𝑘 
w𝑡−1 
𝑘
, where the number of weights is denoted by 
the updated model w𝑡 
𝑘
𝑘
| = |Δw𝑡 
| = 𝑛 for all 𝑘 and 𝑡).  Each client 𝑘 submits the
𝑛 (i.e., |w𝑡 
𝑘
𝑘
𝑘
update Δw𝑡 
to the server, which then updates the
− w𝑡−1  
=  w𝑡 
common model as follows: w𝑡 = w𝑡 −1 + (cid:205)𝑘 ∈K
𝑡 , where
|𝐷𝑘 | is known to the server for all 𝑘 (a client’s update is weighted
with the size of its training data). The server stops training after
a fixed number of rounds 𝑇cl, or when the performance of the
common model does not improve on a held-out data.

|𝐷𝑘 |
(cid:205)𝑗 |𝐷 𝑗 |

Δw𝑘

Note that each 𝐷𝑘 may be generated from different distributions
(i.e., Non-IID case), that is, any client’s local dataset may not be
representative of the population distribution [27]. This can happen,
for example, when not all output classes are represented in every
client’s training data. The federated learning of neural networks is
summarized in Alg. 1. In the sequel, each client is assumed to use
the same model architecture.

Algorithm 1: FL-STANDARD: Federated Learning

1 Server:

2

3

4

5

6

7

8

9

Initialize common model 𝑤0
for 𝑡 = 1 to 𝑇cl do

Select K clients uniformly at random
for each client 𝑘 in K do

Δw𝑘

𝑡 = Client𝑘 (w𝑡 −1)

end
w𝑡 = w𝑡 −1 + (cid:205)𝑘

|𝐷𝑘 |
(cid:205)𝑗 |𝐷 𝑗 |

Δw𝑘
𝑡

end
Output: Global model w𝑡

10
11 Client𝑘 (w𝑘
w𝑘
𝑡 −1,𝑇gd)
12
Output: Model update (w𝑘

𝑡 −1):
𝑡 = SGD(𝐷𝑘, w𝑘

𝑡 − w𝑘

𝑡 −1)

The motivation of federated learning is three-fold: first, it aims
to provide confidentiality of each participant’s training data by

Raouf Kerkouche, Gergely Ács, Claude Castelluccia, and Pierre Genevès

Algorithm 2: Stochastic Gradient Descent
Input: 𝐷 : training data, 𝑇gd : local epochs, w : weights

1 for 𝑡 = 1 to 𝑇gd do
2

Select batch B from 𝐷 randomly
w = w − 𝜂 ∇𝑓 (B; w)

3
4 end

Output: Model w

sharing only model updates instead of potentially sensitive training
data. Second, in order to decrease communication costs, clients can
perform multiple local SGD iterations before sending their update
back to the server. Third, in each round, only a few clients are
required to perform local training of the common model, which
further diminishes communication costs and makes the approach
especially appealing with a large number of clients.

However, several prior works have demonstrated that model
updates do leak potentially sensitive information [29, 33]. Hence,
simply not sharing training data per se is not enough to guarantee
their confidentiality.

2.2 Differential Privacy
Differential privacy allows a party to privately release information
about a dataset: a function of an input dataset is perturbed, so that
any information which can differentiate a record from the rest of
the dataset is bounded [17].

Definition 2.1 (Privacy loss). Let A be a privacy mechanism
which assigns a value in Range(A) to a dataset 𝐷. The privacy loss
of A with datasets 𝐷 and 𝐷 ′ at output 𝑂 ∈ Range(A) is a random
variable P (A, 𝐷, 𝐷 ′, 𝑂) = log Pr[ A (𝐷)=𝑂 ]
where the probability
Pr[ A (𝐷′)=𝑂 ]
is taken on the randomness of A.

Definition 2.2 ((𝜖, 𝛿)-Differential Privacy [17]). A privacy
mechanism A guarantees
for
any database 𝐷 and 𝐷 ′, differing on at most one record,
Pr𝑂∼A (𝐷) [P (A, 𝐷, 𝐷 ′, 𝑂) > 𝜀] ≤ 𝛿.

(𝜀, 𝛿)-differential privacy if

Intuitively, this guarantees that an adversary, provided with the
output of A, can draw almost the same conclusions (up to 𝜀 with
probability larger than 1 − 𝛿) about any record no matter if it is
included in the input of A or not [17]. That is, for any record owner,
a privacy breach is unlikely to be due to its participation in the
dataset.
Moments Accountant. Differential privacy maintains composition;
the privacy guarantee of the 𝑘-fold adaptive composition of
A1:𝑘 = A1, . . . , A𝑘 can be computed using the moments accoun-
tant method [2]. In particular, it follows from Markov’s inequality
that Pr[P (A, 𝐷, 𝐷 ′, 𝑂) ≥ 𝜀] ≤ E[exp(𝜆P (A, 𝐷, 𝐷 ′, 𝑂))]/exp(𝜆𝜀)
for any output 𝑂 ∈ Range(A) and 𝜆 > 0. This implies that A
is (𝜀, 𝛿)-DP with 𝛿 = min𝜆 exp(𝛼 A (𝜆) − 𝜆𝜀), where 𝛼 A (𝜆) =
max𝐷,𝐷′ log E𝑂∼A (𝐷) [exp(𝜆P (A, 𝐷, 𝐷 ′, 𝑂))] is the log of the mo-
ment generating function of the privacy loss. The privacy guaran-
tee of the composite mechanism A1:𝑘 can be computed using that
𝛼 A1:𝑘 (𝜆) ≤ (cid:205)𝑘
Gaussian Mechanism. There are a few ways to achieve DP, including
the Gaussian mechanism [17]. A fundamental concept of all of them
is the global sensitivity of a function [17].

𝑖=1 𝛼 A𝑖 (𝜆) [2].

Privacy-Preserving and Bandwidth-Efficient Federated Learning: An Application to In-Hospital Mortality Prediction

Definition 2.3 (Global 𝐿𝑝 -sensitivity). For any function 𝑓 : D →
R𝑛, the 𝐿𝑝 -sensitivity of 𝑓 is Δ𝑝 𝑓 = max𝐷,𝐷′ ||𝑓 (𝐷) − 𝑓 (𝐷 ′)||𝑝 , for
all 𝐷, 𝐷 ′ differing in at most one record, where || · ||𝑝 denotes the
𝐿𝑝 -norm.

The Gaussian Mechanism [17] consists of adding Gaussian noise
to the true output of a function. In particular, for any function
𝑓 : D → R𝑛, the Gaussian mechanism is defined as adding i.i.d
Gaussian noise with variance (Δ2 𝑓 ·𝜎)2 and zero mean to each coor-
dinate value of 𝑓 (𝐷). Recall that the pdf of the Gaussian distribution
with mean 𝜇 and variance 𝜉 2 is

pdf G (𝜇,𝜉) (𝑥) =

√

1

2𝜋𝜉

exp

(cid:18)

−

(cid:19)

(𝑥 − 𝜇)2
2𝜉 2

(1)

In fact, the Gaussian mechanism draws vector values from a
multivariate spherical (or isotropic) Gaussian distribution which
is described by random variable G(𝑓 (𝐷), Δ2 𝑓 · 𝜎I𝑛), where 𝑛 is
omitted if its unambiguous in the given context.

3 TOWARD FEDERATED LEARNING

RECORD-LEVEL PRIVACY

3.1 The FL-SIGN Protocol
In the FL-STANDARD scheme, presented in Section 2.1, each se-
lected client sends its updated model to the central server. As dis-
cussed previously, this scheme has several drawbacks in terms of
bandwidth and privacy. We propose to limit these drawbacks by
quantizing the model weights as in [9, 21]. More specifically, in the
new scheme, referred to as FL-SIGN in the rest of this paper, each
client sends only the sign of every coordinate value in its param-
eter update vector. The server takes the sign of the sum of signs
per coordinate and scales down the result with a fixed constant
𝛾 (which is in the order of 10−3 in practice) in order to limit the
contribution of each client and adjust convergence. This scaled
aggregated updates are added to the global model.

Algorithm 3: FL-SIGN: Sign Federated Learning

1 Server:

2

3

4

5

6

7

8

9

Initialize common model 𝑤0
for 𝑡 = 1 to 𝑇cl do

Select K clients uniformly at random
for each client 𝑘 in K do
s𝑘
𝑡 = Client𝑘 (w𝑡 −1)

end
w𝑡 = w𝑡 −1 + 𝛾 sign

(cid:16)(cid:205)𝑘 s𝑘

𝑡

(cid:17)

end
Output: Global model w𝑡

10
11 Client𝑘 (w𝑖
w𝑘
12
Output: Model update sign(w𝑘

𝑡 −1):
𝑡 = SGD(𝐷𝑘, w𝑘

𝑡 −1,𝑇gd)

𝑡 − w𝑘

𝑡 −1)

More specifically, FL-SIGN (see Alg. 3) differs from the standard

federated scheme FL-STANDARD (see Alg. 1) as follows:

(1) Each client returns s𝑘

𝑡 = sign(w − w𝑘
𝑡 −1) instead of (w −
𝑡 −1), where sign : R𝑛 → {−1, 1}𝑛 returns the sign of each
w𝑘

coordinate value of the input vector if it is non-zero and a
sign chosen uniformly at random otherwise.

(2) The server sums the sign vectors s𝑘

(cid:16)(cid:205)𝑘 s𝑘

𝑡 sent by each client 𝑘 and
(cid:17). This
computes the sign vector of this sum as sign
is equivalent to take the median of all clients’ signs at every
position of the update vectors. Unlike in Alg. 1, the update
s𝑘
𝑡 is not weighted with client 𝑘’s data size |𝐷𝑘 |, since that
would require the client to send |𝐷𝑘 | to the server which
would enable the adversary to maliciously scale up its sign
vector by sending a fabricated size of its training data.

𝑡

The extreme quantization performed by FL-SIGN reduces the
communication costs of federated learning by a factor of 32 (since
only one bit is sent per parameter instead of 32 bits). Note also
that, if the quantized update vector is sparse, other lossless or
lossy compression techniques can further improve communication
efficiency [22].

3.2 Privacy-Preserving FL-SIGN (FL-SIGN-DP)
In FL-SIGN, a participant only sends the signs of its updates, as
opposed to their actual values, hence it intuitively reveals less infor-
mation about the client’s dataset than the original FL-STANDARD
scheme. In order to experimentally validate this intuition, we imple-
mented the inference attack described in [29] on FL-STANDARD
and FL-SIGN1. Results showed that the attack accuracy dropped
from 92% for FL-STANDARD to 50% for FL-SIGN. While these re-
sults suggest that privacy could be preserved in practice, they do
not provide any strong guarantee.

𝑡 − w𝑘

𝑡 = sign(w𝑘

To reason about the general privacy guarantee of FL-SIGN more
rigorously, consider the sign vector s𝑘
𝑡 −1). Several
attacks have demonstrated [29, 33] that Δw𝑘
𝑡 −1 can be
used to infer the membership of individual records in the training
data due to the strong memorization property of neural networks,
and overfitting in general. As taking the sign of Δw𝑘
𝑡 is a determin-
istic operation and depends on the value of s𝑘
𝑡 , there is no guarantee
that s𝑘

𝑡 does not leak any sensitive information.

𝑡 = w𝑘

𝑡 − w𝑘

In order to obtain theoretically private schemes, we extend FL-
SIGN with Differential Privacy. Our goal is to design a differentially
private scheme that is accurate and also bandwidth efficient (even
for small 𝜀 values).

3.2.1 Privacy and Adversarial Models. We consider an adversary,
or a set of colluding adversaries, who can access any update vector
sent by the server or any clients at each round of the protocol. The
adversary is computationally unbounded but passive (i.e., honest-
but-curious), that is, it follows the learning protocol faithfully and
does not modify any update vector. A plausible adversary is a
participating entity, i.e. a malicious client or server, that wants to
infer the training data used by other participants.

We aim at developing a solution that protects each record of the
clients’ training datasets. For example, in the scenario of collaborat-
ing hospitals we aim at protecting each individual patient record
of all hospital datasets.

1A model was trained for gender classification on the LFW dataset. The adversary’s
goal is to infer from the model updates whether a specific group of individuals in a
client’s dataset are black.

Algorithm 4: FL-SIGN-DP: Bandwidth-Efficient Federated Learn-
ing with Differential Privacy

Algorithm 5: FL-STANDARD-DP: Federated Learning with Dif-
ferential Privacy

Raouf Kerkouche, Gergely Ács, Claude Castelluccia, and Pierre Genevès

1 Server:

2

3

4

5

6

7

8

Initialize common model 𝑤0
for 𝑡 = 1 to 𝑇cl do

Select K clients randomly
for each client 𝑘 in K do
s𝑘
𝑡 = Client𝑘 (w𝑡 −1)

end
w𝑡 = w𝑡 −1 + 𝛾 sign

(cid:16)(cid:205)𝑘 s𝑘

𝑡

(cid:17)

1 Server:

2

3

4

5

6

7

8

Initialize common model 𝑤0
for 𝑡 = 1 to 𝑇cl do

Select K clients randomly
for each client 𝑘 in K do
s𝑘
𝑡 = Client𝑘 (w𝑡 −1)

end
w𝑡 = w𝑡 −1 + 1
|K|

(cid:16)(cid:205)𝑘 s𝑘

𝑡

(cid:17)

end

9
10 Client𝑘 (w𝑘
˜w𝑘
11
Output: sign( ˜w𝑘

𝑡 = DPSGD(𝐷𝑘, w𝑘
𝑡 − w𝑘

𝑡 −1):

𝑡 −1, 𝑆, 𝜎,𝑇gd)
𝑡 −1)

The adversary should not be able to learn from the received
model or its updates whether any particular record was used to
train the model by any other participants.

We believe that this adversarial model is reasonable for the med-
ical application that we consider in this paper: it is very unlikely
that a participating hospital will take the risk of manipulating the
updates that it sends to the server. However, we want to make sure
that it can not infer any sensitive information from the models that
it receives from the server. In other words, we make the assumption
that hospitals may be "curious", but are "honest".

We use Differential Privacy (DP) because it was proposed to
achieve this goal. DP guarantees plausible deniability. Therefore,
any negative privacy impact on an individual, i.e. a patient in the
dataset, cannot be attributed to his involvement in the training
phase (up to 𝜀 and 𝛿). For example, if an insurance company accesses
the model updates or the common model and decides to increase
the price of a patient’s insurance fee, it cannot be because of the
patient’s data.

3.2.2 Operation. To guarantee differential privacy for any indi-
vidual record of a training data, we propose FL-SIGN-DP, depicted
in Alg. 4, which is a synergy of FL-SIGN and differentially private
gradient descent (DPSGD) from [2]. In particular, instead of run-
ning traditional SGD on its local training data, every client executes
DPSGD (depicted in Alg. 6), which guarantees that its output ˜w𝑘
𝑡
does not leak any information that is specific to a single training
sample (up to 𝜀 and 𝛿) by clipping the 𝐿2-norm of the gradients
and perturbing the result with Gaussian noise. The noise scale is
calibrated to 𝑆 and 𝜎, where the latter directly gives 𝜀 and 𝛿 as
shown below. Hence, any further computation which uses ˜w𝑘
𝑡 is
also differentially private.

Notice that the batch is created using downsampling [19, 31] (see
Alg. 7) in order to overcome the imbalanced classes of the training
data. Downsampling guarantees that every batch contains identical
number of samples from every class, and therefore they have similar
magnitude of gradients on average.

Likewise FL-SIGN, FL-SIGN-DP also sends only signs for aggre-

gation, and hence is equally bandwidth efficient.

3.2.3 Privacy analysis. The privacy guarantee of FL-SIGN-DP is
quantified using the moments accountant method from [2]. Let

end

9
10 Client𝑘 (w𝑘
˜w𝑘
11
Output: ˜w𝑘

𝑡 −1):

𝑡 = DPSGD(𝐷𝑘, w𝑘

𝑡 −1, 𝑆, 𝜎,𝑇gd)

𝑡 − w𝑘

𝑡 −1

Algorithm 6: DPSGD(𝐷, w, 𝑆, 𝜎,𝑇gd)
Input: 𝐷 : training data, 𝑇gd : number of iterations, w : weights, 𝑆 :

clipping threshold, 𝜎 : noise scale

1 for 𝑡 = 1 to 𝑇gd do
2

Select batch B from 𝐷 randomly
B′ = Downsampling(B)
for each record 𝑟 in B′ do

∇ ˆ𝑓 (𝑟, w) = ∇𝑓 (𝑟, w)/max (cid:16)1, ||∇𝑓 (𝑟,w) ||2

(cid:17)

𝑆

end
w = w − (𝜂/|B′ |)

(cid:16)(cid:205)𝑟 ∈B′ ∇ ˆ𝑓 (𝑟 ; w) + G (0, 𝜎𝑆I)

(cid:17)

3

4

5

6

7

8 end

Output: Model parameters w

Algorithm 7: Downsampling(B)

Input: Batch B with labels 𝐿1 and 𝐿2

1 Partition B into C1 and C2, where all samples in C1 has label 𝐿1 and

all samples in C2 has label 𝐿2

2 𝑠min = min( |C1 |, |C2 |)
3 B1 ← select 𝑠min samples from C1 uniformly at random
4 B2 ← select 𝑠min samples from C2 uniformly at random

Output: Balanced batch B1 ∪ B2

𝜂0 (𝑥 |𝜉, 𝑞) = pdf G (0,𝜉) (𝑥) and 𝜂1 (𝑥 |𝜉, 𝑞) = (1 − 𝑞)pdf G (0,𝜉) (𝑥) +
𝑞pdf G (1,𝜉) (𝑥) where 𝑞 is the sampling probability of a single record
in a single round. Let

𝛼 G (𝜆|𝑞) = log max(𝐸1 (𝜆, 𝜉, 𝑞), 𝐸2 (𝜆, 𝜉, 𝑞))

(2)

where

and

𝐸1 (𝜆, 𝜉, 𝑞) =

𝐸2 (𝜆, 𝜉, 𝑞) =

∫

R

∫

R

𝜂0 (𝑥 |𝜉, 𝑞) ·

(cid:18) 𝜂0 (𝑥 |𝜉, 𝑞)
𝜂1 (𝑥 |𝜉, 𝑞)

(cid:19)𝜆

𝑑𝑥

𝜂1 (𝑥 |𝜉, 𝑞) ·

(cid:18) 𝜂1 (𝑥 |𝜉, 𝑞)
𝜂0 (𝑥 |𝜉, 𝑞)

(cid:19)𝜆

𝑑𝑥

Theorem 3.1 (Privacy of FL-SIGN-DP). For any 𝛿 > 0, FL-SIGN-
DP is (min𝜆 (𝑇cl ·𝛼 G (𝜆|𝑞1) +𝑇cl · (𝑇gd −1) ·𝛼 G (𝜆|𝑞2) −log 𝛿)/𝜆, 𝛿)-DP,
|B |
where 𝛼 G is defined in Eq. (2), 𝑞1 =
min𝑘 |𝐷𝑘 | .

𝐶 · |B|
min𝑘 |𝐷𝑘 | , and 𝑞2 =

Privacy-Preserving and Bandwidth-Efficient Federated Learning: An Application to In-Hospital Mortality Prediction

(cid:16)

𝑂

FL-SIGN

1
√
𝑇cl𝐶𝑁
(cid:16) 𝑛𝑆𝜎
√
𝑇cl𝐶𝑁
Table 1: Convergence rates when 𝛾 = 𝑂 (1/
𝑇cl

FL-SIGN-DP 𝑂

(cid:17)

(cid:17)

√
𝑇cl), 𝑇gd = 1, |B| =

The proof follows from Theorem 2 in [2] and the fact that a
record is sampled in the very first SGD iteration of every round
if (1) the corresponding client is sampled, which has a probability
of 𝐶, and (2) the batch sampled locally at this client contains the
. However, the
record, which has a probability of at most
adaptive composition of consecutive SGD iterations are considered
where the output of a single iteration depends on the output of
the previous iterations. Therefore, the sampling probability for the
, while the sampling probability
very first batch is 𝑞1 =
for every subsequent SGD iteration within the same round is at
most 𝑞2 =
min𝑘 |𝐷𝑘 | conditioned on the result of the first iteration
(see the proof of Theorem 2 in [2]).

𝐶 · |B |
min𝑘 |𝐷𝑘 |

|B|
min𝑘 |𝐷𝑘 |

|B|

Given a fixed value of 𝛿, 𝜀 is computed numerically as in [2, 30].

3.2.4 Convergence analysis. In Appendix A.1, we analytically com-
(cid:17). Com-
pute that FL-SIGN-DP has a convergence rate of 𝑂
pared to FL-SIGN (see Table 1), the convergence rate is increased
with a factor of 𝑛𝑆𝜎 which is attributed to the Gaussian noise and
can be considered as the “cost of privacy”.

(cid:16) 𝑛𝑆𝜎
√
𝑇cl𝐶𝑁

4 EXPERIMENTAL RESULTS
The goal of this section is to evaluate the performance of our pro-
posed FL-SIGN-DP scheme on a realistic in-hospital mortality pre-
diction scenario. We aim at evaluating its performance with dif-
ferent levels of privacy (i.e. different values of 𝜖) and comparing it
with the performance of the following learning protocols:

• (Non-federated) CENTRALIZED training: The training data
of all hospitals are merged and a single model is trained on
this merged data without any privacy guarantee.

• FL-STANDARD is described in Section 2.1.
• FL-SIGN is described in Section 3.1.
• FL-STANDARD-DP is specified in Alg. 5. It has the same
privacy guarantee as FL-SIGN-DP2 but is less bandwith effi-
cient. Specifically, unlike in FL-SIGN-DP, each client sends
the original (non-quantized) update vector s𝑘
𝑡 −1
to the server, which computes the model update as w𝑡 =
(cid:17). Both FL-SIGN-DP and FL-STANDARD-
w𝑡 −1 + 1
|K |
DP use downsampling (in Alg. 7) to create batches.

(cid:16)(cid:205)𝑘 s𝑘

𝑡 = w𝑘

𝑡 − w𝑘

𝑡

In order to improve the reproducibility of our results, we pub-

lished all the code used in our experiments 3.

2the privacy analysis in Section 3.2.3 also applies to FL-STANDARD-DP
3https://github.com/raouf-kerkouche/Privacy-preserving-and-Bandwith-Efficient-
Federated-Learning-An-Application-to-In-Hospital-Mortality

4.1 The In-hospital Mortality Prediction

Scenario

The ability to accurately predict the risks in the patient’s perspec-
tives of evolution is a crucial prerequisite in order to adapt the care
that certain patients receive [18].

We consider the scenario where several hospitals are collaborat-
ing to train models for in-hospital mortality prediction using our
Federated Learning schemes. This well-studied real-world problem
consists in trying to precisely identify the patients who are at risk
of dying from complications during their hospital stay [5, 18, 36].
As commonly found in the literature [18], for such predictions, we
focus on hospital admissions of adults hospitalized for at least 3
days, excluding elective admissions.

4.2 The Premier Healthcare Database
We used EHR data from the Premier healthcare database4 which is
one of the largest clinical databases in the United States, collecting
information from millions of patients over a period of 12 months
from 415 hospitals in the USA [18]. These hospitals are supposedly
representative of the United States hospital experience [18]. Each
hospital in the database provides discharge files that are dated
records of all billable items (including therapeutic and diagnostic
procedures, medication, and laboratory usage) which are all linked
to a given patient’s admission [18, 24].

The initial snapshot of the database used in our work (before
pre-processing step) comprises the EHR data of 1,271,733 hospital
admissions. Electronic Health Record (EHR) is a digital version of a
patient’s paper chart readily available in hospitals. For developing
supervised learning and specifically deep learning models, we focus
on a specific set of features from EHR data. The features of interest
that capture the patients information are summarized in Table 2.
There is a total of 24,428 features per patient, mainly due to the
variety of drugs possibly served.

The Medication regimen complexity index (MRCI) [26] is an ag-
gregate score computed from a total of 65 items, whose purpose is
to indicate the complexity of the patient’s situation. The minimum
MRCI score for a patient is 1.5, which represents a single tablet or
capsule taken once a day as needed (single medication). However
the maximum is not defined since the number of medications in-
creases the score [26]. In our case, after statistical analysis of our
dataset, we consider the MRCI score as ranging from 2 to 60.

Most real datasets like ours are generally imbalanced with a
skewed distribution between the classes. In our case, the positive
cases (patients who die during their hospital stay) represent only
3% of all patients. Table 3 gives more details about this distribution
after the pre-processing step which is discussed in 4.3.1.

4.3 Data pre-processing & experimental setup
This section describes the experimental setting which is used to
evaluate the accuracy and the privacy of our proposals.

4.3.1 Preprocessing.

(1) Features normalization: we extract from the dataset the
values of each feature represented in Table 2. For gender, we

4https://www.premierinc.com/newsroom/education/premier-healthcare-database-
whitepaper

Raouf Kerkouche, Gergely Ács, Claude Castelluccia, and Pierre Genevès

Table 2: Descriptions of features

Features Descriptions

Age Value in the range of 15 and 89

Gender Male, Female or Unknown

Admission type

Emergency, Urgent, Trauma Center: visits to a trauma center/hospital or Unknown

MRCI Medication regimen complexity index score (ranging from 2 to 60)

Drugs and ICD9 codes

Drugs given to the patient on the 1𝑠𝑡 day of hospitalization. The ICD9 codes [16] are composed
of procedures and diagnosis codes, the first gives details about the medical procedures performed
on the patient and the second about the doctor’s diagnosis of the patient. There is a total of 24,419
possible drugs and ICD9 codes.

Table 3: Number of instances for our case study.

Data
Train
Test

Positive cases Negative cases Ratio

30,775
7,891

947,152
236,736

Total
3.15% 977,927
3.23% 244,627

Table 4: Statistics on the size of the training and testing data
over all the clients

Data Min Max
12,447
804
Train
3,112
201
Test

Mean
3,114.42
779.07

Std
1,913.39
478.39

use one-hot encoding: Male, Female and Unknown. Similarly,
for admission type we use 4 features: Emergency, Urgent,
Trauma Center, and Unknown5. For drugs, we extract 24,419
features which correspond to the different drugs (name and
dosage). A given patient receives only a few of the possible
drugs served, resulting in a very sparse patient’s record. We
use a MinMax normalization for age and MRCI in order to
rescale the values of these features between 0 and 1 (using
MinMaxScaler class of scikit-learn6). The labels that we con-
sider are boolean: true means that the patient died during
his hospital stay while false means she survived.

(2) Hospitals filtering: The dataset contains 415 hospitals,
however, we choose to consider only hospitals with at least
1,000 patients, which results at the end in 314 hospitals. The
reason is to have enough data per hospital for both training
and testing. We split randomly the dataset of each hospital
into disjoint training and testing data (80% and 20% respec-
tively). We merge the test data of all hospitals for the evalu-
ation, which we consider fairer than averaging the metrics
over all the clients (hospitals). The final dataset for testing
contains 244,627 patients, with 7,891 deceased patients and
236,736 non-deceased patients (see Table 3). The statistics
on the size of the clients’ dataset are depicted in Table 4.
(3) Patients filtering: We consider patient and drug informa-
tion of the first day at the hospital so that we can make pre-
dictions 24 hours after admission (as commonly found in the
literature [18, 36]). We filter out the pregnant and new-born

5https://www.resdac.org/cms-data/variables/claim-inpatient-admission-type-code-
ffs
6https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html

patients because the medication types and admission ser-
vices are not the same for theses two categories of patients.
Our model prediction is built without patients’ historical
medical data. This has the advantage to require minimum
patient’s information and to work for new patients.

Imbalanced data. The dataset of each hospital is imbalanced
4.3.2
because the proportion of patients that leave the hospital alive is,
fortunately, much larger than in-hospital dead patients. To deal with
this well-known problem, a standard solution is to use the Weighted
loss function technique or different sampling techniques [19, 31].
In [25, 41] the authors compare empirically the performance of the
weighted loss technique and the sampling techniques, however, they
were not able to define a clear winner as the results differ for each
dataset. In our case, weighted loss7 outperforms downsampling8
technique with FL-STANDARD and FL-SIGN.

However, weighting the loss function results in very inaccurate
models with Differential Privacy. Indeed, the gradients of the under-
represented class (dead patients) are boosted and are therefore
larger than the gradients of the other class. The larger the gap
between the gradients of the two classes, the more difficult to choose
a single clipping threshold 𝑆 to guarantee Differential Privacy. In
particular, if 𝑆 is calibrated to large gradients (i.e., that of samples
from the under-represented class), the added Gaussian noise, whose
variance is 𝑆2𝜎2, will also be large yielding poor model accuracy.
On the other hand, if 𝑆 is calibrated to the small gradients (i.e., that
of samples from the over-represented class), then samples from the
under-represented class will have very small impact on the training
which eventually also results in weak model accuracy (the model
will be biased towards the majority class).

Instead, as it is described in Section 3.2, we use downsampling
[19, 31] which does not require re-weighting the loss and hence
overcomes the above artifact caused by clipping (see Alg. 7 for
more details). We used downsampling in our experiments with all
differentially private learning protocols (FL-STANDARD-DP and
FL-SIGN-DP) in Section 4.49.

4.3.3 Model architecture. As in [5], we use a fully connected neural
network model with the following architecture: two hidden layers
of 200 units, which use a ReLU activation function followed by an
output layer of 1 unit with sigmoid activation function and a binary

7https://scikit-learn.org/stable/modules/generated/sklearn.utils
.class_weight.compute_class_weight.html
8oversampling is not considered because of the privacy constraint.
9We report only the results of the best sampling technique for each scheme (see Table 5
for details).

Privacy-Preserving and Bandwidth-Efficient Federated Learning: An Application to In-Hospital Mortality Prediction

Table 5: Parameter settings. 𝑇cl is the number of federated runs, 𝑇gd is the number of gradient descent iterations per federated
run. 𝑇gd = 𝐸𝑝𝑜𝑐ℎ𝑠 · |𝐷𝑘 |

|B| for client 𝑘 in federated learning, where 𝐸𝑝𝑜𝑐ℎ𝑠 is fixed for all clients.

Algorithms

FL-SIGN-DP
FL-STANDARD-DP
FL-STANDARD
FL-SIGN
CENRALIZED

Parameters
𝑁 = 314; 𝐶 = 3/314; |B| = 300; 𝐷𝑃𝑆𝐺𝐷 (𝜂 = 0.05); 𝑆 = 2; 𝑇cl = 300; 𝑇gd = 1; 𝛾 = 0.005; with downsampling
𝑁 = 314; 𝐶 = 3/314; |B| = 300; 𝐷𝑃𝑆𝐺𝐷 (𝜂 = 0.05); 𝑆 = 2; 𝑇cl = 300; 𝑇gd = 1; with downsampling
𝑁 = 314; 𝐶 = 3/314; |B| = 100; 𝑆𝐺𝐷 (𝜂 = 0.01); 𝐸𝑝𝑜𝑐ℎ𝑠 = 5; 𝑇cl = 300; with weighted loss function
𝑁 = 314; 𝐶 = 3/314; |B| = |𝐷𝑘 |; 𝑆𝐺𝐷 (𝜂 = 0.01); 𝐸𝑝𝑜𝑐ℎ𝑠 = 5; 𝑇cl = 300; 𝑇gd = 5; 𝛾 = 0.001; with weighted loss function
|𝐷 | = 977927; |B| = 100; 𝑆𝐺𝐷 (𝜂 = 0.01); 𝐸𝑝𝑜𝑐ℎ𝑠 = 300; with weighted loss function

cross entropy loss function. This results in 4,926,201 parameters in
total. The hyperparameters used by each of the considered schemes
are summarized in Table 5.

4.3.4 Computational environment. Our experiments were per-
formed on a server running Ubuntu 18.04 LTS equipped with a
Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz, 192GB RAM, and two
NVIDIA Quadro P5000 GPU card of 16 Go each. We use Keras 2.2.0
[13] with a TensorFlow backend 1.12.0 [1] and NumPy 1.14.3 [34] to
implement our models and experiments. We use Python 3.6.5 and
our code runs on a Docker container to simplify reproducibility.

4.3.5 Performance Metrics. We use the following metrics:

P + TN

• Balanced accuracy [10] [7] is computed as 1/2 · ( TP

N ) =
TPR +TNR
and is mainly used with imbalanced data. Here,
2
TPR is the True Positive Rate and TNR is the True Negative
Rate; which is calculated as: TPR = TP
and TNR = TN
,
N
P
where P and N are the number of positive and negative
instances, respectively, and TP and TN are the number of
true positive and true negative instances. We note that tradi-
tional (“non-balanced”) accuracy metrics such as TP +TN
can
P +N
be misleading for very imbalanced data [3]: in our dataset,
the minority class has only 3% of all the training samples
(see Table 3), which means that a biased (and totally useless)
model always predicting the majority class would have a
(non-balanced) accuracy of 97%.

• The Area under the receiver operating characteristic curve [32]
(AuROC ) is also a frequently used accuracy metric [6, 18,
35]. The ROC curve is calculated by varying the prediction
threshold from 1 to 0, when TPR and FPR are calculated at
each threshold. The area under this curve is then used to
measure the quality of the predictions. A random guess has
an AuROC value of 0.5, whereas a perfect prediction has the
largest AuROC value of 1.

4.3.6 Hyperparameters selection. For each scheme, 𝜂 was tuned
from 0.01 to 0.09 with an increment value of 0.01.

The batch |B| is selected from [50,100,400,800,|𝐷𝑘 |], where
client 𝑘’s data size |𝐷𝑘 | differs for each client; the number of
epochs 𝐸𝑝𝑜𝑐ℎ𝑠 is selected from [1,5,10,15,20] for each federated
scheme, and from [100,150,200,250,300,350,400] for the centralized
case. For the federated schemes, we have an additional parameter
which is the number of global rounds 𝑇cl, which is selected from
[100,150,200,250,300,350,400]. The sensitivity 𝑆 was selected from
the reasonable set of [0.5,1,1.5,2,2.5,3]. As in [21], we set 𝛾 to 0.001

for the non-private scheme FL-SIGN, and we increase it to 0.005 for
the private extension FL-SIGN-DP.

The number of hospitals used in the federated schemes are se-
lected from [1,2,3,4,5]. We have to choose one which is large enough
to not slow down the convergence and at the same time small
enough in order to not deteriorate privacy by increasing the sam-
pling probability, which is one of the principal parameter used in
the moments accountant method [2, 30] to compute 𝜖.

The values of 𝜎 can be 1.08, 0.81, 0.63, such that we can reach
an 𝜖 budget of 1, 2, 4 respectively, after 𝑇cl = 300 rounds, which is
needed for convergence.

We reported for each scheme in Table 5 the best parameters and
also the best technique used to handle the imbalanced data problem.

4.3.7 Evaluation Method. We perform 𝑘-fold cross validation with
𝑘 = 5; first, we split randomly the dataset of each hospital into
disjoint training and testing data (80% and 20% respectively). An
entire federated run is executed with this split, and all the metrics
are evaluated in every round on the union of all clients’ testing
data. All metric values of the round with the best balanced metric
are recorded. The whole run is repeated 4 times each with a new
random split of training and testing data, and the minimum and
maximum of the recorded performance metrics over all the 5 runs
are reported.

4.4 Results
The results are summarized in Table 6. A single federated run is
composed of 300 rounds, and the best and the worst value of each
performance metric over the 5 federated runs are reported. In each
round, 3 hospitals are selected randomly for aggregation. Three
privacy levels are considered with FL-SIGN-DP and FL-STANDARD-
DP: 𝜀 = 1, 2, 4 each with 𝛿 = 1/max𝑗 |𝐷 𝑗 | ≤ 1.3 · 10−5. These
values of 𝜀 requires to add Gaussian noise to the gradients with
𝜎 = 1.08, 0.81, 0.63, respectively10.
We make several observations:

• As mentioned in [14], the performance of FL-STANDARD
and CENTRALIZED are close: the balanced accuracy is 0.74
and 0.77, respectively, which confirms experimentally that
Federated Learning is a viable approach for our medical
application. These results are consistent with the results re-
ported in [18], which uses the same dataset with the same
features to train a logistic regression model in a centralized
manner (see results of 𝐷1, Table II, in [18]). For example, Au-
ROC is 77.2% − 77.7% in [18], whereas we get an AuROC of

10computed numerically based on [2, 30]

Raouf Kerkouche, Gergely Ács, Claude Castelluccia, and Pierre Genevès

Privacy

Algorithms

𝜀 = 1

𝜀 = 2

𝜀 = 4

N/A

FL-SIGN-DP
FL-STANDARD-DP
FL-SIGN-DP
FL-STANDARD-DP
FL-SIGN-DP
FL-STANDARD-DP
FL-SIGN
FL-STANDARD
CENTRALIZED

Performance

AuROC
(0.67,0.68)
(0.65,0.68)
(0.68,0.71)
(0.68,0.69)
(0.71,0.72)
(0.70,0.72)
(0.76,0.77)
(0.79,0.81)
(0.82,0.84)

Balanced Accuracy
(0.63,0.64)
(0.61,0.63)
(0.64,0.66)
(0.62,0.64)
(0.65,0.66)
(0.64,0.66)
(0.68,0.70)
(0.73,0.74)
(0.76,0.77)

Table 6: Summary of results. The worst and best value of each metric over 5 federated runs are reported.

82% − 84%, 79% − 81% and 76% − 77% with CENTRALIZED,
FL-STANDARD and FL-SIGN, respectively.

Table 7: Average bandwidth consumption from a client to
the server.

• The performance of FL-SIGN is slightly worse than the
performance of FL-STANDARD; the balanced accuracy is
0.70 for FL-SIGN and 0.74 for FL-STANDARD. However,
FL-SIGN and FL-SIGN-DP reduce the bandwidth consump-
tion by a factor of 32. Table 7 shows that each client sends
only 1.76 Megabytes with FL-SIGN and FL-SIGN-DP, while
56.48 Megabytes are sent with FL-STANDARD and FL-
STANDARD-DP. The bandwidth consumption is calculated
by measuring the average number of bits sent by a client to
the server over the rounds when the client is selected for
aggregation. This is computed as (𝐶 · 𝑇cl · model_size) for
FL-SIGN and FL-SIGN-DP, and (32 · 𝐶 · 𝑇cl · model_size) for
FL-STANDARD and FL-STANDARD-DP, where model_size
is the number of model parameters (i.e., 4,926,201).

• FL-SIGN-DP performs very similarly to FL-STANDARD-DP,
which means that bandwidth efficiency has no real cost
when Differential Privacy is also applied. In fact, the perfor-
mance gap between FL-STANDARD and FL-STANDARD-DP
is larger than between FL-SIGN and FL-SIGN-DP especially
with stronger privacy guarantee (i.e., smaller 𝜀). This shows
that taking the sign of the noisy update and then the median
of the noisy signs over all clients on the server (in Line 8
of Alg.4) is more robust against perturbation than taking
the simple average of the noisy update vectors (in Line 8 of
Alg. 5).

• The results show in general that strong privacy protection
can be provided at the cost of a relatively small performance
degradation. In fact, the balanced accuracy drops by only 10%
when 𝜀 = 1 with FL-SIGN-DP. Furthermore, the performance
degrades very smoothly as the value of 𝜀 decreases (i.e. as
the privacy guarantee gets stronger): the balanced accuracy
of FL-SIGN-DP is 0.66 for 𝜀 = 4, and only drops to 0.64 when
𝜀 = 1.

5 RELATED WORK
This section describes the related work to our proposal. We start
by presenting the work related to the use of machine learning
in medical applications. We then summarize the work related to
differentially private federated learning. Finally, we consider the

Scheme
FL-SIGN and
FL-SIGN-DP
FL-STANDARD and
FL-STANDARD-DP

Bandwidth consumption (Megabytes)

1.76

56.48

work related to the problem of bandwidth reduction in Federated
Learning.

5.1 Medical prediction
The paper [5] investigates possibilities offered by the use of Deep
Learning and Electronic Health Record (EHR) in order to provide
and improve the quality of end-of-life care for hospitalized patients.
Having the information about the patients one year before the date
of the prediction, the authors define four uneven slices windows.
The information collected during the slices windows are used as
features to train a model. The model is then used to predict all
causes of mortality within a period 3–12 month after the date of the
prediction. The authors of [36] also use predictive deep learning
models with EHR data (provided by two hospitals) for tasks such
as predicting inhospital mortality, 30-day unplanned readmission,
prolonged length of stay and all of a patient’s final discharge diag-
noses. The EHR data of each hospital are used separately, and two
personalized models are trained. The EHR data include the data of
adult patients who are hospitalized for at least 24 hours.

In [28], a binary logistic regression analysis is performed in order
to predict which patients will need Palliative Care Needs (PCNs)
based on six risk factors which are: cancer, metastases, age, absence
of relatives, liver cirrhosis, and high level of care at admission.
During the discharge, the treating physician had to report if the
patient had PCNs or not.

The paper [18] develops interpretable models for predicting the
risk of complications during hospital stays. The predictive models
are based on stacked logistic regressions specifically designed to
leverage the evolution of the drugs served during hospital stays.
The models can scale with very large volumes of EHR data but they
do not consider privacy-related issues.

Privacy-Preserving and Bandwidth-Efficient Federated Learning: An Application to In-Hospital Mortality Prediction

The paper [14] proposes to use Federated learning with DP,
more precisely, it uses objective perturbation [11][12]. An empiri-
cal evaluation using two real-world health datasets is performed.
However, using objective perturbation implies to deal with convex
optimization problems. Hence, logistic regression, perceptron and
SVM models are used for the learning tasks. The paper highlights
also that the performance of FL without DP are close to the perfor-
mance of the traditional learning protocol, where the data is shared
and centralized in the same place for the training.

5.2 Bandwidth Optimization in Federated

Learning

Different quantization methods have been proposed to save the
bandwidth and reduce the communication costs in federated learn-
ing. They can be divided into two main groups: unbiased and biased
methods. The unbiased approximation techniques use probabilistic
quantization schemes to compress the stochastic gradient and at-
tempt to approximate the true gradient value as much as possible
[4][42][40][22]. However, biased approximations of the stochastic
gradient can still guarantee convergence both in theory and prac-
tice [8, 23, 37]. In signSGD [8], all the clients calculate the stochastic
gradient based on a single mini-batch and then send the sign vector
of this gradient to the server. The server calculates the aggregated
sign vector by taking the median (majority vote) and sends the
signs of the aggregated signs back to each client.

The main differences between our scheme (FL-SIGN) and

signSGD are as follows:

• FL-SIGN aims to train a common model that is distributed
to a random subset of all clients in every round. However, in
signSGD, all clients start with the same initialized common
model and the server sends the same aggregated model up-
date to every client at each round. Selecting only a random
subset of clients in each round has at least three benefits.
First, FL-SIGN becomes more robust against temporary node
failures. Second, FL-SIGN reduces the communication costs
upstream to the server. Finally, sampling boosts privacy due
to the uncertainty that a specific user’s or client’s data is
used for training or not.

• In FL-SIGN, each client can perform multiple SGD iterations
locally using multiple mini-batches before computing the
model update. In contrast, signSGD always performs one
local SGD iteration with a single mini-batch at every client.
• As all the clients participate at each round in signSGD, the
server only transfers the sign of the aggregated signs to
the clients in every round. Therefore, only a single bit is
transferred per parameter downstream to the clients. In FL-
SIGN, the whole model is transferred but only to a random
subset of clients.

5.3 Differentially Private Federated Learning
Similarly to our FL-SIGN-DP algorithm, another approach [39]
also uses DPSGD [2] in order to hide the record of each client’s
dataset, but the noise is generated in a distributed manner, that is,
untrusted server is assumed. Indeed, the noise is added by each
client during the training, and then the noisy update is sent to the
server. The sum of these noisy updates is sufficiently noised to

provide differential privacy. To protect individual updates which
are not differentially private, homomorphic encryption is used to
guarantee that the adversary can only access the aggregated update
which is sufficiently noised. Notice that the noise can be generated
distributively, because each client performs only a single mini-
batch to compute their model update (i.e., 𝑇gd = 1). By contrast, our
approach (FL-SIGN-DP) works even if 𝑇gd > 1 at the cost of adding
larger magnitude of noise and sends only signs for aggregation.

The paper [20] proposed a solution which faithfully follows the
SignSGD protocol but is not based on federated learning protocol.
The authors use local DP to guarantee client-level-DP. However,
it is widely accepted that the large noise needed for local DP de-
creases accuracy significantly, as the aggregation of the DP updates
increases the noise variance. The paper [21] adapts the SignSGD
protocol to federated learning for a client-level-DP guarantee. The
proposed scheme adds noise in a distributed manner such that the
final noise after the aggregation corresponds to the minimum noise
needed to ensure DP. However, their proposal, that uses a discrete
Gaussian mechanism and needs several bits per parameter, is less
bandwidth efficient than [20] that only sends one bit per parameter.
Our solution is based on [21] but considers record-level guar-
antee instead of client-level guarantee. It therefore requires less
perturbations and reduces bandwidth by sending only one bit per
parameter.

Differential Private Federated Machine Learning has been stud-
ied in the context of medical applications to provide client-level
privacy guarantee [35] or record-level privacy guarantee [6, 15].
Our solution improves the state of the art as it provides record-level
privacy guarantee and optimizes bandwidth efficiency by sending
only one bit per parameter. Furthermore, as opposed to most pub-
lished papers that use public, often synthetic, datasets with limited
size, we evaluated our scheme using a large cohort of real-world
data.

6 ETHICAL CONSIDERATIONS
Our study was approved by our Institutional Review Board (IRB)
process before any research activity began. The EHR dataset is
stored on a server whose security was audited by Inria security
teams.

7 CONCLUSIONS
Real-world data are generally highly imbalanced, our solution aims
to handle this well-known problem while it provides both band-
width efficiency and differentially private guarantee. We experi-
mentally evaluate the performance of our solution for in-hospital
mortality prediction using the Premier Healthcare database, con-
taining about one million records of patients. We consider a scenario
where 314 hospitals are collaborating to train, using our Federated
Learning scheme, a prediction model without exchanging any of
their patients’ data. Our scheme guarantees that no internal or
external adversary that has access to the final model, intermediate
updates or even all the messages that are exchanged during the
training phase can infer any information about any of the patient
data that were used by each hospital.

The accuracy performance results are very encouraging. They
show in general that strong privacy protection can be provided at

the cost of a relatively small performance degradation. Furthermore, 
our scheme reduces the bandwidth consumption by a factor of 32 
compared to standard federated learning schemes, reducing it from 
56.48 to 1.76 Megabytes.

We believe that this paper reports the first large-scale experi-
mental assessment in favor of using privacy-preserving federated 
learning for the purpose of in-hospital mortality prediction. We 
demonstrate that it is possible to benefit from the power of machine 
learning without sacrificing the privacy of patients. Hospitals, and 
other medical institutions, are reluctant to collaborate because they 
often consider their patient medical records as their own intellec-
tual properties. Our scheme protects these intellectual properties 
of participating entities on record-level.

8  ACKNOWLEDGMENTS
This article was developed in the framework of the Grenoble Alpes 
Data Institute, supported by the French National Research Agency 
under the "Investissements d’avenir” program (ANR-15-IDEX-02). 
The research was supported by the NRDI fund of the Ministry of 
Innovation and Technology NRDI Office, and also within the frame-
work of the Artificial Intelligence National Laboratory Program. 
This project has received support from the EU/EFPIA Innovative 
Medicines  Initiative  2  Joint  Undertaking  (MELLODDY  grant 𝑛◦ 
831472). This project has received support from the ANR project 
ANR-16-CE25-0010.

REFERENCES
[1] Martín Abadi, , et al. 2015. TensorFlow: Large-Scale Machine Learning on Hetero-
geneous Systems. http://tensorflow.org/ Software available from tensorflow.org.
[2] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In
ACM CCS.

[3] Josephine Akosa. 2017. Predictive accuracy: a misleading performance measure
for highly imbalanced data. In Proceedings of the SAS Global Forum. 2–5.

[4] Dan Alistarh, Jerry Li, Ryota Tomioka, and Milan Vojnovic. 2016. QSGD: Ran-
domized Quantization for Communication-Optimal Stochastic Gradient Descent.
CoRR abs/1610.02132 (2016). arXiv:1610.02132 http://arxiv.org/abs/1610.02132
[5] Anand Avati, Kenneth Jung, Stephanie Harman, Lance Downing, Andrew Ng,
and Nigam H. Shah. 2018. Improving palliative care with deep learning. BMC
Medical Informatics and Decision Making 18, 4 (12 Dec 2018), 122. https://doi.org/
10.1186/s12911-018-0677-8

[6] Brett K. Beaulieu-Jones, William Yuan, Samuel G. Finlayson, and Zhiwei Steven
Wu. 2018. Privacy-Preserving Distributed Deep Learning for Clinical Data.
arXiv:cs.LG/1812.01484

[7] Mohamed Bekkar, Hassiba Djema, and T.A. Alitouche. 2013. Evaluation mea-
sures for models assessment over imbalanced data sets. Journal of Information
Engineering and Applications 3 (01 2013), 27–38.

[8] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anand-
kumar. 2018. signSGD: compressed optimisation for non-convex problems. CoRR
abs/1802.04434 (2018). arXiv:1802.04434 http://arxiv.org/abs/1802.04434

[9] Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar.
2018. signSGD with Majority Vote is Communication Efficient And Byzantine
Fault Tolerant. CoRR abs/1810.05291 (2018). arXiv:1810.05291 http://arxiv.org/
abs/1810.05291

[10] Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M
Buhmann. 2010. The balanced accuracy and its posterior distribution. In 2010
20th International Conference on Pattern Recognition. IEEE, 3121–3124.

[11] Kamalika Chaudhuri and Claire Monteleoni. 2009. Privacy-preserving logistic
regression. In Advances in neural information processing systems. 289–296.
[12] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. 2011. Differen-
tially private empirical risk minimization. Journal of Machine Learning Research
12, Mar (2011), 1069–1109.

[13] François Chollet et al. 2015. Keras. https://keras.io.
[14] Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla,
Yoonyoung Park, Grace Hsu, and Amar Das. 2019. Differential Privacy-enabled
Federated Learning for Sensitive Health Data. arXiv:cs.LG/1910.02578

Raouf Kerkouche, Gergely Ács, Claude Castelluccia, and Pierre Genevès

[15] Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla,
Yoonyoung Park, Grace Hsu, and Amar Das. 2020. Differential Privacy-enabled
Federated Learning for Sensitive Health Data. arXiv:cs.LG/1910.02578

[16] Marta TERRON CUADRADO. 2019. ICD-9-CM: International Classification of
Diseases, Ninth Revision, Clinical Modification. https://ec.europa.eu/cefdigital/
wiki/display/EHSEMANTIC/ICD-9-CM%3A+International+Classification+of+
Diseases%2C+Ninth+Revision%2C+Clinical+Modification.

[17] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differ-
ential Privacy. Foundations and Trends in Theoretical Computer Science 9, 3–4
(2014).

[18] A. Fejza, P. Genevès, N. Layaïda, and J. Bosson. 2018. Scalable and Interpretable
Predictive Models for Electronic Health Records. In 2018 IEEE 5th International
Conference on Data Science and Advanced Analytics (DSAA). 341–350. https:
//doi.org/10.1109/DSAA.2018.00045

[19] Haibo He and Edwardo A Garcia. 2009. Learning from imbalanced data. IEEE
Transactions on knowledge and data engineering 21, 9 (2009), 1263–1284.
[20] Richeng Jin, Yufan Huang, Xiaofan He, Tianfu Wu, and Huaiyu Dai. 2020.
Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees.
arXiv:cs.LG/2002.10940

[21] Raouf Kerkouche, Gergely Ács, and Claude Castelluccia. 2020. Federated Learning

in Adversarial Settings. arXiv:cs.CR/2010.07808

[22] Jakub Konecný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon. 2016. Federated Learning: Strate-
gies for Improving Communication Efficiency. CoRR abs/1610.05492 (2016).
arXiv:1610.05492 http://arxiv.org/abs/1610.05492

[23] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. 2018. Deep Gradient
Compression: Reducing the Communication Bandwidth for Distributed Training.
In International Conference on Learning Representations, ICLR 2018. https://
openreview.net/forum?id=SkhQHMW0W

[24] Rupa Makadia and Patrick B. Ryan. 2014. Transforming the Premier Perspective®
Hospital Database into the Observational Medical Outcomes Partnership (OMOP)
Common Data Model. In EGEMS.

[25] Kate McCarthy, Bibi Zabar, and Gary Weiss. 2005. Does cost-sensitive learning
beat sampling for classifying rare classes?. In Proceedings of the 1st international
workshop on Utility-based data mining. 69–77.

[26] Margaret Mcdonald, Timothy Peng, Sridevi Sridharan, Janice Foust, Polina Kogan,
Liliana Pezzin, and Penny Feldman. 2012. Automating the medication regimen
complexity index. Journal of the American Medical Informatics Association :
JAMIA 20 (12 2012). https://doi.org/10.1136/amiajnl-2012-001272

[27] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas. 2016. Communication-Efficient Learning of Deep Net-
works from Decentralized Data. In AISTATS.

[28] Cornelia Meffert, Gerta Rücker, Isaak Hatami, and Gerhild Becker. 2016. Identifi-
cation of hospital patients in need of palliative care – a predictive score. BMC
Palliative Care 15 (12 2016). https://doi.org/10.1186/s12904-016-0094-7
[29] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.
2018. Inference Attacks Against Collaborative Learning. CoRR abs/1805.04049
(2018). arXiv:1805.04049 http://arxiv.org/abs/1805.04049

[30] Ilya Mironov, Kunal Talwar, and Li Zhang. 2019. Rényi Differential Privacy of the
Sampled Gaussian Mechanism. CoRR abs/1908.10530 (2019). arXiv:1908.10530
http://arxiv.org/abs/1908.10530

[31] Ajinkya More. 2016. Survey of resampling techniques for improving classification
performance in unbalanced datasets. arXiv preprint arXiv:1608.06048 (2016).

[32] Sarang Narkhede. 2018.

Understanding AUC - ROC Curve.

https://

towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5.

[33] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy
Analysis of Deep Learning: Passive and Active White-box Inference Attacks
against Centralized and Federated Learning. In IEEE Symposium on Security and
Privacy, 2019. 739–753. https://doi.org/10.1109/SP.2019.00065

[34] Travis E Oliphant. 2006. A guide to NumPy. Vol. 1. Trelgol Publishing USA.
[35] Stephen R. Pfohl, Andrew M. Dai, and Katherine Heller. 2019. Federated and Differ-
entially Private Learning for Electronic Health Records. arXiv:cs.LG/1911.05861
[36] Alvin Rajkomar and al. 2018. Scalable and accurate deep learning with electronic
health records. npj Digital Medicine 1, 1 (2018), 18. https://doi.org/10.1038/s41746-
018-0029-1 url, An earlier version appeared in eprint arXiv:1801.07860.
[37] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-bit stochastic
gradient descent and its application to data-parallel distributed training of speech
DNNs. In INTERSPEECH 2014. 1058–1062. http://www.isca-speech.org/archive/
interspeech_2014/i14_1058.html

[38] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-Preserving Deep Learning.
In ACM SIGSAC Conference on Computer and Communications Security, 2015.
1310–1321. https://doi.org/10.1145/2810103.2813687

[39] Stacey Truex and al. 2018. A Hybrid Approach to Privacy-Preserving Federated
Learning. CoRR abs/1812.03224 (2018). arXiv:1812.03224 http://arxiv.org/abs/
1812.03224

[40] Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary B. Charles, Dimitris S.
Papailiopoulos, and Stephen Wright. 2018. ATOMO: Communication-efficient
Learning via Atomic Sparsification. In NeurIPS.

Privacy-Preserving and Bandwidth-Efficient Federated Learning: An Application to In-Hospital Mortality Prediction

of randomness. Therefore, the probability that a vote fails for the
𝑖𝑡ℎ parameter is bounded as

P [𝑍𝑖 ≤ 𝑀/2] ≤

≤

≤

√

(cid:112)E[( ˜𝑔𝑖 − 𝑔𝑖 )2]
𝑀 |𝑔𝑖 |
𝜏 2
𝑖 + 𝑆2𝜎2
√
𝑀 |𝑔𝑖 |

(cid:113)

𝜏𝑖 + 𝑆𝜎
√
𝑀 |𝑔𝑖 |

(by independence)

where the second inequality follows from Assumption 3 and the
fact that the variance of the Gaussian noise is 𝑆2𝜎. The rest of the
derivation is identical to the proof of Theorem 2 in [9].

□

[41] Gary M Weiss, Kate McCarthy, and Bibi Zabar. 2007. Cost-sensitive learning
vs. sampling: Which is best for handling unbalanced classes with unequal error
costs? Dmin 7, 35-41 (2007), 24.

[42] Wei Wen and al. 2017. TernGrad: Ternary Gradients to Reduce Communication
in Distributed Deep Learning. CoRR abs/1705.07878 (2017). arXiv:1705.07878
http://arxiv.org/abs/1705.07878

[43] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. iDLG: Improved Deep

Leakage from Gradients. arXiv preprint arXiv:2001.02610 (2020).

[44] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep Leakage from Gradients.
In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 14747–14756.
http://papers.nips.cc/paper/9617-deep-leakage-from-gradients

A APPENDIX
A.1 Convergence Proofs
The convergence proof of FL-SIGN can be found in [9], whereas
the proof of FL-SIGN-DP is a simple adaptation of Theorem 2 from
[9]. Here we outline only the main deviations from the proof of
that theorem.

Assumptions:
(1) Lower bound: For all 𝑥 and some constant 𝑓 ∗, 𝑓 (𝑥) ≥ 𝑓 ∗,

where 𝑓 denotes the loss/objective function.

(2) Smoothness: Let 𝑔(𝑥) denote the gradient of the objective
function 𝑓 evaluated at 𝑥. Then, for all 𝑥, 𝑦 and some non-
negative constant L = (𝐿1, 𝐿2, . . . , 𝐿𝑛),
|𝑓 (𝑦) − [𝑓 (𝑥) + 𝑔(𝑥)T (𝑦 − 𝑥)]| ≤ 1/2 (cid:213)
𝑖

𝐿𝑖 (𝑦𝑖 − 𝑥𝑖 )2

(3) Variance bound: Upon receiving query 𝑥 ∈ R𝑛, the stochastic
gradient oracle gives us an independent, unbiased estimate
ˆ𝑔 that has bounded variance per coordinate: E[ ˆ𝑔(𝑥)] = 𝑔(𝑥),
E[( ˆ𝑔(𝑥)𝑖 − 𝑔(𝑥)𝑖 )2] ≤ 𝜏 2
𝑖 for a vector of non-negative con-
stants 𝝉 = (𝜏1, 𝜏2, . . . , 𝜏𝑛).

(4) Unimodal, symmetric gradient noise: At any given point 𝑥,
each component of the stochastic gradient vector ˆ𝑔(𝑥) has
unimodal distribution that is also symmetric about the mean.
Note that adding extra Gaussian noise to each gradient compo-
nent for the purpose of differential privacy will not violate Assump-
tion 4.

Theorem A.1. If |B| = 𝑇cl, 𝑇gd = 1, and 𝛾 =

(cid:113) 𝑓0−𝑓∗
| |L| |1𝑇cl

, then

1
𝑇cl

cl−1
𝑇
(cid:213)

𝑡 =0

E| |𝑔𝑡 | |1 ≤

2
√
𝑇cl

(cid:18) | |𝝉 | |1 + 𝑛𝑆𝜎
√
𝐶𝑁

+ (cid:112)| |L| |1 (𝑓0 − 𝑓 ∗)

(cid:19)

Proof. The primary focus of the proof is to bound the probabil-
ity that a client computes the sign of a parameter update correctly.
Let 𝑀 = 𝐶𝑁 . As in [9], let 𝑍𝑖 ∈ [0, 𝑀] denote the number of correct
sign bits received by the aggregator for parameter 𝑖, and 𝑝 denotes
the probability that a honest client computes the correct bit. Let
𝜔 = 𝑝 − 1

2 . According to Theorem 2 in [9],

P [𝑍𝑖 ≤ 𝑀/2] ≤

(cid:112)E[( ˜𝑔𝑖 − 𝑔𝑖 )2]
𝑀 |𝑔𝑖 |

√

where ˜𝑔 is the noisy stochastic gradient. Observe that ˜𝑔 has two
sources of randomness; (1) the stochasticity of the sampling mech-
anism which is modelled by the stochastic gradient oracle (see
Assumption 3), and (2) the Gaussian noise that is introduced in
order to guarantee DP. Importantly, these are independent sources

