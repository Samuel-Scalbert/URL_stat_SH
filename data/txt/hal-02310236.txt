Private Protocols for U-Statistics in the Local Model
and Beyond
James Bell, Aurélien Bellet, Adrià Gascón, Tejas Kulkarni

To cite this version:

James Bell, Aurélien Bellet, Adrià Gascón, Tejas Kulkarni. Private Protocols for U-Statistics in the
Local Model and Beyond. AISTATS 2020 - 23rd International Conference on Artificial Intelligence
and Statistics, Aug 2020, Palermo, Italy. ￿hal-02310236v2￿

HAL Id: hal-02310236

https://inria.hal.science/hal-02310236v2

Submitted on 4 May 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Private Protocols for U -Statistics in the Local Model and Beyond

James Bell
The Alan Turing Institute

Aurélien Bellet
Inria

Adrià Gascón
Google

Tejas Kulkarni
Aalto University

Abstract

In this paper, we study the problem of com-
puting U -statistics of degree 2, i.e., quantities
that come in the form of averages over pairs
of data points, in the local model of diﬀeren-
tial privacy (LDP). The class of U -statistics
covers many statistical estimates of interest,
including Gini mean diﬀerence, Kendall’s tau
coeﬃcient and Area under the ROC Curve
(AUC), as well as empirical risk measures for
machine learning problems such as ranking,
clustering and metric learning. We ﬁrst intro-
duce an LDP protocol based on quantizing
the data into bins and applying randomized
response, which guarantees an (cid:15)-LDP esti-
mate with a Mean Squared Error (MSE) of
O(1/
n(cid:15)) under regularity assumptions on
the U -statistic or the data distribution. We
then propose a specialized protocol for AUC
based on a novel use of hierarchical histograms
that achieves MSE of O(α3/n(cid:15)2) for arbitrary
data distribution. We also show that 2-party
secure computation allows to design a pro-
tocol with MSE of O(1/n(cid:15)2), without any
assumption on the kernel function or data dis-
tribution and with total communication linear
in the number of users n. Finally, we evalu-
ate the performance of our protocols through
experiments on synthetic and real datasets.

√

1

INTRODUCTION

The problem of collecting aggregate statistics from a
set of n users in a way that individual contributions
remain private even from the data analysts has recently
attracted a lot of interest. In the popular local model of
diﬀerential privacy (LDP) (Duchi et al., 2013; Kairouz

Proceedings of the 23rdInternational Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2020, Palermo, Italy.
PMLR: Volume 108. Copyright 2020 by the author(s).

et al., 2014), users apply a local randomizer to their pri-
vate input before sending it to an untrusted aggregator.
In this context, most work has focused on computing
quantities that are separable across individual users,
such as sums and histograms (see Bassily and Smith,
2015; Wang et al., 2017; Kulkarni et al., 2019; Cormode
et al., 2018; Bassily et al., 2017, and references therein).

(cid:80)

n(n−1)

In this paper, we study the problem of privately com-
puting U -statistics of degree 2, which generalize sample
mean statistics to averages over pairs of data points.
Let x1, . . . , xn be a set of n data points drawn i.i.d.
from an unknown ditribution µ over a (discrete or
continuous) domain X . The U -statistic of degree 2
with kernel f , given by Uf,n = 2
i<j f (xi, xj),
is an unbiased estimate of Uf = Ex,x(cid:48)∼µ[f (x, x(cid:48))] with
minimum variance (Hoeﬀding, 1948). The class of
U -statistics covers many statistical estimates of inter-
est, including sample variance, Gini mean diﬀerence,
Kendall’s tau coeﬃcient, Wilcoxon Mann-Whitney hy-
pothesis test and Area under the ROC Curve (AUC)
(Lee, 1990; Mann and Whitney, 1947; Faivishevsky and
Goldberger, 2008). They are also commonly used as
empirical risk measures for machine learning problems
such as ranking, clustering and metric learning (Kar
et al., 2013; Clémençon et al., 2016).

Interestingly, private estimation of U -statistics in the
LDP model for arbitrary kernel functions f and data
distributions µ cannot be straightforwardly addressed
by resorting to standard local randomizers such as the
Laplace mechanism or randomized response. Indeed,
one cannot apply the local randomizer to the terms of
the sum based on the sensitivity of f (as each term
is shared across two users), and perturbing the inputs
themselves can lead to large errors when passed through
the (potentially discontinuous) function f .

In this work, we design and analyze several protocols
for computing U -statistics with privacy and utility
guarantees. More precisely:

1. We introduce a generic LDP protocol based on
quantizing the data into k bins and applying k-
ary randomized response. We show that under an
assumption on either the kernel function f or the

Private Protocols for U -Statistics in the Local Model and Beyond

data distribution µ, the aggregator can construct
an (cid:15)-LDP estimate of Uf,n with a Mean Squared
n(cid:15)).
Error (MSE) of O(1/

√

Uf,n is an unbiased estimate of Uf . Denoting by
ζ1 = Var(f (x1, X2) | x1)) and ζ2 = Var(f (X1, X2),
its variance is given by (Hoeﬀding, 1948; Lee, 1990):

2. For the case of the AUC on a domain of size
2α, whose kernel does not satisfy the regularity
assumption required by our previous protocol, we
design a specialized protocol based on hierarchical
histograms that achieves MSE O(α2 log(1/δ)/n(cid:15)2)
under ((cid:15), δ)-LDP and O(α3/n(cid:15)2) under (cid:15)-LDP, for
arbitrary data distribution.

3. Under a slight relaxation of the local model in
which we allow pairs of users i and j to compute
a randomized version of f (xi, xj) with 2-party se-
cure computation, we show that we can design
a protocol with MSE of O(1/n(cid:15)2), without any
assumption on the kernel function or data distri-
bution and with constant communication for each
of the n users.

4. To evaluate the practical performance of the pro-
posed protocols, we present some experiments on
synthetic and real datasets for the task of comput-
ing AUC and Kendall’s tau coeﬃcient.

The paper is organized as follows. Section 2 gives
some background on U -statistics and local diﬀerential
privacy. In Section 3 we present a generic LDP protocol
based on randomizing quantized inputs. Section 4
introduces a specialized LDP protocol for computing
the Area under the ROC Curve (AUC). In Section 5,
we introduce a generic protocol which operates in a
slightly relaxed version of the LDP model where users
can run secure 2-party computation. We present some
numerical experiments in Section 6, and conclude with
a discussion of our results and future work in Section 7.

2 BACKGROUND

Var(Uf,n) = 2

n(n−1) (2(n − 2)ζ1 + ζ2).

(3)

The above variance is of O(1/n) and is optimal among
all unbiased estimators of Uf that can be computed
from S. This incurs a complex dependence structure,
as each data point appears in n−1 pairs. The statistical
behavior of U -statistics can be investigated using lin-
earization techniques (Hoeﬀding, 1948) and decoupling
methods (de la Pena and Giné, 1999), which provide
tools to reduce their analysis to that of standard i.i.d.
averages. One may refer to (Lee, 1990) for asymptotic
theory of U -statistics, to (Van Der Vaart, 2000) (Chap-
ter 12 therein) and (de la Pena and Giné, 1999) for
nonasymptotic results, and to (Clémençon et al., 2008,
2016) for an account of U -statistics in the context of
machine learning and empirical risk minimization.

2.1.2 Motivating Examples

U -statistics are commonly used as point estimators of
various global properties of distributions, as well as
in statistical hypothesis testing (Lee, 1990; Mann and
Whitney, 1947; Faivishevsky and Goldberger, 2008).
They also come up as empirical risk measures in ma-
chine learning problems with pairwise loss functions
such as bipartite ranking, metric learning and clus-
tering. Below, we give some concrete examples of
U -statistics of broad interest to motivate our private
protocols.

Gini mean diﬀerence. This is a classic measure of
dispersion which is often seen as more informative than
the variance for some distributions (Yitzhaki, 2003).
Letting X ⊂ R, it is deﬁned as

In this section, we introduce some background on U -
statistics and local diﬀerential privacy.

G = 2

n(n−1)

(cid:80)

i<j |xi − xj|,

(4)

2.1 U -Statistics

2.1.1 Deﬁnition and Properties

Let µ be an (unknown) distribution over an input space
X and f : X 2 → R be a pairwise function (assumed to
be symmetric for simplicity) referred to as the kernel.
Given a sample S = {xi}n
i=1 of n observations drawn
from µ, we are interested in estimating the following
population quantity:

Uf = EX1,X2∼µ[f (X1, X2)].
Deﬁnition 1 (Hoeﬀding, 1948). The U -statistic of
degree 2 with kernel f is given by
Uf,n = 2

i<j f (xi, xj).

(cid:80)

(1)

(2)

n(n−1)

i=1 xi.

which is a U -statistic of degree 2 with kernel f (xi, xj) =
|xi − xj|. Gini coeﬃcient, the most commonly used
measure of inequality, is obtained by multiplying G by
(n − 1)/2 (cid:80)n
Remark 1. The variance of a sample, obtained by
replacing the absolute diﬀerence by the squared diﬀer-
ence in (4), is also a U -statistic. However we note that
computing the variance can be achieved by computing
two sums of locally computable variables (xi and x2
i ),
which can be done with existing LDP protocols.

Rényi-2 entropy. Also known as collision entropy,
this provides a measure of entropy between Shannon’s
entropy and min entropy which is used in many appli-
cations involving discrete distributions (see Acharya

James Bell, Aurélien Bellet, Adrià Gascón, Tejas Kulkarni

et al., 2015, and references therein). It is given by

H2 = − ln

(cid:16)

2
n(n−1)

(cid:80)

i<j

I[xi = xj]

(cid:17)

.

(5)

The expression inside the log is a U -statistic of degree
2 with kernel f (xi, xj) = I[xi = xj].

Kendall’s tau coeﬃcient. This statistic measures
the ordinal association between two variables and is
often used as a test statistic to answer questions such as
“does a higher salary make one happier?”. In learning
to rank applications, it is used to evaluate the extent to
which a predicted ranking correlates with the (human-
generated) gold standard (see e.g., Joachims, 2002;
Lapata, 2006). Formally, assuming continuous variables
for simplicity, let X ⊂ R2 and S = {xi = (yi, zi)}n
i=1.
For any i < j, the pairs xi = (yi, zi) and xj = (yj, zj)
are said be concordant if (yi > yj) ∧ (zi > zj) or (yi <
yj) ∧ (zi < zj), and discordant otherwise. Let C and
D be the number of concordant and discordant pairs
in S. Kendall rank correlation coeﬃcient is deﬁned as:

τ =

C − D
C + D

=

1
(cid:1)
(cid:0)n
2

(cid:88)

i<j

sign(yi − yj) sign(zi − zj), (6)

which is a U -statistic of degree 2 with kernel f (xi, xj) =
sign(yi − yj) sign(zi − zj).1

Area under the ROC curve (AUC). In binary
classiﬁcation with class imbalance, the Receiver Oper-
ating Characteristic (ROC) gives the true positive rate
with respect to the false positive rate of a predictor at
each possible decision threshold. The AUC is a pop-
ular summary of the ROC curve which gives a single,
threshold-independent measure of the classiﬁer good-
ness: it corresponds to the probability that the predic-
tor assigns a higher score to a randomly chosen positive
point than to a randomly chosen negative one. AUCs
are widely used as performance metrics in machine
learning (Bradley, 1997; Herschtal and Raskutti, 2004),
and have also been recently studied as fairness measures
(Kallus and Zhou, 2019; Vogel et al., 2020). Formally,
let X ⊂ R × {−1, 1} and S = {xi = (si, yi)}n
i=1 where
for each data point i, si ∈ R is the score assigned to
point i and yi ∈ {−1, 1} is its label. For convenience,
let S + = {si : yi = 1} and S − = {si : yi = −1} and let
n+ = |S +| and n− = |S −|. The AUC is given by

AU C = 1

n+n−

(cid:80)

si∈S+

(cid:80)

sj ∈S− I[si > sj],

(7)

where I[σ] is an indicator variable outputting 1 if the
(cid:1)/n+n−
predicate σ is true and 0 otherwise. Up to a (cid:0)n
factor, it is easy to see that AU C is a U -statistic of
degree 2 with kernel f (xi, xj) = I[si > sj ∧ yi > yj] +
I[si < sj ∧ yi < yj].

2

1One can easily modify the kernel to account for ties.

Machine learning with pairwise losses. Many
machine learning problems involve loss functions that
operate on pairs of points (Kar et al., 2013; Clémençon
et al., 2016). This is the case for instance in metric
learning (Bellet et al., 2015), bipartite ranking (Clé-
mençon et al., 2008) and clustering (Clémençon, 2014).
Empirical risk minimization problems have therefore
the following generic form:

min
θ∈Θ

2
n(n−1)

(cid:80)

i<j (cid:96)θ(xi, xj),

(8)

where θ ∈ Θ are model parameters. The objective
function in (8), as well as its gradient, are U -statistics
of degree 2 with kernels (cid:96)θ and ∇θ(cid:96)θ respectively.

2.2 Local Diﬀerential Privacy

The classic centralized model of diﬀerential privacy
assumed the presence of a trusted aggregator which
processes the private information of individuals and
releases a perturbed version of the result. The local
model instead captures the setting where individuals
do not trust the aggregator and randomize their input
locally before sharing it. This model has received wide
industrial adoption (Erlingsson et al., 2014; Fanti et al.,
2016; Diﬀerential Privacy Team, Apple, 2017; Ding
et al., 2017).

Deﬁnition 2 (Duchi et al., 2013). A local randomizer
R is ((cid:15), δ)-locally diﬀerentially private (LDP) if for all
x, x(cid:48) ∈ X and all possible output O in the range of R:

P r[R(x) = O] ≤ e(cid:15)P r[R(x(cid:48)) = O] + δ.

The special case δ = 0 is called pure (cid:15)-LDP.

Most work in LDP aims to compute quantities that
are separable across individual inputs, such as sums
and histograms (see Bassily and Smith, 2015; Wang
et al., 2017; Kulkarni et al., 2019; Cormode et al., 2018;
Bassily et al., 2017, and references therein). In contrast,
our goal is to design LDP protocols for computing U -
statistics, where each term involves a pair of inputs.

3 GENERIC LDP PROTOCOL
FROM QUANTIZATION

Discrete inputs. We ﬁrst consider the case of discrete
inputs taking one of k values. The possible values of
the kernel function can be written as a matrix A ∈
Rk×k where Aij = f (i, j). In this case, we can set the
local randomizer R to be k-ary randomized response
to generate a perturbed version R(xi) of each input
xi. Let ei denote the vector of length k with a one in
the i-th position and 0 elsewhere. For each perturbed
input in one-hot encoding form eR(xi) we can deduce an
unbiased estimate of exi. As the discrete U -statistic is

Private Protocols for U -Statistics in the Local Model and Beyond

Algorithm 1: LDP algorithm based on quanti-
zation and private histograms

Public Parameters: Privacy budget (cid:15),
quantization scheme π, number of bins k.
Input: (xi ∈ X )i∈[n]
Output: Estimate (cid:98)Uf,n of Uf

1 for each user i ∈ [n] do
2

Form quantized input π(xi) ∈ [k]
For β = k/(k + e(cid:15) − 1), generate ˜xi ∈ [k] s.t.

3

P (˜xi = i) =

(cid:26) 1 − β for i = π(xi),
for i (cid:54)= π(xi),

β/k

(9)

Send ˜xi to the aggregator

4
5 end
6 Return (cid:98)Uf,n computed from ˜x1, . . . , ˜xn and β

a linear function of each of these vectors, computing it
on these unbiased estimates gives an unbiased estimate
(cid:98)Uf,n which can be written as:

(cid:98)Uf,n =

1
(cid:1)
(cid:0)n
2

(cid:88)

1≤i<j≤n

(cid:98)fA(R(xi), R(xj)),

and is itself a U -statistic with kernel (cid:98)fA given by

(cid:98)fA(R(x1), R(x2)) = (1−β)−2(eR(x1)−b)T A(eR(x2)−b),

where 1 − β is the probability of returning the true
input in k-ary randomized response (see Eq. 9) and b
is the vector of length k with every entry β/k. Details
and analysis of this process, leveraging Hoeﬀding’s
decomposition of U -statistics (Hoeﬀding, 1948; Lee,
1990), can be found in Section A.1 of the supplementary
material. The resulting bounds on the variance of (cid:98)Uf,n
are summarized in the following theorem.
Theorem 1. If f (x, x(cid:48)) ∈ [0, 1] for all x, x(cid:48), then

Var( (cid:98)Uf,n) ≤

1
n(1 − β)2 +

(1 + β)2
2n(n − 1)(1 − β)4 .

In order to achieve (cid:15)-LDP with a ﬁxed k this becomes,

Var( (cid:98)Uf,n) ≈

(1 + k/(cid:15))2
n

+

(1 + k/(cid:15))4
2n2

≈

k2
n(cid:15)2 ,

Continuous inputs. For U -statistics on discrete do-
mains, e.g. Renyi-2 entropy, the above strategy can be
applied directly. Possibly more importantly however,
this then leads to a natural protocol for the continu-
ous case. In this protocol (see Algorithm 1), the local
randomizer proceeds by quantizing the input into k
bins (for instance using simple or randomized rounding)
before applying the previous procedure.

There are two sources of error in this protocol. The
ﬁrst one is due to the randomization needed to satisfy
LDP in the quantized domain as bounded in Theorem 1.
The second source of error is due to quantization. In
order to control this error in a nontrivial way, we rely
on an assumption on the kernel function (namely, that
it is Lipschitz) or the data distribution (namely, that it
has Lipschitz density). Under these assumptions and
using an appropriate variant of the kernel function on
the quantized domain, we show that we can bound
the error with respect to the original domain by a
term in O(1/k2) (see Section A.2 of the supplementary
material). This leads to the following result.
Theorem 2. For simplicity, assume bounded domain
X = [0, 1] and kernel values f (x, y) ∈ [0, 1] for all
x, y ∈ X . Let π correspond to simple rounding, (cid:15) > 0,
k ≥ 1 and β = k/(k + e(cid:15) − 1). Then Algorithm 1
satisﬁes (cid:15)-LDP. Furthermore:

• If f is Lf -Lipschitz in each of its arguments, then

MSE( (cid:98)Uf,n) is less than or equal to

1
n(1 − β)2 +

(1 + β)2
2n(n − 1)(1 − β)4 +

L2
f
2k2 .

• If dµ/dλ is Lµ-Lipschitz w.r.t. some measure λ,

then MSE( (cid:98)Uf,n) is less than or equal to

4L2
µ
k2 +

1
n(1 − β)2 +

(1 + β)2
2n(n − 1)(1 − β)4 +

4L4
µ
k4 .
Remark 2. The use of simple rounding is not optimal
in many situations. In the case of sum, and possibly of
the Gini coeﬃcient, it would be more accurate if ran-
domized rounding were used instead of simple rounding.
We leave this investigation for later work.

Setting k so as to balance the quantization and estima-
tion errors leads to the following corollary.
Corollary 1. Under the conditions of Theorem 2, for
(cid:15) ≤ 1 and large enough n, taking k = n1/4
L(cid:15) leads to
MSE( (cid:98)Uf,n) = O(L/
n(cid:15)), where L corresponds to Lf or
Lµ depending on the assumption.

√

√

This result gives concrete error bounds for U -statistics
whose kernel is Lipschitz, for arbitrary data distribu-
tions. One important example is the Gini mean diﬀer-
ence, whose corresponding kernel f (xi, xj) = |xi − xj|
is 1-Lipschitz. On the other hand, for U -statistics with
non-Lipschitz kernels, the data distribution must be
suﬃciently smooth (if not, it is easy to construct cases
that make the algorithm fail).

4 LOCALLY PRIVATE AUC

In this section, we describe an algorithm for computing
AUC (7), whose kernel is discontinuous and therefore

James Bell, Aurélien Bellet, Adrià Gascón, Tejas Kulkarni

hλ

h0

h1

h00

h01

h10

h11

=

5

2

3

1

1

2

1

(cid:88)

Recursed nodes at level m

Discarded nodes at level m

Active nodes at level m + 1

Level m

(cid:88)

(cid:88)

Figure 1: Hierarchical histogram h for multiset
{0, 1, 2, 2, 3} over the domain {0, 1, 2, 3}.

Level m + 1

non-Lipschitz. We assume X to be an ordered domain
of size d, that is with each datum in [0..d − 1]. Note
that all data is in practice discrete when represented
in ﬁnite precision, so this is general. For simplicity of
presentation we will assume that (i) d = 2α for some
integer α, and (ii) that the classes of the data, the yi,
are public.

Our solution for computing AUC in the local model
relies on a hierarchical histogram construction that has
been considered in previous works for private collection
of high-dimensional data (Chan et al., 2012), heavy hit-
ters (Bassily et al., 2017), and range queries (Kulkarni
et al., 2019). A hierarchical histogram is essentially a
tree data structure on top of a histogram where each
internal node is labelled with the sum of the values in
the interval covered by it (see Figure 1). That allows to
answer any range query about u by checking the value
associated with O(log |u|) nodes in the tree. We ﬁrst
deﬁne an exact version of such hierarchical histograms
and explain how to compute AUC from one.

Notation on trees. We represent a binary tree h of
depth α with integer node labels as a total mapping
from a preﬁx-closed set of binary strings of length at
most α to the integers. We refer to the i-th node in
level l of the tree by the binary representation of i
padded to length l from the left with zeros. With this
notation, hλ is the label of the root node, as we use λ
to denote the empty string, h0 (resp. h1) is the integer
label of the left (resp. right) child of the root of h, and
in general hp is the label of the node at path p from
the root, i.e. the label of the node reached by following
left or right children from the root according to the
value of p (0 indicates left and 1 indicates right). Let
bi be the i-th node in the bottom level. For two binary
strings p, p(cid:48) ∈ {0, 1}∗ we denote the preﬁx relation by
p(cid:48) (cid:22) p, and their concatenation as p · p(cid:48).

Deﬁnition 3. Let S = {s1, . . . , sn} be a multiset, with
si ∈ [0..d − 1]. A hierarchical histogram of S is a total
mapping h : {0, 1}≤log(d) → Z deﬁned as h(b) = |{s ∈
S | ∃b(cid:48) ∈ {0, 1}∗ : b · b(cid:48) = bs}|. For simplicity, we
denote h(b) by hb.

Algorithm. We use hierarchical histograms to com-
pute AUC as follows. Let S+ and S− be the samples
of the positive and negative classes from which we

Figure 2: Our algorithm can be seen as a breath-ﬁrst
traversal of a tree, where at each level some nodes are
selected for their subtrees to be explored further.

want to estimate AUC. Let h+ and h− be hierarchi-
cal histograms for S+ and S−. Note that h+
λ = n+
and h−
λ = n−. We can now deﬁne the unnormalized
AUC, denoted UAUC, over hierarchical histograms re-
cursively by letting UAUC(h+, h−, p) be 0, if p is a leaf,
and otherwise setting:

UAUC(h+, h−, p) = h+

p·1h−

p·0 +

(cid:88)

i∈{0,1}

UAUC(h+, h−, p·i) .

1

Thus we have AUC(S+, S−) = AUC(h+, h−, λ) =
n+n− UAUC(h+, h−, λ).
The above deﬁnition naturally leads to an algorithm
that proceeds by traversing the trees h+, h− top-down
from the root λ, accumulating the products of counts
from h+, h− at nodes that correspond to entries in h+
that are bigger than entries in h−. We now deﬁne a
diﬀerentially private analogue. Later we will describe
an eﬃcient frequency oracle which can be used to com-
pute an LDP estimate ˆh of a hierarchical histogram h
of n values in a domain of size 2α. This will provide
the following necessary properties (i) ˆh is unbiased, (ii)
Var(ˆh) ≤ v, with v deﬁned as Cnα for some small
constant C (iii) the ˆhp are pairwise independent and
(iv) Each level of ˆh is independent of the other levels.
Our private algorithm for computing an estimate of
UAUC is then deﬁned in terms of parameters n+ and
n−, v+ and v− (bounding the variance of ˆh+ and ˆh−
respectively), and a > 1 is a small number depending
on n+, n−, α and C.

√

For a symbol ℵ we write ℵ± to simultaneously refer to
ℵ+ and ℵ−. Let ˜h±
p = max(ˆh±
p ,
p =
√
p = max(ˆh−
av+/2) and ˜h−
max(ˆh+
p ,
p ,
av−/2), and
√
v−v+. Our private estimate is deﬁned as
let τ = a
follows. If p is a leaf then (cid:91)UAUC(ˆh+, ˆh−, p) is 0, else if
˜h+
p

˜h−
p < τ then it is given by

av±/2), i.e. ˜h+

√

(cid:80)

i∈{0,1}

ˆh+
p·i

(cid:80)

i∈{0,1}

ˆh−
p·i .

1
2

Otherwise, it is given recursively by

ˆh+
p·1

p·0 + (cid:80)
ˆh−

i∈{0,1}

(cid:91)UAUC(ˆh+, ˆh−, p · i) .

(10)

Private Protocols for U -Statistics in the Local Model and Beyond

(cid:80)

(cid:80)

i∈0,1

ˆh+
p·i

As before, this deﬁnition leads to an algorithm. Note
that the only diﬀerence with its non-private analogue
is that this procedure does not recurse into subtrees
whose contribution to the UAUC is upper bounded
suﬃciently tightly. More concretely, the server starts
by querying ˆh+, ˆh− at the root, namely with p = λ. If
p is a leaf then we return 0 as the AUC. Otherwise, the
algorithm checks whether ˜h+
˜h−
p < τ . If so, then the
p
algorithm concludes that there is not much to gain in
exploring the subtrees rooted at p·0 and p·1, and returns
ˆh−
p·i as an estimate of 1
1
2 h+
p . This
i∈0,1
2
ˆh−
estimate might seem equivalent to 1
p , but takes
2
the previous form for a technical reason that is made
clear in the proof. In this case we call p a discarded
node. On the other hand, if ˜h+
˜h−
p ≥ τ , the algorithm
p
proceeds as its non-private analogue, accumulating the
contribution to the UAUC from the direct subtrees of p
and recursing into nodes p · 0 and p · 1. In this case
we refer to p as a recursed node. Thus every node
p ∈ {0, 1}≤α will be either recursed, a leaf or there
will be a discarded node p(cid:48) such that p(cid:48) (cid:22) p. This is
depicted in Figure 2.

p h−

ˆh+
p

Analysis. Note that our algorithm has two sources of
error: (i) the one incurred by discarding nodes and (ii)
the error in estimating the contribution to the UAUC
of the recursed nodes. The threshold τ is carefully
chosen to balance these two errors.

2

m∈[α]

E(ER
m

m∈[α] ER

Let Rm be the set of nodes recursed on at level m.
Our accuracy proof starts by bounding the expected
value of |Rm| (see Lemma 4 in Section B.1 of the
supplementary) by a quantity B that is independent
of m. We now describe a central argument to our
accuracy proof, stated in the next theorem. Let ER
m be
the contribution to the error by nodes in Rm. Then,
the total contribution to the error by recursed nodes
is ER = (cid:80)
) =
(cid:80)

m. A useful identity is E(ER2

), as we can bound E(ER
), for any m,
m
in terms of B (see detailed proof in the supplementary).
Note that this identity follows from E(ER
m(cid:48)) = 0,
with m(cid:48) > m. The latter would hold if errors ER
m
and ER
m(cid:48) were independent, since our frequency oracle
is unbiased. However, errors at a given level are not
independent of previous levels. However E(ER
mER
m(cid:48)) =
0 because the conditional expectation of ER
m(cid:48) with
respect to the answers of the frequency oracles up to
level m(cid:48) is 0 i.e. ER
m(cid:48) is a martingale diﬀerence
sequence. The idea of conditioning on previous levels
is used several times in our proofs, also to bound the
error due to discarded nodes.

1 , . . . , ER

mER

2

Next, we state our accuracy result, which is proven
in detail in Section B.1 of the supplementary. Our
proof tracks constants: this is important for practical
purposes, and we show empirically in Section D.1 that

our bound is in fact quite tight.

√

Theorem 3. If α ≤

n and the following holds:

1. E(ˆh±
ased.

p − h±

p ) = 0 i.e. frequency estimates are unbi-

2. E((ˆh±

p − h±

p )2) ≤ v± i.e. MSE of frequency estima-

tor is bounded by v± = Cn±α.

3. For distinct p, p(cid:48) ∈ {0, 1}≤α with |p| = |p(cid:48)|, ˆh±

p and
ˆh±
p(cid:48) are independent i.e. the frequency estimates
are pairwise independent.

4. For all m ≤ log(d), the lists (ˆh±

p )p∈{0,1}≤m and

(ˆh±

p )p∈{0,1}>m are independent of each other.

Then, MSE((cid:91)UAUC) is given by

Cn−n+α2(cid:16)

2n + (4a + 1) min(n−, n+) +

√
21
√

2nCα
a − 1

(cid:17)

Instantiating ˆh. So far, Theorem 3 does not yield a
complete algorithm as it does not specify an algorithm
for computing estimates ˆh of a hierarchical histogram
that satisfy the conditions of Theorem 3. In Section B.2
of the supplementary, we show how to instantiate such
an algorithm in a communication-eﬃcient manner by
combining ideas from Bassily et al. (2017), in particular
the use of the Hadamard transform, with an modiﬁed
version of the protocol from Kulkarni et al. (2019).
This leads to the following result.

Theorem 4. There is a one-round non-interactive
protocol for computing AUC in the local model with
MSE bounded by O(α2 log(1/δ)/n(cid:15)2) under ((cid:15), δ)-LDP
and O(α3/n(cid:15)2) under (cid:15)-LDP. Every user submits one
bit, and the server does O(nlog(d)) computation and
requires O(log(d)) additional reconstruction space.

5 GENERIC PROTOCOLS FROM

2PC

So far, we have proposed a specialized LDP protocol
for AUC, and a generic LDP protocol which requires
some assumption on the kernel function or the data
distribution to guarantee nontrivial error bounds. We
conjecture that no LDP protocol can guarantee non-
trivial error for arbitrary kernels and distributions, but
we leave this as an open question for future work.

In this section, we slightly relax the model of LDP
by allowing pairs of users i and j to compute a ran-
domized version ˜f (xi, xj) of their kernel value f (xi, xj)
with 2-party secure computation (2PC). This gives rise
to a computational diﬀerential privacy (CDP) model
Mironov et al. (2009). Unsurprisingly, we show that in

James Bell, Aurélien Bellet, Adrià Gascón, Tejas Kulkarni

this model we can match the MSE of O( ln(1/δ)
) for com-
puting regular (univariate) averages in the ((cid:15), δ)-LDP
model by using advanced composition results (Dwork
et al., 2010). However, such a protocol requires O(n2)
communication as all pairs of users need to compute
˜f (xi, xj) via 2PC, and does not satisfy pure (cid:15)-DP.

n(cid:15)2

Proposed protocol. To address these limitations, we
propose that the aggregator asks only a (random) sub-
set of pairs of users (i, j) to submit their randomized
kernel value ˜f (xi, xj). The idea is to trade-oﬀ between
the error due to privacy (which increases as more pairs
are used, due to budget splitting) and the subsampling
error (for not averaging over all pairs). Given a posi-
tive integer P (which should be thought of as a small
constant independent of n) and assuming n to be even
for simplicity, we propose the following protocol:

1. Subsampling: The aggregator samples P indepen-
dent permutations σ1, . . . , σP ∈ Sn of the set of
users {1, . . . , n}. This deﬁnes a set of P n/2 pairs
P = (σp(2i − 1, 2i))p∈[P ],1≤i≤N/2.

2. Perturbation: For each pair of users (i, j) ∈ P,
users compute ˜f (xi, xj) via 2PC and sends it to
the aggregator.

3. Aggregation: The aggregator computes an estimate

of Uf as a function of { ˜f (xi, xj)}(i,j)∈P .

Analysis. We have the following result for the Laplace
mechanism applied to real-valued kernel functions (the
extension to randomized response for discrete-valued
kernels is straightforward). The proof relies on an exact
characterization of the subsampling error by leveraging
results on the variance of incomplete U -statistics (Blom,
1976), see Section C.1 of the supplementary for details.

Theorem 5 (2PC subsampling protocol with Laplace
mechanism). Let (cid:15) > 0, P ≥ 1 and assume that the
kernel f has values in [0, 1]. Consider our subsampling
protocol above with ˜f (xi, xj) = f (xi, xj) + ηij where
ηij ∼ Lap(P/(cid:15)), and the estimate computed as (cid:98)Uf,n =
˜f (xi, xj). Then the protocol satisﬁes (cid:15)-
2
P n
CDP, has a total communication cost of O(P n) and

(i,j)∈P

(cid:80)

MSE( (cid:98)Uf,n) =

2(P − 1)(cid:0)1 −

(cid:16)

2
P n
+ (cid:0)1 +

(cid:1)ζ1

1
n − 1
2P
n(cid:15)2 ,

+

P − 1
n − 1

(cid:17)

(cid:1)ζ2

where ζ1 and ζ2 are deﬁned as in (3).

The MSE in Theorem 5 is of O( 1
n(cid:15)2 ). Remarkably,
this shows that the O(1/n) variance of the estimate that
uses all pairs is preserved when subsampling only O(n)
pairs. This is made possible by the strong dependence
structure in the O(n2) terms of the original U -statistic.

P n + P

As expected, P rules a trade-oﬀ between the errors
due to subsampling and to privacy: the larger P , the
smaller the former but the larger the latter (as each
user must split its budget across P pairs). The optimal
value of P depends on the kernel function and the data
distribution (through ζ1 and ζ2) on the one hand, and
the privacy budget (cid:15) on the other hand. This trade-oﬀ,
along with the optimality of the proposed subsampling
schemes, are discussed in more details Section C.1 of the
supplementary material. In practice and as illustrated
in our experiments, P can be set to a small constant.

Implementing 2PC. Securely computing the ran-
domized kernel value ˜f (xi, xj) can be done eﬃciently
for many kernel functions and local randomizers of
interest, as the number of parties involved is limited to
2. We assume semi-honest parties (see Goldreich, 2004,
for a deﬁnition of this threat model). A suitable 2PC
technique in this application are garbled circuits (Yao,
1986; Lindell and Pinkas, 2009; Evans et al., 2018),
which are well-suited to compute Boolean comparisons
as required in several of the kernels mentioned in Sec-
tion 2.1.2. The circuits for computing the kernels can
then be extended with output perturbation following
ideas from Dwork et al. (2006) and Champion et al.
(2019). We refer to Section C.2 of the supplement for
details on design and complexity.

Remark 3 (Beyond 2PC). One could further relax
the model to allow multi-party secure computation with
more than two parties, e.g. by extending the garbled
circuit computing the kernel with secure aggregation
over the P n pairs before performing output perturba-
tion. This would recover the utility of centralized DP
at the cost of much more computation and quadratic
communication, which is not practical, as well as ro-
bustness. More interesting trade-oﬀs may be achieved
by securely aggregating small subsets of pairs. We leave
the careful analysis of such extensions to future work.

6 EXPERIMENTS

AUC. We use the Diabetes dataset (Strack et al.,
2014) for the binary classiﬁcation task of determining
whether a patient will be readmitted in the next 30 days
after being discharged. We train a logistic regression
model s which is used to score data points in [0, 1], and
apply our protocol to privately compute AUC on the
test set. Patients readmitted before 30 days form the
positive class. Class sizes are shown in Figure 3. Class
information is not considered sensitive, as opposed
to the score s(x) on private user data s(x), which
includes detailed medical information. Figure 3 shows
the standard error achieved by our protocol for diﬀerent
values of the domain size d. For each value of d we
run our protocol with inputs s(fp(s(xi), d)), where fp

Private Protocols for U -Statistics in the Local Model and Beyond

2PC primitive to compute ˜f (xi, xj) is simulated. The
results shown in Figure 4 show that the 2PC protocol
with all pairs performs worst due to composition. The
randomized response protocol performs slightly better,
thanks to the small domain size. Finally, our 2PC
protocol with subsampling achieves the lowest error by
roughly an order of magnitude in high privacy regimes
((cid:15) ≤ 2) while keeping the communication cost linear
in n. As predicted by our analysis, P = 1 is best in
high privacy regimes, where the error due to privacy
dominates the subsampling error. We also see that P >
1 can be used to reduce the overall error in low privacy
regimes (for (cid:15) = 10 one can use an even larger P to
match the error of the randomized response protocol).

7 CONCLUDING REMARKS

In this paper, we tackled the problem of computing
U -statistics from private user data, covering many sta-
tistical quantities of broad interest which were not
addressed by previous private protocols.

Relative merits of our protocols. Our three proto-
cols are largely complementary, insofar as each of them
is well-suited to speciﬁc situations. Our ﬁrst protocol
(quantization followed by randomized response) can
gracefully handle cases where the kernel is Lipschitz or
the data is discrete in a small domain. It may also work
well for non-Lipschitz kernels when quantizing data into
few bins does not lose too much information (e.g., when
the data distribution is smooth enough). As the latter
hypothesis is diﬃcult to assess in advance, we argue
that there can exist specialized protocols that work
well for non-Lipschitz kernels on continuous data or
large discrete domains. Our second protocol illustrates
this for the case of AUC: we leverage a hierarchical
histogram structure to scale much better with the num-
ber of bins than the ﬁrst protocol (see Section D for
experiments comparing these two protocols). Finally, if
one is willing to slightly relax the LDP model to allow
pairwise communication among users, and if the kernel
can be computed eﬃciently via 2PC, our third protocol
is expected to perform best in terms of accuracy.

Extension to higher degrees. While we focused
on pairwise U -statistics, our ideas can be extended to
higher degrees. A prominent example is the Volume
Under the ROC Surface, the generalization of the AUC
to multi-partite ranking (Clémençon et al., 2013).

Future work. A promising direction for future work
is to develop private multi-party algorithms for learning
with pairwise losses (Kar et al., 2013; Clémençon et al.,
2016) by combining private stochastic gradient descent
for standard empirical risk minimization (Bassily et al.,
2014; Shokri and Shmatikov, 2015) and our protocols
to compute the gradient estimates.

Figure 3: Mean and std. dev. (over 20 runs) of the
absolute error of our AUC protocol on the scores of a
logistic regression model trained on a Diabetes dataset.

Figure 4: Mean and standard deviation (over 20 runs)
of the absolute error in KTC on Tripadvisor dataset.

denotes a discretization into the domain [0..d − 1]. The
plot shows that the protocol is quite robust to the
choice of d, and that increasing (cid:15) beyond 2 does not
improve results signiﬁcantly. Recall that the error of
our AUC protocol depends on the size of the smallest
class, which is quite small here (only 693 examples).

Kendall’s tau. We use the Tripadvisor dataset (Wang,
2010). The dataset consists of discrete user ratings
(from scale -1 to 5) for hotels in San Francisco over
many service quality metrics such as room service,
location, room cleanliness, front desk service etc. After
discarding the records with missing values, we have over
246K records. Let xi = (yi, zi) be ratings given by user
i to the room (yi) and the cleanliness (zi). The goal
is to privately estimate the Kendall’s tau coeﬃcient
(KTC) between these two variables, whose true value
is 0.58. We compare the privacy-utility trade-oﬀ of our
randomized response protocol (Algorithm 1 without
quantization, since inputs can take only 6 × 6 = 36
values), our 2PC protocol based on subsampling, and
the 2PC protocol that computes all pairs and relies on
advanced composition (for which we set δ = 1e−8). The

0246810104103102101Standard errorn=2463992PC protocol with all pairs,=1e82PC protocol with subsampling, P=12PC protocol with subsampling, P=5RR-based protocol (Algorithm 1)James Bell, Aurélien Bellet, Adrià Gascón, Tejas Kulkarni

Acknowledgments

J. B. was supported by The Alan Turing Institute un-
derthe EPSRC grant EP/N510129/1, and the UK Gov-
ernment’s Defence & Security Programme in support
of the Alan Turing Institute. A. B. was supported by
grants ANR-16-CE23-0016-01 and ANR-18-CE23-0018-
03, by the European Union’s Horizon 2020 Research
and Innovation Program under Grant Agreement No.
825081 COMPRISE and by a grant from CPER Nord-
Pas de Calais/FEDER DATA Advanced data science
and technologies 2015-2020. A. B. thanks Jan Ramon
for useful discussions. Work partly done when A. G.
was at The Alan Turing Institute and Warwick Univer-
sity, and supported by The Alan Turing Institute under
the EPSRC grant EP/N510129/1, and the UK Gov-
ernment’s Defence & Security Programme in support
of the Alan Turing Institute. T. K. thanks Graham
Cormode for arranging a trip to Inria Lille. His visit
was supported by Marie Curie Grant 618202.

References

Acharya, J., Orlitsky, A., Suresh, A. T., and Tyagi, H.
(2015). The Complexity of Estimating Rényi Entropy.
In SODA.

Bassily, R., Nissim, K., Stemmer, U., and Thakurta,
A. G. (2017). Practical locally private heavy hitters.
In NIPS.

Bassily, R. and Smith, A. (2015). Local, private,
eﬃcient protocols for succinct histograms. In STOC.

Bassily, R., Smith, A. D., and Thakurta, A. (2014).
Private Empirical Risk Minimization: Eﬃcient Algo-
rithms and Tight Error Bounds. In FOCS.

Bellet, A., Habrard, A., and Sebban, M. (2015). Met-
ric Learning. Morgan & Claypool Publishers.

Blom, G. (1976). Some properties of incomplete U -
statistics. Biometrika, 63(3):573–580.

Bradley, A. P. (1997). The use of the area under
the ROC curve in the evaluation of machine learning
algorithms. Pattern Recognition, 30(7):1145–1159.

Champion, J., Shelat, A., and Ullman, J. (2019). Se-
curely sampling biased coins with applications to dif-
ferential privacy. IACR Cryptology ePrint Archive,
2019:823.

Chan, T. H., Shi, E., and Song, D. (2012). Privacy-
preserving stream aggregation with fault tolerance. In
Financial Cryptography.

Clémençon, S., Bellet, A., and Colin, I. (2016). Scaling-
up Empirical Risk Minimization: Optimization of
Incomplete U-statistics. Journal of Machine Learning
Research, 13:165–202.

Clémençon, S., Lugosi, G., and Vayatis, N. (2008).
Ranking and empirical risk minimization of U -
statistics. The Annals of Statistics, 36(2):844–874.

Clémençon, S., Robbiano, S., and Vayatis, N. (2013).
Ranking data with ordinal labels: Optimality and
pairwise aggregation. Machine Learning, 91(1):67–
104.

Cormode, G., Kulkarni, T., and Srivastava, D. (2018).
Marginal release under local diﬀerential privacy. In
SIGMOD.

de la Pena, V. and Giné, E. (1999). Decoupling: from
Dependence to Independence. Springer.

Diﬀerential Privacy Team, Apple (2017). Learning
with privacy at scale.

Ding, B., Kulkarni, J., and Yekhanin, S. (2017). Col-
lecting telemetry data privately. In NIPS.

Duchi, J. C., Jordan, M. I., and Wainwright, M. J.
(2013). Local privacy and statistical minimax rates.
In FOCS.

Dwork, C., Kenthapadi, K., McSherry, F., Mironov,
I., and Naor, M. (2006). Our data, ourselves: Privacy
via distributed noise generation. In EUROCRYPT.

Dwork, C., Rothblum, G. N., and Vadhan, S. (2010).
Boosting and Diﬀerential Privacy. In FOCS.

Erlingsson, U., Pihur, V., and Korolova, A. (2014).
Rappor: Randomized aggregatable privacy-preserving
ordinal response. In CCS.

Evans, D., Kolesnikov, V., and Rosulek, M. (2018).
A pragmatic introduction to secure multi-party com-
putation. Foundations and Trends in Privacy and
Security, 2(2-3):70–246.

Faivishevsky, L. and Goldberger, J. (2008). ICA based
on a Smooth Estimation of the Diﬀerential Entropy.
In NIPS.

Fanti, G., Pihur, V., and Erlingsson, Ú. (2016). Build-
ing a RAPPOR with the unknown: Privacy-preserving
In
learning of associations and data dictionaries.
PoPETs.

Clémençon, S. (2014). A statistical view of cluster-
ing performance through the theory of U-processes.
Journal of Multivariate Analysis, 124:42–56.

Goldreich, O. (2004). The Foundations of Cryptog-
raphy - Volume 2, Basic Applications. Cambridge
University Press.

Private Protocols for U -Statistics in the Local Model and Beyond

Vogel, R., Bellet, A., and Clémençon, S. (2020). Learn-
ing Fair Scoring Functions: Fairness Deﬁnitions, Al-
gorithms and Generalization Bounds for Bipartite
Ranking. arXiv preprint arXiv:2002.08159.

Wang, H. (2010). Trip advisor data. http://www.
preflib.org/data/combinatorial/trip/.

Wang, T., Blocki, J., Li, N., and Jha, S. (2017). Lo-
cally diﬀerentially private protocols for frequency es-
timation. In USENIX Security Symposium.

Yao, A. C. (1986). How to generate and exchange
secrets (extended abstract). In FOCS.

Yitzhaki, S. (2003). Gini’s Mean Diﬀerence: A Su-
perior Measure of Variability for Non-Normal Distri-
butions. Metron International Journal of Statistics,
61(2):285–316.

Herschtal, A. and Raskutti, B. (2004). Optimising
area under the ROC curve using gradient descent. In
ICML.

Hoeﬀding, W. (1948). A class of statistics with asymp-
totically normal distribution. Annals of Mathematics
and Statistics, 19:293–325.

Joachims, T. (2002). Optimizing search engines using
clickthrough data. In KDD.

Kairouz, P., Oh, S., and Viswanath, P. (2014). Ex-
tremal mechanisms for local diﬀerential privacy. In
NIPS.

Kallus, N. and Zhou, A. (2019). The Fairness of Risk
Scores Beyond Classiﬁcation: Bipartite Ranking and
the xAUC Metric. In NeurIPS.

Kar, P., Sriperumbudur, B. K., Jain, P., and Karnick,
H. (2013). On the Generalization Ability of Online
Learning Algorithms for Pairwise Loss Functions. In
ICML.

Kulkarni, T., Cormode, G., and Srivastava, D. (2019).
Answering range queries under local diﬀerential pri-
vacy. In SIGMOD.

Lapata, M. (2006). Automatic Evaluation of Infor-
mation Ordering: Kendall’s Tau. Computational Lin-
guistics, 32(4):471–484.

Lee, A. (1990). U -statistics: Theory and practice.
Marcel Dekker, Inc., New York.

Lindell, Y. and Pinkas, B. (2009). A proof of security
of yao’s protocol for two-party computation. Journal
of Cryptology, 22(2):161–188.

Mann, H. B. and Whitney, D. R. (1947). On a Test of
Whether one of Two Random Variables is Stochasti-
cally Larger than the Other. Annals of Mathematical
Statistics, 18(1):50–60.

Mironov, I., Pandey, O., Reingold, O., and Vadhan,
S. P. (2009). Computational Diﬀerential Privacy. In
CRYPTO.

Shokri, R. and Shmatikov, V. (2015). Privacy-
preserving deep learning. In CCS.

Strack, B., DeShazo,
J. P., Gennings, C.,
Olmo, J. L., Ventura, S., Cios, K. J., and
Clore, J. N. (2014).
https:
//archive.ics.uci.edu/ml/datasets/diabetes+
130-us+hospitals+for+years+1999-2008.

Diabetes data.

Van Der Vaart, A. W. (2000). Asymptotic Statistics.
Cambridge University Press.

