On Limited-Memory Subsampling Strategies for Bandits
Dorian Baudry, Yoan Russac, Olivier Cappé

To cite this version:

Dorian Baudry, Yoan Russac, Olivier Cappé. On Limited-Memory Subsampling Strategies for Bandits.
ICML 2021- International Conference on Machine Learning, Jul 2021, Vienna / Virtual, Austria. ￿hal-
03265442￿

HAL Id: hal-03265442

https://hal.science/hal-03265442

Submitted on 20 Jun 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

On Limited-Memory Subsampling Strategies for Bandits

Dorian Baudry * 1 Yoan Russac * 2 Olivier Cappé 2

Abstract
There has been a recent surge of interest in non-
parametric bandit algorithms based on subsam-
pling. One drawback however of these approaches
is the additional complexity required by random
subsampling and the storage of the full history
of rewards. Our ﬁrst contribution is to show
that a simple deterministic subsampling rule, pro-
posed in the recent work of Baudry et al. (2020)
under the name of “last-block subsampling”, is
asymptotically optimal in one-parameter expo-
nential families. In addition, we prove that these
guarantees also hold when limiting the algorithm
memory to a polylogarithmic function of the time
horizon. These ﬁndings open up new perspec-
tives, in particular for non-stationary scenarios
in which the arm distributions evolve over time.
We propose a variant of the algorithm in which
only the most recent observations are used for
subsampling, achieving optimal regret guaran-
tees under the assumption of a known number of
abrupt changes. Extensive numerical simulations
highlight the merits of this approach, particularly
when the changes are not only affecting the means
of the rewards.

1. Introduction

In the K-armed stochastic bandit model, the learner repeat-
edly picks an action among K available alternatives and only
observes the rewards associated with her actions. By inter-
acting with the environment, the learner aims at maximizing
her expected sum of rewards and needs to sequentially adapt
her decision strategy in light of the information gained up
to now. In this model, over-conﬁdent policies are provably
suboptimal and a proper trade-off between exploitation and
exploration has to be found.

*Equal contribution 1Univ. Lille, CNRS, Inria, Centrale
Lille, UMR 9198-CRIStAL, F-59000 Lille, France 2DI ENS,
CNRS, Inria, ENS, Université PSL, Paris, France. Correspon-
dence to: Dorian Baudry <dorian.baudry@inria.fr>, Yoan Russac
<yoan.russac@ens.fr>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

Multi-armed bandits models have been used to address a
wide range of sequential optimization tasks under uncer-
tainty: online recommendation (Li et al., 2011; 2016), strate-
gic pricing (Bergemann & Välimäki, 1996) or clinical trials
(Zelen, 1969; Vermorel & Mohri, 2005) to name a few. In
its standard formulation the multi-armed bandit model pos-
tulates that the distributions of the rewards obtained when
drawing the different arms remain constant over time. How-
ever, in some scenarios the stationary assumption is not
realistic. In clinical trials, the disease to defeat may mutate
and the initially optimal treatment could become subopti-
mal compared to another candidate (Gorre et al., 2001). In
strategic pricing problems, the price maximizing the proﬁt
of a given asset can evolve with the introduction of a new
product on the market (Eliashberg & Jeuland, 1986). For on-
line recommendation systems, the preferences of the users
are likely to evolve (Wu et al., 2018) and collected data
becomes progressively obsolete.

During the past ten years, several works have considered
non-stationary variants of the multi-armed bandit model,
proposing methods that can be grouped into two main cat-
egories: they either actively try to detect modiﬁcations in
the distribution of the arms with changepoint detection algo-
rithms (Liu et al., 2017; Cao et al., 2019; Auer et al., 2019;
Chen et al., 2019; Besson et al., 2020) or they passively
forget past information (Garivier & Moulines, 2011; Raj &
Kalyani, 2017; Trovo et al., 2020). To some extent, all of
these methods require some knowledge on the distribution
to obtain theoretical guarantees.

To balance exploration and exploitation, the algorithms men-
tioned so far are based on one of the two standard building
blocks introduced in the bandit literature: Upper Conﬁdence
Bound (UCB) constructions (Auer et al., 2002) or Thomp-
son Sampling (TS) (Thompson, 1933). However, there has
been a recent surge of interest for alternative non-parametric
bandit strategies (Kveton et al., 2019a;b; Riou & Honda,
2020). Instead of using prior information on the reward dis-
tributions as in Thompson sampling or of building tailored
upper-conﬁdence bounds (Cappé et al., 2013) those meth-
ods only use the empirical distribution of the data. These
algorithms are non-parametric in the sense that the exact
same implementation can be used with different probability
distributions, while still achieving optimal regret guarantees
(in a sense to be deﬁned in Section 2 below).

On Limited-Memory Subsampling Strategies for Bandits

In particular, subsampling algorithms (Baransi et al., 2014;
Chan, 2020; Baudry et al., 2020) have demonstrated their
potential thanks to their ﬂexibility and strong theoretical
guarantees. From a high level perspective, they all rely on
the same two components. (1) subsampling: the arms that
have been pulled a lot are randomized by sampling only a
fraction of their history. (2) duels: the arms are pulled based
on the outcomes of duels between the different pairs of
arms. Note that the term duel, which we will also use in the
following, refers to the algorithmic principle of comparing
the arms two by two, based on their subsamples. It is totally
unrelated to the dueling bandit framework introduced by
Yue & Joachims (2009).

Scope and contributions
In this paper, we build on the
Last-Block Subsampling Duelling Algorithm (LB-SDA) in-
troduced by Baudry et al. (2020) but for which no theoretical
guarantees were provided. This approach is of interest be-
cause of its simplicity and its computational efﬁciency com-
pared to other strategies based on randomized subsampling.
We ﬁrst prove that for stationary environments LB-SDA is
asymptotically optimal in one-parameter exponential family
models and therefore matches the guarantees obtained by
Baudry et al. (2020) for randomized subsampling schemes.
The main technical challenge is to devise an alternative
to the diversity condition used in their work, which was
speciﬁcally designed for randomized subsampling schemes.

Furthermore, we show that, without additional changes,
these guarantees still hold for a variant of the algorithm
using a limited memory of the observations of each arm. We
prove that storing Ω (cid:0)(log T )2(cid:1) observations instead of T is
sufﬁcient to ensure the asymptotic guarantees, making the
algorithm more tractable for larger time horizons. To the
best of our knowledge, this paper is the ﬁrst to propose an
asymptotically optimal subsampling algorithm with poly-
logarithmic storage of rewards under general assumptions.

Building a subsampling algorithm based on the most recent
observations makes it an ideal candidate for a passively for-
getting policy. Our third contribution is to propose a natural
extension of the LB-SDA strategy to non-stationary environ-
ments. By limiting the extent of the time window in which
subsampling is allowed to occur, one obtains a passively
forgetting non-parametric bandit algorithm, which we refer
to as Sliding Window Last Block Subsampling Duelling
Algorithm (SW-LB-SDA). To analyze the performance of
this algorithm, we assume an abruptly changing environ-
ment in which the reward distributions change at unknown
time instants called breakpoints. We show that SW-LB-
SDA guarantees a regret of order O((cid:112)ΓT T log(T )) for
any abruptly changing environment with at most ΓT break-
points, thus matching the lower bounds from Garivier &
Moulines (2011), up to logarithmic factors. The only re-
quired assumption is that, during each stationary phase, the

reward distributions belong to the same one-parameter expo-
nential family for all arms. Due to its non-parametric nature,
this algorithm can thus be used in many scenarios of interest
beyond the standard bounded-rewards / change-in-the-mean
framework. We discuss some of these scenarios in Section 5,
where we validate numerically the potential of the approach
by comparing it with a variety of state-of-the-art algorithms
for non-stationary bandits.

2. Preliminaries

The algorithms to be presented below are designed for the
stochastic K-armed bandit model, which is the most studied
setting in the bandit literature. We introduce in this section
the two variants of this basic model that will be considered in
the paper: stationary and abruptly changing environments.

Stationary environments When the environment is sta-
tionary, the K arms are characterized by the reward distribu-
tions (νk)k≤K and their associated means (µk)k≤K, with
µ(cid:63) = maxk∈{1,...,K} µk denoting the highest expected re-
ward. We denote by (Yk,s)s∈N the i.i.d. sequence of rewards
from arm k. Following Chan (2020), our algorithm operates
in successive rounds, whose length varies between 1 and
K time steps. At each round r, the leader denoted (cid:96)(r) is
deﬁned and (K − 1) duels with the remaining arms called
challengers are performed. Denoting by Nk(r) the number
of pulls of arm k up to the round r the leader is the arm that
has been most pulled. Namely,

(cid:96)(r) = argmaxk∈{1,...,K}Nk(r) .

(1)

When several arms are candidate for the maximum number
of pulls, the one with the largest sum of rewards is chosen.
If this is still not sufﬁcient to obtain a unique arm, the
leader is chosen at random among the arms maximizing both
criteria. At round r, a subset Ar ⊂ {1, ..., K} is selected
by the learner based on the outcomes of the duels against
(cid:96)(r). Next, all arms in Ar are drawn, yielding Yk,Nk(r) for
k ∈ Ar, where Nk(r) = (cid:80)r
s=1

1(k ∈ As).

The regret is deﬁned as the expected difference between
the highest expected reward and the rewards collected by
playing the sequence of arms (At)t≤T :

RT = E

(cid:34) T

(cid:88)

(µ(cid:63) − µAt)

(cid:35)

.

t=1

For distributions in one-parameter exponential families, the
lower bound of Lai & Robbins (1985) states that no strategy
can systematically outperform the following asymptotic
regret lower bound

lim inf
T →∞

RT
log(T )

≥

(cid:88)

k:µk<µ(cid:63)

µ(cid:63) − µk
kl(µk, µ(cid:63))

.

On Limited-Memory Subsampling Strategies for Bandits

Abruptly changing environments
In Section 4, we con-
sider abruptly changing environments. The number of break-
points up to time T , denoted ΓT , is deﬁned by

well as under-explored arms for which Nk(r) ≤ (cid:112)log(r).
If Ar+1 is empty, only the leader is pulled. Combining these
elements gives LB-SDA detailed below.

ΓT =

T −1
(cid:88)

t=1

1{∃k, νk,t (cid:54)= νk,t+1}.

The time instants (t1, ..., tΓT ) associated to these break-
points deﬁne ΓT + 1 stationary phases where the reward
distributions are ﬁxed. Note that in this model, the change
do not need to affect all arms simultaneously. In such en-
vironments, letting µ(cid:63)
t = maxk∈{1,...,K} µk,t denote the
best arm at time t, the performance of a policy is measured
through the dynamic regret deﬁned as

RT = E

(cid:35)

(µ(cid:63)

t − µAt)

.

(cid:34) T

(cid:88)

t=1

We will explain how to extend the notion of leader to this
setting in Section 4.

In the non-stationary case, the lower bound for the regret
takes a different form: for any strategy, there exists an
abruptly changing instance such that E[RT ] = Ω(
T ΓT )
(Garivier & Moulines, 2011; Seznec et al., 2020). Note that
in the bandit literature, there is also another, more general,
way of characterizing non-stationary environments based
on a variational distance introduced by Besbes et al. (2014).
In this work, we however only consider the case of abruptly
changing environments.

√

3. LB-SDA in Stationary Environments

In this section we detail the subsampling strategy used in the
LB-DSA algorithm and obtain asymptotically optimal regret
guarantees for its performance. In Section 3.3, we consider
the variant of LB-SDA in which the memory available to
the algorithm is strongly limited.

3.1. Last Block Sampling

Compared to the algorithms analyzed in (Baudry et al.,
2020) where the sampler is randomized, we consider a de-
terministic sampler. At round r, the duel between arm
k (cid:54)= (cid:96)(r) and the leader consists in comparing the aver-
age reward from arm k with the average reward computed
only from the last Nk(r) observations of the leader. The
challenger k thus wins its duel if

Algorithm 1 LB-SDA
Input: K arms, horizon T
Initialization: t ← 1, r ← 1, ∀k ∈ {1, ..., K}, Nk ← 0
while t < T do

A ← {}, (cid:96) ← leader(N, Y )
if r = 1 then

A ← {1, . . . , K} (Draw each arm once)

else

for k (cid:54)= (cid:96) ∈ {1, ..., K} do

if Nk ≤ (cid:112)log(r) or ¯Yk,Nk ≥ ¯Y(cid:96),N(cid:96)−Nk+1:N(cid:96)
then

A ← A ∪ {k}

if |A| = 0 then
A ← {(cid:96)}

for k ∈ A do

Pull arm k, observe reward Yk,Nk+1, Nk ← Nk + 1,
t ← t + 1

r ← r + 1

Baransi et al. (2014) propose interesting arguments explain-
ing why subsampling methods work. Essentially, if the
sampler allows enough diversity in the duels, the probabil-
ity of repeatedly selecting a suboptimal arm is small. On
the sampler side, this condition is satisﬁed when out of a
large number of duels between two arms there is a reason-
able amount of them with non-overlapping subsamples. We
prove that last block sampling satisﬁes such property. The
second requirement concerns the distribution of the arms,
and has been formulated by Baransi et al. (2014) who intro-
duced the balance function of a family of distributions. In
particular, Chan (2020) shows that introducing an asymp-
log r is enough
totically negligible sampling obligation of
to make subsampling suitable when the arms come from
the same one-parameter exponential family of distributions.
log r samples at round r,
Namely, if each arm has at least
the diversity of duels will guarantee each arm to be pulled
enough. This exploration rate does not have to be tuned
and is not detrimental in practice : for an horizon of, say,
T = 106 it only forces each arm to be sampled at least 4
times.

√

√

¯Yk,Nk(r) ≥ ¯Y(cid:96)(r),N(cid:96)(r)(r)−Nk(r)+1:N(cid:96)(r)(r) ,

(2)

3.2. Regret Analysis of LB-SDA

where ¯Yk,i:j = 1
n=i Yk,n denotes the average com-
puted on the j − i + 1 observations of arm k between its
i-th and j-th pull, and ¯Yk,n is a shortcut for ¯Yk,1:n.

j−i+1

(cid:80)j

We consider that the arms come from the same one-
parameter exponential family of distributions PΘ, i.e., that
there exists a function g : R × Θ (cid:55)→ R such that any arm k
has a density of the form

At each round, the set Ar+1 includes all of the challengers
that have defeated the leader, according to Equation (2), as

gk(x) = g(x, θk) = eθkx−Ψ(θk)g(x, 0) ,

On Limited-Memory Subsampling Strategies for Bandits

where Ψ(θk) = log (cid:2)(cid:82) eθkxg(x, 0) dx(cid:3). This assumption is
standard in the literature and covers a broad range of bandits
applications. The exact knowledge of the family of distri-
butions of the arms (e.g Bernoulli, Gaussian with known
variance, Poisson, etc.) can be used to calibrate algorithms
like Thompson Sampling (Kaufmann et al., 2012), KL-UCB
(Cappé et al., 2013) or IMED (Honda & Takemura, 2015) in
order to reach asymptotic optimality. Recently, subsampling
algorithms like SSMC (Chan, 2020) and RB-SDA (Baudry
et al., 2020) have been proved to be optimal without know-
ing exactly PΘ. This means that the same algorithm can run
on Bernoulli or Gaussian distributions and achieve optimal-
ity. We ﬁrst prove that LB-SDA matches these theoretical
guarantees. We denote kl(µ, µ(cid:48)) the Kullback-Leibler diver-
gence between two distributions of mean µ and µ(cid:48) in the
exponential family PΘ.
Theorem 1 (Asymptotic optimality of LB-SDA). For any
bandit model ν = (ν1, . . . , νK) ⊂ P K
Θ where PΘ is any
one-parameter exponential family of distributions, the regret
of LB-SDA satisﬁes, for all ε > 0,

R(T ) ≤

(cid:88)

k:µk<µ(cid:63)

1 + ε
kl(µk, µ(cid:63))

log(T ) + C(ν, ε) ,

where C(ν, ε) is a problem-dependent constant.

Proof sketch We assume without loss of generality that
there is a unique optimal arm denoted k(cid:63). The analysis of
Chan (2020) and Baudry et al. (2020) shows that for any
SDA algorithm the number of pulls of a suboptimal arm
may be bounded as follow.

Lemma 1 (Lemma 4.1 in Baudry et al. (2020)). For any
suboptimal arm k (cid:54)= k(cid:63), the expected number of pulls of k
is upper bounded by

E[Nk(T )] ≤

1 + ε
kl(µk, µ(cid:63))

log(T ) + Ck(ν, ε)

+ 32

T
(cid:88)

r=1

P(Nk(cid:63) (r) ≤ (log r)2) ,

(3)

where Ck(ν, ε) is a problem-dependent constant.

The next step consists in upper bounding the probability
that the best arm is not pulled "enough" during a run of
the algorithm. This part is more challenging and relies
on the notion of diversity in the subsamples provided by
the subsampling algorithm. This notion was introduced by
Baransi et al. (2014) to analyze the Best Empirical Sampled
Average (BESA) algorithm. Intuitively, random block sam-
pling (Baudry et al., 2020) or sampling without replacement
(Baransi et al., 2014) explore different part of the history
thus bringing diversity in the duels. Unfortunately, this prop-
erty is not satisﬁed by deterministic samplers. Nonetheless,

with a careful examination of the relation implied by the
deterministic nature of last-block subsampling it is possible
to prove that the number of pulls of the optimal arm is large
enough with high probability.

Lemma 2. The probability that the optimal arm is not
pulled enough by LB-SDA can be upper bounded as follows

+∞
(cid:88)

r=1

P (cid:0)Nk(cid:63) (r) ≤ (log r)2(cid:1) ≤ Ck(cid:63) (ν) ,

for some constant Ck(cid:63) (ν).

Plugging the result of Lemma 2 in Lemma 1 gives the
asymptotic optimality of LB-SDA (Theorem 1). The proof
of Lemma 2 is reported in Appendix A.

3.3. Memory-Limited LB-SDA

One of our main motivations for studying LB-SDA is its
simplicity and efﬁciency. Yet, all existing subsampling
algorithms (Baransi et al., 2014; Chan, 2020; Baudry et al.,
2020) as well as the vanilla version of LB-SDA have to store
the entire history of rewards for all the arms. In this section,
we explain how to modify LB-SDA to reduce the storage
cost while preserving the theoretical guarantees.

The fact that LB-SDA is asymptotically optimal means that,
when T is large, the arm with the largest mean is most often
the leader with all of its challengers having a number of
pulls that is of order O(log T ) only. With duels based on
the last block, this would mean in particular that only the
last O(log T ) observations from the optimal arm should be
stored and that previous observations will never be used
again in practice. Based on this intuition, one might think
that keeping only log(T )/(µ(cid:63)−µk)2 observations is enough
for LB-SDA. However, this could only be done with the
knowledge of the gaps that are unknown.

We propose instead to limit the storage memory of each arm
at round r to a value of the form

mr = max (cid:0)M, (cid:6)C(log r)2(cid:7)(cid:1) ,
where C > 0 and M ∈ N. M ensures that a minimum
number of samples are stored during the ﬁrst few rounds.
Following the deﬁnition of Agrawal & Goyal (2012), we
then deﬁne the set of saturated arms at a round r as

Sr = {k ∈ {1, . . . , K} : Nk(r) ≥ mr} .

The only modiﬁcation of LB-SDA is the following: at each
round r, if a saturated arm is pulled then the newly collected
observation replaces the oldest observation in its history.
The pseudo code of LB-SDA with Limited Memory (LB-
SDA-LM) is given in Appendix B and the following result
shows that it keeps the same asymptotical performance as
LB-SDA under general assumptions on mr.

On Limited-Memory Subsampling Strategies for Bandits

Theorem 2 (Asymptotic optimality of LB-SDA with Lim-
ited Memory). For any bandit model ν = (ν1, . . . , νK) ⊂
P K
Θ where PΘ is any one-parameter exponential family of
distributions, if mr/ log(r) → ∞, the regret of memory-
limited LB-SDA satisﬁes, for all ε > 0,

RT ≤

(cid:88)

k:µk<µ(cid:63)

1 + ε
kl(µk, µ(cid:63))

log(T ) + C (cid:48)(ν, ε, M) ,

where M = (m1, m2, . . . , mT ) denotes the sequence
(mr)r∈N and C (cid:48)(ν, ε, M) is a problem-dependent constant.

The proof of this theorem is reported in Appendix B, which
provides precise estimates of the dependence of C (cid:48)(ν, ε, M)
with respect to the parameters, and in particular, with respect
to the sequence M. Note that LB-SDA-LM remains an
anytime algorithm because the storage constraint does not
depend on the time horizon T but only on the current round.

3.4. Storage and Computational Cost

To the best of our knowledge, LB-SDA-LM is the only
subsampling bandit algorithm that does not require to store
the full history of rewards. We report in Table 1 estimates of
the computational cost of LB-SDA-LM and its competitors.

Table 1. Storage and computational cost at round T for existing
subsampling algorithms.

Algorithm

Storage

Comp. cost
Best-Worst case

BESA
(Baransi et al., 2014)

SSMC
(Chan, 2020)

RB-SDA
(Baudry et al., 2020)

LB-SDA
(this paper)

LB-SDA-LM
(this paper)

O(T )

O((log T )2)

O(T )

O(1)-O(T )

O(T )

O(log T )

O(T )

O(1)-O(log T )

O((log T )2) O(1)-O(log T )

The computational cost can be broken into two parts: (a)
the subsampling cost and (b) the computation of the means
of the samples. We assume that drawing a sample of size n
without replacement has O(n) cost and that computing the
mean of this subsample costs another O(n). Furthermore, at
round T , each challenger to the best arm has about O(log T )
samples. This gives an estimated cost of O (cid:0)(log T )2(cid:1) for
BESA (Baransi et al., 2014). For RB-SDA (Baudry et al.,
2020) the estimated cost is O(log(T )), because the sam-
pling cost for random block sampling is O(1) and only the
sample mean has to be recomputed at each round.

For the three deterministic algorithms (namely SSMC (Chan,
2020), LB-SDA, LB-SDA-LM), when the leader arm wins
all its duels, its sample mean can be updated sequentially at
cost O(1). This is the best case in terms of computational
cost. However, when a challenger arm is pulled, SSMC
requires a full screening of the leader’s history, with O(T )
cost, while LB-SDA and LB-SDA-LM only need the com-
putation of the mean of the last O(log T ) samples from the
leader.

4. LB-SDA in Non-Stationary Environments

In stationary environments, LB-SDA achieves optimal re-
gret rates, even when its decisions are constrained to use
at most O((log T )2) observations. One might think that
this argument itself is sufﬁcient to address non-stationary
scenarios as the duels are performed mostly using recent ob-
servations. However, the latter is only true for the best arm
and in the case where an arm that has been bad for a long
period of time suddenly becomes the best arm, adapting to
the change would still be prohibitively slow. For this reason,
LB-SDA has to be equipped with an additional mechanism
to perform well in non-stationary environments.

4.1. SW-LB-SA: LB-SDA with a Sliding-Window

We keep a round-based structure for the algorithm, where,
at each round r, duels between arms are performed and the
algorithm subsequently selects the subset of arms Ar that
will be pulled. In contrast to Section 3.3, where a constraint
on storage related to the number of pulls was added, here,
we use a sliding window of length τ to limit the historical
data available to the algorithm to that of the last τ rounds.

(cid:96)(r − 1)
(cid:96)(r − 1)
(cid:96)(r − 1)

n
i
w

l
o

o

s

e

round rrr

(cid:96)(r)
(cid:96)(r)
(cid:96)(r)

r − τ
r − τ
r − τ

r − 1
r − 1
r − 1

r − τ + 1
r − τ + 1
r − τ + 1

rrr

Figure 1. Illustration of a passive leadership takeover with a slid-
ing window τ = 4 when the standard deﬁnition of leader is used.
The bold rectangle correspond to the leader. A blue square is added
when an arm has an observation for the corresponding round and
the red square correspond to the information that will be lost at the
end of the round due to the sliding window.

Modiﬁed leader deﬁnition The introduction of a sliding
window requires a new deﬁnition for the leader. By analogy
with the stationary case, the leader could be deﬁned as the
arm that has been pulled the most during the τ last rounds.

On Limited-Memory Subsampling Strategies for Bandits

Algorithm 2 SW-LB-SDA
Input: K arms, horizon T , τ length of sliding window
Initialization: t ← 1, r ← 1, ∀k ∈ {1, ..., K}, Nk ← 0,

N τ

k ← 0

while t < T do

A ← {}, (cid:96) ← leader(N, Y, τ )
if r = 1 then

A ← {1, . . . , K} (Draw each arm once)

else

for k (cid:54)= (cid:96) ∈ {1, ..., K} do

if N τ

k ≤ (cid:112)log(τ ) or Dτ
A ← A ∪ {k}

k (r) = 1 then

else

k = ¯Yk,Nk−N τ
(cid:98)µτ
k +1:Nk
N = min(N τ
k , N τ
(cid:96) )
(cid:96),k = ¯YN(cid:96)−N +1:N(cid:96)
(cid:98)µτ
if (cid:98)µτ

k ≥ (cid:98)µτ
(cid:96),k then
A ← A ∪ {k}

if |A| = 0 then
A ← {(cid:96)}

for k ∈ A do

Pull arm k, observe reward Yk,Nk+1
Update Nk ← Nk + 1, N τ

k ← N τ

k + 1, t ← t + 1

for k ∈ {1, ..., K} do

if k ∈ Ar−τ +1 then
k ← N τ
k − 1

N τ

r ← r + 1

s=r−τ

1 (r − 1) = N τ

k (r) = (cid:80)r−1

However, with the inclusion of the sliding window, a new
phenomenon, which we call passive leadership takeover,
1 (k ∈ As+1),
can occur. Let us deﬁne N τ
the number of times arm k has been pulled during the last
τ rounds and consider a situation with 3 arms {1, 2, 3}.
Assume that the leader is arm 1 and at a round (r − 1)
we have N τ
2 (r − 1). If the leader has been
pulled τ rounds away and wins its duel against arm 2 but
looses against arm 3, only arm 3 will be pulled at round r.
Consequently, at round r, arm 2 will have a strictly larger
number of pulls than arm 1 without having actually defeated
the leader. This situation, illustrated on Figure 1, is not
desirable as it can lead to spurious leadership changes. We
ﬁx this by imposing that any arm has to defeat the current
leader to become the leader itself. Deﬁne,

Br = {k ∈ Ar+1 ∩ {N τ

k (r + 1) ≥ min(r, τ )/K}} .

Then for any r ∈ N, the leader at round r + 1 is deﬁned as
(cid:96)τ (r+1) = argmaxk∈{1,...,K}N τ
(cid:96)τ (r)(r+1) <
min(r, τ )/(2K) and the argmax is taken over Br ∪ {(cid:96)τ (r)}
otherwise. This modiﬁed deﬁnition of the leader ensures
that an arm can become the leader only after earning at

k (r+1) if N τ

least τ /K samples and winning a duel against the current
leader, or if the leader loses a lot of duels and its number
of samples falls under a ﬁxed threshold. Thanks to this
deﬁnition it holds that N τ
(cid:96)τ (r)(r) ≥ min(r, τ )/(2K). More
details are given in Appendix C.

k (r), satisfying Dτ

Additional diversity ﬂags As in the vanilla LB-SDA, we
use a sampling obligation to ensure that each arm has a
minimal number of samples. However, in contrast to the
stationary case, this very limited number of forced samples
may not be sufﬁcient to guarantee an adequate variety of du-
els, due to the forgetting window. To this end, the sampling
obligation is coupled with a diversity ﬂag. We deﬁne it as a
binary random variable Dτ
k (r) = 1 only
when, for the last (cid:100)(K −1)(log τ )2(cid:101) rounds the three follow-
ing conditions are satisﬁed: 1) some arm k(cid:48) (cid:54)= k has been
leader during all these rounds, 2) k(cid:48) has not been pulled, and
k (r) ≤ (log τ )2. In
3) k has not been pulled and satisfy N τ
practice, there is a very low probability that these conditions
are met simultaneously but this additional mechanism is
required for the theoretical analysis. Note that the diversity
ﬂags have no impact on the computational cost of the al-
gorithm as they require only to store the number of rounds
since the last draw of the different arms (which can be up-
dated recursively) as well as the last leader takeover. Arms
that raise their diversity ﬂag are automatically added to the
set of pulled arms.

Bringing these parts together, gives the pseudo-code of SW-
LB-SDA in Algorithm 2.

4.2. Regret Analysis in Abruptly Changing

Environments

In this section we aim at upper bounding the dynamic regret
in abruptly changing environments, as deﬁned in Section 2.
Our main result is the proof that the regret of SW-LB-SDA
matches the asymptotic lower bound of Garivier & Moulines
(2011).
Theorem 3 (Asymptotic optimality of SW-LB-SDA). If the
time horizon T and number of breakpoint ΓT are known,
choosing τ = O((cid:112)T log(T )/ΓT ) ensures that the dynamic
regret of SW-LB-SDA satisﬁes

RT = O((cid:112)T ΓT log T ) .

To prove this result we only need to assume that, during
each stationary period, the rewards come from the same
one-parameter exponential family of distributions. In con-
trast, current state-of-the-art algorithms for non-stationary
bandits typically require the assumption that the rewards are
bounded to obtain similar guarantees. Hence, this result is
of particular interest for tasks involving unbounded reward
distributions that can be discrete (e.g Poisson) or contin-
uous (e.g Gaussian, Exponential). SW-LB-SDA can also

On Limited-Memory Subsampling Strategies for Bandits

be used for general bounded rewards with the same perfor-
mance guarantees by using the binarization trick (Agrawal
& Goyal, 2013). Note however, that the knowledge of the
horizon T and the estimated number of change point ΓT is
still required to obtain optimal rates, which is an interest-
ing direction for future works on this approach (Auer et al.,
2019; Besson et al., 2020). We provide a high-level outline
of the analysis behind Theorem 3 and the complete proof is
given in Appendix C.

Regret decomposition For the ΓT + 1 stationary phases
[tφ, tφ+1−1] with φ ∈ {1, . . . , ΓT }, we deﬁne rφ as the ﬁrst
round where an observation from the phase φ was pulled.
Introducing the gaps ∆φ
k = µ∗
− µtφ,k and denoting the
tφ
optimal arm k(cid:63)
φ, we can rewrite the regret as




ΓT(cid:88)

rφ+1−2
(cid:88)

RT = E



(cid:88)

1 (k ∈ Ar+1) ∆φ
k



φ=1

r=rφ−1

k(cid:54)=k(cid:63)
φ

ΓT(cid:88)

(cid:88)

=

φ=1

k(cid:54)=k∗
φ

E[N φ

k ]∆φ
k ,

k = (cid:80)rφ+1−2

where we deﬁne N φ
1(k ∈ Ar+1) the number
r=rφ−1
of pulls of an arm k during a phase φ when it is suboptimal.
Note that the quantities tφ, rφ and ∆φ
k for the different
stationary phases φ are only required for the theoretical
analysis and the algorithm has no access to those values. We
highlight that the sequence (rφ)φ≥1 is a random variable
that depends on the trajectory of the algorithm. However, we
show in Appendix C that this causes no additional difﬁculty
for upper bounding the regret. We introduce δφ = tφ+1 − tφ
the length of a phase φ. Combining elements from the proofs
of Garivier & Moulines (2011) and that of Theorem 1, we
ﬁrst provide an upper bound on E[N φ
k ] for any suboptimal
arm k during the phase φ as
δφAφ,τ
k
τ

k,2 + cφ,τ
k,3 .

k,1 + cφ,τ

k ] ≤ 2τ +

+ cφ,τ

E[N φ

k = bφ
k > 0, along with the terms cφ,τ

In this decomposition we deﬁne Aφ,τ
k log(τ ) for some
constant bφ
k,2 and cφ,τ
k,1 , cφ,τ
k,3 ,
which all represents a different technical aspect of the regret
decomposition of SW-LB-SDA. Before interpreting them
we start with their formal deﬁnition,





cφ,τ
k,1 = E

rφ+1−2
(cid:88)

1

(cid:16)

(cid:17)
k (r, Aφ,τ
Gτ
k )



 ,

r=rφ+2τ −2





cφ,τ
k,2 = E

rφ+1−2
(cid:88)

r=rφ+2τ −2





cφ,τ
k,3 = E

rφ+1−2
(cid:88)

r=rφ+2τ −2

1 (cid:0)(cid:96)τ (r) = k∗

φ, Dτ

k (r) = 1(cid:1)



 ,



1 (cid:0)(cid:96)τ (r) (cid:54)= k∗

φ

(cid:1)

 ,

where Gτ

k (r, n) is equal to

{k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ n, Dτ

k (r) = 0} .

k

Bounding individual terms The three terms have intu-
itive interpretation and summarize well the technical contri-
butions behind Theorem 3. To some extent they all rely on
the notion of saturated arms deﬁned in Section 3.3 and that
we reﬁne in Appendix C for the problems considered in this
section (mainly by properly tuning Aφ,τ
in the theoretical
analysis).
First, cφ,τ
k,1 is an upper bound on the expectation of the num-
ber of times a saturated suboptimal arm can defeat the opti-
mal leader (i.e (cid:96)τ (r) = k∗
φ). To prove this result we establish
a new concentration inequality for Last-Block Sampling in
the context of SW-LB-SDA.
The second term cφ,τ
k,2 controls the probability that the diver-
sity ﬂag is activated when the optimal arm k∗
φ is the leader.
We prove that if this event happen, then k∗
φ has necessarily
lost at least one duel against a saturated sub-optimal arm,
and that this event has only a low probability.
The term cφ,τ
k,3 is the most difﬁcult to handle, the main chal-
lenge is to upper bound the probability that the optimal arm
is not saturated after a large number of rounds.

In Appendix C we provide the complete analysis of each of
these terms and a full description of all the technical results
that led to Theorem 3.

5. Experiments

Limiting the storage in stationary environments.
In our
ﬁrst experiment1 reported on Figure 3, we compare LB-SDA
and LB-SDA-LM on a stationary instance with K = 2 arms
with Bernoulli distributions for a horizon T = 10000. We
add natural competitors (Thompson Sampling (Thompson,
1933), kl-UCB (Cappé et al., 2013)), that know ahead of
the experiment that the reward distributions are Bernoulli
and are tuned accordingly. The arms satisfy (µ1, µ2) =
(0.05, 0.15) with a gap ∆ = 0.1. We run LB-SDA-LM
with a memory limit mr = log(r)2 + 50, which gives a
storage ranging from 50 to 150 samples (much smaller than
the horizon T = 10000). The regret are averaged on 2000
independent replications and the upper and lower quartiles
are reported. In this setup LB-SDA-LM performs similarly
to KL-UCB, and the impact of limiting the memory is mild,
when compared to LB-SDA. This illustrates that even with
relatively small gaps (here 0.1), a substantial reduction of
the storage can be done with only minor loss of performance
with LB-SDA-LM.

1The code for obtaining the different ﬁgures reported in
the paper is available at https://github.com/YRussac/
LB-SDA.

On Limited-Memory Subsampling Strategies for Bandits

Figure 2. Evolution of the means: Left, Bernoulli arms (Fig. 4); Right, Gaussian arms (Figs. 5 and 6).

Figure 3. Cost of storage limitation on a Bernoulli instance. The
reported regret are averaged over 2000 independent replications.

Figure 4. Performance on a Bernoulli instance averaged on 2000
independent replications.

Empirical performance in abruptly changing environ-
ments.
In the second experiment, we compare differ-
ent state-of-the-art algorithms on a problem with K = 3
Bernoulli-distributed arms. The means of the distributions
are represented on the left hand side of Figure 2 and the
performance averaged on 2000 independent replications
are reported on Figure 4. Two changepoint detection al-
gorithms, CUSUM (Liu et al., 2017) and M-UCB (Cao
et al., 2019) are compared with progressively forgetting
policies based on upper conﬁdence bound, SW-klUCB and
D-klUCB adapted from Garivier & Moulines (2011), or
Thompson sampling, DTS (Raj & Kalyani, 2017) and SW-
TS (Trovo et al., 2020). We also add EXP3S (Auer et al.,
2002) designed for adversarial bandits and our SW-LB-SDA
algorithm for the comparison. The different algorithms
make use of the knowledge of T and ΓT .

To allow for fair comparison, we use for SW-LB-SDA, the
same value of τ = 2(cid:112)T log(T )/ΓT that is recommended
for SW-UCB (Garivier & Moulines, 2011). D-UCB uses the
discount factor suggested by Garivier & Moulines (2011),

1/(1 − γ) = 4(cid:112)T /ΓT . The changepoint detection algo-
rithms need extra information such has the minimal gap for
a breakpoint and the minimum length of a stationary phase.
For M-UCB, we set w = 800 and b = (cid:112)w/2 log(2KT 2)
as recommended by Cao et al. (2019) but set the amount
of exploration to γ = (cid:112)KΓT log(T )/T following Besson
et al. (2020). In practice, using this value rather than the
theoretical suggestion from Cao et al. (2019) improved
signiﬁcantly the empirical performance of M-UCB for
the horizon considered here. For CUSUM, α and h are
tuned using suggestions from Liu et al. (2017), namely
α = (cid:112)ΓT /T log(T /ΓT ) and h = log(T /ΓT ). On this
speciﬁc instance, using ε = 0.05 (to satisfy Assumption 2
of Liu et al. (2017)) and M = 50 gives good performance.
For the EXP3S algorithm, following (Auer et al., 2002) the
parameters α and γ are tuned as follows: α = 1/T and
γ = min(1, (cid:112)K(e + ΓT log(KT )/((e − 1)T ).
This problem is challenging because a policy that focuses
on arm 1 to minimize the regret in the ﬁrst stationary phase
also has to explore sufﬁciently to detect that the second arm
is the best in the second phase. SW-LB-SDA has perfor-

On Limited-Memory Subsampling Strategies for Bandits

mance comparable to the forgetting TS algorithms and is
the best performing algorithm in this scenario. Note that
both TS algorithms use the assumption that the arms are
Bernoulli whereas SW-LB-SDA does not. SW-klUCB per-
forms better than D-klUCB and its regret closely matches
the one from the changepoint detection algorithms. By ob-
serving the lower and the upper quartiles, one sees that the
performance of CUSUM vary much more than the other
algorithms depending on its ability to detect the breakpoints.
Finally, EXP3S, which can adapt to more general adversar-
ial settings, lags behind the other algorithms in this abruptly
changing stochastic environment.

Figure 6. Performance on a Gaussian instance with time dependent
standard deviations averaged on 2000 independent replications.

Changes affecting the variance. The last experiment fea-
tures the same Gaussian means but with different standard
errors. The standard error takes the values 0.5, 0.25, 1 and
0.25, respectively, in the four stationary phases. The al-
gorithms based on upper conﬁdence bound are given the
maximum standard error σ = 1, whereas SW-LB-SDA is
not provided with any information of this sort. Figure 6
shows that the non-parametric nature of SW-LB-SDA is ef-
fective, with a signiﬁcant improvement over state-of-the-art
methods in such settings.

Acknowledgements

The PhD of Dorian Baudry is funded by a CNRS80 grant.

Figure 5. Performance on a Gaussian instance with a constant stan-
dard deviation of σ = 0.5 averaged on 2000 independent runs.

In the third experiment with ΓT = 3 breakpoints, the K = 3
arms comes from Gaussian distributions with a ﬁxed stan-
dard deviation of σ = 0.5 but time dependent means. The
evolution of the arm’s means is pictured on the right of
Figure 2 and Figure 5 displays the performance of the al-
gorithms. CUSUM and M-UCB can not be applied in this
setting because CUSUM is only analyzed for Bernoulli dis-
tributions and M-UCB assume that the distributions are
bounded. Even if no theoretical guarantees exist for Thomp-
son sampling with a sliding window or discount factors,
when the distribution are Gaussian with known variance,
we add them as competitors. The analysis of SW-UCB
and D-UCB was done under the bounded reward assump-
tion but the algorithms can be adapted to the Gaussian
case. Yet, the tuning of the discount factor and the slid-
ing window had to be adapted to obtain reasonable perfor-
mance, using τ = 2(1 + 2σ)(cid:112)T log(T )/ΓT for D-UCB
and γ = 1 − 1/(4(1 + 2σ))(cid:112)ΓT /T for SW-UCB (consid-
ering that, practically, most of the rewards lie under 1 + 2σ).
For reference, Figure 5 also displays the performance of the
UCB1 algorithm that ignores the non-stationary structure.
Clearly, SW-LB-SDA, in addition of being the only algo-
rithm analyzed in this setting with unbounded rewards, also
has the best empirical performance.

On Limited-Memory Subsampling Strategies for Bandits

References

Agrawal, S. and Goyal, N. Analysis of thompson sampling
for the multi-armed bandit problem. In Conference on
learning theory, pp. 39–1. JMLR Workshop and Confer-
ence Proceedings, 2012.

Agrawal, S. and Goyal, N. Further optimal regret bounds
In Artiﬁcial intelligence and

for thompson sampling.
statistics, pp. 99–107. PMLR, 2013.

Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time
analysis of the multiarmed bandit problem. Machine
learning, 47, 2002.

Auer, P., Gajane, P., and Ortner, R. Adaptively tracking the
best bandit arm with an unknown number of distribution
changes. In Conference on Learning Theory, pp. 138–158,
2019.

Baransi, A., Maillard, O.-A., and Mannor, S. Sub-sampling
In Joint European Confer-
for multi-armed bandits.
ence on Machine Learning and Knowledge Discovery
in Databases, pp. 115–131. Springer, 2014.

Baudry, D., Kaufmann, E., and Maillard, O.-A. Sub-
sampling for efﬁcient non-parametric bandit exploration.
Advances in Neural Information Processing Systems, 33,
2020.

Bergemann, D. and Välimäki, J. Learning and strategic pric-
ing. Econometrica: Journal of the Econometric Society,
pp. 1125–1149, 1996.

Besbes, O., Gur, Y., and Zeevi, A. Stochastic multi-armed-
bandit problem with non-stationary rewards. In Advances
in neural information processing systems, pp. 199–207,
2014.

Besson, L., Kaufmann, E., Maillard, O.-A., and Seznec, J.
Efﬁcient change-point detection for tackling piecewise-
stationary bandits. Prepint, December 2020.

Cao, Y., Wen, Z., Kveton, B., and Xie, Y. Nearly optimal
adaptive procedure with change detection for piecewise-
In The 22nd International Confer-
stationary bandit.
ence on Artiﬁcial Intelligence and Statistics, pp. 418–427.
PMLR, 2019.

Cappé, O., Garivier, A., Maillard, O.-A., Munos, R., Stoltz,
G., et al. Kullback–leibler upper conﬁdence bounds for
optimal sequential allocation. The Annals of Statistics, 41
(3):1516–1541, 2013.

Chan, H. P. The multi-armed bandit problem: An efﬁcient
nonparametric solution. The Annals of Statistics, 48(1):
346–373, 2020.

Chen, Y., Lee, C.-W., Luo, H., and Wei, C.-Y. A new
algorithm for non-stationary contextual bandits: Efﬁcient,
optimal and parameter-free. In Conference on Learning
Theory, pp. 696–726. PMLR, 2019.

Eliashberg, J. and Jeuland, A. P. The impact of competi-
tive entry in a developing market upon dynamic pricing
strategies. Marketing Science, 5(1):20–36, 1986.

Garivier, A. and Moulines, E. On upper-conﬁdence bound
arXiv

policies for non-stationary bandit problems.
preprint arXiv:0805.3415, 2008.

Garivier, A. and Moulines, E. On upper-conﬁdence bound
policies for switching bandit problems. In International
Conference on Algorithmic Learning Theory, pp. 174–
188. Springer, 2011.

Gorre, M. E., Mohammed, M., Ellwood, K., Hsu, N., Paque-
tte, R., Rao, P. N., and Sawyers, C. L. Clinical resistance
to sti-571 cancer therapy caused by bcr-abl gene mutation
or ampliﬁcation. Science, 293(5531):876–880, 2001.

Honda, J. and Takemura, A. Non-asymptotic analysis of a
new bandit algorithm for semi-bounded rewards. Journal
of Machine Learning Research, 16:3721–3756, 2015.

Kaufmann, E., Korda, N., and Munos, R. Thompson sam-
pling: An asymptotically optimal ﬁnite-time analysis. In
Algorithmic Learning Theory - 23rd International Con-
ference, ALT, 2012.

Kveton, B., Szepesvari, C., Ghavamzadeh, M., and Boutilier,
C. Perturbed-history exploration in stochastic multi-
armed bandits. arXiv preprint arXiv:1902.10089, 2019a.

Kveton, B., Szepesvari, C., Vaswani, S., Wen, Z., Latti-
more, T., and Ghavamzadeh, M. Garbage in, reward
out: Bootstrapping exploration in multi-armed bandits.
In International Conference on Machine Learning, pp.
3601–3610. PMLR, 2019b.

Lai, T. L. and Robbins, H. Asymptotically efﬁcient adaptive
allocation rules. Advances in applied mathematics, 6(1):
4–22, 1985.

Li, L., Chu, W., Langford, J., and Wang, X. Unbiased
ofﬂine evaluation of contextual-bandit-based news article
recommendation algorithms. In Proceedings of the fourth
ACM international conference on Web search and data
mining, pp. 297–306, 2011.

Li, S., Karatzoglou, A., and Gentile, C. Collaborative ﬁl-
tering bandits. In Proceedings of the 39th International
ACM SIGIR conference on Research and Development in
Information Retrieval, pp. 539–548, 2016.

On Limited-Memory Subsampling Strategies for Bandits

Liu, F., Lee, J., and Shroff, N. A change-detection based
framework for piecewise-stationary multi-armed bandit
problem. arXiv preprint arXiv:1711.03539, 2017.

Raj, V. and Kalyani, S. Taming non-stationary bandits: A
bayesian approach. arXiv preprint arXiv:1707.09727,
2017.

Riou, C. and Honda, J. Bandit algorithms based on thomp-
son sampling for bounded reward distributions. In Algo-
rithmic Learning Theory, pp. 777–826. PMLR, 2020.

Seznec, J., Menard, P., Lazaric, A., and Valko, M. A single
algorithm for both restless and rested rotting bandits. In
International Conference on Artiﬁcial Intelligence and
Statistics, pp. 3784–3794. PMLR, 2020.

Thompson, W. R. On the likelihood that one unknown
probability exceeds another in view of the evidence of
two samples. Biometrika, 25(3/4):285–294, 1933.

Trovo, F., Paladino, S., Restelli, M., and Gatti, N. Sliding-
window thompson sampling for non-stationary settings.
Journal of Artiﬁcial Intelligence Research, 68:311–364,
2020.

Vermorel, J. and Mohri, M. Multi-armed bandit algorithms
and empirical evaluation. In European conference on
machine learning, pp. 437–448. Springer, 2005.

Wu, Q., Iyer, N., and Wang, H. Learning contextual bandits
in a non-stationary environment. In The 41st International
ACM SIGIR Conference on Research & Development in
Information Retrieval, pp. 495–504, 2018.

Yue, Y. and Joachims, T. Interactively optimizing informa-
tion retrieval systems as a dueling bandits problem. In
Proceedings of the 26th Annual International Conference
on Machine Learning. ACM, 2009.

Zelen, M. Play the winner rule and the controlled clinical
trial. Journal of the American Statistical Association, 64
(325):131–146, 1969.

On Limited-Memory Subsampling Strategies for Bandits

Organization of the appendix

The appendix is organized as follows:

• In Section A we provide some details on our analysis for the vanilla LB-SDA algorithm.

• In Section B explain how to adapt LB-SDA when a limited memory is used and derive an upper-bound for the regret of

this variant of LB-SDA.

• In Section C a detailed analysis of LB-SDA with a sliding window in any abruptly changing environment is proposed.

A. Analysis of LB-SDA

A.1. Proof of Lemma 2

Before establishing our main result for LB-SDA, we introduce the balance function of an arm, which was ﬁrst deﬁned in
(Baransi et al., 2014).

Assume that the K arm are characterized by the reward distributions (ν1, ..., νK). Assume that there is a unique optimal
arm associated to the arm k(cid:63).
Deﬁnition 1. Letting νk,j denote the distribution of the sum of j independent variables drawn from νk, and Fνk,j its
corresponding CDF. the balance function of arm k is

αk(M, j) = EX∼νk(cid:63) ,j

(cid:16)(cid:0)1 − Fνk,j (X)(cid:1)M (cid:17)

.

If we draw one sample from a distribution νk(cid:63),j, and M independent samples from another distribution νk,j, the balance
function αk(M, j) quantiﬁes the probability that each sample from νk,j is larger than the sample from νk(cid:63),j. The index j
represents itself the fact that these variables are built as the sum of j independent random variables from the same distribution
(respectively νk(cid:63) and νk). This function has been studied in detail in (Baudry et al., 2020) (Appendix G and H), and we will
use its properties to prove the following result.

Lemma 2. The probability that the optimal arm is not pulled enough by LB-SDA can be upper bounded as follows

+∞
(cid:88)

r=1

P (cid:0)Nk(cid:63) (r) ≤ (log r)2(cid:1) ≤ Ck(cid:63) (ν) ,

for some constant Ck(cid:63) (ν).

Proof. The main problem with the last block sampling is that if both the leader and a given challenger are not played for
some time, the index used in their duels remain the same due to the deterministic nature of the sampler. As a consequence
this challenger is never played as long as the leader remains the same. If this situation occur too often, this would limit the
diversity for the duels played by the optimal arm k(cid:63) against suboptimal leaders. We show that this is not possible by proving
that the leader will be played a large number of times, which necessarily brings some diversity. To measure this, we deﬁne
the quantity of duels won by the leaders at the different rounds as

Wr = 1 +

r−1
(cid:88)

s=1

1(As+1 = {(cid:96)(s)}) ,

where we added 1 to consider the ﬁrst round where every arm is pulled once. For any trajectory this quantity is linear in r.
Lemma 3. With Wr = 1 + (cid:80)r−1

1(As+1 = {(cid:96)(s)}), for any round r under LB-SDA it holds that

s=1

Wr = N(cid:96)(r)(r) ≥ r/K .

Before using Lemma 3, we recall the sampling obligation rule introduced in Section 3. and that we use to consider rounds
log r samples is pulled. We
where the optimal arm has enough samples. At any round r each arm with less than f (r) =

√

On Limited-Memory Subsampling Strategies for Bandits

focus on rounds where we are sure that arm k(cid:63) has been pulled "enough", and compute the probability that it has lost a
lot of duels after this moment. In particular, we consider ar as the smallest round satisfying f (ar) ≥ f (r) − 1, ensuring
Nk(cid:63) (ar) ≥ (cid:98)f (r) − 1(cid:99). This round is exactly (cid:100)f −1(f (r) − 1)(cid:101), that can be computed as

f −1(f (r) − 1) = exp (cid:0)(f (r) − 1)2(cid:1)

= exp (cid:0)f (r)2 + 1 − 2f (r)(cid:1)
= f −1(f (r)) exp(−2f (r) + 1)
= r × exp(−2f (r) + 1) .

This means that for any γ ∈ (0, 1), if r is large enough to satisfy f (r) ≥ 1−log γ
then ar ≤ γr. For the rest of the proof we
consider the number of duels lost by the arm k(cid:63) after the round ar against unique subsamples of a suboptimal leader. The
number of duels won by the leader between the rounds ar and r is equal to Wr − War . Out of those duels, at most (log r)2
of them can concern the optimal arm k(cid:63) because Nk(cid:63) (r) ≤ log(r)2. Consequently, there is at least Wr − War − (log r)2
duels won by a suboptimal leader between rounds ar and r. Using Lemma 3 and War ≤ ar one has,

2

Wr − War − (log r)2 ≥

≥

r
K
r
K

− ar − (log r)2

− γr − (log r)2 .

To simplify the expression we just write that for any β ∈ (0, 1) there exists a constant r(β, K) satisfying ∀r ≥ r(β, K),

Wr − War − (log r)2 ≥ β

r
K

.

(4)

Under Nk(cid:63) (r) ≤ (log r)2 we are sure that there exists some j ∈ {1, ..., (cid:98)(log r)2(cid:99)} such that a fraction 1/(log r)2 of the
duels counted above have been played with Nk(cid:63) (r) = j. Let us denote (cid:102)Wr = Wr − War − (log r)2 and show this by
contradiction. Out of those duels, we denote (cid:102)Wr,j the number of duels played with Nk(cid:63) (r) = j. If we assume that for all
j ≤ (cid:98)(log r)2(cid:99), there is strictly less than

r
K duels played with Nk(cid:63) (r) = j. The following would hold,

β
(log r)2

Wr − War − (log r)2 = (cid:102)Wr =

(cid:98)(log r)2(cid:99)
(cid:88)

(cid:102)Wr,j <

(cid:98)(log r)2(cid:99)
(cid:88)

j=1

j=1

β
(log r)2

r
K

< β

r
K

.

There is a contradiction with Equation (4) and means there is a j ≤ (cid:98)(log r)2(cid:99) and βr/((log r)2K) duels such that k(cid:63)
competes using its same block of observations of size j.

Furthermore, with the same argument we are sure that a fraction 1/(K − 1) of these duels is played against the same leader
k ∈ {2, . . . , K}. We would now like to obtain duels with non-overlapping blocks. Even if the blocks are all consecutive,
waiting for j steps is enough to ensure that they are not overlapping. Taking a fraction 1/j of the duels from the previous
subsets is hence enough to guarantee this.

Finally, we conclude that for any β ∈ (0, 1) there exists a constant r(β, K) such that for any round r > r(β, K), under the
event {N1(r) ≤ (log r)2} there exists some k ∈ {2, . . . , K} and some j ∈ {(cid:98)f (r) − 1(cid:99), (cid:98)(log r)2(cid:99)} such as arm k(cid:63) lost at
K(K−1)(log r)2j duels against non-overlapping blocks of arm k while k is the leader and k(cid:63) has exactly j observations.
least β
This term correspond exactly to the balance function αk(M, j) from Deﬁnition 1, with M = β
K(K−1)(log r)2j , hence we
can upper bound

r

r

T
(cid:88)

r=1

P (cid:0)Nk(cid:63) (r) ≤ (log r)2(cid:1) ≤ r(β, K) +

K
(cid:88)

T
(cid:88)

(cid:98)(log r)2(cid:99)
(cid:88)

k=2

r=r(β,K)

j=(cid:98)log(r)−1(cid:99)

(cid:18)

αk

β

r
K(K − 1)(log r)2j

(cid:19)

, j

.

Remark 1. The fact that the duels concern non-overlapping blocks of arm k is necessary to obtain independent samples. It
is also important that those duels are based on exactly j observations in order to introduce the balance function.

We conclude the proof using the following lemma which is proved in the next section.

Lemma 4. If the arms k and k(cid:63) come from the same one-parameter exponential family of distributions it holds that

On Limited-Memory Subsampling Strategies for Bandits

T
(cid:88)

(cid:98)(log r)2(cid:99)
(cid:88)

r=r(β,K)

j=(cid:98)log(r)−1(cid:99)

(cid:18)

αk

β

r
K(K − 1)(log r)2j

(cid:19)

, j

= O(1) .

A.2. Proof of Auxiliary Results
Lemma 3. With Wr = 1 + (cid:80)r−1

s=1

1(As+1 = {(cid:96)(s)}), for any round r under LB-SDA it holds that

Wr = N(cid:96)(r)(r) ≥ r/K .

Proof. We consider any trajectory of the bandit algorithm. For this trajectory we consider the sequence of the rounds where
a change of leader occurred and write them as the (potentially inﬁnite) set Y = [r0, r1, r2, . . . ]. These are basically all the
rounds r satisfying (cid:96)(r) (cid:54)= (cid:96)(r − 1). r0 = 1 as it is the ﬁrst round where we start deﬁning the leader in the algorithm, and it
holds that N(cid:96)(1)(1) = 1 as every arm is drawn once at the ﬁrst round. As the leader was not deﬁned before it holds that
W1 = 1 = N(cid:96)(1)(1) so the property holds in r0. As a ﬁrst step, we show that the property is valid for all ri when i ∈ N. Let
i ∈ N, we assume that the property holds in ri and we consider the round ri+1. It holds that

Wri+1 = Wri +

ri+1−1
(cid:88)

s=ri

1(As+1 = (cid:96)(s)) .

The sum is exactly the number of duels won by the arm that is leader during the interval [ri, ri+1 − 1] and it holds that
(cid:80)ri+1−1
1(As+1 = (cid:96)(s)) = N(cid:96)(ri)(ri+1) − N(cid:96)(ri)(ri). Furthermore, when a change of leader happens the number of
s=ri
elements of the new and former leader are the same, i.e. N(cid:96)(ri+1)(ri+1) = N(cid:96)(ri)(ri+1). This is due to the fact that when a
challenger reaches the history size of the leader then the arm with the largest mean is chosen as the leader. In particular,
if the challenger has a lower index than the leader at this round it cannot take the leadership at the next round as it will
otherwise lose its duel against the leader. For this reason, the only possibility for a challenger to take the leadership is to
reach to number of samples of the leader and to have a better index at this moment. We can write

Wri+1 = Wri +

ri+1−1
(cid:88)

s=ri

1(As+1 = {(cid:96)(s)})

= Wri + N(cid:96)(ri)(ri+1) − N(cid:96)(ri)(ri)
= Wri + N(cid:96)(ri+1)(ri+1) − N(cid:96)(ri)(ri)
= N(cid:96)(ri)(ri) + N(cid:96)(ri+1)(ri+1) − N(cid:96)(ri)(ri)
= N(cid:96)(ri+1)(ri+1) .

(Inductive step)

Therefore, if the property holds in ri then it holds in ri+1 which gives the result. The extension to any round is obtained
with similar arguments: ∀r /∈ Y, ∃i : ri < r < ri+1. Then we write

Wr =Wri +

r−1
(cid:88)

s=ri

1(As+1 = (cid:96)(s))

=N(cid:96)(ri)(ri) + (N(cid:96)(ri)(r) − N(cid:96)(ri)(ri))
=N(cid:96)(ri)(r) = N(cid:96)(r)(r) ,

where the last inequality comes from the fact that the leader is unchanged between the rounds ri and r. We conclude the
proof by using the property that as the leader always has a number of samples larger than r/K, as it is the arm with the
largest number of pulls at each round.

Lemma 4. If the arms k and k(cid:63) come from the same one-parameter exponential family of distributions it holds that

On Limited-Memory Subsampling Strategies for Bandits

T
(cid:88)

(cid:98)(log r)2(cid:99)
(cid:88)

r=r(β,K)

j=(cid:98)log(r)−1(cid:99)

(cid:18)

αk

β

r
K(K − 1)(log r)2j

(cid:19)

, j

= O(1) .

Before proving this result we prove an intermediary result that will also be useful to handle the balance function in the
proof for switching bandits in Appendix C. This result was already presented in (Chan, 2020), but we provide its proof for
completeness.

Lemma 5. Let F1 and F2 be the cdf of two distributions with respective means µ1 and µ2, µ1 > µ2. For any integer j ≥ 1
we denote F1,j and F2,j the cdf of the sum of j independent random variables drawn respectively from F1 and F2, and
α(M, j) = EX∼F1,j

(cid:0)(1 − F2,j(X))M (cid:1) the balance function of these two distributions. For any u ∈ R it holds that

α(M, j) ≤ F1,j(u) + (1 − F2,j(u))M .

Furthermore, if we assume that F1 and F2 come from the same one-parameter exponential family of distributions, for any
u ∈ [0, 1] satisfying F2(u) ≤ F2(µ2) the following result holds

α(M, j) ≤ e−jkl(θ2,θ1)u + (1 − u)M ,

where kl(θ2, θ1) is the Kullback-Leibler divergence between F2 and F1, expressed with their canonical parameters θ1 and
θ2.

Proof. We prove the ﬁrst result, that is valid for any distribution F1 and F2 and is a direct property of the deﬁnition of the
balance function. For u ∈ R, it holds that

(cid:90) +∞

α(M, j) =

(1 − F2,j(x))M dF1,j(x)

−∞

(cid:90) u

−∞

≤

(1 − F2,j(x))M dF1,j(x) +

(cid:90) +∞

u

(1 − F2,j(x))M dF1,j(x)

≤ F1,j(u) + (1 − F2,j(u))M .

We now assume that F1 and F2 come from the same one-parameter exponential family of distributions. In this case they
admit a density fθ(y) = f (y, 0)eη(θ)y−ψ(θ) for some natural parameter θ ∈ R. We write θ1 the parameter of F1, and θ2 the
parameter of F2. We then deﬁne some y1, ..., yj ∈ Rj. If the sequence y1, . . . , yj satisﬁes (cid:80)j

u=1 yu ≤ jµ2, it holds that

j
(cid:89)

u=1

fθ1(yu) =

j
(cid:89)

u=1

e(η(θ1)−η(θ2))yu−(ψ(θ1)−ψ(θ2))fθ2 (yu) ≤ e−jkl(θ2,θ1)

j
(cid:89)

u=1

fθ2 (yu) .

where we write kl(θ2, θ1) for the Kullback-Leibler divergence between F1 and F2. This inequality ﬁrst ensures that for all
x ≤ µ2

F1,j(x) ≤ e−jkl(θ2,θ1)F2,j(x) .

If we insert this expression in the ﬁrst result, we have that for any u ∈ [0, 1] satisfying F2(u) ≤ F2(µ2) the following result
holds

α(M, j) ≤ e−jkl(θ2,θ1)u + (1 − u)M .

Remark 2. The second result is particularly interesting because there is a trade-off in the choice of u. If we want to upper
bound α(M, j) by a relatively small quantity we need to choose small values for u, however if u is too small then the second

On Limited-Memory Subsampling Strategies for Bandits

term may become too large. In particular, making the approximation (1 − u)M ≈ e−M u provides an optimal scaling of u of
the form

u∗ =

jkl(θ2, θ1) + log M
M

,

and as a consequence

α(M, j) ≤ e−jkl(θ2,θ1)u∗ + (1 − u∗)M

≤

≤

=

jkl(θ2, θ1) + log M
M
jkl(θ2, θ1) + log M
M

e−jkl(θ2,θ1) + eM log

(cid:16)

1− jkl(θ2,θ1)+log M

M

(cid:17)

e−jkl(θ2,θ1) + C1

e−jkl(θ2,θ1)
M

jkl(θ2, θ1) + log M + C1
M

e−jkl(θ2,θ1) ,

for some constant C1.

With these technical results we can now ﬁnish the proof of Lemma 4 by simply replacing M by its value in the double sum.

Proof. We denote αk the balance function between the arm k(cid:63) and an arm k and want to upper bound

T
(cid:88)

(cid:98)(log r)2(cid:99)
(cid:88)

r=r(β,K)

j=(cid:98)

√

log r−1(cid:99)

(cid:18)

αk

β

r
K(K − 1)(log r)2j

(cid:19)

, j

.

We directly use the second result of Lemma 5, and choose the tuning of u from Remark 2.
αk

and try to extract the order of ar,j just in terms of r and j we obtain

r
K(K−1)(log r)2j , j

(cid:16)

(cid:17)

β

If we write ar,j =

ak,j = Or,j

(cid:18) j2(log r)2
r

e−jkl(θk,θk(cid:63) )

(cid:19)

.

We then upper bound the term in j2 by another (log r)4 using the upper limit on the sum on j, hence the only term left in j
is e−jkl(θ2,θ1), which sums in a term of order exp(−

log r). So we then obtain a term of the form

√

T
(cid:88)

(cid:98)(log r)2(cid:99)
(cid:88)

r=r(β,K)

j=(cid:98)

√

log r−1(cid:99)

(cid:18)

αk

β

r
K(K − 1)(log r)2j

(cid:19)

, j

= O

(cid:32) T

(cid:88)

r=1

(log r)6e−

√

log r

(cid:33)

r

.

We conclude, using that for any integer k > 1, (log r)k = o(e

√

log r). Hence

(log r)6e−

√

log r

r

(cid:18)

= o

1
r(log r)2

(cid:19)

,

which is the general term of a convergent series. Hence we ﬁnally obtain

T
(cid:88)

(cid:98)(log r)2(cid:99)
(cid:88)

r=r(β,K)

j=(cid:98)

√

log r−1(cid:99)

(cid:18)

αk

β

r
K(K − 1)(log r)2j

(cid:19)

, j

= O(1) .

B. LB-SDA with a limited memory

On Limited-Memory Subsampling Strategies for Bandits

In this section the variant of LB-SDA using a limited storage memory introduced in Section 3.3 is analyzed. After introducing
a few notations, we present a detailed version of the algorithm. We then provide a detailed proof of Theorem 2.

B.1. Notation for the Proof of Theorem 2

General notations for the stationary case:

• K number of arms

• νk distribution of the arm k, with mean µk. We assume that ∀k, νk ∈ PΘ, a one-parameter exponential family.

• We assume that µ1 = maxk∈[K] µk so we call the (unique) optimal arm "arm 1".

• Ik(x) some large deviation rate function of the arm k, evaluated in x. For one-parameter exponential families this

function will always be the KL-divergence between νk and the distribution from the same family with mean x.

• Nk(r) number of pull of arm k up to (and including) round r.

• Yk,i reward obtained at the i-th pull of arm k.

• ¯Yk,i mean of the i-th ﬁrst reward of arm k, ¯Yk,n:m mean of the rewards of k on a subset of indices n < m: ¯Yk,n:m =

1
m−n+1

(cid:80)m

i=n Yk,i. If m − n = s, then ¯Yk,s and ¯Yk,n:m have the same distribution.

• (cid:96)(r) leader at round r, (cid:96)(r) = argmax

Nk(r).

k∈{1,...,K}

• Ar set of arms pulled at a round r.

• RT regret up to (and including) round T .

Notations for the regret analysis, part relying on concentration:

• Z r = {(cid:96)(r) (cid:54)= 1}, the leader used at round r + 1 is suboptimal.

• Dr = {∃u ∈ {(cid:98)r/4(cid:99), ..., r} such that (cid:96)(u − 1) = 1}, the optimal arm has been leader at least once between (cid:98)r/4(cid:99) and

r.

• Bu = {(cid:96)(u) = 1, k ∈ Au+1, Nk(u) = N1(u) − 1 for some arm k}, the optimal arm is leader in u but loses its duel

against arm k, that have been pulled enough to possibly take over the leadership at next round.

• Cu = {∃k (cid:54)= 1, Nk(u) ≥ N1(u), ˆYk,Su
duel against the suboptimal leader.

1 (Nk(u),N1(u)) ≥ ˆY1,N1(u)}, the optimal arm is not the leader and has lost its

• Lr = (cid:80)r

u=(cid:98)r/4(cid:99)

1Cu .

B.2. The algorithm

Before giving the algorithm, we introduce additional notations that are used in the statement of the algorithm. The stored
history for the arm k at round r is denoted Hk(r). At round r when comparing the leader (cid:96)(r) and the arm k (cid:54)= (cid:96)(r) the
last block of the history of (cid:96)(r) is used and is denoted S(Hk(r), H(cid:96)(r)). In particular, when both arms are saturated their
entire history of length mr is used for the duel. The Last Block Subsampling Duelling Algorithm with Limited Memory is
reported in Algorithm 3

On Limited-Memory Subsampling Strategies for Bandits

Algorithm 3 LB-SDA with Limited Memory
Input: K arms, horizon T , mr storage limitation
Initialization: t ← 1, r = 1 ∀k ∈ {1, ..., K}, Nk ← 0, Hk = {}
while t < T do

A ← {}, (cid:96) ← leader(N, t)
if r = 1 then

A ← {1, . . . , K} (Draw each arm once)

else

for k (cid:54)= (cid:96) ∈ {1, ..., K} do
√

if Nk ≤

log r or ¯Yk,Hk > ¯Y(cid:96),S(Hk,H(cid:96)) then

A ← A ∪ {k}

if |A| = 0 then
A ← {l}

for k ∈ A do

if card(Hk) ≥ mr then

pop(Hk) // Removing the oldest observation
Pull arm k, observe reward Yk,Nk+1, Nk ← Nk + 1, t ← t + 1

Hk = Hk ∪ {Yk,Nk+1} // Append the new observation

r ← r + 1

B.3. Proof of Theorem 2

The beginning of the proof of Baudry et al. (2020) is valid for LB-SDA, however it has to be rewritten completely to
introduce the storage limitation. We use the same notation as in Section 3.3 and introduce a sequence mr of allowed memory
for each arm at a round r. In the beginning of the proof we do not make any assumption on the sequence mr except that
mr/ log(r) → +∞, which is required in the statement of Theorem 2. We further assume that mr is an integer for any round
r, which does not change anything for the algorithm but simpliﬁes the notations for the proof. In this section, without loss of
generality, we assume that the arm 1 is the unique optimal arm µ1 = maxk∈[K] µk. We also recall that the arms are assumed
to come from the same one-parameter exponential family of distributions.

In terms of notation, we remark that if Nk(r) ≥ mr and (cid:96)(r) (cid:54)= k then the duel between k and (cid:96)(r) is the comparison
between ¯Yk,Nk(r)−mr:Nk(r) and ¯Y(cid:96)(r),N(cid:96)(r)(r)−mr:N(cid:96)(r)(r). Otherwise, if Nk(r) ≤ mr and (cid:96)(r) (cid:54)= k then the duel is the
comparison between ¯Yk,Nk(r) and ¯Y(cid:96)(r),N(cid:96)(r)(r)−Nk(r):N(cid:96)(r)(r), which is the same as for the vanilla LB-SDA.
We recall that the set of saturated arms at round r is deﬁned as

Sr = {k ∈ {1, . . . , K} : Nk(r) ≥ mr} .

(5)

However, we do not change the deﬁnition of the leader that is still deﬁned as (cid:96)(r) = argmaxk≤KNk(r) nor the corresponding
tie-breaking rules. All along the proof we will use the Chernoff inequality, that states that for any exponential family of
distribution and any x, y satisfying x < µk < y, then P( ¯Yk,n ≤ x) ≤ e−kl(x,µk) and P( ¯Yk,n ≥ y) ≤ e−kl(y,µk). To simplify
the notation for each arm k we deﬁne the real number xk = µ1+µk
∈ (µk, µ1), and write ωk = min(kl(xk, µ1), kl(xk, µk)).
Hence, we will write most of our results using concentration with this value ωk for arm k.
We write Nk(T ) as Nk(T ) = 1 + (cid:80)T −1

1(k ∈ Ar+1). The ﬁrst step of the proof is to decompose the number of pulls

2

r=1

On Limited-Memory Subsampling Strategies for Bandits

according to the events {(cid:96)(r) = 1} and k ∈ Sr,

E[Nk(T )] = 1 + E

(cid:34)T −1
(cid:88)

(cid:35)
1(k ∈ Ar+1, (cid:96)(r) (cid:54)= 1)

(cid:34)T −1
(cid:88)

(cid:35)
1(k ∈ Ar+1, k /∈ Sr, (cid:96)(r) = 1)

+ E

r=1

r=1

(cid:34)T −1
(cid:88)

(cid:35)
1(k ∈ Ar+1, k ∈ Sr, (cid:96)(r) = 1)

+ E

≤ 1 + E

r=1
(cid:34)T −1
(cid:88)

r=1

(cid:35)
1((cid:96)(r) (cid:54)= 1)

+ E

(cid:34)T −1
(cid:88)

(cid:35)
1(k ∈ Ar+1, k /∈ Sr, (cid:96)(r) = 1)

r=1

(cid:34)T −1
(cid:88)

(cid:35)
1(k ∈ Ar+1, k ∈ Sr, (cid:96)(r) = 1)

.

+ E

r=1

We ﬁrst study the term E1 = E
and use that under k ∈ Sr the index of both arms
will be a subsample of size mr of their history. We start the sum on the rounds at 2m1 because two arms cannot be saturated
before this round is reached, so it holds that

1(k ∈ Ar+1, k ∈ Sr, (cid:96)(r) = 1)

(cid:104)(cid:80)T −1
r=1

(cid:105)

E1 ≤

≤

≤

≤

≤

T −1
(cid:88)

r=2m1

T −1
(cid:88)

r=2m1

T −1
(cid:88)

r=2m1

P ((cid:96)(r) = 1, k ∈ Ar+1, Nk(r) ≥ mr, N1(r) ≥ mr)

P (cid:0)(cid:96)(r) = 1, k ∈ Ar+1, Nk(r) ≥ mr, N1(r) ≥ mr, ¯Yk,Nk(r)−mr+1:Nk(r) ≥ ¯Y1,N1(r)−mr+1:N1(r)

(cid:1)

P (cid:0)Nk(r) ≥ mr, ¯Yk,Nk(r)−mr+1:Nk(r) ≥ xk

(cid:1) +

T −1
(cid:88)

r=2m1

P (cid:0)N1(r) ≥ mr, ¯Y1,N1(r)−mr+1:N1(r) ≤ xk

(cid:1)

T −1
(cid:88)

r
(cid:88)

r=2m1

nk=mr

T −1
(cid:88)

r
(cid:88)

r=2m1

nk=mr

P (cid:0) ¯Yk,nk−mr+1:nk ≥ xk, Nk(r) = nk

(cid:1) +

T −1
(cid:88)

r
(cid:88)

r=2m1

n1=mr

P (cid:0) ¯Y1,n1−mr+1:n1 ≤ xk, N1(r) = n1

(cid:1)

P (cid:0) ¯Yk,nk−mr+1:nk ≥ xk

(cid:1) +

T −1
(cid:88)

r
(cid:88)

r=2m1

n1=mr

P (cid:0) ¯Y1,n1−mr+1:n1 ≤ xk

(cid:1)

≤ 2

T −1
(cid:88)

r=2m1

re−mrωk ,

where we used two main elements: 1) if two random variables X and Y satisfy X ≥ Y then for any threshold η it holds that
either X ≥ η or Y ≤ η (third line), and 2) the empirical averages of the ﬁxed blocks of observations satisfy the Chernoff
concentration inequality. Using the notation, we introduced

P( ¯Y1,n1−mr+1:n1 ≤ xk) = P( ¯Y1,mr ≤ xk) ≤ e−mrωk

P( ¯Yk,nk−mr+1:nk ≥ xk) = P( ¯Yk,mr ≥ xk) ≤ e−mrωk .

and

Therefore, the following holds

T −1
(cid:88)

r=1

P(k ∈ Ar+1, k ∈ Sr, (cid:96)(r) = 1) ≤ 2

T −1
(cid:88)

r=2m1

re−mrωk .

(6)

We then study E2 = E
n0(T ) holds or not at each round, for some n0(T ) that will be speciﬁed later.

1(k ∈ Ar+1, k /∈ Sr, (cid:96)(r) = 1)

(cid:104)(cid:80)T −1
r=1

(cid:105)

. We further distinguish two cases, whenever Nk(r) ≤

On Limited-Memory Subsampling Strategies for Bandits

E2 ≤ n0(T ) + E

(cid:34)T −1
(cid:88)

r=1

1(k ∈ Ar+1, k /∈ Sr, (cid:96)(r) = 1, Nk(r) ≥ n0(T ))

.

(cid:35)

We then use that on the event k /∈ Sr the duels played between k and 1 will be the classical duel with the last block: k will
compete with its empirical mean and 1 with the mean of its last block of size Nk(r). We deﬁne some ηk ∈ (µk, µ1) and
write

E2 ≤ n0(T ) + E

≤ n0(T ) + E

(cid:34)T −1
(cid:88)

r=1
(cid:34)T −1
(cid:88)

r=1

1(k ∈ Ar+1, k /∈ Sr, (cid:96)(r) = 1, Nk(r) ≥ n0(T ))

(cid:35)

(cid:35)
1(k ∈ Ar+1, ¯Yk,Nk(r) ≥ ¯Y1,N1(r)−Nk(r)+1:N1(r), (cid:96)(r) = 1, Nk(r) ≥ n0(T ))

≤ n0(T ) +

T −1
(cid:88)

r=1

P (cid:0)k ∈ Ar+1, ¯Yk,Nk(r) ≥ ηk, Nk(r) ≥ n0(T )(cid:1)

+

T −1
(cid:88)

r=1

P (cid:0)k ∈ Ar+1, ¯Y1,N1(r)−Nk(r)+1:N1(r) ≤ ηk, (cid:96)(r) = 1, Nk(r) ≥ n0(T ), N1(r) ≥ n0(T )(cid:1) ,

where we used the same trick as for E1 to obtain the last result.

We then use a union bound on the values of Nk(r) for the ﬁrst sum and on both Nk(r) and N1(r) for the second sum,
leading to

E2 ≤ n0(T ) +

T −1
(cid:88)

r=1

T −1
(cid:88)

T −1
(cid:88)

+

T −1
(cid:88)

nk=n0(T )
n1(cid:88)

r=1

n1=n0(T )

nk=n0(T )

P (cid:0)k ∈ Ar+1, ¯Yk,nk ≥ ηk, Nk(r) = nk

(cid:1)

P (cid:0)k ∈ Ar+1, ¯Y1,n1−nk+1:n1 ≤ ηk, Nk(r) = nk, N1(r) = n1

(cid:1)

≤ n0(T ) +

T −1
(cid:88)

P (cid:0) ¯Yk,nk ≥ ηk

(cid:1) +

T −1
(cid:88)

T −1
(cid:88)

P (cid:0) ¯Y1,n1−nk+1:n1 ≤ ηk

(cid:1) ,

nk=n0(T )

nk=n0(T )

n1=n0(T )

where we used that (cid:80)T −1
r=1
in the second term). Using the Chernoff inequality, we write

1(k ∈ Ar+1, Nk(r) = nk) ≤ 1 to remove the sums in r (simply ignoring the event N1(r) = n1

E2 ≤ n0(T ) +

e−n0(T )kl(ηk,µk)
1 − e−kl(ηk,µk)

+ T

e−n0(T )kl(ηk,µ1)
1 − e−kl(ηk,µ1)

.

1+ε

We then calibrate n0(T ) and ηk in order to makes these terms converge properly. We deﬁne ε > 0 and state n0(T ) =
kl(µk,µ1) log T . We then use the continuity of the kullback-leibler divergence on (µk, µ1) to state that for any δ > 0, there
exists some ε > 0 and ηk ∈ (µk, µ1) satisfying kl(ηk, µ1) ≥ kl(µk, µ1) − δ ≥ kl(µk,µ1)
. This means that for any ε > 0,
there exists some ηk > 0 satisfying T e−n0(T )kl(ηk,µ1) ≤ T e−n0(T )

kl(µk ,µ1 ) log T ≤ 1. Hence, for any ε > 0 it holds that

1+ε

1+ε

where Ck,ε is a constant.

E2 ≤

1 + ε
I1(µk)

log T + Ck,ε ,

Combining these results we can write a ﬁrst decomposition of E[Nk(T )] as

On Limited-Memory Subsampling Strategies for Bandits

E[Nk(T )] ≤ 1 +

1 + ε
I1(µk)

log T + 2

T −1
(cid:88)

r=2m1

re−mrωk + Ck,ε +

T −1
(cid:88)

r=2m1

P((cid:96)(r) (cid:54)= 1) .

(7)

We remark that this expression provides an explicit dependence in mr in the second term, that justiﬁes the condition
in Theorem 2 for mr ( namely, mr/(log r) → +∞). Indeed, this condition is sufﬁcient to ensure for instance that
mr ≥ 3
ωk

log r for r large enough, making the term inside the sum a o(r−2).

The next step is to prove that (cid:80)T −1
P((cid:96)(r) (cid:54)= 1) = o(log T ). As in the proof of (Chan, 2020) this part causes a lot of
r=1
technical challenges, and we need to deﬁne several new events to analyze the different scenarios that could lead a suboptimal
arm to be the leader at a round r. In the next steps we will consider the same events as in the original proof, but the storage
limitation will add some complexity to the task. We will use the following property, issued from the deﬁnition of the leader

(cid:96)(r) = k ⇒ Nk(r) ≥

(cid:109)

(cid:108) r
K

.

However, adding the storage constraint we have that for any r satisfying r ≥ Kmr the leader has necessarily more than mr
observations. For this reason, its history will be truncated to the mr last observations. However, we leverage the property
that when r is reasonably large, mr is large enough to guarantee a good concentration of the empirical mean of the saturated
arms around their true mean. We will explain how this can be done in this section. We deﬁne ar = (cid:6) r
(cid:7), and write the
following decomposition

4

P ((cid:96)(r) (cid:54)= 1) = P ({(cid:96)(r) (cid:54)= 1} ∩ Dr) + P (cid:0){(cid:96)(r) (cid:54)= 1} ∩ ¯Dr(cid:1) .

(8)

We deﬁne Dr the event under which the optimal arm has been leader at least once in [ar, r].

Dr = {∃u ∈ [ar, r] such that (cid:96)(u) = 1}.

We now explain how to upper bound the term in the left hand side of Equation (8). We look at the rounds larger than some
round r0 that will be speciﬁed later in the proof.

B.3.1. ARM 1 HAS BEEN LEADER ar AND r

We introduce a new event

Bu = {(cid:96)(u) = 1, k ∈ Au+1, Nk(u) = N1(u) − 1 for some arm k} .

Under the event Dr, {(cid:96)(r) (cid:54)= 1} can only be true only if the leadership has been taken over by a suboptimal arm at some
round between ar and r, that is

{(cid:96)(r) (cid:54)= 1} ∩ Dr ⊂ ∪r−1
u=ar

{(cid:96)(u) = 1, (cid:96)(u + 1) (cid:54)= 1} ⊂ ∪r−1
u=ar

Bu .

(9)

Indeed, a leadership takeover can only happen after a challenger has defeated the leader while having at least the same
number of observations minus one (however this situation is necessary but not sufﬁcient to cause a change of leader, hence
the strict inclusion).
We now upper bound (cid:80)T −1
P(Bu). We use the notation br = (cid:100)ar/K(cid:101) representing the minimum of samples of
r=r0
the leader at the round ar. Hence we are sure that under Bu arm 1 had at least bu observations when it lost the duel that cost
it the leadership.

(cid:80)r

u=ar

We then take an union bound on all the suboptimal arms k ∈ {2, ..., K}, deﬁning

Bu = ∪K

k=2Bu

k := {(cid:96)(u) = 1, k ∈ Au+1, Nk(u) = N1(u) − 1} ,

which ﬁxes the speciﬁc suboptimal arm that could have taken the leadership.

On Limited-Memory Subsampling Strategies for Bandits

Choosing xk, ωk as in the previous section we can write

T −1
(cid:88)

r
(cid:88)

r=r0

u=ar

P(Bu

k ) = E

≤ E

(cid:124)

+ E

(cid:124)

(cid:34) T −1
(cid:88)

r
(cid:88)

r=r0
(cid:34) T −1
(cid:88)

u=ar

r
(cid:88)

r=r0

u=ar

1((cid:96)(u) = 1, k ∈ Au+1, N1(u) = Nk(u) + 1)

(cid:35)

(cid:35)
1((cid:96)(u) = 1, k ∈ Au+1, N1(u) = Nk(u) + 1, k /∈ Su)

(cid:34) T −1
(cid:88)

r
(cid:88)

(cid:35)
1((cid:96)(u) = 1, k ∈ Au+1, N1(u) = Nk(u) + 1, k ∈ Su)

.

(cid:123)(cid:122)
B1

(cid:125)

r=r0

u=ar

(cid:123)(cid:122)
B2

(cid:125)

We proceed similarly as in the previous part, analyzing separately the case k ∈ Su and the case k /∈ Su with Su deﬁned in
Equation (5). We start with the term B1,

B1 ≤ E

≤ E

+ E

(cid:34) T −1
(cid:88)

r
(cid:88)

r=r0
(cid:34) T −1
(cid:88)

u=ar

r
(cid:88)

r=r0
(cid:34) T −1
(cid:88)

u=ar

r
(cid:88)

r=r0

u=ar

1(N1(u) ≥ br, ¯Yk,Nk(u) ≥ ¯Y1,N1(u)−Nk(u)+1:N1(u), N1(u) = Nk(u) + 1, k ∈ Au+1, k /∈ Su)

1(N1(u) ≥ br, ¯Yk,Nk(u) ≥ xk, N1(u) = Nk(u) + 1, k ∈ Au+1, k /∈ Su)

(cid:35)

(cid:35)

(10)

(cid:35)
1(N1(u) ≥ br, ¯Y1,N1(u)−Nk(u)+1:N1(u) ≤ xk, N1(u) = Nk(u) + 1, k ∈ Au+1, k /∈ Su)

.

(11)

We now separately upper bound each of these two terms. First,

(10) ≤ E

(cid:34) T −1
(cid:88)

r
(cid:88)

mu−1
(cid:88)

(cid:35)
1(Nk(u) = nk, k ∈ Au+1, ¯Yk,nk ≥ xk)

r=r0
(cid:34) T −1
(cid:88)

u=ar

nk=br−1

r
(cid:88)

r
(cid:88)

r=r0

u=ar

nk=br−1



(cid:35)
1(Nk(u) = nk, k ∈ Au+1, ¯Yk,nk ≥ xk)



T −1
(cid:88)

r
(cid:88)

1( ¯Yk,nk ≥ xk)

r=r0

nk=br−1

r
(cid:88)

u=ar
(cid:124)

1(Nk(u) = nk)1(k ∈ Au+1)

(cid:123)(cid:122)
≤1






(cid:125)

≤ E

≤ E







≤

≤

≤

T −1
(cid:88)

r
(cid:88)

r=r0

nk=br−1

T −1
(cid:88)

r
(cid:88)

P( ¯Yk,nk ≥ xk)

exp (−nkωk)

r=r0

nk=br−1

T −1
(cid:88)

r=r0

e−(br−1)ωk
1 − e−ωk

.

We remark that by deﬁnition br ≥ ar/K ≥ r/(4K) and using r0 ≥ 8, we conclude that

(10) ≤

e(1− 2

K )ωk

(1 − e−ωk )(1 − e−ωk/(4K))

.

On Limited-Memory Subsampling Strategies for Bandits

As the subsampling in LB-SDA is deterministic, thanks to N1(r) = Nk(u) + 1 we obtain the same result for (11),

(11) ≤ E

(cid:34) T −1
(cid:88)

r
(cid:88)

r
(cid:88)

(cid:35)
1( ¯Y1,2:nk+1 ≤ xk)1(Nk(u) = nk)1(k ∈ Au+1)

r=r0

u=ar

nk=br−1





≤ E

T −1
(cid:88)

r
(cid:88)

1( ¯Y1,2:nk+1 ≤ xk)

r=r0

nk=br−1







r
(cid:88)

u=ar
(cid:124)

1(Nk(u) = nk)1(k ∈ Au+1)

(cid:123)(cid:122)
≤1






(cid:125)

≤

≤

T −1
(cid:88)

r
(cid:88)

r=r0

nk=br−1

P( ¯Y1,nk ≤ xk)

e(1− 2

K )ωk

(1 − e−ωk )(1 − e−ωk/(4K))

.

We then control B2. For B2 the condition N1(u) = Nk(u) + 1 will not be used but instead we use Equation (6) already
established in the previous section.

r
(cid:88)

u=1

P(k ∈ Au+1, k ∈ Su, (cid:96)(u) = 1) ≤ 2

r
(cid:88)

u=2m1

ue−muωk ,

which leads to

B2 = E

(cid:34) T −1
(cid:88)

r
(cid:88)

(cid:35)
1((cid:96)(u) = 1, k ∈ Au+1, N1(u) = Nk(u) + 1, k ∈ Su)

r=r0

u=ar

T −1
(cid:88)

≤

r
(cid:88)

r=r0

u=max(ar,2m1)

2ue−muωk .

Then, if consider r0 = min{r : ar ≥ 2m1} we can further upper bound B2 by

B2 ≤

T −1
(cid:88)

r
(cid:88)

r=r0

u=ar

2ue−muωk

T −1
(cid:88)

r

r
(cid:88)

≤ 2

2e−muωk

r=r0

u=ar

≤ 2

T −1
(cid:88)

r=r0

r2e−mar ωk .

We ﬁrst use this result without commenting its dependence in the sequence (mr)r≥1. Summing on all suboptimal arms k
we obtain

T −1
(cid:88)

r=r0

P ({(cid:96)(r) (cid:54)= 1} ∩ Dr) ≤ 2

(cid:34)

K
(cid:88)

e(1− 2

K )ωk

(1 − e−ωk )(1 − e−ωk/(4K))

k=2

+

T −1
(cid:88)

r=r0

(cid:35)

r2e−mar ωk

.

(12)

Hence, the sums of the probability that arm 1 is not the leader while it has already been before is upper bounded by two
terms: a problem-dependent constant, and a term that depends of the sequence of memory limits (mr)r≥1. We can further
analyze this second term. First, we remark that contrarily to the term in mr in Equation (7) this time we have both r2 and
mar instead of mr, with ar = (cid:100)r/4(cid:101). Hence, for a ﬁxed r the term of the sum is larger in this case. However, the constraint
mr/ log(r) → +∞ is again sufﬁcient to ensure a proper convergence of this sum to a constant with the same arguments.
This is mainly because the choice of ar as a fraction of r ensures that mar will be sufﬁciently large.

On Limited-Memory Subsampling Strategies for Bandits

B.3.2. ARM 1 HAS NEVER BEEN LEADER BETWEEN ar AND r

The idea in this part is to leverage the fact that if the optimal arm is not leader between (cid:98)r/4(cid:99) and r, then it has necessarily
lost a lot of duels against the current leader at each round. We then use the fact that when the leader has been drawn
"enough", concentration prevents this situation with large probability. We introduce

Lr =

r
(cid:88)

u=ar

1Cu ,

with Cu deﬁned as Cu = {∃k (cid:54)= 1, (cid:96)(u) = k, 1 /∈ Au+1}. The following holds

P((cid:96)(r) (cid:54)= 1 ∩ ¯Dr) ≤ P(Lr ≥ r/4) .

This result comes from (Chan, 2020), along with the direct use of the Markov inequality to provide the upper bound

P(Lr ≥ r/4) ≤

E(Lr)
r/4

=

4
r

r
(cid:88)

u=ar

P(Cu) .

(13)

(14)

We further decompose the probability of P(Cu) in two parts depending on the value of the number of selections of arm 1.
For the next steps we deﬁne the following events, {N1(u) ≤ C/4 log(u)} and {N1(u) ≥ C/4 log(u)}, for some constant
C that is not known by the algorithm and that we will deﬁne later. This idea handle the memory limit through this parameter
C. Indeed, we only know that the sequence (mr)r≥1 satisﬁes mr/(log(r)) → +∞. For this reason, we know that for any
C > 0 there exists a round rC such that for any r ≥ rC then mr ≥ C log(r).

Using Equation (13) and Equation (14), we have

T −1
(cid:88)

r=r0

P({(cid:96)(r) (cid:54)= 1} ∩ D

r

) ≤

+

4
r

4
r

T −1
(cid:88)

r=r0
(cid:124)

T −1
(cid:88)

r=r0
(cid:124)

r
(cid:88)

P

u=ar

(cid:18)

N1(u) ≤

C
4

log(u)

(cid:123)(cid:122)
B

(cid:19)

(cid:125)

r
(cid:88)

P

u=ar

(cid:18)

Cu, N1(u) ≥

(cid:19)

log(u)

.

C
4

(cid:123)(cid:122)
D

(cid:125)

Again, D can be upper bounded by splitting the cases when the optimal arm is saturated or not. We also introduce
Cu
k = {(cid:96)(u) = k, 1 /∈ Au+1} for any k ∈ {2, . . . , K} and obtain



D ≤

K
(cid:88)

k=2

T −1
(cid:88)





r=r0

(cid:124)

4
r

r
(cid:88)

(cid:18)

P

u=ar

Cu
k , N1(u) ≥

C
4

(cid:19)

log(u), 1 ∈ Su

+

(cid:123)(cid:122)
Dk,1

(cid:125)

r
(cid:88)

(cid:18)

P

u=ar

4
r

T −1
(cid:88)

r=r0
(cid:124)

Cu
k , N1(u) ≥

C
4

log(u), 1 /∈ Su

(cid:123)(cid:122)
Dk,2



(cid:19)






(cid:125)

.

For the event featuring {1 ∈ Su} we can use the result of the previous sections because in the event we consider there is no
difference between (cid:96)(r) = 1 and (cid:96)(r) = k when both arms are saturated. Following the proof for obtaining Equation (6),
one has

r
(cid:88)

u=ar

P(1 /∈ Au+1, 1 ∈ Su, (cid:96)(u) = k) ≤ 2

r
(cid:88)

u=ar

ue−muωk .

(15)

With this result we then obtain

On Limited-Memory Subsampling Strategies for Bandits

Dk,1 =

≤

≤

T −1
(cid:88)

r=r0

T −1
(cid:88)

r=r0

T −1
(cid:88)

r=r0

4
r

4
r

4
r

r
(cid:88)

u=ar

r
(cid:88)

u=ar

r
(cid:88)

u=ar

P (Cu

k , 1 ∈ Su)

P (1 /∈ Au+1, 1 ∈ Su, (cid:96)(u) = k)

2ue−muωk

(Equation (15))

≤ 8

T −1
(cid:88)

r
(cid:88)

r=r0

u=ar

e−muωk

≤ 8

T −1
(cid:88)

r=r0

re−mar ωk ,

Dk,2 ≤

≤

≤

≤

≤

T −1
(cid:88)

r=r0

T −1
(cid:88)

r=r0

T −1
(cid:88)

r=r0

T −1
(cid:88)

r=r0

T −1
(cid:88)

r=r0

So ﬁnally

r
(cid:88)

u=ar

r
(cid:88)

P(Cu

k , N1(u) ≥

C
4

log(u), 1 /∈ Su)

P( ¯Yk,Nk(u)−N1(u)+1:Nk(u) > ¯Y1,N1(u), N1(u) ≥

u=ar
(cid:20)

1
1 − e−ωk

e− C

4 log(ar)ωk +

r
1 − e−ωk

e− C

4 log(ar)ωk

(cid:21)

4
r

4
r

4
r

4(r + 1)
r(1 − e−ωk )

e− C

4 log(ar)ωk

6
1 − e−ωk

e− C

4 log(ar)ωk .

C
4

log(u), 1 /∈ Su, Nk(u) > N1(u))

D ≤

(cid:34)

K
(cid:88)

8

T −1
(cid:88)

k=2

r=r0

re−mar ωk +

T −1
(cid:88)

r=r0

6
1 − e−ωk

(cid:35)

e− C

4 log(ar)ωk

.

At this step we remark that we need to choose the constant C large enough in order to make this sum converge to a constant.
We remind here, that C is only an analysis parameter. We then consider the term B. As in Baudry et al. (2020) we transform
the double sum in a simple sum by simply counting the number of times each term is included. For any integer s and any
round r, the term 4

s only if as ≤ r ≤ s. With the value ar = (cid:6) r

(cid:7) we obtain

4

T
(cid:88)

B =

4
r
r=r0
If we remark that (cid:80)r

r
(cid:88)

(cid:18)

P

N1(u) ≤

C
4

(cid:19)

log(u)

=

T
(cid:88)

(cid:32) r

(cid:88)

(cid:33)

1(t ∈ [r, 4r])

P

4
t

(cid:18)

N1(r) ≤

(cid:19)

log(u)

.

C
4

u=ar

r=r0
1(t ∈ [s, 4s]) ≤ (4s − s + 1) × 4

4
t

t=1

t=1

s ≤ 16, we ﬁnally get:

T
(cid:88)

r=r0

P({(cid:96)(r) (cid:54)= 1} ∩ D

r

) ≤ r0 + 16

T
(cid:88)

(cid:18)

P

r=r0

N1(r) ≤

C
4

(cid:19)

log(r)

+ D(ν).

(16)

Combining (12) and (16) yields

T
(cid:88)

r=r0

P ((cid:96)(r) (cid:54)= 1) ≤ r0 + 16

T
(cid:88)

P

r=r0

(cid:18)

N1(r) ≤

(cid:19)

log(r)

C
4

+ D(cid:48)

k(ν)

On Limited-Memory Subsampling Strategies for Bandits

for some constant D(cid:48)
but asymptotically the dominant terms are the same as in the proof of the vanilla LB-SDA algorithm.

k(ν) that depends on k and ν. Hence, the storage limit may introduce larger constant terms in the proof,

The last step is to show that we can upper the last term as we did in Appendix A. To do so, we only need to prove that
if r0 is large enough and {N1(r) ≤ C/4 log(r)}, then the arm 1 has not been saturated for a long time. This way we
would handle the saturation exactly as we handled the forced exploration (which is still present here) in the proof for the
vanilla LB-SDA. To do so, we deﬁne the function m−1(x) = inf{r : mr ≥ x}. If we had exactly mr = C log r then this
function would be m−1(x) = exp(x/C). Up to choosing a slightly larger r0, we consider that for any r > r0 we also have
m−1(C/4 log r) ≤ exp(C/4 log(r)C −1) = r1/4. Hence, after the round r0 we are sure that arm 1 has never been saturated
since the round r1/4, hence we can apply the same sketch of proof as in Appendix A to conclude that

T
(cid:88)

(cid:18)

P

r=r0

N1(r) ≤

C
4

(cid:19)

log(r)

= O(1) .

C. Proof for Switching Bandits

As explained in the main paper bounding E[N φ
k ], the number of pulls of a suboptimal arm k during a phase φ is sufﬁcient to
control the dynamic regret. During the phase φ the best arm is denoted k∗
φ. We consider the SW-LB-SDA policy with a
sliding window of size τ . We also deﬁne ˆδφ = rφ+1 − rφ, the random number of rounds in the phase φ. Due to the sliding
1 (k ∈ As+1), i.e.
window, we use the deﬁnition of the leader introduced in Section 4 and recall that N τ
number of times arm k has been pulled during the τ last rounds.
Then for any r ∈ N, the leader at round r + 1 is deﬁned as

k (r) = (cid:80)r−1

s=r−τ

(cid:96)τ (r + 1) =

(cid:40)

argmaxk∈{1,...,K}N τ
argmaxk∈Br∪{(cid:96)τ (r)}N τ

k (r + 1) otherwise

k (r + 1) if N τ

(cid:96)τ (r)(r + 1) < min(r, τ )/(2K)

C.1. Details for SW-LB-SDA Implementation

With our new deﬁnition of the leader, it could happen that for some rounds the leader is not the arm with the largest
number of samples when K ≥ 3. We give an example of such a behavior: assume that the ﬁrst round is r = 1, there are
2n + m rounds and K = 3 arms drawn in the following order (1 arm per round): m pulls of arm 1, followed by n > m
pulls of arm 3 and then n − m pulls of arm 1. If the length of the sliding window is τ = 2n and the leader at the round
(m + n + (n − m) = 2n) is 1, then we see that 1 will lose samples during the next m rounds. If for those m successive
rounds only the arm 2 is pulled, then 1 will stay leader with n − m samples while 3 still have n samples. At the end (round
2n + m), the leader is arm 1, we have N τ
3 (2n + m) = n. This example highlights that is it
possible that the leader is not the arm that has been played the most with a sliding window.

1 (2n + m) = n − m < N τ

For this reason, the duels are slightly different to the stationary case. The index of the leader for duels against an arm with
a larger number of samples is simply the mean of its observations collected during the last τ rounds. Indeed, in this case
both arms have a large number of samples hence subsampling is not necessary. This explain why the term ˆµτ
(cid:96),k is used in
Algorithm 2.

On Limited-Memory Subsampling Strategies for Bandits

C.2. Analysis

We use the notation introduced in Section 4. The beginning of the proof takes elements from Garivier & Moulines (2008)
and Baudry et al. (2020). For k (cid:54)= k∗

φ and an arbitrary function Aφ,τ

k , we write

N φ

k =

rφ+1−2
(cid:88)

r=rφ−1

1 (k ∈ Ar+1)

rφ+1−2
(cid:88)

≤ 2τ +

1 (k ∈ Ar+1)

r=rφ+2τ −2

rφ+1−2
(cid:88)

(cid:16)

1

≤ 2τ +

k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ Aφ,τ

k

(cid:17)

r=rφ+2τ −2

rφ+1−2
(cid:88)

(cid:16)

1

+

r=rφ+2τ −2

k ∈ Ar+1, N τ

k (r) < Aφ,τ

k

rφ+1−2
(cid:88)

(cid:17)

+

r=rφ+2τ −2

1 (cid:0)k ∈ Ar+1, (cid:96)τ (r) (cid:54)= k∗

φ

(cid:1)

≤ 2τ +

rφ+1−2
(cid:88)

(cid:16)

1

k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ Aφ,τ

k , Dτ

k (r) = 0

(cid:17)

rφ+1−2
(cid:88)

+

r=rφ+2τ −2

1 (cid:0)(cid:96)τ (r) = k∗

φ, Dτ

k (r) = 1(cid:1)

r=rφ+2τ −2

rφ+1−2
(cid:88)

(cid:16)

1

+

r=rφ+2τ −2

k ∈ Ar+1, N τ

k (r) < Aφ,τ

k

rφ+1−2
(cid:88)

(cid:17)

+

r=rφ+2τ −1

1 (cid:0)k ∈ Ar+1, (cid:96)τ (r) (cid:54)= k∗

φ

(cid:1) .

We then use the following lemma.

Lemma 6 (Adaptation of Lemma 25 from (Garivier & Moulines, 2008)).

rφ+1−2
(cid:88)

r=rφ+2τ −2

1 (k ∈ Ar+1, N τ

k (r) < A) ≤

(cid:98)δφA
τ

.

Therefore,

N φ

k ≤ 2τ +

(cid:98)δφAφ,τ
k
τ

+

rφ+1−2
(cid:88)

(cid:16)

1

r=rφ+2τ −2
(cid:124)

k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ Aφ,τ

k , Dτ

k (r) = 0

(cid:17)

rφ+1−2
(cid:88)

+

r=rφ+2τ −2
(cid:124)

1 (cid:0)(cid:96)τ (r) = k∗

φ, Dτ

k (r) = 1(cid:1)

(cid:123)(cid:122)
cφ,τ
k,2

r=rφ+2τ −1
(cid:124)

(cid:125)

(cid:123)(cid:122)
cφ,τ
k,3

(cid:123)(cid:122)
cφ,τ
k,1

rφ+1−2
(cid:88)

+

1 (cid:0)(cid:96)τ (r) (cid:54)= k∗

φ

(cid:125)

.

(cid:1)

(cid:125)

We control the expectation of these terms separately.

C.2.1. UPPER BOUNDING E[cφ,τ
k,1 ]

We recall that

E[cφ,τ

k,1 ] = E





rφ+1−2
(cid:88)

(cid:16)

1

r=rφ+2τ −2

k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ Aφ,τ

k , Dτ

k (r) = 0

(cid:17)



 .

We start by stating a lemma on the concentration of subsample means in Last Block sampling that is crucial for the proof.

On Limited-Memory Subsampling Strategies for Bandits

Lemma 7. We consider a stationary phase φ and the multi-arm bandit model characterized by (νφ
φ denote
the arm with the largest mean. For each arm we assume there exists a continuous rate function Ik satisfying Ik(x) = 0 if
x = E

k and Ik(x) ≥ 0 otherwise. Furthermore,

K). Let k∗

1 , . . . , νφ

X∼νφ

k (X) = µφ

∀x > µφ
∀y < µφ

k , P (cid:0) ¯Yn ≥ x(cid:1) ≤ e−nIk(x) ,
k , P (cid:0) ¯Yn ≤ y(cid:1) ≤ e−nIk(y) .

Then, for any constant n ∈ N satisfying n ≥ f (τ ) =

√

log τ , by letting ˜n = min(n, (cid:98)τ /(2K)(cid:99)) it holds that





E

rφ+1−2
(cid:88)

r=rφ+2τ −2

1 (cid:0)k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ n, Dτ

k (r) = 0(cid:1)



 ≤ δφ(τ + 1)

e−˜nωk
1 − e−ωk

,

(17)

where we deﬁned ωk = min
the sliding window. Similarly,

(cid:16)

Ik

(cid:16) 1
2 (µφ

k + µφ
k∗
φ

(cid:17)
)

, Ik∗

φ

(cid:16) 1
2 (µφ

k + µφ
k∗
φ

(cid:17)(cid:17)
)

, and δφ is the length of the phase and τ the size of





E

rφ+1−2
(cid:88)

(cid:16)

1

r=rφ+τ −2

φ /∈ Ar+1, (cid:96)τ (r) = k, N τ
k∗
k∗
φ

(r) ≥ n

(cid:17)



 ≤ δφ(τ + 1)

e−˜nωk
1 − e−ωk

.

(18)

Proof. We start with the ﬁrst claim. Under the considered event, an arm k can be drawn for three reason: 1) Dτ
diversity ﬂag of this arm is raised 2) N τ
φ. In our case, as Dτ
leader k∗
against k∗
φ.

k (r) = 1, the
log τ , the forced exploration is used, or 3) k has won its duel against the
φ is leader then k has won its duel

k (r) ≤
k (r) = 0 and N τ

log τ , if k is pulled while k∗

k (r) ≥ n ≥

√

√

Under this event, the duel between k and k∗
φ is a comparison between the mean of two blocks containing at least
min(n, τ /(2K)) observations because of the deﬁnition of the leader. As in Baudry et al. (2020) we use that for any
k(r) ≥ ξk or (cid:98)µτ
threshold ξk, k wins the duel only if either (cid:98)µτ
(cid:96),k(r) ≤ ξk. For the sake of simplicity in our results we choose
k + µφ
ξk as the number satisfying ξk = 1
), and this choice will remain the same for the rest of the paper. We then write
k∗
φ

2 (µφ





A = E

rφ+1−2
(cid:88)

1 (cid:0)k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥ n, Dτ

k (r) = 0(cid:1)





r=rφ+2τ −2





≤ E

rφ+1−2
(cid:88)

(cid:16)

1

r=rφ+2τ −2





≤ E

rφ+1−2
(cid:88)

(cid:16)

1

k ∈ Ar+1, {(cid:98)µτ

k(r) ≥ ξk ∪ (cid:98)µτ

φ,k(r) ≤ ξk}, N τ

k∗
φ

k∗

(r) ≥ τ /(2K), N τ

k (r) ≥ n

(cid:17)





k ∈ Ar+1, (cid:98)µτ

φ,k(r) ≤ ξk, N τ

k∗
φ

k∗

(r) ≥ τ /(2K), N τ

k (r) ≥ n

(cid:17)





r=rφ+2τ −2


rφ+1−2
(cid:88)

+ E



(cid:16)

1

k ∈ Ar+1, (cid:98)µτ

k(r) ≥ ξk, N τ
k∗
φ

(r) ≥ τ /(2K), N τ

k (r) ≥ n

(cid:17)



 .

r=rφ+2τ −2

First note that for a given arm k all possible blocks of observations are uniquely described by two quantities: N φ
number of observations of arm k from the beginning of the phase φ and N τ
last τ rounds. We will use this property to bound the two previous sums.

k (r) the
k (r) number of observations of arm k over the

Starting by the simpler term featuring the arm k, we use

(cid:16)

1

k ∈ Ar+1, (cid:98)µτ

k(r) ≥ ξk, N τ
k∗
φ

(r) ≥

, N τ

k (r) ≥ n

(cid:17)

τ
2K

≤ 1 (k ∈ Ar+1, (cid:98)µτ

k(r) ≥ ξk, N τ

k (r) ≥ n) .

(19)

k is deﬁned by N φ

N φ
equal to 1, it implies that there is a block of length at least n with a mean at least ξk. More formally, when introducing

1(k ∈ As+1). For a given round r if the indicator from the RHS of Equation (19) is

k (r) = (cid:80)r−1

s=rφ−1

On Limited-Memory Subsampling Strategies for Bandits

Sn,m
k

the following holds,

(r) = {k ∈ Ar+1, (cid:98)µτ

k(r) ≥ ξk, N φ

k (r) = m + n − 1, N τ

k (r) = n} ,

{k ∈ Ar+1, (cid:98)µτ

k(r) ≥ ξk, N τ

k (r) ≥ n} ⊂

ˆδφ
(cid:91)

ˆδφ
(cid:91)

nk=n

mk=1

Snk,mk
k

(r) .

(20)

For the sake of clarity, we denote Yk,1, ..., Yk,ˆδφ
the set of possible rewards for the arm k for the phase φ. If the indicator
function equals one for a given round r0, then {k ∈ Ar0+1} holds. The same block (same value for both n and m) can not
be used for upcoming rounds because N φ
k (r0). More speciﬁcally, for the arm k
for any possible block there is at most one round for which the indicator function can be 1., i.e.

k (r0 + 1) will satisfy N φ

k (r0 + 1) = 1 + N φ

ˆδφ
(cid:88)

ˆδφ
(cid:88)

rφ+1−2
(cid:88)

nk=n

mk=1

r=rφ+2τ −2

1 (Snk,mk
k

(r)) ≤

ˆδφ
(cid:88)

ˆδφ
(cid:88)

nk=n

mk=1

1 (cid:0) ¯Yk,mk:mk+nk−1 ≥ ξk

(cid:1) .

Similarly, we denote Yk(cid:63)

φ,1, ..., Yk(cid:63)

φ,ˆδφ

the set of possible rewards for the arm k(cid:63)

φ and let

Sn,m
k(cid:63)
φ

We also have

(r) = {k ∈ Ar+1, (cid:98)µτ

k(cid:63)

φ,k(r) ≤ ξk, N φ

k∗
φ

(r) = m + n − 1, N τ
k∗
φ

(r) = n} .

{k ∈ Ar+1, (cid:98)µτ

φ,k(r) ≤ ξk, N τ

k(cid:63)
φ

k(cid:63)

(r) ≥ n(cid:48)} ⊂

ˆδφ
(cid:91)

ˆδφ
(cid:91)

n(cid:63)=n(cid:48)

m(cid:63)=1

Sn(cid:63),m(cid:63)
k(cid:63)
φ

(r) .

(21)

The main difference here is that several rounds can use the same block of observations of k(cid:63)
when the indicator function equals 1 the arm k is drawn instead of k(cid:63)
N τ
k∗
φ

(r) can not remain unchanged for more than τ steps because of the sliding window. This implies in particular,

φ. This can be explained because
φ and the previous argument do not hold anymore. Yet,

ˆδφ
(cid:88)

ˆδφ
(cid:88)

rφ+1−2
(cid:88)

n(cid:63)=n(cid:48)

m(cid:63)=1

r=rφ+2τ −2

1(Sn(cid:63),m(cid:63)
k(cid:63)
φ

(r)) ≤ τ

ˆδφ
(cid:88)

ˆδφ
(cid:88)

1

n(cid:63)=n(cid:48)

m(cid:63)=1

(cid:16) ¯Yk∗

φ,m(cid:63):m(cid:63)+n(cid:63)−1 ≤ ξk

(cid:17)

.

Bringing things together and applying the previous inequality with n(cid:48) = (cid:98)τ /(2K)(cid:99) we obtain

A ≤ E





ˆδφ
(cid:88)

ˆδφ
(cid:88)

τ 1

m(cid:63)=1

n(cid:63)=n(cid:48)

(cid:16) ¯Yk∗

φ,m(cid:63):m(cid:63)+n(cid:63)−1 ≤ ξk

ˆδφ
(cid:88)

ˆδφ
(cid:88)

(cid:17)

+

mk=1

nk=n

1 (cid:0) ¯Yk,mk:mk+nk−1 ≥ ξk

(cid:1)

 .



We then have to handle carefully the fact that (cid:98)δφ is actually a random variable depending on the bandit algorithm. Indeed, as
several arms can be pulled at each round we don’t know what will be the length of a phase in terms of rounds. However, this
quantity is upper bounded by the actual length of the phase in terms of arms pulled δφ.

Thus, using the concentration inequality corresponding to the family of distributions for an appropriate rate function we can
write

δφ
(cid:88)

δφ
(cid:88)

τ P

A ≤

(cid:16) ¯Yk∗

φ,m(cid:63):m(cid:63)+n(cid:63)−1 ≤ ξk

δφ
(cid:88)

δφ
(cid:88)

(cid:17)

+

mk=1

nk=n

P (cid:0) ¯Yk,mk:mk+nk−1 ≥ ξk

(cid:1)

m(cid:63)=n

n(cid:63)=n(cid:48)

δφ
(cid:88)

δφ
(cid:88)

≤

n(cid:63)=n(cid:48)

m(cid:63)=1
(cid:32)
τ

≤ δφ

−n(cid:63)Ik∗
φ

(ξk)

τ e

+

δφ
(cid:88)

δφ
(cid:88)

e−nkIk(ξk)

−n(cid:48)Ik∗
φ

(ξk)

e

1 − e

−Ik∗
φ

(ξk)

+

mk=n

nk=n
(cid:33)

e−nIk(ξk)
1 − e−Ik(ξk)

≤ δφ(τ + 1)

e−(cid:101)nωk
1 − e−ωk

,

where in the last inequality we have introduced ˜n = min(n, n(cid:48)) = min(n, (cid:98)τ /(2K)(cid:99)).

On Limited-Memory Subsampling Strategies for Bandits

Finally, the proof of the second statement is a direct adaptation of this proof by inverting k and k∗
Dφ

φ is not drawn it has necessarily lost its duel against the leader k.

k (r) = 0 because if k∗

φ. We don’t need the event

We then remark that Equation (17) in Lemma 7 can be used to upper bound term cφ,τ
that Aφ,τ

k ≤ τ /(2K) it holds that

k,1 , by replacing n by Aφ,τ

k . Assuming

E[cφ,τ

k,1 ] ≤ δφ(τ + 1)

k ωk

e−Aφ,τ
1 − e−ωk

.

(22)

C.2.2. UPPER BOUNDING E[cφ,τ
k,2 ]

We recall that,

E[cφ,τ

k,2 ] = E





rφ+1−2
(cid:88)

r=rφ+2τ −2

1 (cid:0)(cid:96)τ (r) = k∗

φ, Dτ

k (r) = 1(cid:1)



 .

To upper bound E[cφ,τ
k,2 ] we have to study the probability that the optimal arm for the phase φ loses (cid:100)(K − 1)(log τ )2(cid:101)
successive duels while being leader. We derive in Lemma 8 an intuitive consequence of this property: the optimal arm has
necessarily lost at least one duel against a concentrated arm.

Lemma 8. Consider K arms, and assume that some arm k has been leader for M consecutive rounds, M ≤ τ . For any m
satisfying (K − 1)m ≤ M , if k has lost more than (K − 1)m duels then it has lost at least one duel against an arm with
more than m samples.

Proof. We assume that arm k has been leader for M consecutive rounds and that arm k lost strictly more than (K − 1)m
duels. We also assume that all the challengers that have won against the arm k have less than m samples. There exists an
arm k(cid:48) (cid:54)= k such that k(cid:48) won at least m + 1 duels against arm k while having less than m samples by assumption. We denote
the rounds corresponding to the ﬁrst m + 1 wins r1, . . . , rm+1. The following holds,

N τ

k(cid:48)(rm+1) = N τ

k(cid:48)(r1) + m −

rm+1
(cid:88)

s=r1

1(k(cid:48) ∈ As−τ +1) .

As the number of rounds where k(cid:48) wins against k is smaller than τ , we have (cid:80)rm+1
s=r1
this in the previous equation gives,

1(k(cid:48) ∈ As−τ +1) ≤ N τ

k(cid:48)(r1). Plugging

We have the contradiction and it concludes the proof.

Nk(cid:48)(rm+1, τ ) ≥ m .

k,2 , the optimal arm k∗

Under the event cφ,τ
k (r) = 1, and k∗
φ
is the leader, it means that the leader has not changed for (cid:100)(K − 1)(log τ )2(cid:101) successive rounds and hast lost more than
(K − 1)(log τ )2 duels. All the conditions for applying Lemma 8 are met. Using Lemma 8 and the fact that the diversity ﬂag
cannot be activated in r if it has already been activated in the last (cid:100)(K − 1)(log τ )2(cid:101) rounds it holds that

φ is the leader and the diversity ﬂag for the arm k is raised. If Dτ

1 (cid:0)(cid:96)τ (r) = k∗

φ, Dτ

k (r) = 1(cid:1) ≤

(cid:88)

r−1
(cid:88)

k(cid:48)(cid:54)=k∗
φ

s=r−(cid:100)(K−1)(log τ )2(cid:101)

1((cid:96)τ (s) = k∗

φ, N τ

k(cid:48)(s) ≥ (log τ )2, k(cid:48) ∈ As+1, Dτ

k(cid:48)(s) = 0) .

Furthermore, we can add that an event {(cid:96)τ (r) = k∗
with at most one event Dτ
(cid:100)(K − 1)(log τ )2(cid:101) rounds. Hence, combining these results we obtain

(23)
k (s) ≥ (log τ )2, k ∈ As+1, Dτ
k (s) = 0} can only be associated
k (r) = 1 for some r. Indeed, if the diversity ﬂag is activated it cannot be anymore before at least

φ, N τ

rφ+1−2
(cid:88)

r=rφ+2τ −2

1((cid:96)τ (r) = k∗

φ, Dτ

k (r) = 1) ≤

(cid:88)

rφ+1−2
(cid:88)

k(cid:48)(cid:54)=k∗
φ

r=rφ+2τ −2

1(k(cid:48) ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k(cid:48)(r) ≥ (log τ )2, Dτ

k(cid:48)(r) = 0) .

Applying Lemma 7 with n = (log τ )2 gives,

On Limited-Memory Subsampling Strategies for Bandits

E[cφ,τ

k,2 ] ≤

(cid:88)

k(cid:48)(cid:54)=k∗
φ

δφ(τ + 1)

e−(log τ )2ωk(cid:48)
1 − e−ωk(cid:48)

.

(24)

C.2.3. UPPER BOUNDING cφ,τ
k,3

We recall that,

E[cφ,τ

k,3 ] = E





rφ+1−2
(cid:88)

r=rφ+2τ −1



1 (cid:0)(cid:96)τ (r) (cid:54)= k∗

φ

(cid:1)

 .

As for the stationary case the trickiest part is to prove that the leader is the best arm with high probability. We will ﬁrst
look at the terms involving the event that the best arm has already been leader after the ﬁrst τ rounds of the phase, and then
analyze the situation where it has never been leader. As the upper bound for cφ,τ
k,3 is difﬁcult to obtain, we break this section
into different parts.

Part 1: the optimal arm has been leader between r − τ and r − 1

If the best arm has already been leader between r − τ and r − 1 then it has necessarily lost its leadership at some intermediate
round. Loosing the leadership can be done in two different ways. The ﬁrst one called the active leadership takeover
corresponds to the case where an arm takes the leadership by winning against the leader. The second one, passive leadership
takeover is simply the case where the leader loses so many duels that its number of samples falls below τ /(2K). We handle
the ﬁrst case similarly as in Baudry et al. (2020), while for the second we use Lemma 8.

We denote D(r) = {∃s ∈ [r − τ, r − 1] : (cid:96)τ (s) = k∗

B(r) = (cid:8)∃s ∈ [r − τ, r − 1] : (cid:96)τ (s) = k∗

φ} and we will upper bound P((cid:96)τ (r) (cid:54)= k∗
(cid:8)(cid:96)τ (s) = k∗
(cid:9) = ∪r−1

φ, (cid:96)τ (s + 1) (cid:54)= k∗
φ

s=r−τ

φ, D(r)). We introduce,
φ, (cid:96)τ (s + 1) (cid:54)= k∗
φ

(cid:9) .

One has,

1((cid:96)τ (r) (cid:54)= k∗

φ, D(r)) ≤ 1(B(r)) .

The change of leader can happen under three different scenarios: 1) some arm k takes the leadership after winning against
k∗
φ (active takeover), 2) arm k∗
φ loses the leadership because its number of samples falls below the threshold τ /(2K) and 3)
some arm takes the leadership after being pulled because of the diversity ﬂag. We remark that the activation of the diversity
ﬂag for some arm k cannot lead to a leadership takeover by arm k if (log τ )2 ≤ τ /K, so this scenario can only happen for
relatively small values of τ . These properties can be formulated as
(cid:8)(cid:96)τ (s) = k∗
φ, (cid:96)τ (s + 1) (cid:54)= k∗
φ

φ, (cid:96)τ (s + 1) = k, k ∈ As+1, Dτ

(cid:8)(cid:96)τ (s) = k∗

k (s) = 0(cid:9)

(cid:9) ⊂ ∪k(cid:54)=k∗

φ

(cid:110)

(cid:96)τ (s) = k∗
∪
∪ (cid:8)(cid:96)τ (s) = k∗

φ, N τ
φ, ∃k (cid:54)= k∗

(cid:96)τ (s)(s + 1) ≤ τ /(2K)

(cid:111)

φ : (cid:96)τ (s + 1) = k, Dτ

k (s) = 1(cid:9) .

Using this property it holds that

rφ+1−2
(cid:88)

r=rφ+2τ −1

1((cid:96)τ (r) (cid:54)= k∗

φ, D(r)) ≤

≤

+

+

rφ+1−2
(cid:88)

1(B(r))

r=rφ+2τ −1

rφ+1−2
(cid:88)

r−1
(cid:88)

(cid:88)

r=rφ+2τ −1

s=r−τ

k(cid:54)=k∗
φ

1 (cid:0)k ∈ As+1, (cid:96)τ (s) = k∗

φ, (cid:96)τ (s + 1) = k, Dτ

k (s) = 0(cid:1)

rφ+1−2
(cid:88)

r−1
(cid:88)

(cid:16)

1

(cid:96)τ (s) = k∗

φ, N τ

(cid:17)
(cid:96)τ (s)(s + 1) ≤ τ /(2K)

r=rφ+2τ −1

s=r−τ

rφ+1−2
(cid:88)

r−1
(cid:88)

(cid:88)

r=rφ+2τ −1

s=r−τ

k(cid:54)=k∗
φ

1 (cid:0)(cid:96)τ (s) = k∗

φ, (cid:96)τ (s + 1) = k, Dτ

k (s) = 1(cid:1) .

On Limited-Memory Subsampling Strategies for Bandits

We remark that if we reorganize the sums in s and r each element in the range [rφ + 2τ − 1, rφ+1 − 2] will appear at most τ
times, which leads to

rφ+1−2
(cid:88)

r=rφ+2τ −1

1((cid:96)τ (r) (cid:54)= k∗

φ, D(r)) ≤

rφ+1−2
(cid:88)

r=rφ+2τ −2
(cid:124)

(cid:88)

τ

k(cid:54)=k∗
φ

1 (cid:0)(cid:96)τ (r) = k∗

φ, (cid:96)τ (r + 1) = k, k ∈ Ar+1, Dτ

k (r) = 0(cid:1)

(cid:123)(cid:122)
C1

(cid:125)

rφ+1−2
(cid:88)

(cid:16)

τ 1

(cid:96)τ (r) = k∗

φ, N τ

(cid:96)τ (r)(r + 1) ≤ τ /(2K)

(cid:17)

(cid:125)

+

+

r=rφ+2τ −2
(cid:124)

rφ+1−2
(cid:88)

r=rφ+2τ −1
(cid:124)

(cid:88)

τ

k(cid:54)=k∗
φ

(cid:123)(cid:122)
C2

1 (cid:0)(cid:96)τ (r) = k∗

φ, (cid:96)τ (r + 1) = k, Dτ

k (r) = 1(cid:1)

.

(cid:123)(cid:122)
C3

(cid:125)

We then upper bound separately the three terms. We can upper bound C1 using Lemma 7 replacing n by the value τ /K − 2,

E[C1] ≤

(cid:88)

τ E





rφ+1−2
(cid:88)

(cid:16)

1

r=rφ+2τ −2

k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥

τ
K

− 2, Dτ

k (r) = 0

(cid:17)





k(cid:54)=k∗
φ

(cid:88)

k(cid:54)=k∗
φ

≤

δφτ (τ + 1)

e−(τ /K−2)ωk
1 − e−ωk

.

To handle C2 we will use Lemma 8. The deﬁnition of the leader ensures that when one arm takes the leadership is does it
with at least τ /K observations. Hence, to make this number go below the threshold τ /(2K), k∗
φ has to lose at least τ /(2K)
duels between the moment this arm took the leadership and the round r. There are two possibilities. The ﬁrst one is that k∗
φ
was leader for at least τ rounds: as the index of each arms are computed from observations that have been all drawn under
the leadership of k∗
φ while having more than τ /K − 1 observations, which results in an
active leadership takeover by this arm. Hence, a passive change of leader can only happen if k∗
φ was leader for less than τ
τ
rounds. In this case, we apply Lemma 8, it ensures that k∗
2K(K−1) (cid:99)
observations during the time it was leader. Formally,

φ lost at least one duel with an arm with more than (cid:98)

φ then at least one arm has to beat k∗

(cid:110)

(cid:96)τ (r) = k∗

φ, N τ
k∗
φ

(r + 1) ≤ τ /(2K)

(cid:111)

⊂ ∪r−1

s=r−τ

(cid:26)

∃k (cid:54)= k∗

φ : k ∈ As+1, (cid:96)τ (s) = k∗

φ, N τ

k (s) ≥

(cid:22)

τ
2K(K − 1)

(cid:23)(cid:27)

.

We can write

E[C2] = τ E





rφ+1−2
(cid:88)

1((cid:96)τ (r) = k∗

φ, N τ
k∗
φ

(r + 1) ≤ τ /(2K))





r−1
(cid:88)

(cid:18)

1

s=r−τ

k ∈ As+1, (cid:96)τ (s) = k∗

φ, N τ

k (s) ≥

(cid:22)

τ
2K(K − 1)

(cid:23)

(cid:19)

, Dτ

k (s) = 0





(cid:18)

1

k ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k (r) ≥

(cid:22)

(cid:23)

τ
2K(K − 1)

(cid:19)

, Dτ

k (r) = 0





r=rφ+2τ −2


rφ+1−2
(cid:88)

≤ τ

(cid:88)

k(cid:54)=k∗
φ

E



r=rφ+2τ −2


rφ+1−2
(cid:88)

E



r=rφ+2τ −2
e−(cid:98)

δφτ 2(τ + 1)

≤ τ 2 (cid:88)
k(cid:54)=k∗
φ

(cid:88)

≤

k(cid:54)=k∗
φ

τ

2K(K−1) (cid:99)ωk

1 − e−ωk

.

In the second to last inequality, we have used that the terms can appear at most τ times and the last inequality result from
Lemma 7.

On Limited-Memory Subsampling Strategies for Bandits

We now focus on the term C3. We use that {(cid:96)τ (s + 1) = k, Dτ
(log τ )2 ≤ τ /K, the activation of the diversity ﬂag is not sufﬁcient to take over the leadership. We recall that,

k (s) = 1} can happen only if τ /K ≤ (log τ )2 because if

E[C3] = E





rφ+1−2
(cid:88)

(cid:88)

τ

r=rφ+2τ −2

k(cid:54)=k∗
φ

1 (cid:0)(cid:96)τ (r) = k∗

φ, (cid:96)τ (r + 1) = k, Dτ

k (r) = 1(cid:1)



 .

Using Equation (23), and letting b = (cid:100)(K − 1)(log τ )2(cid:101), one has

E[C3] ≤ τ

(cid:88)

E





rφ+1−2
(cid:88)

(cid:88)

r−1
(cid:88)

k(cid:54)=k∗
φ

r=rφ+2τ −2

k(cid:48)(cid:54)=k∗
φ

s=r−b

1(k(cid:48) ∈ As+1, (cid:96)τ (s) = k∗

φ, N τ

k(cid:48)(s) ≥ (log τ )2, Dτ

k(cid:48)(s) = 0)1 (cid:0)τ /K ≤ (log τ )2(cid:1)





≤ τ (K − 1)

(cid:88)

k(cid:48)(cid:54)=k∗
φ

1 (cid:0)τ /K ≤ (log τ )2(cid:1) E





rφ+1−2
(cid:88)

r=rφ+2τ −2

1(k(cid:48) ∈ Ar+1, (cid:96)τ (r) = k∗

φ, N τ

k(cid:48)(r) ≥ (log τ )2, Dτ

k(cid:48)(r) = 0)

 .



As 1 (cid:0)τ /K ≤ (log τ )2(cid:1) is deterministic, we conclude by applying Lemma 7.

E[C3] ≤ (K − 1)

δφτ (τ + 1)

e−(log τ )2ωk
1 − e−ωk

(cid:88)

k(cid:54)=k∗
φ

1 (cid:0)τ /K ≤ (log τ )2(cid:1) .

We then use the condition on τ to simply upper bound C3 by

E[C3] ≤ (K − 1)

δφτ (τ + 1)

e−(τ /K)ωk
1 − e−ωk

.

(cid:88)

k(cid:54)=k∗
φ

We observe that the three terms E[C1], E[C2] and E[C3] have very similar upper bounds, so we ﬁnally regroup them in a
single term using

≤ τ /K − 2 ≤ τ /K.

(cid:107)

(cid:106)

τ
2K(K−1)

E[C1] + E[C2] + E[C3] ≤ 3δφτ 2(τ + 1)(K − 1)

e−(cid:98)

τ

2K(K−1) (cid:99)ωk

1 − e−ωk

.

(cid:88)

k(cid:54)=k∗
φ

Part 2: the optimal arm has never been the leader after the 2τ ﬁrst observations of the phase.

We now aim at upper bounding E
between r − τ and r − 1. To do so, we use that

r=rφ+2τ −2

(cid:104)(cid:80)rφ+1−2

(cid:105)
1(D(r)c)

, where D(r)c is the event that k∗

φ has never been the leader

D(r)c ⊂

(cid:40) r−1
(cid:88)

s=r−τ

1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ

(cid:1) ≥

(cid:41)

τ
2

,

and as in Chan (2020) we would like to handle this term using the Markov inequality. However, the problem in non-stationary
environment is that the index of the sum is a random variable. Hence, to get back to a sum with a deterministic number of
terms we introduce the set Rφ = [rφ + 2τ − 1, rφ+1 − 2] and write





E

rφ+1−2
(cid:88)

r=rφ+2τ −1

1(D(r)c)


 = E

(cid:34) T

(cid:88)

r=2τ

(cid:35)

1(D(r)c, r ∈ Rφ)

T
(cid:88)

r=2τ

T
(cid:88)

r=2τ

T
(cid:88)

≤

≤

≤

E [1(D(r)c, r ∈ Rφ)]

(cid:32) r−1
(cid:88)

s=r−τ
(cid:32) r−1
(cid:88)

P

P

1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ

(cid:1) ≥

(cid:33)

τ
2

, r ∈ Rφ

1(r ∈ Rφ)1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ

(cid:1) ≥

(cid:33)

τ
2

.

r=2τ

s=r−τ

On Limited-Memory Subsampling Strategies for Bandits

At this step we can use the Markov inequality, and obtain





E

rφ+1−2
(cid:88)

r=rφ+2τ −1



1(D(r)c)

 ≤

T
(cid:88)

r=2τ

E

2
τ

(cid:34) r−1
(cid:88)

s=r−τ

1(r ∈ Rφ)1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ

(cid:35)

(cid:1)

≤ E

(cid:34) T

(cid:88)

r=2τ

1(r ∈ Rφ)

2
τ

r−1
(cid:88)

s=r−τ

1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ

(cid:35)

(cid:1)





≤ E

(cid:88)

r∈Rφ

2
τ

r−1
(cid:88)

s=r−τ

1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ





(cid:1)





= E

rφ+1−2
(cid:88)

r=rφ+2τ −1

2
τ

r−1
(cid:88)

s=r−τ

1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ



(cid:1)

 .

Hence,

where,





E

rφ+1−2
(cid:88)

r=rφ+2τ −1

1(D(r)c)


 ≤ E





rφ+1−2
(cid:88)

r=rφ+2τ −1

2
τ

r−1
(cid:88)

s=r−τ

1 (cid:0)k∗

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
φ

(cid:1)

 ≤ D1 + D2 ,







D1 = E

rφ+1−2
(cid:88)

r=rφ+2τ −1





D2 = E

rφ+1−2
(cid:88)

r=rφ+2τ −1

2
τ

2
τ

(cid:16)

r−1
(cid:88)

1

s=r−τ

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
k∗

φ, N τ
k∗
φ

(s) ≥ Aφ,τ
k∗
φ

(cid:17)





1

(cid:16)

N τ
k∗
φ

(s) ≤ Aφ,τ
k∗
φ

(cid:17)



 .

r−1
(cid:88)

s=r−τ

The different rounds can appear at most τ times in the double sum. Using this and the second equation of Lemma 7, D1 can
be upper bounded

D1 ≤ 2E





rφ+1−2
(cid:88)

(cid:16)

1

r=rφ+2τ −2

φ /∈ Ar+1, (cid:96)τ (r) (cid:54)= k∗
k∗

φ, N τ
k∗
φ

(r) ≥ Aφ,τ
k∗
φ

(cid:17)



 ≤ 2δφ(τ + 1)

(cid:88)

k(cid:54)=k∗
φ

ωk

−Aφ,τ
k∗
φ

e
1 − e−ωk

.

(r, τ ) ≤ Aφ,τ
Contrarily to the stationary case, we cannot work directly with D2 and have to further decompose 1(Nk∗
).
k∗
φ
Indeed, the proof in the stationary case use the sparsity of the observations of k∗
φ when it has not been pulled a lot, and the
fact that in this case it has necessarily lost a lot of duel while having a ﬁxed sample size. This is not the case in the non
stationary environment, as for instance if k∗
φ has been pulled a lot in the previous windows its index may change a lot. To
avoid this we split the event according to the values of N τ

φ

k (r − τ ).

(cid:16)

1

N τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

(cid:17)

≤ 1

(cid:16)

N τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

, N τ
k∗
φ

(r − τ ) > Aφ,τ
k∗
φ

(cid:17)

(cid:16)

+ 1

N τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

, N τ
k∗
φ

(r − τ ) ≤ Aφ,τ
k∗
φ

(cid:17)

.

We then write D2 = 2(D3 + D4), with





D3 = E

rφ+1−1
(cid:88)

(cid:16)

1

r=rφ+2τ −1





D4 = E

rφ+1−1
(cid:88)

(cid:16)

1

r=rφ+2τ −1

N τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

, N τ
k∗
φ

(r − τ ) > Aφ,τ
k∗
φ

N τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

, N τ
k∗
φ

(r − τ ) ≤ Aφ,τ
k∗
φ

(cid:17)



 ,

(cid:17)



 .

On Limited-Memory Subsampling Strategies for Bandits

D3 can be upper bounded using Equation (18) in Lemma 7. Indeed, if N τ
, for large
k∗
φ
φ can not be the leader and lost at least one duel against a suboptimal leader while having exactly Aφ,τ
enough values of τ , k∗
k∗
φ
samples between round r − τ and round r − 1, thus

(r − τ, τ ) > Aφ,τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

and N τ
k∗
φ

(cid:110)

N τ
k∗
φ

(r) ≤ Aφ,τ
k∗
φ

, N τ
k∗
φ

(r − τ ) > Aφ,τ
k∗
φ

(cid:111)

⊂ ∪r−1

s=r−τ

(cid:110)

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
k∗

φ, N τ
k∗
φ

(s) = Aφ,τ
k∗
φ

(cid:111)

.

We use the same trick as for D1 and D2 to handle the sums and write





D3 ≤ E

rφ+1−1
(cid:88)

r−1
(cid:88)

(cid:16)

1

φ /∈ As+1, (cid:96)τ (s) (cid:54)= k∗
k∗

φ, N τ
k∗
φ

(s) = Aφ,τ
k∗
φ

(cid:17)





s=r−τ

r=rφ+2τ −1


rφ+1−1
(cid:88)

≤ τ E



r=rφ+2τ −1

(cid:16)

1

φ /∈ Ar+1, (cid:96)τ (r) (cid:54)= k∗
k∗

φ, N τ
k∗
φ

(r) = Aφ,τ
k∗
φ

(cid:17)



 .

We can directly use Lemma 7, however we remark that as we do not have to use an union bound on the values of N τ
k∗
φ
can remove the factor 1/(1 − e−ωk ). Hence, we ﬁnally get

we

D3 ≤ δφτ (τ + 1)e

−Aφ,τ
k∗
φ

ωk

.

(r − τ ) ≤ Aφ,τ
k∗
φ

We then handle D4 by using the arguments introduced by Baransi et al. (2014) with some novelty due to the sliding window.
Indeed, we remark that if both N τ
different
k∗
φ
index in the entire window [r − τ, r − 1]. This is due to the fact that the index change only if k∗
φ is pulled (can happen at
most Aφ,τ
times) or if k∗
φ loses one observation from the window [r − 2τ, r − τ − 1] due to the sliding window (which can
k∗
φ
also happen at most Aφ,τ
times). Thanks to these properties we know that during the interval [r − τ, r − 1] we are sure that
k∗
φ
φ lost at least τ − Aφ,τ
duels, and that a fraction 1/2Aφ,τ
k∗
k∗
k∗
φ
φ

φ competes with at most 2Aφ,τ
k∗
φ

of them occurred while the index of k∗

φ remained the same.

(r) ≤ Aφ,τ
k∗
φ

and N τ
k∗
φ

, then k∗

Our objective is to highlight a property similar to the balance condition. To do so we need to identify the fraction of the
duels played by k∗
φ with the same index and against non-overlapping blocks (i.e of mutually independent means) of any
suboptimal arm k ∈ {1, . . . , K}, k (cid:54)= k∗
φ. To avoid cumbersome notations we summarize the elements that allow this
conclusion, ﬁrst recalling the arguments of the previous paragraph:

• k∗

φ lost at least τ − Aφ,τ
k∗
φ

duels in the window [r − τ, r − 1]

• A fraction 1/(2Aφ,τ
k∗
φ

) of them has been played with a ﬁxed index for k∗

φ, i.e with the subsample mean of the same

√

block. With a forced exploration B(τ ) =

log τ this block can have any size between

√

log τ and Aφ,τ
k∗
φ

.

• Among those duels, a fraction of at least 1/(K − 1) of them has been played against the same suboptimal arm k (cid:54)= k∗
φ.

The next step is to identify the proportion of these duels that have been played against non-overlapping blocks of k. As in
the proof for the stationary case we proceed in 2 steps. First we identify the number of different duels (i.e the index of k is
not based on the same block of observations of k) played by k∗
φ against k. However, thanks to the diversity ﬂag we know a
new duel happens after at most each (K − 1)(log τ )2 rounds. So we further process the set of duels previously identiﬁed
stating that:

• A fraction of

1

(K−1)(log τ )2 has been played against different index of k based on different blocks of observations from

the history of k, thanks to the diversity ﬂag.

• As the blocks are of maximum size Aφ,τ
k∗
φ

a fraction at least 1/Aφ,τ
k∗
φ

of them are non-overlapping.

On Limited-Memory Subsampling Strategies for Bandits

We put all these elements together to state that there exist some β ∈ (0, 1) such that for any value of τ large enough k∗

φ lost

(cid:37)

βτ
2(K−1)2(log τ )2(Aφ,τ
k∗
φ
j . Summing on all the arms, rounds, possible interval (index n) and size of the history of k∗

duels against non-overlapping blocks of some challenger k, with a ﬁxed index. We

φ (index j), we

)2

(cid:36)

at least C τ =

write this event Eτ
obtain

D4 ≤E








(cid:88)

rφ+1−1
(cid:88)

(cid:22)

2

Aφ,τ
k∗
φ
(cid:88)

k(cid:54)=k∗
φ

r=rφ+2τ −1

n=1

(cid:23)

(cid:23)

(cid:22)

Aφ,τ
k∗
φ
(cid:88)

√

j=

log τ



1(Eτ
j )






.

As these events do not depend on r and on n we have

D4 ≤2δφAφ,τ
k∗
φ

≤2δφAφ,τ
k∗
φ

(cid:23)

(cid:22)

Aφ,τ
k∗
φ
(cid:88)

(cid:88)

k(cid:54)=k∗
φ

√

j=

log τ

(cid:23)

(cid:22)

Aφ,τ
k∗
φ
(cid:88)

(cid:88)

k(cid:54)=k∗
φ

√

j=

log τ

E (cid:2)1(Eτ

j )(cid:3)

αφ
k (C τ , j) .

Here αk is the balance function, as deﬁned in Appendix A. We index these functions by φ and k in order to denote the
balance function between k∗

φ and k in the phase φ. We recall the deﬁnition of αk, for any integer M

αφ
k (M, j) = E

X∼νφ
k∗
φ

(cid:16)

(1 − F φ

k,j(X))M (cid:17)

,

k(cid:48) is the distribution of the sum of j random variables drawn from the distribution of an arm k(cid:48) in the phase φ, and
k(cid:48),j its cdf. We then use the Lemma 5, introduced and proved in Appendix A. We recall that this result state that for any

αk(C τ , j) ≤ e

−jkl(µφ

k ,µk∗
φ

)

u + (1 − u)Cτ

.

) = ωφ

k , and choose the value u = 3 log τ
Cτ

. Thanks to this choice, there exist a constant γ > 1 such that

where νφ
F φ
u ≤ µφ

k it holds that

We write kl(µφ

k , µk∗

φ

(1 − u)Cτ

= exp (C τ log(1 − u))

(cid:18)

C τ log

(cid:18)

1 −

= exp

(cid:19)(cid:19)

3 log τ
C τ

≤ γ exp (−3 log τ )

≤

γ
τ 3 .

If we plug this expression to upper bound the sums we obtain

D4 ≤ 2δφAφ,τ
k∗
φ

(cid:88)

k(cid:54)=k∗
φ

≤ 2δφAφ,τ
k∗
φ

(cid:88)

k(cid:54)=k∗
φ



(cid:23)

(cid:22)

Aφ,τ
k∗
φ
(cid:88)

√

log τ

j=


√

log τ ωφ
k

e−
1 − e−ωφ


√

k

(cid:20)
e−jωφ

k

(cid:21)

3 log τ
C τ +

γ
τ 3

3 log τ
C τ +

γAφ,τ
k∗
φ
τ 3





≤ 2δφAφ,τ
k∗
φ

(K − 1)



log τ ωφ

e−
1 − e−ωφ

3 log τ
C τ +

γAφ,τ
k∗
φ
τ 3



 ,

On Limited-Memory Subsampling Strategies for Bandits

ωφ

where ωφ = mink(cid:54)=k∗
terms in the regret analysis. Indeed, if we only look at the order of Aφ,τ
k∗
φ
(cid:16)
e−

k . Even if these terms look impressive we explain in the next section that they are not ﬁrst order
, C τ , we can use the same argument as in the
√

proof of Lemma 4. Considering that for any integer k > 1, (log τ )k = o

we obtain that asymptotically D4 is a

log rω(cid:17)

φ

(cid:16) δφ

τ log τ k(cid:48)

o

(cid:17)

for any integer k(cid:48) ≥ 1.

C.3. Summary: Upper Bound on the Dynamic Regret

Objective Due to the many terms introduced in the analysis we provide in this section a clariﬁcation of the ﬁnal terms in
the regret. First of all we recall the decomposition introduced in the Section 4 to control the number of pulls of a suboptimal
arm during a phase φ ∈ [1, ΓT ],

E[N φ

k ] ≤ 2τ +

δφAφ,τ
k
τ

+ E[cφ,τ

k,1 ] + E[cφ,τ

k,2 ] + E[cφ,τ

k,3 ] .

Results of Section C We ﬁrst provide the results we obtained in Appendix C, that are true for any value of the
sliding window τ and the function Aφ,τ
k , that we will properly calibrate later. We also recall that for any sub-
optimal arm k in a phase φ we deﬁned a constant ωφ
k (written ωk in the proof as the phase is explicit), satisfying
k + µφ
k + µφ
ωφ
k = min
k∗
k∗
φ
φ

2 (µφ
, 1

µφ
k , 1

2 (µφ

(cid:17)(cid:17)
)

µφ
k∗
φ

(cid:17)
)

, kl

kl

(cid:16)

(cid:16)

(cid:16)

.

We ﬁrst obtained an upper bound on E[cφ,τ
when the best one is leader, and E[cφ,τ
diversity ﬂag when the best arm is leader. These upper bounds are

k,1 ], which controls the probability that a "concentrated" suboptimal arm k is pulled
k,2 ], that represents the expectation of the number of pulls of the arm k because of the

E[cφ,τ

k,1 ] ≤ δφ(τ + 1)

k ωk

e−Aφ,τ
1 − e−ωk

, E[cφ,τ

k,2 ] ≤ δφ(τ + 1)

e−(log τ )2ωk(cid:48)
1 − e−ωk(cid:48)

.

(cid:88)

k(cid:48)(cid:54)=k∗
φ

We then provided an upper bound of E[cφ,τ
k,3 ] composed of multiple terms. This is because this term represents the expectation
of the number of rounds when the best arm is not leader. To provide a general overview, this term is composed of two parts:
the ﬁrst one for the cases when the best arm has already been leader in the last τ rounds, and the case when the best arm has
never been leader in the last τ round. The ﬁrst general scenario was handled by the constants C1, C2 and C3, that we upper
bounded in expectation by,

E [C1 + C2 + C3] ≤ 3δφτ 2(τ + 1)(K − 1)

(cid:88)

e−(cid:98)

τ

2K(K−1) ωk(cid:48)(cid:99)

1 − e−ωk(cid:48)

k(cid:48)(cid:54)=k∗
φ

.

We observe that this term has a larger order in τ than the previous one before the exponential, but as a larger term in the
exponential that compensates. After that, we handled the cases when the best arm has never been leader in . We distinguish
again different cases. The terms D1 and D3 provide terms that share similar order with the ones we obtained before, namely:

D1 ≤ 2δφ(τ + 1)

e−Aφ,τ
k(cid:48) ωk(cid:48)
1 − e−ωk(cid:48)

(cid:88)

k(cid:48)(cid:54)=k∗
φ

and D3 ≤ δφτ (τ + 1)e−(log τ )2ωk

The last term is the one that corresponds to the balance condition in the stationary case. Its adaptation to the non-stationary
case was non trivial but we could provide an upper bound, leveraging on the properties detailed in Appendix A. We obtained

D4 ≤ 2δφAφ,τ
k∗
φ

(K − 1)





(cid:36)

where C τ =

βτ
2(K−1)2(log τ )2(Aφ,τ
k∗
φ

)2

(cid:37)
.

√

log τ ωφ

e−
1 − e−ωφ

3 log τ
C τ +

γAφ,τ
k∗
φ
τ 3



 ,

On Limited-Memory Subsampling Strategies for Bandits

Tuning of the parameters The previous results allow to control precisely the dynamic regret of SW-LB-SDA for general
values of τ and the constants of the problem. We ﬁrst remark that one could tune each of the constants Aφ,τ
to optimize
k∗
φ
the term in each phase. However, in this paragraph we propose a more general asymptotic analysis that proves that an
optimal tuning of τ allows the algorithm to reach optimal guarantees. To catch this generality we will simply deﬁne
Aφ,τ
k }. With these new deﬁnitions we
k∗
φ
can regroup several terms together, and obtain for τ > K

= A(τ ) = B log τ for some constant B, and deﬁne ω = minφ∈[1,ΓT ]{mink(cid:54)=k∗

ωφ

φ

E[N φ

k ] ≤2τ +

δφA(τ )
τ

+

2δφ(τ + 1)K
1 − e−ω

+ 3δφτ 2(τ + 1)(K − 1)2 e−(cid:98)

Kδφτ (τ + 1)
1 − e−ω

e−A(τ )ω +

τ

2K(K−1) ω(cid:99)

1 − e−ω

+ 2δφA(τ )(K − 1)

e−(log τ )2ω

(cid:34)

√

log τ ω

e−
1 − e−ω

(cid:35)

3 log τ
C τ +

γA(τ )
τ 3

As the only term that depends on the phase is δφ it is now straightforward to sum on the phases and the arms to obtain the
dynamic regret, recalling that (cid:80)ΓT
φ=1 δφ = T . Without loss of generality, we also assume that for all φ and for all k (cid:54)= k∗
φ,
∆φ

k ≤ 1.

RT =

ΓT(cid:88)

(cid:88)

φ=1

k(cid:54)=k∗
φ

E[N φ

k ]∆φ

k

≤ 2(K − 1)τ ΓT +
(cid:123)(cid:122)
E1

(cid:124)

(K − 1)T A(τ )
τ

+

2T (τ + 1)K(K − 1)
1 − e−ω
(cid:123)(cid:122)
E2

(cid:124)

e−A(τ )ω

(cid:125)

(cid:125)

+

T K(K − 1)τ (τ + 1)
1 − e−ω

(cid:124)

(cid:123)(cid:122)
E3

e−(log τ )2ω
(cid:125)

+

(cid:124)

3T (K − 1)τ 2(τ + 1)(K − 1)2
1 − e−ω

e−(cid:98)

τ

2K(K−1) ω(cid:99)

(cid:125)

(cid:123)(cid:122)
E4

+ 2T A(τ )(K − 1)2

(cid:34)

(cid:124)

√

log τ ω

e−
1 − e−ω
(cid:123)(cid:122)
E5

3 log τ
C τ + (K − 1)

γA(τ )
τ 3

(cid:35)

(cid:125)

Knowing the horizon T and an order of the number of breakpoints ΓT we propose a tuning for τ in
prove that the only ﬁrst order terms in the decomposition are the terms in E1.
First, as log τ is of order log T , choosing A(τ ) = 6
ω log τ ensures that E2 is upper bounded by a constant. Then, the terms
E3 and E4 are also both upper bounded by constants as the term in the exponent dominates the polynomial in τ before it.
The term E5 is a bit more touchy. Indeed, its second component causes no difﬁculty and is upper bounded by a constant.
However, for the ﬁrst term we need to use the fact C τ is of order τ / log(τ )j, hence there exists some integer j(cid:48) such that the
dominant term in E5 is of order T
e−
log τ ω = o(log(τ )−1)
(for instance). Hence, thanks to the log terms E5 is of lower order than E1. Finally, we obtain

log τ ω. As in Appendix A we use that (log τ )j(cid:48)

τ × (log τ )j(cid:48)

. We then

e−

√

√

(cid:113) T log T
ΓT

This concludes the proof of Theorem 3.

RT = O((cid:112)T ΓT log T ) .

