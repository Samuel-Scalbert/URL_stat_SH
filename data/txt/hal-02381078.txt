Different Flavors of Attention Networks for Argument
Mining
Johanna Frau, Milagro Teruel, Laura Alonso Alemany, Serena Villata

To cite this version:

Johanna Frau, Milagro Teruel, Laura Alonso Alemany, Serena Villata. Different Flavors of Attention
Networks for Argument Mining. FLAIRS 2019 - 32th International Florida Artificial Intelligence
Research Society Conference, May 2019, Sarasota, United States. ￿hal-02381078￿

HAL Id: hal-02381078

https://hal.science/hal-02381078

Submitted on 27 Nov 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Different Flavors of Attention Networks for Argument Mining

Johanna Frau,1 Milagro Teruel,1 Laura Alonso Alemany,1 Serena Villata,2
1Natural Language Processing Group, FAMAFyC-UNC, C´ordoba, Argentina.
2Universit´e Cˆote d’Azur. CNRS, Inria, I3S, France.
jfrau@famaf.unc.edu.ar, lauraalonsoalemany@unc.edu.ar, mteruel@unc.edu.ar, villata@i3s.unice.fr

Abstract

Argument mining is a rising area of Natural Language Pro-
cessing (NLP) concerned with the automatic recognition and
interpretation of argument components and their relations.
Neural models are by now mature technologies to be ex-
ploited for automating the argument mining tasks, despite the
issue of data sparseness. This could ease much of the man-
ual effort involved in these tasks, taking into account hetero-
geneous types of texts and topics. In this work, we evaluate
different attention mechanisms applied over a state-of-the-art
architecture for sequence labeling. We assess the impact of
different ﬂavors of attention in the task of argument compo-
nent detection over two datasets: essays and legal domain.
We show that attention not only models the problem better
but also supports interpretability.

Introduction
Argument Mining (Peldszus and Stede 2013; Lippi and Tor-
roni 2016; Cabrio and Villata 2018) tackles a very complex
phenomenon, involving several levels of human communi-
cation and cognition. It has been deﬁned as “the general task
of analyzing discourse on the pragmatics level and applying
a certain argumentation theory to model and automatically
analyze the data at hand” (Habernal and Gurevych 2017).
Due to the complexity of the task, data-driven approaches
require a huge amount of data to properly characterize the
phenomena and ﬁnd patterns that can be exploited by a clas-
siﬁer. However, most of the corpora annotated for this task
are small, and they cannot be used in combination because
they are based on different theoretical frameworks (e.g., ab-
stract and structured argumentation) or cover different gen-
res (e.g., political debates, social network posts). Our pri-
mary research domain is argument mining in legal cases,
where corpora are especially scarce.

In this paper, we analyze the utility of neural attention
mechanisms to improve the tasks of Argument Mining. It
has been shown that some kind of attention improves the
performance of neural-based argument mining systems Stab
et al. (2018). It seems that attention mechanisms concentrate
the classiﬁer on the most determinant elements to take into
account, and thus direct the training of the classiﬁer to a bet-
ter convergence point. Moreover, attention has a promise of

interpretability, since the parts of the input that receive more
attention can be recognized.

We assess the impact of adding an attention mechanism
to a state-of-the-art neural model (BiLSTM with charac-
ter embeddings and a CRF layer) (Reimers and Gurevych
2017)1.We apply different ﬂavours of attention and compare
their impact on performance. We assess the impact of perfor-
mance over two very different corpora for Argument Min-
ing: a corpus of persuasive essays (Stab and Gurevych 2017)
and a small corpus of the legal domain, consisting of only 8
judgments (Teruel et al. 2018) of the European Court of Hu-
man Rights (ECHR).2. We focus on the argument compo-
nent detection task, distinguishing claims and justiﬁcations
or focusing on claim detection alone.

We show that attention mechanisms consistently improve
performance. Additionally, it helps to identify words that are
important for the Argument Mining task, useful for guide-
lines and selection of examples for annotation.

In the rest of the paper we present relevant work, describe
the architecture of the attention-based system, then we detail
our experimental setting and we discuss the obtained results.

Related work

In this section, we report on the approaches proposed in
the area of Argument Mining employing recurrent neural
networks and attention mechanisms. For state-of-the-art on
argument mining, we refer the reader to (Cabrio and Vil-
lata 2018). Recurrent neural networks have been success-
fully applied to the problem of Argument Mining. In Eger et
al. (2017), a single model is trained to jointly classify argu-
mentative components and the relations of attack and sup-
port between them. The model is evaluated over the dataset
of argumentative essays. Stab et al. (2018) applied atten-
tion to crowd-sourced general domain argumentative essays
from the Web, to measure how relevant each word on the
input is, with respect to the topic of the essay.

Our ﬁnal aim is to ﬁnd a good system for argument min-
ing in legal cases as the ones in the ECHR corpus. In legal
documents, the length and complexity of the texts is bigger
than for essays, there is no pre-deﬁned topic and the spans to

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf
2hudoc.echr.coe.int

The Thirty-Second International Florida Artificial Intelligence Research Society Conference (FLAIRS-32)173be detected are intra-sentential, which makes the task con-
siderably more complex than for essays.

Architecture

In this section, we ﬁrst describe the Bidirectional Long
Short-Term Memory (BiLSTM) we adopted for our Ar-
gument Mining task, and second we present the attention
mechanism we empowered the BiLSTM with.

BiLSTM architecture

Our proposed model adds an attention mechanism over
the recurrent architecture implemented by (Reimers and
Gurevych 2017), which we will brieﬂy describe in this sec-
tion. The base of the model is a bidirectional recurrent layer
with LSTM units. The input layer is composed of three parts:
a word embedding, a character embedding and a casing
embedding. The word embedding uses pre-trained weights,
supporting any type of word vectors. The character embed-
dings generate a representation for each word by perform-
ing either a convolution operation or a recurrence operation
at a character level. The objective of this type of embed-
dings is to generate a representation of the out-of-vocabulary
(OOV) words based on their morphology. The weights for
both methods are trained with the network. The casing em-
bedding generates a very small representation of the word
using features like presence of digits or all uppercase charac-
ters. Finally, the model replaces the traditional classiﬁcation
layer, a dense layer with a softmax activation, with a Condi-
tional Random Field (CRF) layer (Huang, Xu, and Yu 2015).
A CRF takes into account the surrounding labels to calculate
the label for the current word. This mechanism should pro-
duce more consistent labels, to avoid for example starting a
claim in the middle of a premise.

The attention mechanism

Attention mechanisms, in simple terms, weight the output
of a layer with importance scores. These scores are used to
increase the values of inputs which are relevant to the pre-
diction of the correct output, and to decrease the values of in-
puts that are not relevant. Different types of attention mech-
anisms arise depending on how we calculate the attention
scores: we can use only the information from each example
individually, or use the whole set of examples.

In this work, we propose a simple attention mechanism
applied directly to the input words of the sentence, visu-
alized in Figure 1. The goal of placing the attention layer
before the recurrent layer is to weight which words in the
sequence are relevant to the task we want to solve. This
type of attention, called inner attention, was ﬁrstly proposed
by (Wang, Liu, and Zhao 2016).

The attention layer added to the base RNN model is com-
posed of two parts: one that calculates the attention scores,
and a merge function ⊗ that combines them with the original
input. As a result of this combination, the attention scores
regulate how much of the initial value of the cells in the input
are passed to the following layer, increasing or alternatively
turning off certain values.

The attention scores are calculated using a fully con-
nected layer. This layer has the following parameters: a
weight matrix W A, a bias bA vector, and an activation func-
tion f . Once we obtain the attention scores, the merge func-
tion (in this case multiplication) is applied pointwise. Let it
be x a single sequence with n timesteps and m features (in-
put of the network), s the attention scores, and xA the output
of the attention layer, then the operations performed inside
the attention layer are:

s = f (xW A + bA)

xA = x ⊗ s

(1)

Note that, to obtain the attention score si (corresponding
to the i-th input on the sequence), we are multiplying each
row xi by the parameter W A. There is an important impli-
cation to this equation: the attention scores for each cell in
the input are computed taking into account only the features
in that timestep input, and not using any information present
on the rest of the elements in the sequence. We call this ap-
proach word attention.

This may seem counter intuitive, as we expect the impor-
tance of a word in the sentence not to be only determined
by itself, but also by the words with which it co-occurs, by
the property of compositionality of sentential semantics. We
have also explored a different attention mechanism, where
the attention score for feature j in instance xi is calculated
using the value of feature j in all timesteps of the sequence.
The model, which only differs in two transpositions, is de-
scribed in Equation . We call this approach context attention.

s = f (xT W A + bA)

xA = x ⊗ sT

(2)

As we can see, all the operations of the two attention lay-
ers are differentiable, and we can optimize its parameters
along with the training of the original model.

The selection of the activation function f has also an im-
portant impact. We propose three different options: linear
activation, a sigmoid function, and an hyperbolic tangent
function. If we use linear activation, the values of the atten-
tion scores are not limited to any range, and could possibly
explode to huge values, become negative or vanish to nearly
zeros. However, in our experiments the values remain in rea-
sonable intervals (between 1 and 10). Furthermore, linear
activation makes the attention scores a linear combination
of the original values, but we expect the pointwise multi-
plication with the mean score of the instance to effectively
add expressiveness to the model. The sigmoid function, on
the other hand, introduces non linearity and forces the at-
tention scores to the (0, 1) interval. After the application
of the scores, all the input activations are decreased. This
could lead to a faster vanishing of the neuron’s signals as
they propagate through the network. Finally, the hyperbolic
tangent (tanh) function does not have this problem, as it con-
straints the scores to the interval (-1, 1). However, it enables
negative attention, which does not have an intuitive interpre-
tation.

174Figure 1: Attention mechanism applied to the Char embedding + BiLSTM + CRF architecture.

Experimental Setting

Datasets
We applied the model on two datasets developed for the
Argument Mining task of argument component detection.
The essays dataset consists of 400 argumentative essays
(148,000 words) written by students in response to contro-
versial topics (Stab and Gurevych 2017). In this dataset,
the argument components are separated in Claims, Major
Claims and Premises, but we collapsed the distinction be-
tween Major Claims and Claims to make it homogeneous
with the ECHR corpus.

Our primary research interest concerns a small cor-
pus of the European Court of Human Rights (ECHR
dataset) (Teruel et al. 2018), with 8 judgments (28,847
words). It has, 329 claims and 401 premises. As input for
the attention learners, each sequence is an entire paragraph,
and the classiﬁer is expected to detect all components within
the paragraph.

A major difference between these two corpora, besides
their sizes, is the length of the sequences. For the essays
corpus, the longest paragraph is 98 words long, while the
longest paragraph in the ECHR documents is 317 words
long. The maximal length determines the length of the se-
quences given to the classiﬁer for each corpus. The longer
the sequence, the more difﬁcult the task of calculating
context attention, as context distributes attention along the
whole length of the sequence.

Evaluation procedure and metrics
To evaluate our models using the ECHR dataset, we use a 8
fold cross validation where, in each fold, we leave out one
entire document for testing, instead of just leaving out a sub-
set of examples. We consider this setting to be more repre-
sentative of the real case scenario of a production system,

where the new instances are documents never seen before.
As the essays dataset has more documents, we perform the
evaluation on 9 holdout documents.

The evaluation metric used is the F1-score, the harmonic
mean between precision and recall. All values reported are
averaged between classes with a weighted strategy, to avoid
giving too much weight to the minority B- classes.

Hyperparameter search

During the experiment phase, we noted that, in the ECHR
dataset, models were highly sensitive to the architecture and
hyperparameter selection. Contrary to the results on previ-
ous works, simpler models without character embeddings
and CRF do not perform necessarily worse. As a result, we
use a random search to ﬁnd successful combinations of hy-
perparameters. The evaluated combinations are:

• BiLSTM layer with identical layers of 30, 50, 100, or 200

units.

• Mini batches sizes of 30, 50, 100, or 200.

• Dropout on the LSTM layer of 0.1, 0.2, 0.3, 0.4, or 0.5.

• Classiﬁcation layer with softmax activation and with CRF

activation.

• Character embeddings with convolutions and with recur-

rence, resulting on vectors of sizes 16, 32 or 64.

The optimizer used is Adam, with a clip norm of 1. All
classiﬁers are trained during 50 epochs, stopping if there is
no improvement for 10 epochs.

Evaluation
In this section, we ﬁrst discuss the results we obtain with
our classiﬁer for argument component detection empowered

175with attention, distinguishing claims and premises and fo-
cusing on claim detection alone. Claim detection is a sim-
pler, better deﬁned task which still retains utility in applica-
tions. Indeed, Teruel et al. (2018) show that inter-annotator
agreement is higher for claims than for premises. Thus the
training and evaluation data for claim detection are more
consistent, therefore it provides for a fair evaluation.

Secondly, we present how attention mechanisms can be
used to provide an explanation of the output of the Argument
Mining system, to be integrated to increase consistency of
annotations (as in guidelines for annotators) or as a criterion
to select examples to be annotated.

Classiﬁer performance
Tables 1 and 2 show the best results obtained after hyperpa-
rameter search for the essays corpus and the ECHR corpus,
respectively. As could be expected, we obtain better results
for the task of claim detection than when we distinguish
claims, premises and non-components. Also, for this task,
attention does not provide an improvement over not using
attention.

The performance for the ECHR is also worse than for es-
says. This is probably due to various factors: the essays cor-
pus is bigger, but also sentences in that corpus are shorter,
and argument components are less complex.

Table 1: Best results obtained for claim detection (left) and
component detection (right) after hyperparameter search in
the essays corpus.

No attention
Word + lin
Word + sig
Word + tanh
Context + lin
Context + sig
Context + tanh

Claim Detection Claim - Premise
Acc
0.844
0.831
0.829
0.841
0.786
0.797
0.781

Acc
0.696
0.684
0.700
0.717
0.709
0.703
0.702

F1
0.841
0.834
0.833
0.837
0.756
0.799
0.754

F1
0.683
0.684
0.695
0.704
0.699
0.691
0.698

Table 2: Best results obtained for claim detection (left) and
component detection (right) after hyperparameter search in
the ECHR corpus.

Claim Detection Claim - Premise
Acc
0.805
0.824
0.822
0.821

Acc
0.661
0.668
0.680
0.674

F1
0.652
0.673
0.683
0.682

F1
0.793
0.816
0.810
0.814

No attention
Word + lin
Word + sig
Word + tanh

Concerning the conﬁguration of the classiﬁers, we can see
that word attention systematically obtains better results than
context attention, with context attention performing worse
than a system with no attention in some cases. It has to be
noted that context attention has a very strong parameter, the
length of the context to be used to ﬁnd attention. The longer
the context, the more computational resources are required.

This is an interesting parameter to explore and may have a
big impact in improving the performance of context atten-
tion, especially if we take into account the extensive length
of paragraphs in the ECHR corpus. This will be explored in
future work.

Results for the rest of hyperparameters are not conclusive.
In Figure 2 we can see the distribution of F1-Scores on dif-
ferent conﬁgurations of classiﬁers for the task of argumen-
tative component detection, for the ECHR and the essays
dataset, respectively. These classiﬁers were trained for the
selection of hyperparameters with random combinations, as
described in the previous section.

We can see that there is a wide range of variability across
results. For the essays corpus, we can see that the difference
in performance between word attention and context atten-
tion falls within the range of variability of different param-
eters with the same kind of attention. This means that the
combination of hyperparameters affects performance more
importantly than the kind of attention. However, word at-
tention tends to have slightly better performance and more
stable (less variable results).

In the case of the ECHR corpus, word attention performs
clearly better than context attention, beyond the impact of
the rest of hyperparameters. This is probably due to the fact
that sequences given to the classiﬁer are whole paragraphs,
which are thrice longer than the sequences for the essays
corpus. The task of context modelling is more difﬁcult when
sequences are longer. In contrast, the formulaic, repetitive
nature of legal text may be useful to concentrate attention
in particular words or word sequences, which is particularly
adequate for word attention mechanisms.

We cannot see an important difference in variability
across activation functions: the three kinds of activation
present wide ranges of variability.

However, the effects that activation have in the distribu-
tion of the attention scores are intrinsically different. For
both datasets, the scores given with the sigmoid and tanh
activation concentrate on values greater than 0.9, and few
words obtain a low attention score. In contrast, linear ac-
tivation concentrates the scores in a middle value, usually
close to 3, with a gaussian-like distribution. We have ob-
served classiﬁers where linear activation concentrates the at-
tention scores close to zero, effectively shutting down the
signals from irrelevant words. We are currently working on
establishing a correspondence between these patterns of dis-
tribution of attention and improvements in performance or
interpretability.

Visualizing attention
To further explore the effects of attention, we visualize the
attention scores assigned by a model. We propose a visual-
ization where the intensity of the color over each word in the
text denotes its attention. The scale of intensity is linear, nor-
malized according to attention values. The color itself repre-
sents the label assigned by the classiﬁer: red for claims, blue
for premises, gray for the O class. Words misclassiﬁed by
the model are underlined.

In Figure 3 we show the labels and attention scores as-
signed by a word attention, sigmoid classiﬁer for claim de-

176Figure 2: Distribution of F1-Scores on different architectures in the ECHR and Essays dataset, for the argumentative component
classiﬁcation task.

Figure 3: Attention scores assigned to a fragment of the
ECHR corpus for the claim detection task. The model uses
word attention with sigmoid activation.

tection on a fragment of the ECHR corpus.

We can see that words with low attention are irrelevant to
detect claims, like stopwords. In contrast, words with high
attention are very relevant, as is the case of ”inadmissible /
admissible” or ”conclude”. This kind of information is very
useful to include in annotation guidelines to enhance inter-
annotator agreement, and also to speed up annotation, by
focusing the attention of annotators in more important infor-
mation.

Context attention provides an entirely different kind of in-
formation, as can be seen in Figure 4, showing the result of
a classiﬁer with context attention in the essays corpus, dis-
tinguishing claims and premises.

Context attention weights differently the word on each oc-
currence. It systematically gives more importance to words
at the beginning of the sentence and to the two words at the
end of the sentence, because they are strong signals of the
beginning of an argument component, whereas the rest of
words in the sentence are not. This behaviour has its roots

Figure 4: Attention scores assigned to a fragment of the
essays corpus for the component detection (claim-premise)
task. The model uses context attention with linear activation.

on the argument structure, as it is not likely that a new com-
ponent will start at the end of the sentence.

Punctuation acting as sentence separators ( . ) receive a
high value of attention, as they denote the start of a new
component. In the case of commas ( , ) we have seen mixed
results, as they can be part of a component as well. It is inter-
esting to note that this behaviour is captured by both types
of attention.

Regarding activation functions, the linear activation pre-
viously described favours more evenly distributed, non-
extreme attention scores. As a result, it generates a less satu-
rated visualization than the sigmoid activation. We consider
this type of activation better suitable for explaining the re-
sults, as we can identify words in more distributed attention
values.

With the word attention mechanism, we can extract the
words with more attention. We show them in a wordcloud
on Figure 5. This visualization highlights the words that are
more important to decide any classiﬁcation label. In the case
of the essays, where the language and the argumentation are
more informal, we see mainly modal verbs, connectors and
adverbs. The implication is that the model is not deﬁning

177consistency of annotations, improving inter-annotator agree-
ment. It will also be used to select examples with words for
which the model has no strong attention, thus helping ex-
plore regions of the space that are not well characterized yet.

Acknowledgements
The authors have received funding from the European
Union’s Horizon 2020 research and innovation programme
under the Marie Skodowska-Curie grant agreement No
690974 for the project MIREL: MIning and REasoning with
Legal texts.

References
Cabrio, E., and Villata, S. 2018. Five years of argument
mining: a data-driven analysis. In Lang, J., ed., Proceedings
of IJCAI 2018, 5427–5433.
Eger, S.; Daxenberger, J.; and Gurevych, I. 2017. Neural
end-to-end learning for computational argumentation min-
ing. In Proceedings of the 55th ACL, 11–22.
Habernal, I., and Gurevych, I. 2017. Argumentation min-
ing in user-generated web discourse. Comput. Linguist.
43(1):125–179.
Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirectional LSTM-
CRF models for sequence tagging. CoRR abs/1508.01991.
Lippi, M., and Torroni, P. 2016. Argumentation mining:
State of the art and emerging trends. ACM Trans. Internet
Techn. 16(2):10:1–10:25.
Peldszus, A., and Stede, M. 2013. From argument diagrams
to argumentation mining in texts: A survey. IJCINI 7(1):1–
31.
Reimers, N., and Gurevych, I.
2017. Reporting Score
Distributions Makes a Difference: Performance Study of
LSTM-networks for Sequence Tagging. In Proceedings of
the 2017 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 338–348.
Stab, C., and Gurevych, I.
Parsing argumenta-
tion structures in persuasive essays. Comput. Linguist.
43(3):619–659.
Stab, C.; Miller, T.; and Gurevych, I. 2018. Cross-topic ar-
gument mining from heterogeneous sources using attention-
based neural networks. CoRR abs/1802.05758.
Teruel, M.; Cardellino, C.; Cardellino, F.; Alemany, L. A.;
and Villata, S.
Increasing Argument Annotation
Reproducibility by Using Inter-annotator Agreement to Im-
prove Guidelines. In Proceedings of the LREC 2018.
Wang, B.; Liu, K.; and Zhao, J. 2016. Inner attention based
recurrent neural networks for answer selection. In Proceed-
ings of the 54th ACL, 1288–1297.

2017.

2018.

Figure 5: Words with most attention in the essays corpus,
with word attention. Size is proportional to attention score.

the argumentation by the semantics of its words, instead us-
ing the intention of the speaker represented at the start of
the phrase. Adverbs like really and somehow exemplify the
more sentimental nature of the argumentations in student
essays. This information can be readily included in guide-
lines or to select new instances for annotation. For example,
we can select instances without words for which we already
have obtained high attention values in already annotated in-
stances, thus gaining insight for new words.

Conclusions
In this paper, we present an analysis of the impact of neural
attention mechanisms in the task of training Argument Min-
ing systems for argument component detection. Argument
Mining is a particularly challenging task for machine learn-
ing approaches because training corpora are typically small,
and the phenomena to represent are very complex, with im-
pact in various linguistic levels, thus it suffers from acute
data sparseness. Our assumption was that attention mech-
anisms could help concentrate this sparse information. We
have found that some attention mechanisms do improve the
performance of Argument Mining systems, if only slightly.
We have found that for the task of claim detection, which is
more well-deﬁned, the performance is higher, but the im-
provement provided by attention mechanisms is approxi-
mately the same in both tasks. We have also found that
word-level attention produces consistently better results than
context-level attention. However, an important parameter of
context-level attention, the length of the sequence, remains
to be explored. The rest of the hyperparameters do not seem
to provide consistently better results, although they produce
important variations in performance.

Even if the improvement in performance is small, we have
gained on explanainability of the results: we can now ana-
lyze how classiﬁers make errors with more insight, as we
have stronger indicators of the information wherefrom they
make their conclusions. This helps to improve the training
corpora and the classiﬁers that exploit them.

These results have been found to hold on two very dif-
ferent corpora: a reference corpus of student essays and our
primary focus of research, a small corpus of judgments of
the European Court of Human Rights. We expect that the in-
terpretability provided by attention mechanisms, focused on
words, will help in enhancing this corpus, by increasing the

178