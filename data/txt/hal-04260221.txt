Argument-based Detection and Classification of Fallacies
in Political Debates
Pierpaolo Goffredo, Mariana Chaves, Serena Villata, Elena Cabrio

To cite this version:

Pierpaolo Goffredo, Mariana Chaves, Serena Villata, Elena Cabrio. Argument-based Detection
and Classification of Fallacies in Political Debates. EMNLP 2023 - Conference on Empirical
Methods in Natural Language Processing, Dec 2023, Singapore (SG), Singapore. pp.11101-11112,
￿10.18653/v1/2023.emnlp-main.684￿. ￿hal-04260221￿

HAL Id: hal-04260221

https://hal.science/hal-04260221

Submitted on 20 Dec 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

11101
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11101–11112
December 6-10, 2023 ©2023 Association for Computational Linguistics

Argument-basedDetectionandClassificationofFallaciesinPoliticalDebatesPierpaoloGoffredo1,MarianaChavesEspinoza1,ElenaCabrio1,SerenaVillata11UniversitéCôted’Azur,CNRS,Inria,I3S,Francegoffredo@i3s.unice.fr,machaves@i3s.unice.fr,elena.cabrio@univ-cotedazur.fr,serena.villata@univ-cotedazur.frAbstractFallaciesareargumentsthatemployfaultyrea-soning.Giventheirpersuasiveandseeminglyvalidnature,fallaciousargumentsareoftenusedinpoliticaldebates.Employingthesemis-leadingargumentsinpoliticscanhavedetri-mentalconsequencesforsociety,sincetheycanleadtoinaccurateconclusionsandinvalidinferencesfromthepublicopinionandthepol-icymakers.Automaticallydetectingandclassi-fyingfallaciousargumentsrepresentsthereforeacrucialchallengetolimitthespreadofmis-leadingormanipulativeclaimsandpromoteamoreinformedandhealthierpoliticaldis-course.Ourcontributiontoaddressthischal-lengingtaskistwofold.First,weextendtheElecDeb60To16datasetofU.S.presidentialdebatesannotatedwithfallaciousarguments,byincorporatingthemostrecentTrump-Bidenpresidentialdebate.Weincludeupdatedtoken-levelannotations,incorporatingargumentativecomponents(i.e.,claimsandpremises),there-lationsbetweenthesecomponents(i.e.,supportandattack),andsixcategoriesoffallaciousar-guments(i.e.,AdHominem,AppealtoAuthor-ity,AppealtoEmotion,FalseCause,SlipperySlope,andSlogans).Second,weperformthetwofoldtaskoffallaciousargumentdetectionandclassificationbydefiningneuralnetworkarchitecturesbasedonTransformersmodels,combiningtext,argumentativefeatures,anden-gineeredfeatures.Ourresultsshowtheadvan-tagesofcomplementingtransformer-generatedtextrepresentationswithnon-textualfeatures.1IntroductionFallaciousargumentshavebeenfirstlydefinedasdefectiveinferences,i.e.,logicallyinvalidtypesofarguments(Eemeren,2001).Morerecently,inamorepragmaticperspective,fallaciousargumentshavebeendefinedasinfringementsofperformancerulescharacteristicofaparticularidealtypeofargu-mentativeengagement(EemerenandGrootendorst,1987)andasillicitdialecticalshiftsacrossdiffer-entdialoguetypes,highlightingthattheattemptedmoveisinappropriatewithrespecttoitspragmaticapplicationcontext(Walton,1995).Despitetheiremploymentinmanyscenarios(e.g.,onlinedis-cussionplatformsandblogs,TVroundtables),anaturaltestbedofthisformofmisleadingargu-mentationispoliticaldebate.Forinstance,theadhominemfallacy,wheretheplausibilityofthear-gumentdependsonthecredentials,personalback-groundandpastactionsofthespeaker,isprobablyoneofthefallacylabelsthatismostoftenthrownaroundinpoliticaldebate.Thiskindofargumentsmaysoundconvincingandhasthegoaltomisleadtheaudience,persuadingitaboutthevalidityoftheargument.Giventhepotentialnefariousim-pactofthesemisleadingargumentsonthesociety,identifyingandclassifyingfallaciousargumentsisthereforeamainopenchallengeinArgumentMin-ing(AM)(CabrioandVillata,2018;LawrenceandReed,2019;Lauscheretal.,2022),andinNLPingeneral.Existingapproachesintheliterature(Habernaletal.,2017,2018;Goffredoetal.,2022;Vija-yaraghavanandVosoughi,2022;Alhindietal.,2022;Vorakitphanetal.,2022;Sahaietal.,2021)mainlyconcentratedontheclassificationoffalla-cioustextsnippetsoverafinitesetoflabels,leavingthechallengingissueofidentifyingthefallacioustextsnippetanditsboundariesunder-investigated.Inthispaper,wetacklethisopenresearchques-tiononadatasetofpoliticaldebatesfromtheU.S.presidentialcampaignsfrom1960to2020.Moreprecisely,thecontributionofthispaperistwofold.First,weextendanexistingresourceofU.S.politicaldebatesfromthepresidentialcam-paigns(1960-2016)annotatedbothwithargumentcomponentsandrelations,andsixfallacycate-gories(namely,AdHominem,AppealtoAuthor-ity,AppealtoEmotion,FalseCause,SlipperySlopeandSlogans).Thisnewresource,namedtheElecDeb60to20dataset,includesnowalsothedebatesofthe2020presidentialcampaign(Trump-11102

Biden)withalltherelatedannotations.Second,weproposeanewapproach,basedonTransformers,todetectthefallacioustextsnippetsinthesede-bates,andthenclassifythemalongthesixfallacycategories.Thisapproachencodestheargumentcomponents(i.e.,Premise,Claim),theargumentrelations(i.e.,Support,Attack)andthePoStagstosuccessfullyidentifyandclassifyfallaciousar-guments.Experimentalresultsshowthatthepro-posedapproachoutperformsstandardbaselinesandconcurrentapproacheswithanaveragef1-scoreof0.74,withourproposedmodelnamedMultiFusionBERT,onthetaskoffallaciousargumentdetectionandclassification.Whilstmostofthecomputationalapproachestar-getingfallaciousargumentationfocusonthepureclassificationofsuchnefariouscontent(Habernaletal.,2017,2018;Jinetal.,2022;Goffredoetal.,2022;VijayaraghavanandVosoughi,2022;Alhindietal.,2022),theoriginalityofourcontributionisthatitproposes,tothebestofourknowledge,thefirstneuralarchitecturetobothdetectfallaciousar-gumentsandclassifytheminpoliticaldebates,andoneoftheveryfewapproachestotacklethistaskingeneral,outperformingcompetingapproaches(Vo-rakitphanetal.,2022;Sahaietal.,2021).Theurgencytostudyfallaciesinpoliticaldis-courseiscrucialbothfromthephilosophicalandthepoliticalperspective.Itemphasizestheneedtoscrutinizepoliticalargumentsforsoundreason-ingratherthandeception.Understandinglogicalflawsisamainissueforinformeddecision-making,enablingtherecognitionandassessmentoffalla-ciousargumentsinpoliticaldiscourse.Moreover,examininghowfallaciesareemployedinpoliticaldebatesrevealstheirstrategicroleininfluencingopinionanddivertingattention.Thisstrategicuseoffallaciesmirrorsthesubtletiesoflanguage,em-phasizingtheinterplaybetweenrhetoric,philoso-phy,andpoliticalcommunication.Furthermore,itoffersvaluableinsightsintothedynamicsofpublicdiscourseandtheneedforcriticalanalysis(Walton,1987,1995).2RelatedWorkOvertheyears,therehasbeenagrowinginterestinthefieldofNLPtothedetectionoffallaciesandrelatedphenomena,includingmisinformationandpropaganda(DaSanMartinoetal.,2020b).ThepioneeringworkofDaSanMartinoetal.(2019b)onfallaciesinnewspapernewshasbeenasignif-icantsourceofinspirationinthisarea.Recently,researchershavemadesignificantprogressiniden-tifyingandclassifyingfallacieswithindiscourse.Inthissection,wediscusstheapproachesproposedintheliteratureonthetwotasksoffallacyclassi-fication(Section2.1)andfallacydetection(Sec-tion2.2).2.1FallacyClassificationForinstance,Habernaletal.(2017)aimedtoim-provefallacydetectionbycreatingapubliclyavail-ablesoftwarecalled“Argotario”.Itservesasaneducationalgamingplatformandameanstogatherdatafromthecrowdforannotatingfallacytypesineverydayarguments.Inasubsequentstudy,Haber-naletal.,2018releasedannotateddatasetsoffalla-ciousargumentsinEnglishandGerman.Theycon-ductedexperimentsusingSupportVectorMachineandBiLSTMmodelswithGermanwordvectorstoclassifysixfallacioustopictypes(accuracy=50.9%,macro-F1score=42.1%).Jinetal.(2022)proposesanarchitecturebasedonasimpleclassifierthatincorporatesthestruc-turalinformationoffallacies.Theyaugmentastandardpre-trainedlanguagemodelforclassifi-cationwithatemplatethatincludesmodifiedas-sumptionsandpremisesbasedonawell-definedmaskingscheme.TheyachieveanF1scoreof58.77%ontheclassificationtaskover13classes.Theyusedadatasetcollectedfromonlineteachingmaterials,specificallydesignedtoteachandteststudents’understandingoflogicalfallacies.Inamorecontextualapproach,Goffredoetal.(2022)useadatasetfromU.S.PresidentialElec-tionDebatesFrom1960to2016.Thisdatasetincorporatesinformationsuchasthecontextandargumentativefeatures(i.e.,argumentcomponentsandrelations)foreachfallacy.Theproposedar-chitectureusesoneclassifierforeachfeatureinordertocalculatetheloss.Onasentenceclassifica-tiontaskencompassingsixfallacycategories,theirapproachachievesanF1scoreof84%.Toaddresstheidentificationandclassificationoffine-grainedpropagandatweets,Vijayaragha-vanandVosoughi(2022)proposedanend-to-endtransformers-basedapproachthatconsidersaddi-tionalfeatureslikecontext,relationalinforma-tion,andexternalknowledgethroughdataaug-mentation.Theirdatasetconsistsofapproximately211Ktweetsannotatedwith19classes(18propa-gandatypesand1non-propaganda).Theapproach11103

yieldeda64%F1scoreonatextclassificationtask.Alhindietal.(2022)introducedaninstruction-basedpromptinamultitaskconfigurationusingtheT5modeltoclassifyfallacies.Thismethod-ologyinvolvedleveragingmultiplefallacy-baseddatasets,includingPropaganda(DaSanMartinoetal.,2019b),Logic(Jinetal.,2022),Argotario(Habernaletal.,2017),Covid-19(Musietal.,2022),andClimate(Jinetal.,2022).Theirap-proachenabledtheidentificationof28distinctfal-laciesacrossvariousdomainsandgenres,facilitat-ingtheanalysisofmodelsizeandpromptselection,andinvestigatingtheimpactofannotationqualityonmodelperformance,potentiallysupplementedwithexternalknowledge.Theresultsobtainedus-ingtheirT5-largemodelyieldedF1scoresof41%forPropaganda,62%forLogic,59%forArgotario,26%forCovid-19,and17%forClimate,respec-tively.2.2FallacyDetectionandClassificationTaskVorakitphanetal.(2022)proposesasystemcapa-bleofautomaticallyidentifyingpropagandames-sagesandclassifyingthembasedonthepropa-gandatechniquesemployed.Thesystemadoptsapipelineapproachthatfirstlydetectsthetextsnip-petcontainingpotentialpropaganda,andthenper-formsclassificationbyleveragingsemanticandargumentativefeatures.Twostandardbenchmarks,NLP4IF’19(DaSanMartinoetal.,2019a)andSe-mEval’20datasets(DaSanMartinoetal.,2020a),wereusedforthispropagandadetectionandclas-sificationtask.Thebinaryclassificationstep,uti-lizingBERT,achieveda72%F1score,whilethesentence-spanmulti-classclassificationtask(14classes)achieveda64%microF1scoreusingaRoBERTa-basedarchitecture.Fallacydetectionandtoken-levelclassificationtaskswerealsoperformedinSahaietal.(2021).Theynarrowedtheirstudyto8fallacytypesandcreatedacorpusoffallaciousargumentsbyanno-tatingusercommentsonReddit.Theyperformedfallacyclassificationatcomment-levelandtoken-level,relyingonBERTandMGN(DaSanMartinoetal.,2019a)models.Theinclusionoftheconver-sationcontext,representedbytheparentcommentorsubmissiontitle,wasusedtoenhancethepre-dictions.Fine-tunedBERTwithaclassificationheadofalinearlayerreportedthebestresultsonthetokenclassificationtask(macroF1=53%).Ourpresentworkexpandsthescopeofthesefallacystudiesinseveralways.Firstly,weemployacorpusspecificallycreatedforfallacydetectioninpoliticaldebates,whichhasbeenunderinves-tigatedinthiscontexteventhoughfallaciousar-gumentationisamainissueinpoliticaldiscourse.Secondly,whilethemajorityofexistingstudiesfo-cusonfallacyclassification,inthispaperwefocusonthefallacydetectiontask,whichisofmainim-portanceinreal-lifescenarioswheretextorspeechlackspre-segmentationandaclearbinaryclassifi-cationintofallaciesornon-fallacies.Lastly,ourstudycapitalizesontheannotationsofargumen-tationfeaturesinourdataset,anelementthatisoftenabsentinothercorpora,allowingustogobeyondpuretext-basedapproaches,andtorelyontheargumentationstructure.3ElecDeb60to20DatasetToeffectivelyaddressthetaskofdetectingandclassifyingfallaciousargumentswithinpoliticaldebates,wedecidedtorelyontheElecDeb60To16dataset(Haddadanetal.,2019;Goffredoetal.,2022).ItcomprisesteleviseddebatesfromU.S.presidentialelectioncampaignsspanningfrom1960to2016.ThesedebatesweresourcedfromthewebsiteoftheCommissiononPresidentialDebates1,whichopenlyprovidestranscriptsofdebatesbroadcastedontelevisionandfeaturingtheprominentcandidatesforpresidentialandvice-presidentialnominationsintheUnitedStates.Allinformationonthiswebsiteisaccessibletothepublic.ConsideringthemostrecentpresidentialelectionbetweenTrumpandBidenoccurredin2020,weexpandedthedatasetwiththetranscriptsofthedebatesofthiselectioncampaigntoincludeupdatedannotations,incorporatingargumentativecomponentssuchasClaimsandPremises,aswellastherelationsbetweenthesecomponents,i.e.,SupportandAttack.Asaresultofthisannotationupdate,thedatasetisrenamedasElecDeb60to202,reflectingthecoverageofdebatesspanningfrom1960to2020.Thisupdateddatasetprovidesamorecomprehensiveandcontemporarycollectionoffal-laciesinpoliticaldebates,enhancingtherelevanceandapplicabilityofthedataforfurtheranalysisandresearchinthefield.Thisresourceisavaluablebenchmarkforin-vestigatingpotentialconnectionsbetweenspecific1https://www.debates.org/voter-education/debate-transcripts/2https://github.com/pierpaologoffredo/FallacyDetection11104

argumentcomponentsandrelationsthatunderlietheoccurrenceoffallaciousarguments.3.1AnnotatedFallaciesDuringtheannotationprocessoffallacieswithintheU.S.politicaldebatesofthe2020presidentialelection,werelyonthesixcategoriesbasedontheannotationschemeproposedbyDaSanMartinoetal.(2019a),thecategorizationoutlinedbyWal-ton(1987),andtheannotationofthepreviousde-batesinthefirstversionofthedatasetwiththesefal-lacycategories(Goffredoetal.,2022).Hence,weadoptedthefollowingcategories:AdHominem,Ap-pealtoAuthority,AppealtoEmotion,FalseCause,SlipperySlope,andSlogans.Below,weprovideaconcisedescriptionofeachofthesesixcategories.AdHominem.Whentheargumentbecomesanexcessiveattackonanarguer’sposition(Walton,1987).AppealtoEmotion.Theunessentialloadingoftheargumentwithemotionallanguagetoexploittheaudienceemotionalinstinct.AppealtoAuthority.Itoccurswhenthearguerreliesontheendorsementofanauthorityfigureoragroupconsensuswithoutprovidingsufficientevidence.Itmayalsoinvolvethecitationofnon-expertsorthemajoritytosupporttheirclaim.SlipperySlope.Thisfallacyimpliesthatanim-probableorexaggeratedconsequencecouldresultfromaparticularaction.FalseCause.Themisinterpretationofthecorre-lationoftwoeventsforcausation(Walton,1987).Slogan.Itisabriefandstrikingphraseusedtoprovokeexcitementoftheaudience,andisoftenaccompaniedbyanothertypeoffallacycalledar-gumentbyrepetition.3.2AnnotationPhaseTheupdatedannotationswereconductedfollow-ingtheannotationschemeintroducedinHaddadanetal.(2019);Goffredoetal.(2022).Followingthisapproach,eachdebatewasdividedintosections,startingwitheitheramoderator/panelistoranau-diencememberaskingaquestiononanewtopic.Tofacilitatetheannotationprocess,thesemanticannotationplatformINCEpTION(Klieetal.,2018)wasused.Twoannotators,withexpertiseincomputationallinguistics,independentlyannotatedthenewpor-tionofthedataset(Trumpvs.Bidendebates)byidentifyingargumentativecomponents,relations,andfallacies.Tomaintainobjectivityandpreventbias,theannotationprocessforargumentativecom-ponentswasperformedonrawdata,withoutanypre-existingfallacyannotations.Thisapproachwasadoptedtoensurethattheannotationprocessremainsunbiasedandfreefromanypreconceivednotionsrelatedtofallacies.Asetof50sentencesrandomlyextractedfromthedebateswasannotatedtoassessInter-AnnotatorAgreement(IAA),andtheresults,visualizedinTable1,indicateasub-stantiallevelofagreementbetweentheannotators.MeasureValueObservedAgreement0.857Krippendorff’sα0.757Table1:IAAagreementover50sentencesrandomlyextractedfromthedebatesTrump-Biden.3.3StatisticsandDataAnalysisTable2summarizestheTrumpvs.Biden’sde-batesannotationspercategoryandargumentativefeatures.Wetokenizedtheannotatedfallaciousar-gumentstocomputetheaveragenumberofwordsineachcategory.InlinewiththeguidelinesofGoffredoetal.(2022),Slogansistheshortestwith5.0tokensonaverage3,whereasSlipperySlopewasthelongestwith20.5tokensonaverage.CategoryFreqAvgTokArg.FeatureFreqAdHominem624,6Claims1513AppealtoAuthority1718,6Premise332AppealtoEmotion1476,81SupportRel.400FalseCause00AttackRel.112SlipperySlope420,5Slogans25Total2329,25Total2357Table2:Distributionofannotatedfallaciespercate-goryandargumentativefeaturesofTrumpvs.Biden’sdebates.Thetrainandtestsetsplitwasperformedconsid-eringtheentirenewdatasetElecDeb60to20.Thetrainingsetaccountsfor90%ofthedataset,whiletheremaining10%constitutesthetestset.Thedistributionoffallacylabelsisasfollows:Appeal-toEmotion(59.94%),AppealtoAuthority(15.20%),3Sloganisbydefinitionashortandstrikingphrase.11105

AdHominem(13.58%),FalseCause(46.93%),Slip-peryslope(3.97%),andSlogans(2.63%).Inthelastdebate,themostusedfallaciesareAppealtoE-motionandAdHominem,confirmingthetrendofthepreviousdebates.Behindthisstrategy,therearemanyreferencestotheCOVID-19pandemicandsomepersonalissuesofthetwocandidatesex-ploitedduringthedebates.Despitebeingdistinctandunrelated,thesetwotopicsheldsignificantim-portanceandconsistentlyfueledintensedebates.4FallacyDetectionWecastthefallacydetectiontaskasaninformationextractionproblem,wherethegoalistoidentifyandclassifyinthedebatesthetextualsnippetscor-respondingtothesixcategoriesoffallaciesanno-tatedinthecontextofapoliticaldebate(seeSection3.1forthelistoffallaciesandtheirdescription).WerelyontheBIO/IOBdataformat,andspecifictagsareassignedtoannotatethefallacies,i.e.,B-AdHominem,I-AdHominem,B-AppealtoAuthority,I-AppealtoAuthority,B-AppealtoEmotion,I-AppealtoEmotion,B-FalseCause,I-FalseCause,B-Slipperyslope,I-Slipperyslope,B-Slogans,I-Slogans,O.Thefallacydetectionandclassificationtaskscon-sistthereforeinassigningoneofthesethirteenpre-definedlabelstoeachtoken.Tohavearicherrepresentationoffallacyanno-tations,webuildacontextualframeworkthatin-cludesthesentencecontainingthefallacy,aswellastheprecedingandfollowingsentences.Whenthefallacioussentenceisthefirstorlastinthedialogue,theprecedingorfollowingsentenceisexcluded.4.1MethodToaddresstheabove-mentionedtasks,weemploytransformer-basedarchitecturesinboththeirba-sicconfigurationandinaspecializedconfigurationdesignedfortokenclassification4,drawinginspira-tionfrompreviousstudiesonfallacydetectionandclassification(DaSanMartinoetal.,2020a;Vorak-itphanetal.,2022;Goffredoetal.,2022)thathaveprovidedempiricalevidenceoftheadvantagesofcomplementingtransformer-generatedtextrepre-sentationswithnon-textualfeatures.Moreover,weenhancethespecializedarchitecturebyincludingadditionalargumentativefeatures.4https://huggingface.co/docs/transformers/tasks/token_classification4.1.1BaselinesBERT+(Bi)LSTM(s)Thesimplestmodelscon-sistofapre-trainedBERTmodelfollowedbyei-ther(i)anLSTMlayerandadenselayer,or(ii)aBiLSTMlayerwith0.2dropout,anLSTMlayer,andadenselayer.Theweightsofthetransformerarekeptfrozenduringtraining.Thetextservesasinputforthetransformer,andweextractthelasthiddenstates(i.e.,theembeddedrepresenta-tionofeachtoken).Thisoutputisthenpassedontothesubsequentlayers.Inthecasewhereargumentativefeaturesareincludedinthemodel,weconcatenatethelasthiddenstatesofthetrans-formerwiththeone-hot-encodedrepresentationoftheargumentcomponentsandrelationships.ThisconcatenatedfeaturerepresentationisthenfedintothenextRNN-basedlayers.AllmodelsusedAdamoptimizerwithdefaultPyTorchparameters.BertForTokenClassificationisatransformer-basedmodelrelyingonabidirectionalap-proachtocapturecontextualinformationfromsurroundingwords.Wetestedtwocheckpoints:bertbaseuncasedandbert-large-cased-finetuned-conll03-engl.DebertaForTokenClassificationisbasedonamodifiedtransformerarchitecturewithimprove-mentslike“de-coupledattention”and“cross-layerparametersharing”forenhancedlanguagemodelingcapabilities.Thecheckpointusedismicrosoft/deberta-base.ElectraForTokenClassificationreliesonanovelpre-trainingmethodcalled"dis-criminativepre-training,"whereageneratorandadiscriminatoraretrainedtoenhancethequalityofthelearnedrepresentations.Thecheckpointusedis:bhadresh-savani/electra-base-discriminator-finetuned-conll03-english.DistilbertForTokenClassificationisadistilledversionofBERTthatretainsmuchofitsperformancewhilesignificantlyreducingthemodelsizeandcomputationalresourcesre-quiredfortrainingandinference.Thecheck-pointusedare:distilbert-base-casedanddistilbert-base-uncased.4.1.2MultiFusionBERTRelyingontheresultsobtainedbythebaselinesonthefallacydetectiontaskon11106

ModelAvgmacroF1ScoreBERT+LSTM0.4697BERT+LSTM(comp.andrel.features)0.5142BERT+BiLSTM+LSTM0.5495BERT+BiLSTM+LSTM(comp.andrel.features)0.5614BertFTCbert-base-uncased0.7096BertFTCdbmdz/bert-large-cased-finetuned-conll03-english0.7237DebertaFTCmicrosoft/deberta-base0.7222ElectraFTCbhadresh-savani/electra-base-discriminator-finetuned-conll03-english0.4033DistilbertFTCdistilbert-base-cased0.7010DistilbertFTCdistilbert-base-uncased0.7047MultiFusionBERT(comp.,rel.andPoSfeatures)0.7394Table3:AveragemacroF1scoresforfallacydetection(BIOlabelsaremerged)usingdifferentmodels.Thescoresarebasedonanaverageof3runs,exceptforBERT+(Bi)LSTM(s)models,whichwereevaluatedusing10runs.(FTCstandsfor“ForTokenClassification)FeaturesAvgmacroComponentsRelationshipsPoSF1Score✓0.6922✓0.6922✓0.7212✓✓0.7278✓✓0.7166✓✓0.7166✓✓✓0.7394Table4:AveragemacroF1scoresforfallacydetection(BIOlabelsaremerged)usingMultiFusionBERTanddifferentfeatures.Thescoresarebasedonanaverageof3runs.theElecDeb60to20datasetreportedinTable35,weselecttheBertFTCmodel(bert-large-cased-finetuned-conll03-eng.)tobeincludedinourproposedarchitecture.DespitethegoodperformancesofBertFTC,weproposetoenhanceitscapabilitiestodetectfallacioustextbyintegratingand“fusing”addi-tionalfeatures,namelyargumentativecomponents(Claim,Premise),argumentativerelations(Support,Attack),andPart-of-Speech(PoS)tags.Argumentativefeatureswereincludedtoim-provethemodel’sunderstandingofanargumentunderlyingstructure,enablingittodetectwhenitslogicalstructureiscompromised.Sincefallaciesinvolvefaultyreasoning,wehypothesizedthatpro-vidingthisinformationtothemodelwouldberele-vant.ResultsinTable4provideempiricalevidencetosupportthishypothesis.Inparticular,ithasbeenshownthatincludingargumentationfeaturesin-5ThereportedresultsrepresenttheaverageperformancebasedonthemacroaverageF1score.creasesthemodelperformancesinthecontextoffallacyclassification(Goffredoetal.,2022).SuchfeaturescanbeextractedbyspecificannotationsassociatedwitheachfallacyintheElecDeb60to20dataset:thetypeofargumentativecomponentinwhichthefallacymaybepresent,andtheargumen-tativerelationsamongthesedifferentcomponents.TheintegrationofPoSinformationisdrivenbytheobservationthatcertainfallaciesexhibitdis-tinctivelanguagepatternsthatcanbemoreeas-ilydiscernedusingPoStagging.Forexample,intheLoadedLanguagefallacy(asubcategoryoftheAppealToEmotioncategory),theintensityofasentenceisoftenincreasedbyusingemotionallyloadedphrases,expressedthroughtheuseofasen-timentlexicon,particularlyconcerningadjectivesandadverbs.Similarly,intheAdHominemfallacy,wherethefocusshiftsfromattackingtheargumenttotargetingthecharacter,motives,orpersonalqual-itiesofthepoliticalopponent,thereferencetothisopponentisexpressedusinganounorpronounandsubsequentlyemploysadjectiveswithnegativeconnotations.Figure1illustratestheproposedmodel,calledMultiFusionBERT,forthedetectionandclassi-ficationoffallaciesinpoliticaldebates.Multi-FusionBERTcomputeslogits(L)foreachfea-turebyemployingaspecializedTokenForClassi-ficationTransformermodeladaptedtothenum-beroflabels:3forcomponentsandrelations,and17forpart-of-speechtags.Thearchitecturesforargumentativefeaturesforcomponentsandrela-tionssharethesameparameters,enablingustoobtainlogitsforbothcomponentsandrelations.Anadditionalmodel,basedonthenumberofPoS11107

Figure1:MultiFusionBERTwithjointlossapproach.tags(i.e.,17),isusedtoobtainlogitsforPoSfeatures.Consequently,distinctlossesarecom-putedforeachmodel:fallacyloss(lossfal),com-ponentloss(losscmp),relationloss(lossrel),andpart-of-speechloss(losspos).Theseindividuallossesarecombinedbymultiplyingthemwithanarbitraryαvalueof0.1,yieldingaunifiedaveragelossreferredtoasthejointloss(Vorak-itphanetal.,2021).Inourstudy,weoptedforempiricallyinvestigatingtheoptimalalphavaluethatyieldedsuperiorperformance,asevidencedbyourexperiments(seeAppendixDfortheex-haustiveevaluation).Theback-propagationfunc-tionincorporatesalllossesinthefollowingway:jointloss=α∗(lossfal+losscmp+lossrel+lossPoS)Nloss,whereNlossdenotesthenumberoflossesconsid-eredbythemodel.Weconductedanexplorationofvariousvaluesfortheαparameter.4.2ExperimentalSetupAllmodelshavebeenfine-tunedusingtheElecDeb60to20dataset.TheimplementationwasbasedonHuggingFace6version4.30.andonPy-Torch1.7.0.AllmodelsutilizetheAdamopti-mizer,withagradientclippingsetto10,adropoutof0.1,alearningrateof4e−05andatrainingbatchsizeof8andatestbatchsizeof4.Thetrainingpro-cessconsistsof4epochs,duringwhichthemodelsarefine-tunedandoptimized.Thedatasetwassplitwith90%fortrainingand10%fortesting,ensur-ingbalanceddistributiononthefallacylabelsusingstratification.Thepartitioningwasperformedus-ingthetrain_and_test_splitfunctionfromthe6https://huggingface.co/docs/transformers/indexscikit-learn7library.Therandomseedwassetto42.ThePoStagswereobtainedusingthespaCy8library.Themaximumtensorsizeissetto256,ensuringthatallnecessarytextinformationisin-cludedwithouttruncation.TherepresentationofthisencodingisshowedintheAppendixA.Theproposedneuralarchitecturecontains328millionparameters9.Thislargeparametercountenablesthemodeltocaptureintricatepatternsanddependencieswithinthedata,enhancingitscapac-ityforcomplexinformationprocessingandgen-eratingmoreaccuratepredictions.Weevaluatedthe“MultiFusion”approachonthebaselinemodelthatshowedthebestresults.Despitehavingap-proximatelyhalfofthetrainableparametersofBERT(e.g.,65MforDistilBERTvs.109MforBERT),BERToutperformedDistilBERT.Conse-quently,weadoptedBERTasthearchitectureforimplementingtheapproachdescribedabove.WeutilizedtheNvidiaQuadroRTX8000GPU(32GB)forourexperiments.Theaverageruntimewasof21minutesfortrainingandtestingalltheconfigurationsofourmodels.5EvaluationTable3presentstheresultsofthetestedmodelsforfallaciesdetectioninthepoliticaldebates.ResultsarecalculatedusingthemacroaverageF1metric,consideringthefollowingfallacylabels:(i)AdHominem,(ii)AppealtoAuthority,(iii)AppealtoEmotion,(iv)FalseCause,(v)SlipperySlope,(vi)Slogans,and(vii)Other(BandIlabelsaremerged).Despitetherelativelysmallersizeofthedatasetandthetaskcomplexity,theresultsob-tainedfromthedifferentmodelsarepromising.Asintroducedbefore,amongthebaselines,BERT“db-mdz/bert-large-cased-finetuned-conll03-english”achievedthebestperformance.Thus,MultiFu-sionBERT,incorporatingargumentativefeatures(componentsandrelations)aswellasPoStags,significantlyoutperformedtheothermodels(theperformanceincreasewithrespecttoBertFTCisof2.12%).7https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html8https://spacy.io9ThestandardBertFTCmodelisaroundthreetimessmaller,approximately109millionparameters.11108

5.1AblationTestsTobetteranalyzetheimpactofthedifferentfea-turesincorporatedinourarchitecture,wecarriedoutablationtests.Table4presentstheresultsobtainedbyMultiFusionBERTusingallpossi-blecombinationsof(i)argumentativecomponents,(ii)argumentativerelations,and(iii)contextPoStags.Incorporatingargumentativecomponents,re-lations,andPoSfeaturesindividuallyorinpairsresultedinadeclineinperformancecomparedtothebestbaselineresults(i.e.,BertFTC“dbmdz/bert-large-cased-finetuned-conll03-eng.),withanaveragedegradationof4.35%acrossthedifferentconfigurations(excludingtheoneconsideringallthreefeatures).Incontrast,whenallthreefeaturesareincluded(asdescribedinSection4.1.2)asig-nificantimprovementinmodelperformanceisob-served,highlightingtheimportanceofconsideringallofthemtogetherforfallacydetection.5.2ErrorAnalysisLabelprecisionrecallf1-scoresupportAdHominem0.990.770.87739AppealtoAuthority0.900.780.831’049AppealtoEmotion0.820.770.792’224FalseCause0.820.860.84321Slipperyslope0.900.880.89332Slogans0.000.000.0049O0.900.950.937’914accuracy0.8912’628macroavg0.760.720.7412’628weightedavg0.890.890.8912’628Table5:ClassificationreportofFallacyDetectionandClassificationwithBandIlabelsmerged.Table5providesanin-depthanalysisofMulti-FusionBERT’sperformancesonthetestset,con-sideringthedifferenttargetlabels10.Notably,theidentificationoftokenslabeledasSlogansexhibitsthepoorestresults,despitebeingrelativelyeasiertorecognizeforhumans.Thiscanbeduetothelimitedpresenceofexamples/tokensinboththetrainingandthetestset11.Inaddition,theseresultspointoutthatrecognizingsloganswithinpoliti-caldebatesinvolvesfactorsbeyondsyntacticandargumentativefeatures(mostlysemanticsandprag-matics).Onthecontrary,tokenslabeledas“Slip-perySlope”and“FalseCause”(with332and321examples,respectively)aremuchbetterclassified10AdetailedanalysisoftheperformanceswithBIOtokensisprovidedinAppendixB11AdetailedtablewiththecountofeachtokencanbefoundinAppendixCFigure2:NormalizedconfusionmatrixMultiFusionBERT.BIOlabelsaremerged.Normalizationisper-formedusingthenumberoftrueelementsineachclass.bythemodel,showingthehighestperformances(0.89and0.84).Thedefinitionof“SlipperySlope”revolvesaroundportrayingimprobableorexagger-atedconsequencesarisingfromaspecificaction,andargumentativecomponentsareoftenusedtothecause,aswellassemanticnuanceswellcap-turedbythemodel.Theresultsobtainedfortheotherlabelsareinlinewiththosein(Goffredoetal.,2022)fortheclassificationtaskonly.Theadditionofnewfal-laciousexamplesfromthe2020debateskeptun-changedthedistributionoffallacieswithrespecttothepreviousdebates,suggestingthatthedetectionoffallacioussnippetsremainsconsistentandstableacrossdifferentdebatecontexts.ForabetterunderstandingofthepredictionsmadebytheMultiFusionBERTweanalyzethenormalizedconfusionmatrixvisualizedinFigure2.Thenormalizedversionispreferredduetothedatasetclassimbalance.Notably,althoughtheclasswiththehighestF1scoreisO(representingnon-fallacies),theconfusionmatrixrevealsthatthemodeltendstoover-predictinstancesinthiscate-gory.AsobservedinthecolumnofthepredictedOclass,falsepositivesarethemostprevalentinthenon-fallacioustokens.Moreover,FalseCauseandAppealtoEmotionaretheclassesthatthemodelsmisinterpretthemostasnonfallacious.Inasmallerproportion,themodelmisclassifiesinstancesofAp-pealtoAuthorityasAppealtoEmotion.Table6showsafewmisclassifiedfallacysnip-pets.Inthefirstexample,theargumentismisclas-sifiedasAppealtoEmotioninsteadofAppeltoAuthority,becausethemodelismisledbytheword11109

FallacysnippetTruePred.fallacyfallacyFranklinRooseveltsaidin1932thattheonlythingwehavetofearisfearitself.AppealtoAuthorityAppealtoEmotionAsthePresidentsaidtheothernight,therewillalwaysbetrou-blesinthisol’world,buttheUnitedStatesofAmericacanbecountedontoprovidethevisionthattheworldlooksforfromtheUnitedStatesofAmerica.OAppealtoAuthorityandOButasAdmiralYarnellhassaid,andhe’sbeensupportedbymostmilitaryauthority,theseislandsthatwe’renowtalkingaboutarenotworththebonesofasingleAmericansoldier;andIknowhowdifficultitistosustaintroopsclosetotheshoreunderartillerybombard-ment.AppealtoAuthorityAppealtoEmotionandOInaplacelikeChicago,wherethousandsofpeoplehavebeenkilled,thousandsoverthelastnumberofyears,infact,almost4,000havebeenkilledsinceBarackObamabecamepresident,overallal-most4,000peopleinChicagohavebeenkilled.Wehavetobringbacklawandorder.FalseCauseFalseCauseandOTable6:Examplesofmisclassificationusingthebest-performingmodel.Underlinedtextistohighlightthetruelabelforeachtoken,whereasBoldisforthepre-dictedfallacyandItalicforpredictedOtokens.“fear,”whichcarriesanimportantemotionalconno-tation.Inthenextexample,theargumentiserro-neouslyclassifiedasbeinganAppealtoAuthorityargumentwhilstitisnotafallaciousargument(O).ThethirdexampleshowsanotherinstancewherethemodelconfusesAppealtoAuthoritywithAp-pealtoEmotion,whilealsofailingtoidentifypartofthefallacyingeneral.Thethirdandthefourthexamplesshowwherethemodelpartiallyidentifiesthecorrectfallacyoritsabsence.6ConclusionExistingargumentationschemes(Walton,1995)toidentifyflawedandinvalidformsofreasoningof-tenfallshortwhenappliedtofallaciousargumentsemployedinreal-worldcontextslikepoliticalde-bates.Totacklethischallenge,thecontributionofthispaperistwofold.First,weextendedtheElecDeb60to16datasetbyincorporatingtheTrumpvs.Biden2020presidentialdebatealongwithargumentativeannotationsandfallacies.Second,weproposedandevaluatedMultiFusionBERT,atransformer-basedarchitecturethatcombinesthedebatetext,theargumentativefeatures(i.e.,com-ponentsandrelations),andengineeredfeaturestoperformthefallacydetectionandclassificationtask.Ourresultshighlightthemainroleofargumentativefeaturesinthecorrectidentificationandclassifica-tionoffallaciousarguments.Thisapproachyieldsanaverageperformanceimprovementof2.12%comparedtobaselinemethodsandcompetingap-proaches.Asfutureresearch,weintendtodelvedeeperintofallaciousargumentationbyintegratingknowl-edgeinordertoaddressmorechallengingfallacycategorieslikecausalones,wherereasoningandknowledge-basedfeaturesarerequiredtoidentifythefallacy.Ourfurtherobjectiveistogeneratevalidargumentsfromidentifiedfallaciousonesandtheircontext.Additionally,achallengeweaimtotackleistoexplorewaystocountertheformalinvalidityoffallaciousargumentsthroughthegen-erationofnewarguments.7LimitationsSomelimitationsofthisworkrequireadiscussion.Firstly,theusedtrainingcorpusisfocusedonUSpoliticaldebates,whichrestrictstheapplicabilityofthemodeltoEnglish-languagecontextsonly.Fur-thermore,theimbalanceddistributionoflabelshadanoticeableimpactonthemodel’sperformanceanditsabilitytogeneralizeduringprediction.Forinstance,thelabel"Slogan"wassignificantlyunder-representedcomparedtootherlabels,furtheraffect-ingthemodel’sperformance.Finally,itisimpor-tanttoconsiderthattheGPUrequirements,specif-icallytheneedforNvidiaRTX8000with32GBVRAM,maypresentlimitationsonthepracticalutilizationofthesemodelsinresource-constrainedenvironments.Theselimitationshighlighttheneedforfurtherresearchtoaddressthedatasetlimita-tionswithrespecttotheemployedlanguageandthelabelbalance,toimprovethemodelarchitec-ture,andexploreadditionalstrategiestoenhancefallacydetectionthroughknowledgeinjection.8AcknowledgmentsThisworkwaspartlysupportedbytheFrenchgov-ernment,throughthe3IACôted’AzurInvestmentsintheFutureprojectmanagedbytheNationalRe-searchAgency(ANR)withthereferencenumberANR-19-P3IA-0002.Thisworkwaspartlysup-portedalsobyEUHorizon2020projectAI4Media,undercontractno.951911(https://ai4media.eu/).ThisworkhasbeenpartiallysupportedbytheANRprojectATTENTION(ANR-21-CE23-0037).11110

ReferencesTariqAlhindi,TuhinChakrabarty,ElenaMusi,andSmarandaMuresan.2022.Multitaskinstruction-basedpromptingforfallacyrecognition.InProceed-ingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages8172–8187,AbuDhabi,UnitedArabEmirates.AssociationforComputationalLinguistics.ElenaCabrioandSerenaVillata.2018.Fiveyearsofargumentmining:adata-drivenanalysis.InProceed-ingsoftheTwenty-SeventhInternationalJointCon-ferenceonArtificialIntelligence,IJCAI2018,July13-19,2018,Stockholm,Sweden,pages5427–5433.ijcai.org.GiovanniDaSanMartino,AlbertoBarrón-Cedeño,andPreslavNakov.2019a.FindingsoftheNLP4IF-2019sharedtaskonfine-grainedpropagandadetection.InProceedingsoftheSecondWorkshoponNaturalLan-guageProcessingforInternetFreedom:Censorship,Disinformation,andPropaganda,pages162–170,HongKong,China.AssociationforComputationalLinguistics.GiovanniDaSanMartino,AlbertoBarrón-Cedeño,HenningWachsmuth,RostislavPetrov,andPreslavNakov.2020a.SemEval-2020task11:Detectionofpropagandatechniquesinnewsarticles.InPro-ceedingsoftheFourteenthWorkshoponSemanticEvaluation,pages1377–1414,Barcelona(online).InternationalCommitteeforComputationalLinguis-tics.GiovanniDaSanMartino,StefanoCresci,AlbertoBarrón-Cedeño,SeunghakYu,RobertoDiPietro,andPreslavNakov.2020b.Asurveyoncomputa-tionalpropagandadetection.InProceedingsoftheTwenty-NinthInternationalJointConferenceonArti-ficialIntelligence,IJCAI-20,pages4826–4832.Inter-nationalJointConferencesonArtificialIntelligenceOrganization.Surveytrack.GiovanniDaSanMartino,SeunghakYu,AlbertoBarrón-Cedeno,RostislavPetrov,andPreslavNakov.2019b.Fine-grainedanalysisofpropagandainnewsarticle.InProceedingsofthe2019conferenceonempiricalmethodsinnaturallanguageprocessingandthe9thinternationaljointconferenceonnatu-rallanguageprocessing(EMNLP-IJCNLP),pages5636–5646.FransH.VanEemeren.2001.Fallacies.Criticalcon-ceptsinargumentationtheory,page135–164.FransH.VanEemerenandRobGrootendorst.1987.Fallaciesinpragma-dialecticalperspective.Argu-mentation,1(3):283–301.PierpaoloGoffredo,ShohrehHaddadan,VorakitVorak-itphan,ElenaCabrio,andSerenaVillata.2022.Fal-laciousargumentclassificationinpoliticaldebates.InProceedingsoftheThirty-FirstInternationalJointConferenceonArtificialIntelligence,IJCAI-22,pages4143–4149.InternationalJointConferencesonArtificialIntelligenceOrganization.MainTrack.IvanHabernal,RaffaelHannemann,ChristianPol-lak,ChristopherKlamm,PatrickPauli,andIrynaGurevych.2017.Argotario:Computationalargu-mentationmeetsseriousgames.InProceedingsofthe2017ConferenceonEmpiricalMethodsinNat-uralLanguageProcessing:SystemDemonstrations,pages7–12,Copenhagen,Denmark.AssociationforComputationalLinguistics.IvanHabernal,PatrickPauli,andIrynaGurevych.2018.AdaptingSeriousGameforFallaciousArgumenta-tiontoGerman:Pitfalls,Insights,andBestPrac-tices.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).ShohrehHaddadan,ElenaCabrio,andSerenaVillata.2019.Yes,wecan!miningargumentsin50yearsofUSpresidentialcampaigndebates.InProceedingsofthe57thAnnualMeetingoftheAssociationforCom-putationalLinguistics,pages4684–4690,Florence,Italy.AssociationforComputationalLinguistics.ZhijingJin,AbhinavLalwani,TejasVaidhya,Xi-aoyuShen,YiwenDing,ZhihengLyu,MrinmayaSachan,RadaMihalcea,andBernhardSchölkopf.2022.Logicalfallacydetection.arXivpreprintarXiv:2202.13758.Jan-ChristophKlie,MichaelBugert,BetoBoullosa,RichardEckartdeCastilho,andIrynaGurevych.2018.TheINCEpTIONplatform:Machine-assistedandknowledge-orientedinteractiveannotation.InProceedingsofthe27thInternationalConferenceonComputationalLinguistics:SystemDemonstrations,pages5–9,SantaFe,NewMexico.AssociationforComputationalLinguistics.AnneLauscher,HenningWachsmuth,IrynaGurevych,andGoranGlavas.2022.Scientiapotentiaest-ontheroleofknowledgeincomputationalargumentation.Trans.Assoc.Comput.Linguistics,10:1392–1422.JohnLawrenceandChrisReed.2019.Argumentmin-ing:Asurvey.Comput.Linguistics,45(4):765–818.ElenaMusi,MyrtoAloumpi,ElinorCarmi,SimeonYates,andKayO’Halloran.2022.Developingfakenewsimmunity:fallaciesasmisinformationtriggersduringthepandemic.OnlineJournalofCommunica-tionandMediaTechnologies,12(3).SaumyaSahai,OanaBalalau,andRoxanaHorincar.2021.Breakingdowntheinvisiblewallofinfor-malfallaciesinonlinediscussions.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages644–657,Online.AssociationforComputationalLinguistics.11111

PrashanthVijayaraghavanandSoroushVosoughi.2022.TWEETSPIN:Fine-grainedpropagandadetectioninsocialmediausingmulti-viewrepresentations.InProceedingsofthe2022ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages3433–3448,Seattle,UnitedStates.AssociationforComputationalLinguistics.VorakitVorakitphan,ElenaCabrio,andSerenaVillata.2021.”Don’tdiscuss”:InvestigatingSemanticandArgumentativeFeaturesforSupervisedPropagandistMessageDetectionandClassification.InRANLP2021-RecentAdvancesinNaturalLanguagePro-cessing,Varna/Virtual,Bulgaria.VorakitVorakitphan,ElenaCabrio,andSerenaVillata.2022.Protect:Apipelineforpropagandadetectionandclassification.InCLiC-it2021-ItalianConfer-enceonComputationalLinguistics.D.N.Walton.1987.InformalFallacies:TowardsaThe-oryofArgumentCriticisms.Pragmatics&beyondcompanionseries.J.BenjaminsPublishingCompany.D.N.Walton.1995.APragmaticTheoryofFallacy.Studiesinrhetoricandcommunication.UniversityofAlabamaPress.AEncodingArepresentationofthedataset’sencodedexampleisillustratedintheFigure3,demonstratinghowitispreparedforinputintothearchitecturefortokenlabelprediction.Notably,theargumentfeaturesalignwiththeoffsetmappingapproachemployedbythetokenizerduringtokenization.Eachargumentativefeatureisrepresentedbyatensoroflength256(themaximumtokenlengthof216isused)filledwithlabelIDs(0:None,1:Claim/Support,2:Premise/Attack)uptothemax-imumlength.ThesameprocessisappliedtothePart-of-Speechtensor,whereeachtagisconvertedintoitscorrespondingID(0:ADJ,1:ADP,2:ADV,3:AUX,4:CCONJ,5:DET,6:INTJ,7:NOUN,8:NUM,9:PART,10:PRON,11:PROPN,12:PUNCT,13:SCONJ,14:SYM,15:VERB,16:X).BDetailedModelPerformanceInthisappendixsection,acomprehensiveclassifi-cationreportofthebest-performingmodelispre-sented,consideringallBIOlabelsandallthethreefeatures(argumentativecomponents,argumenta-tiverelationsandPoStags).Thisreportprovidesathoroughassessmentofthemodelperformance,offeringvaluableinsightsintoitsaccuracyandef-fectivenessinrecognizingandclassifyingdifferentcategoriestoeverytoken.Figure3:EncodedexampleofasingleitemofthedatasetElecDeb60to20.Labelprecisionrecallf1-scoresupportB-AdHominem1.000.190.3127B-AppealtoAuthority0.750.500.6030B-AppealtoEmotion0.720.390.51120B-FalseCause0.750.330.469B-Slipperyslope0.330.120.188B-Slogans0.000.000.005I-AdHominem0.980.790.88712I-AppealtoAuthority0.900.780.841’019I-AppealtoEmotion0.810.780.792’104I-FalseCause0.810.870.84312I-Slipperyslope0.890.890.89324I-Slogans0.000.000.0044O0.900.950.937’914accuracy0.8812’628macroavg0.680.510.5612’628weightedavg0.880.880.8812’628Table7:Classificationreportoffallacyentityclassifica-tionwithBIOlabels.CTokenDistributionTable8showsthedistributionoftheBIOtokensamongthetrainingandtestset.DAnalysisofDifferentαValuesMultiFusionBERT’slossfunctionisdefinedasjointloss=α∗(lossfal+losscmp+lossrel+lossPoS)Nloss,11112

LabelTrainTestTotalB-AdHominem24327270B-AppealtoAuthority27230302B-AppealtoEmotion1’0731201’193B-FalseCause84993B-Slipperyslope71879B-Slogans47552I-AdHominem4’8557125’567I-AppealtoAuthority8’7071’0199’726I-AppealtoEmotion17’4082’10419’512I-FalseCause3’0763123’388I-Slipperyslope2’4873242’811I-Slogans25444298O74’4937’91482407Total113’07012’628125’698Table8:DistributionofBIOfallacytagsamongthetrainandtestset.αFeaturesAvgmacroComponentsRelationshipsPoSF1Score0.1✓0.6922✓0.6922✓0.7212✓✓0.7278✓✓0.7166✓✓0.7166✓✓✓0.73940.3✓0.7054✓0.7054✓0.7214✓✓0.6889✓✓0.7160✓✓0.7160✓✓✓0.70840.5✓0.7057✓0.7057✓0.6817✓✓0.7366✓✓0.7054✓✓0.7054✓✓✓0.7070Table9:MultiFusionBERT’saveragemacroF1scoresforfallacydetectionusingdifferentfeaturesandvaluesoftheαparameter.Thescoresarebasedonanaverageof3runs.BandIlabelsweremerged.whereNlossdenotesthenumberoflossesconsid-eredbythemodel.Weconductedanexplorationofvariousvaluesfortheαparameter.Table9showsthattheimpactoftheparametervariesdependingonthefeaturesusedinthemodel.Thatis,noneofthevaluesofαyieldsageneralizedimprovementinthemacroF1scoreacrossallfeaturecombinations.