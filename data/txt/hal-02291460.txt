Weighted Linear Bandits for Non-Stationary
Environments
Yoan Russac, Claire Vernade, Olivier Cappé

To cite this version:

Yoan Russac, Claire Vernade, Olivier Cappé. Weighted Linear Bandits for Non-Stationary Envi-
ronments. NeurIPS 2019 - 33rd Conference on Neural Information Processing Systems, Dec 2019,
Vancouver, Canada. ￿hal-02291460v2￿

HAL Id: hal-02291460

https://inria.hal.science/hal-02291460v2

Submitted on 19 Mar 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Weighted Linear Bandits for Non-Stationary
Environments

Yoan Russac
CNRS, Inria, ENS, Université PSL
yoan.russac@ens.fr

Claire Vernade
Deepmind
vernade@google.com

Olivier Cappé
CNRS, Inria, ENS, Université PSL
olivier.cappe@cnrs.fr

Abstract

We consider a stochastic linear bandit model in which the available actions corre-
spond to arbitrary context vectors whose associated rewards follow a non-stationary
linear regression model. In this setting, the unknown regression parameter is al-
lowed to vary in time. To address this problem, we propose D-LinUCB, a novel
optimistic algorithm based on discounted linear regression, where exponential
weights are used to smoothly forget the past. This involves studying the devia-
tions of the sequential weighted least-squares estimator under generic assumptions.
As a by-product, we obtain novel deviation results that can be used beyond non-
stationary environments. We provide theoretical guarantees on the behavior of
D-LinUCB in both slowly-varying and abruptly-changing environments. We ob-
tain an upper bound on the dynamic regret that is of order d2/3B1/3
T T 2/3, where
BT is a measure of non-stationarity (d and T being, respectively, dimension and
horizon). This rate is known to be optimal. We also illustrate the empirical perfor-
mance of D-LinUCB and compare it with recently proposed alternatives in simulated
environments.

1

Introduction

Multi-armed bandits offer a class of models to address sequential learning tasks that involve
exploration-exploitation trade-offs.
In this work we are interested in structured bandit models,
known as stochastic linear bandits, in which linear regression is used to predict rewards [1, 2, 22].

A typical application of bandit algorithms based on the linear model is online recommendation where
actions are items to be, for instance, efﬁciently arranged on personalized web pages to maximize some
conversion rate. However, it is unlikely that customers’ preferences remain stable and the collected
data becomes progressively obsolete as the interest for the items evolve. Hence, it is essential to
design adaptive bandit agents rather than restarting the learning from scratch on a regular basis. In
this work, we consider the use of weighted least-squares as an efﬁcient method to progressively forget
past interactions. Thus, we address sequential learning problems in which the parameter of the linear
bandit is evolving with time.

Our ﬁrst contribution consists in extending existing deviation inequalities to sequential weighted
least-squares. Our result applies to a large variety of bandit problems and is of independent interest.
In particular, it extends the recent analysis of heteroscedastic environments by [18]. It can also be
useful to deal with class imbalance situations, or, as we focus on here, in non-stationary environments.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

As a second major contribution, we apply our results to propose D-LinUCB, an adaptive linear bandit
algorithm based on carefully designed exponential weights. D-LinUCB can be implemented fully
recursively —without requiring the storage of past actions— with a numerical complexity that is
comparable to that of LinUCB. To characterize the performance of the algorithm, we provide a uniﬁed
regret analysis for abruptly-changing or slowly-varying environments.

The setting and notations are presented below and we state our main deviation result in Section 2.
Section 3 is dedicated to non-stationary linear bandits: we describe our algorithms and provide regret
upper bounds in abruptly-changing and slowly-varying environments. We complete this theoretical
study with a set of experiments in Section 4.

1.1 Model and Notations

The setting we consider in this paper is a non-stationary variant of the stochastic linear bandit problem
considered in [1, 22], where, at each round t ≥ 1, the learner

• receives a ﬁnite set of feasible actions At ⊂ Rd;
• chooses an action At ∈ At and receives a reward Xt such that

(1)
t ∈ Rd is an unknown parameter and ηt is, conditionally on the past, a

t (cid:105) + ηt,

Xt = (cid:104)At, θ(cid:63)

where θ(cid:63)
σ−subgaussian random noise.

The action set At may be arbitrary but its components are assumed to be bounded, in the sense that
(cid:107)a(cid:107)2 ≤ L, ∀a ∈ At. The time-varying parameter is also assumed to be bounded: ∀t, (cid:107)θ(cid:63)
t (cid:107)2 ≤ S. We
further assume that | (cid:104)a, θ(cid:63)
t (cid:105) | ≤ 1, ∀t, ∀a ∈ At, (obviously, this could be guaranteed by assuming
that L = S = 1, but we indicate the dependence in L and S in order to facilitate the interpretation
of some results). For a positive deﬁnite matrix M and a vector x, we denote by (cid:107)x(cid:107)M the norm
√

x(cid:62)M x.

The goal of the learner is to minimize the expected dynamic regret deﬁned as

R(T ) = E

(cid:34) T

(cid:88)

t=1

(cid:35)

max
a∈At

(cid:104)a, θ(cid:63)

t (cid:105) − Xt

=

T
(cid:88)

t=1

max
a∈At

(cid:104)a − At, θ(cid:63)

t (cid:105) .

(2)

Even in the stationary case —i.e., when θ(cid:63)
this model.

t = θ(cid:63)—, there is, in general, no single ﬁxed best action in

When making stronger structural assumption on At, one recovers speciﬁc instances that have also
been studied in the literature. In particular, the canonical basis of Rd, At = {e1, . . . , ed}, yields the
familiar —non contextual— multi-armed bandit model [20]. Another variant, studied by [15] and
others, is obtained when At = {e1 ⊗ at, . . . , ek ⊗ at}, where ⊗ denotes the Kronecker product and
at is a time-varying context vector shared by the k actions.

1.2 Related Work

There is an important literature on online learning in changing environments. For the sake of
conciseness, we restrict the discussion to works that consider speciﬁcally the stochastic linear bandit
model in (1), including its restriction to the simpler (non-stationnary) multi-armed bandit model.
Note that there is also a rich line of works that consider possibly non-linear contextual models in the
case where one can make probabilistic assumptions on the contexts [10, 23].

Controlling the regret with respect to the non-stationary optimal action deﬁned in (2) depends on the
assumptions that are made on the time-variations of θ(cid:63)
t . A generic way of quantifying them is through
a variation bound BT = (cid:80)T −1
s+1(cid:107)2 [4, 6, 11], similar to the penalty used in the group fused
Lasso [8]. The main advantage of using the variation budget is that is includes both slowly-varying
and abruptly-changing environments. For the K−armed bandits with known BT , [4–6] achieve the
tight dynamic regret bound of O(K 1/3B1/3
T T 2/3). For linear bandits, [11, 12] propose an algorithm
based on the use of a sliding-window and provide a O(d2/3B1/3
T T 2/3) dynamic regret bound; since
this contribution is close to ours, we discuss it further in Section 3.2.

s=1 (cid:107)θ(cid:63)

s − θ(cid:63)

2

√

A more speciﬁc non-stationary setting arises when the number of changes in the parameter is bounded
by ΓT , as in traditional change-point models. The problem is usually referred to as switching bandits
or abruptly-changing environments. It is, for instance, the setting considered in the work by Garivier
and Moulines [14], who analyzed the dynamic regret of UCB strategies based on either a sliding-
window or exponential discounting. For both policies, they prove upper bounds on the regret in
ΓT T ) when ΓT is known. They also provide a lower bound in a speciﬁc non-stationary setting,
O(
showing that R(T ) = Ω(
T ). The algorithm ideas can be traced back to [19]. [28] shows that an
horizon-independent version of the sliding window algorithm can also be analyzed in a slowly-varying
setting. [17] analyze windowing and discounting approaches to address dynamic pricing guided by a
(time-varying) linear regression model. Discount factors have also been used with Thomson sampling
in dynamic environments as in [16, 26].

√

(cid:15)2 + 1

ΓT KT ) if ΓT is known. [7] achieves a rate of O(

In abruptly-changing environments, the alternative approach relies on change-point detection [3, 7,
9, 29, 30]. A bound on the regret in O(( 1
∆ ) log(T )) is proven by [30], where (cid:15) is the smallest
gap that can be detected by the algorithm, which had to be given as prior knowledge. [9] proves a
√
√
minimax bound in O(
ΓT KT ) without any prior
knowledge of the gaps or ΓT . In the contextual case, [29] builds on the same idea: they use a pool of
LinUCB learners called slave models as experts and they add a new model when no existing slave
is able to give good prediction, that is, when a change is detected. A limitation however of such an
approach is that it can not adapt to some slowly-varying environments, as will be illustrated in Section
4. From a practical viewpoint, the methods based either on sliding window or change-point detection
require the storage of past actions whereas those based on discount factors can be implemented fully
recursively.

Finally, non-stationarity may also arise in more speciﬁc scenarios connected, for instance, to the
decaying attention of the users, as investigated in [21, 24, 27]. In the following, we consider the
general case where the parameters satisfy the variation bound, i.e., (cid:80)T −1
t+1(cid:107)2 ≤ BT and
we propose an algorithm based on discounted linear regression.

t=1 (cid:107)θ(cid:63)

t − θ(cid:63)

2 Conﬁdence Bounds for Weighted Linear Bandits

In this section, we consider the concentration of the weighted regularized least-squares estimator,
when used with general weights and regularization parameters. To the best of our knowledge there is
no such results in the literature for sequential learning —i.e., when the current regressor may depend
on the random outcomes observed in the past. The particular case considered in Lemma 5 of [18]
(heteroscedastic noise with optimal weights) stays very close to the unweighted case and we show
below how to extend this result. We believe that this new bound is of interest beyond the speciﬁc
model considered in this paper. For the sake of clarity, we ﬁrst focus on the case of regression models
with ﬁxed parameter, where θ(cid:63)

t = θ(cid:63), for all t.

First consider a deterministic sequence of regularization parameters (λt)t≥1. The reason why these
should be non-constant for weighted least-squares will appear clearly in Section 3. Next, deﬁne by
Ft = σ(X1, . . . , Xt) the ﬁltration associated with the random observations. We assume that both the
actions At and positive weights wt are predictable, that is, they are Ft−1 measurable.

Deﬁning by

ˆθt = arg min
θ∈Rd

(cid:32) t

(cid:88)

s=1

ws(Xs − (cid:104)As, θ(cid:105))2 + λt(cid:107)θ(cid:107)2
2

(cid:33)

the regularized weighted least-squares estimator of θ(cid:63) at time t, one has

ˆθt = V −1

t

t
(cid:88)

s=1

wsAsXs where Vt =

t
(cid:88)

s=1

wsAsA(cid:62)

s + λtId,

(3)

and Id denotes the d-dimensional identity matrix. We further consider an arbitrary sequence of
positive parameters (µt)t≥1 and deﬁne the matrix

(cid:101)Vt =

t
(cid:88)

s=1

w2

sAsA(cid:62)

s + µtId.

3

(4)

(cid:101)V is strongly connected to the variance of the estimator ˆθt, which involves the squares of the weights
(w2
s)s≥1. For the time being, µt is arbitrary and will be set as a function of λt in order to optimize
the deviation inequality.

We then have the following maximal deviation inequality.
Theorem 1. For any Ft-predictable sequences of actions (At)t≥1 and positive weights (wt)t≥1 and
for all δ > 0,


∀t, (cid:107)ˆθt − θ(cid:63)(cid:107)Vt (cid:101)V −1
t Vt

P

≤

λt√
µt

S + σ

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log(1/δ) + d log

(cid:32)

1 +

L2 (cid:80)t
s=1 w2
s
dµt

(cid:33)

 ≥ 1 − δ.

The proof of this theorem is deferred to the appendix and combines an argument using the method
of mixtures and the use of a proper stopping time. The standard result used for least-squares [20,
Chapter 20] is recovered by taking µt = λt and wt = 1 (note that (cid:101)Vt is then equal to Vt). When
the weights are not equal to 1, the appearance of the matrix (cid:101)Vt is a consequence of the fact that the
variance terms are proportional to the squared weights w2
t , while the least-squares estimator itself is
deﬁned with the weights wt. In the weighted case, the matrix Vt (cid:101)V −1
t Vt must be used to deﬁne the
conﬁdence ellipsoid.

An important property of the least-squares estimator is to be scale-invariant, in the sense that
multiplying all weights (ws)1≤s≤t and the regularization parameter λt by a constant leaves the
estimator ˆθt unchanged. In Theorem 1, the only choice of sequence (µt)t≥1 that is compatible with
this scale-invariance property is to take µt proportional to λ2
t Vt becomes
scale-invariant (i.e. unchanged by the transformation ws (cid:55)→ αws) and so does the upper bound of
(cid:107)ˆθt − θ(cid:63)(cid:107)Vt (cid:101)V −1
in Theorem 1. In the following, we will stick to this choice, while particularizing
t Vt
the choice of the weights wt to allow for non-stationary models.

t : then the matrix Vt (cid:101)V −1

It is possible to extend this result to heteroscedastic noise, when ηt is σt sub-Gaussian and σt is Ft−1
measurable, by deﬁning (cid:101)Vt as (cid:80)t
s + µtId. In the next section, we will also use an
extension of Theorem 1 to the non-stationary model presented in (1) . In this case, Theorem 1 holds
(cid:1), where r is an arbitrary time index (proposition
with θ(cid:63) replaced by V −1
s + λtθ(cid:63)
r
3 in Appendix). The fact that r can be chosen freely is a consequence of the assumption that the
sequence of L2-norms of the parameters (θ(cid:63)

s=1 wsAsA(cid:62)

t )t≥1 is bounded by S.

s AsA(cid:62)

s=1 w2

s θ(cid:63)

sσ2

(cid:0)(cid:80)t

t

3 Application to Non-stationary Linear Bandits

In this section, we consider the non-stationary model deﬁned in (1) and propose a bandit algorithm in
Section 3.1, called Discounted Linear Upper Conﬁdence Bound (D-LinUCB), that relies on weighted
least-squares to adapt to changes in the parameters θ(cid:63)
t . Analyzing the performance of D-LinUCB
in Section 3.2, we show that it achieves reliable performance both for abruptly changing or slowly
drifting parameters.

3.1 The D-LinUCB Algorithm

Being adaptive to parameter changes indeed implies to reduce the inﬂuence of observations that are
far back in the past, which suggests using weights wt that increase with time. In doing so, there
are two important caveats to consider. First, this can only be effective if the sequence of weights
is growing sufﬁciently fast (see the analysis in the next section). We thus consider exponentially
increasing weights of the form wt = γ−t, where 0 < γ < 1 is the discount factor.
Next, due to the absence of assumptions on the action sets At, the regularization is instrumental in
obtaining guarantees of the form given in Theorem 1. In fact, if wt = γ−t while λt does not increase
sufﬁciently fast, then the term log(cid:0)1 + (L2 (cid:80)t
s)/(dµt)(cid:1) will eventually dominate the radius
s=1 w2
of the conﬁdence region since we choose µt proportional to λ2
t . This occurs because there is no
guarantee that the algorithm will persistently select actions At that span the entire space. With this
in mind, we consider an increasing regularization factor of the form λt = γ−tλ, where λ > 0 is a
hyperparameter.

4

Note that due to the scale-invariance property of the weighted least-square estimator, we can equiva-
lently consider that at time t, we are given time-dependent weights wt,s = γt−s, for 1 ≤ s ≤ t and
that ˆθt is deﬁned as

(cid:0)

arg min
θ∈Rd

t
(cid:88)

s=1

γt−s(Xs − (cid:104)As, θ(cid:105))2 + λ/2(cid:107)θ(cid:107)2
2

(cid:1).

For numerical stability reasons, this form is preferable and is used in the statement of Algorithm 1. In
the analysis of Section 3.2 however we revert to the standard form of the weights, which is required to
apply the concentration result of Section 1. We are now ready to describe D-LinUCB in Algorithm 1.

Algorithm 1: D-LinUCB
Input: Probability δ, subgaussianity constant σ, dimension d, regularization λ, upper bound for

actions L, upper bound for parameters S, discount factor γ.

Initialization: b = 0Rd , V = λId, (cid:101)V = λId, ˆθ = 0Rd
for t ≥ 1 do

Receive At, compute βt−1 =

λS + σ

√

(cid:114)

2 log (cid:0) 1

δ

(cid:1) + d log

(cid:16)

1 + L2(1−γ2(t−1))

λd(1−γ2)

(cid:17)

for a ∈ At do

Compute UCB(a) = a(cid:62) ˆθ + βt−1

(cid:112)

At = arg max a(UCB(a))
Play action At and receive reward Xt
Updating phase: V = γV + AtA(cid:62)

b = γb + XtAt, ˆθ = V −1b

a(cid:62)V −1 (cid:101)V V −1a

t + (1 − γ)λId, (cid:101)V = γ2 (cid:101)V + AtA(cid:62)

t + (1 − γ2)λId

3.2 Analysis

As discussed previously, we consider weights of the form wt = γ−t (where 0 < γ < 1) in the
D-LinUCB algorithm. In accordance with the discussion at the end of Section 1, Algorithm 1 uses
µt = γ−2tλ as the parameter to deﬁne the conﬁdence ellipsoid around ˆθt−1. The conﬁdence ellipsoid
Ct is deﬁned as (cid:8)θ : (cid:107)θ − ˆθt−1(cid:107)Vt−1 (cid:101)V −1
≤ βt−1
(cid:115)

(cid:9) where

t−1Vt−1

√

βt =

λS + σ

2 log(1/δ) + d log

1 +

.

(5)

(cid:18)

(cid:19)

L2(1 − γ2t)
λd(1 − γ2)

Using standard algebraic calculations together with the remark above about scale-invariance it is
easily checked that at time t Algorithm 1 selects the action At that maximizes (cid:104)a, θ(cid:105) for a ∈ At and
θ ∈ Ct. The following theorem bounds the regret resulting from Algorithm 1.
Theorem 2. Assuming that (cid:80)T −1
s − θ(cid:63)
bounded for all γ ∈ (0, 1) and integer D ≥ 1, with probability at least 1 − δ, by

s+1(cid:107)2 ≤ BT , the regret of the D-LinUCB algorithm is

s=1 (cid:107)θ(cid:63)

RT ≤ 2LDBT +

4L3S
λ

γD
1 − γ

T + 2

√

√

2βT

(cid:115)

(cid:18)

dT

T log(1/γ) + log

1 +

L2
dλ(1 − γ)

(cid:19)

.

(6)

The ﬁrst two terms of the r.h.s. of (6) are the result of the bias due to the non-stationary environment.
The last term is the consequence of the high probability bound established in the previous section and
an adaptation of the technique used in [1].

We give the complete proof of this result in appendix. The high-level idea of the proof is to isolate bias
and variance terms. However, in contrast with the stationary case, the conﬁdence ellipsoid Ct does
not necessarily contain (with high probability) the actual parameter value θ(cid:63)
t due to the (unknown)
bias arising from the time variations of the parameter. We thus deﬁne

¯θt = V −1
t−1

(cid:32)t−1
(cid:88)

s=1

γ−sAsA(cid:62)

s θ(cid:63)

s + λγ−(t−1)θ(cid:63)
t

(cid:33)

5

which is an action-dependent analogue of the parameter value θ(cid:63) in the stationary setting (although
this is a random value). As mentioned in section 2, ¯θt does belong to Ct with probability at least 1 − δ
(see Proposition 3 in Appendix). The regret may then be split as

RT ≤ 2L

T
(cid:88)

t=1

(cid:107)θ(cid:63)

t − ¯θt(cid:107)2 +

T
(cid:88)

t=1

(cid:104)At, θt − ¯θt(cid:105)

(with probability at least 1 − δ),

where (At, θt) = arg max(a∈At,θ∈Ct)(cid:104)a, θ(cid:105). The rightmost term can be handled by proceeding as in
the case of stationary linear bandits, thanks to the deviation inequality obtained in Section 2. The ﬁrst
term in the r.h.s. can be bounded deterministically, from the assumption made on (cid:80)T −1
s+1(cid:107)2.
In doing so, we introduce the analysis parameter D that, roughly speaking, corresponds to the window
length equivalent to a particular choice of discount factor γ: the bias resulting from observations that
are less than D time steps apart may be bounded in term of D while the remaining ones are bounded
globally by the second term of the r.h.s. of (6). This sketch of proof is substantially different from
the arguments used by [11] to analyze their sliding window algorithm (called SW-LinUCB). We refer
to the appendix for a more detailed analysis of these differences. Interestingly, the regret bound of
Theorem 2 holds despite the fact that the true parameter θ(cid:63)
t may not be contained in the conﬁdence
ellipsoid Ct−1, in contrast to the proof of [14].

s=1 (cid:107)θ(cid:63)

s − θ(cid:63)

It can be checked that, as T tends to inﬁnity, the optimal choice of the analysis parameter D is to take
D = log(T )/(1 − γ). Further assuming that one may tune γ as a function of the horizon T and the
variation upper bound BT yields the following result.
Corollary 1. By choosing γ = 1 − (BT /(dT ))2/3, the regret of the D-LinUCB algorithm is asymp-
totically upper bounded with high probability by a term O(d2/3B1/3

T T 2/3) when T → ∞.

This result is favorable as it corresponds to the same order as the lower bound established by [4].
More precisely, the case investigated by [4] corresponds to a non-contextual model with a number
of changes that grows with the horizon. On the other hand, the guarantee of Corollary 1 requires
horizon-dependent tuning of the discount factor γ, which opens interesting research issues (see also
[11]).

4 Experiments

This section is devoted to the evaluation of the empirical performance of D-LinUCB. We ﬁrst consider
two simulated low-dimensional environments that illustrate the behavior of the algorithms when
confronted to either abrupt changes or slow variations of the parameters. The analysis of the previous
section, suggests that D-LinUCB should behave properly in both situations. We then consider a more
realistic scenario in Section 4.2, where the contexts are high-dimensional and extracted from a data
set of actual user interactions with a web service.

For benchmarking purposes, we compare D-LinUCB to the Dynamic Linear Upper Conﬁdence Bound
(dLinUCB) algorithm proposed by [29] and with the Sliding Window Linear UCB (SW-LinUCB)
of [11]. The principle of the dLinUCB algorithm is that a master bandit algorithm is in charge of
choosing the best LinUCB slave bandit for making the recommendation. Each slave model is built
to run in each one of the different environments. The choice of the slave model is based on a lower
conﬁdence bound for the so-called badness of the different models. The badness is deﬁned as the
number of times the expected reward was found to be far enough from the actual observed reward on
the last τ steps, where τ is a parameter of the algorithm. When a slave is chosen, the action proposed
to a user is the result of the LinUCB algorithm associated with this slave. When the action is made,
all the slave models that were good enough are updated and the models whose badness were too high
are deleted from the pool of slaves models. If none of the slaves were found to be sufﬁciently good, a
new slave is added to the pool.

The other algorithm that we use for comparison is SW-LinUCB, as presented in [11]. Rather than
using exponentially increasing weights, a hard threshold is adopted. Indeed, the actions and rewards
included in the l-length sliding window are used to estimate the linear regression coefﬁcients. We
expect D-LinUCB and SW-LinUCB to behave similarly as they both may be shown to have the same
sort of regret guarantees (see appendix).

In the case of abrupt changes, we also compare these algorithms to the Oracle Restart LinUCB
(LinUCB-OR) strategy that would know the change-points and simply restart, after each change, a

6

new instance of the LinUCB algorithm. The regret of this strategy may be seen as an empirical lower
bound on the optimal behavior of an online learning algorithm in abruptly changing environments.

In the following ﬁgures, the vertical red dashed lines correspond to the change-points (in abrupt
changes scenarios). They are represented to ease the understanding but except for LinUCB-OR, they
are of course unknown to the learning algorithms. When applicable, the blue dashed lines correspond
to the average detection time of the breakpoints with the dLinUCB algorithm. For D-LinUCB the
discount parameter is chosen as γ = 1 − ( BT
dT )2/3. For SW-LinUCB the window’s length is set to
l = ( dT
)2/3, where d = 2 in the experiment. Those values are theoretically supposed to minimize the
BT
asymptotic regret. For the Dynamic Linear UCB algorithm, the badness is estimated from τ = 200
steps, as in the experimental section of [29].

4.1 Synthetic data in abruptly-changing or slowly-varying scenarios

Figure 1: Performances of the algorithms in the abruptly-changing environment (on the left), and, the
slowly-varying environment (on the right). The upper plots correspond to the estimated parameter and
the lower ones to the accumulated regret, both are averaged on N = 100 independent experiments

t = (1, 0); for t ∈ [[1000, 2000]], θ(cid:63)

t = (−1, 0); for t ∈ [[2000, 3000]], θ(cid:63)

In this ﬁrst experiment, we observe the empirical performance of all algorithms in an abruptly
changing environment of dimension 2 with 3 breakpoints. The number of rounds is set to T = 6000.
The light blue triangles correspond to the different positions of the true unknown parameter θ(cid:63)
t :
before t = 1000, θ(cid:63)
t = (0, 1);
and, ﬁnally, for t > 3000, θ(cid:63)
t = (0, −1). This corresponds to a hard problem as the sequence of
parameters is widely spread in the unit ball. Indeed it forces the algorithm to adapt to big changes,
which typically requires a longer adaptation phase. On the other hand, it makes the detection of
changes easier, which is an advantage for dLinUCB. In the second half of the experiment (when
t ≥ 3000) there is no change, LinUCB struggles to catch up and suffers linear regret for long periods
after the last change-point. The results of our simulations are shown in the left column of Figure 1.
On the top row we show a 2-dimensional scatter plot of the estimate of the unknown parameters
ˆθt every 1000 steps averaged on 100 independent experiment. The bottom row corresponds to the
regret averaged over 100 independent experiments with the upper and the lower 5% quantiles. In this
environment, with 1-subgaussian random noise, dLinUCB struggles to detect the change-points. Over
the 100 experiments, the ﬁrst change-point was detected in 95% of the runs, the second was never
detected and the third only in 6% of the runs, thus limiting the effectiveness of the dLinUCB approach.
When decreasing the variance of the noise, the performance of dLinUCB improves and gets closer to

7

the performance of the oracle restart strategy LinUCB-OR. It is worth noting that for both SW-LinUCB
and D-LinUCB, the estimator ˆθt adapts itself to non-stationarity and is able to follow θ(cid:63)
t (with some
delay), as shown on the scatter plot. Predictably, LinUCB-OR achieves the best performance by
restarting exactly whenever a change-point happens.

The second experiment corresponds to a slowly-changing environment. It is easier for LinUCB to
keep up with the adaptive policies in this scenario. Here, the parameter θ(cid:63)
t starts at (1 and moves
continuously counter-clockwise on the unit-circle up to the position [0, 1] in 3000 steps. We then have
a steady period of 3000 steps. For this sequence of parameters, BT = (cid:80)T −1
t+1(cid:107)2 = 1.57.
The results are reported in the right column of Figure 1. Unsurprisingly, dLinUCB does not detect
any change and thus displays the same performance as LinUCB. SW-LinUCB and D-LinUCB behaves
similarly and are both robust to such an evolution in the regression parameters. The performance of
LinUCB-OR is not reported here, as restarting becomes ineffective when the changes are too frequent
(here, during the ﬁrst 3000 time steps, there is a change at every single step). The scatter plot also
gives interesting information: ˆθt tracks θ(cid:63)
t quite effectively for both SW-LinUCB and D-LinUCB but
the two others algorithms lag behind. LinUCB will eventually catch up if the length of the stationary
period becomes larger.

t=1 (cid:107)θ(cid:63)

t − θ(cid:63)

4.2 Simulation based on a real dataset

Figure 2: Behavior of the different algorithms on large-dimensional data

D-LinUCB also performs well in high-dimensional space (d = 50). For this experiment, a dataset
providing a sample of 30 days of Criteo live trafﬁc data [13] was used. It contains banners that
were displayed to different users and contextual variables, including the information of whether the
banner was clicked or not. We kept the categorical variables cat1 to cat9 , together with the variable
campaign, which is a unique identiﬁer of each campaign. Beforehand, these contexts have been one-
hot encoded and 50 of the resulting features have been selected using a Singular Value Decomposition.
θ(cid:63) is obtained by linear regression. The rewards are then simulated using the regression model with
an additional Gaussian noise of variance σ2 = 0.15. At each time step, the different algorithms
have the choice between two 50-dimensional contexts drawn at random from two separate pools of
10000 contexts corresponding, respectively, to clicked or not clicked banners. The non-stationarity
is created by switching 60% of θ(cid:63) coordinates to −θ(cid:63) at time 4000, corresponding to a partial class
inversion. The cumulative dynamic regret is then averaged over 100 independent replications. The
results are shown on Figure 2. In the ﬁrst stationary period, LinUCB and dLinUCB perform better
than the adaptive policies by using all available data, whereas the adaptive policies only use the most
recent events. After the breakpoint, LinUCB suffers a large regret, as the algorithm fails to adapt to the
new environment. In this experiment, dLinUCB does not detect the change-point systematically and
performs similarly as LinUCB on average, it can still outperform adaptive policies from time to time
when the breakpoint is detected as can be seen with the 5% quantile. D-LinUCB and SW-LinUCB adapt
more quickly to the change-point and perform signiﬁcantly better than the non-adaptive policies after
the breakpoint. Of course, the oracle policy LinUCB-OR is the best performing policy. The take-away
message is that there is no free lunch: in a stationary period by using only the most recent events
SW-LinUCB and D-LinUCB do not perform as good as a policy that uses all the available information.
Nevertheless, after a breakpoint, the recovery is much faster with the adaptive policies.

8

References

[1] Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits.

In Advances in Neural Information Processing Systems, pages 2312–2320, 2011.

[2] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research, 3(Nov):397–422, 2002.

[3] P. Auer, P. Gajane, and R. Ortner. Adaptively tracking the best arm with an unknown number of

distribution changes. In European Workshop on Reinforcement Learning 14, 2018.

[4] O. Besbes, Y. Gur, and A. Zeevi. Stochastic multi-armed-bandit problem with non-stationary

rewards. In Advances in neural information processing systems, pages 199–207, 2014.

[5] O. Besbes, Y. Gur, and A. Zeevi. Non-stationary stochastic optimization. Operations research,

63(5):1227–1244, 2015.

[6] O. Besbes, Y. Gur, and A. Zeevi. Optimal exploration-exploitation in a multi-armed-bandit

problem with non-stationary rewards. Available at SSRN 2436629, 2018.

[7] L. Besson and E. Kaufmann. The generalized likelihood ratio test meets klucb: an improved
algorithm for piece-wise non-stationary bandits. arXiv preprint arXiv:1902.01575, 2019.

[8] K. Bleakley and J.-P. Vert. The group fused lasso for multiple change-point detection. arXiv

preprint arXiv:1106.4199, 2011.

[9] Y. Cao, W. Zheng, B. Kveton, and Y. Xie. Nearly optimal adaptive procedure for piecewise-
stationary bandit: a change-point detection approach. arXiv preprint arXiv:1802.03692, 2018.

[10] Y. Chen, C.-W. Lee, H. Luo, and C.-Y. Wei. A new algorithm for non-stationary contextual
bandits: Efﬁcient, optimal, and parameter-free. arXiv preprint arXiv:1902.00980, 2019.

[11] W. C. Cheung, D. Simchi-Levi, and R. Zhu. Learning to optimize under non-stationarity. arXiv

preprint arXiv:1810.03024, 2018.

[12] W. C. Cheung, D. Simchi-Levi, and R. Zhu. Hedging the drift: Learning to optimize under

non-stationarity. arXiv preprint arXiv:1903.01461, 2019.

[13] Diemert Eustache, Meynet Julien, P. Galland, and D. Lefortier. Attribution modeling increases
In Proceedings of the AdKDD and TargetAd

efﬁciency of bidding in display advertising.
Workshop, KDD, Halifax, NS, Canada, August, 14, 2017. ACM, 2017.

[14] A. Garivier and E. Moulines. On upper-conﬁdence bound policies for switching bandit problems.
In International Conference on Algorithmic Learning Theory, pages 174–188. Springer, 2011.

[15] A. Goldenshluger and A. Zeevi. A linear response bandit problem. Stoch. Syst., 3(1):230–261,

2013.

[16] N. Gupta, O.-C. Granmo, and A. Agrawala. Thompson sampling for dynamic multi-armed
bandits. In 2011 10th International Conference on Machine Learning and Applications and
Workshops, volume 1. IEEE, 2011.

[17] N. B. Keskin and A. Zeevi. Chasing demand: Learning and earning in a changing environment.

Mathematics of Operations Research, 42(2):277–307, 2017.

[18] J. Kirschner and A. Krause. Information directed sampling and bandits with heteroscedastic

noise. arXiv preprint arXiv:1801.09667, 2018.

[19] L. Kocsis and C. Szepesvári. Discounted ucb. In: 2nd Pascal Challenge Workshop, 2006.

[20] T. Lattimore and C. Szepesvári. Bandit Algorithms. Cambridge University Press, 2019.

[21] N. Levine, K. Crammer, and S. Mannor. Rotting bandits. In Advances in Neural Information

Processing Systems, pages 3074–3083, 2017.

9

[22] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized

news article recommendation. In WWW, 2010.

[23] H. Luo, C.-Y. Wei, A. Agarwal, and J. Langford. Efﬁcient contextual bandits in non-stationary

worlds. arXiv preprint arXiv:1708.01799, 2017.

[24] Y. Mintz, A. Aswani, P. Kaminsky, E. Flowers, and Y. Fukuoka. Non-stationary bandits with

habituation and recovery dynamics. arXiv preprint arXiv:1707.08423, 2017.

[25] V. H. Peña, T. L. Lai, and Q.-M. Shao. Self-normalized processes: Limit theory and Statistical

Applications. Springer Science & Business Media, 2008.

[26] V. Raj and S. Kalyani. Taming non-stationary bandits: A bayesian approach. arXiv preprint

arXiv:1707.09727, 2017.

[27] J. Seznec, A. Locatelli, A. Carpentier, A. Lazaric, and M. Valko. Rotting bandits are no harder

than stochastic ones. arXiv preprint arXiv:1811.11043, 2018.

[28] L. Wei and V. Srivatsva. On abruptly-changing and slowly-varying multiarmed bandit problems.

In 2018 Annual American Control Conference (ACC), pages 6291–6296. IEEE, 2018.

[29] Q. Wu, N. Iyer, and H. Wang. Learning contextual bandits in a non-stationary environment. In
The 41st International ACM SIGIR Conference on Research & Development in Information
Retrieval, SIGIR ’18, pages 495–504, New York, NY, USA, 2018. ACM.

[30] J. Y. Yu and S. Mannor. Piecewise-stationary bandit problems with side observations. In
Proceedings of the 26th Annual International Conference on Machine Learning, pages 1177–
1184. ACM, 2009.

10

Appendix

A Conﬁdence Bounds for Weighted Linear Bandits

A.1 Preliminary results

In this section we give the main results for obtaining Theorem 1. For the sake of conciseness all the
results will be stated with σ-subgaussian noises but the proofs will be done with the particular value
of σ = 1. The model we consider is the one deﬁned by equation (1), where we recall that (ηs)s is,
conditionally on the past, a sequence of σ-subgaussian random noises. The results of this section are
close to the one proposed in [1] but our results are valid with a sequence of predictable weights.
We introduce the quantity St = (cid:80)t
s + µtId. When the
regularization term is omitted, let (cid:101)Vt(0) = (cid:80)t
s . The ﬁltration associated with the
random observations is denoted Ft = σ(X1, . . . , Xt) such that At is Ft−1-measurable and ηt is
Ft-measurable. The weights are also assumed to be predictable. The following lemma is an extension
to the weighted case of Lemma 8 of [1].
Lemma 1. Let (wt)t≥1 be a sequence of predictable and positive weights. Let x ∈ Rd be arbitrary
and consider for any t ≥ 1

s=1 wsAsηs and (cid:101)Vt = (cid:80)t
s=1 w2

sAsA(cid:62)

sAsA(cid:62)

s=1 w2

Mt(x) = exp

(cid:18) 1
σ

x(cid:62)St −

x(cid:62) (cid:101)Vt(0)x

(cid:19)

.

1
2

Let τ be a stopping time with respect to the ﬁltration {Ft}∞
well-deﬁned and

t=0. Then Mτ (x) is almost surely

∀x ∈ Rd, E[Mτ (x)] ≤ 1.

Proof. First, we prove that ∀x ∈ Rd, (Mt(x))∞
Let x ∈ Rd,

t=0 is a super-martingale.

E[Mt(x)|Ft−1] = E

(cid:104)

exp

(cid:16)

x(cid:62)St−1 + x(cid:62)wtAtηt − 1/2x(cid:62)( (cid:101)Vt−1(0) + w2
t AtA(cid:62)
(cid:21)

t )x

= Mt−1(x)E

(cid:20)
exp(x(cid:62)wtAtηt −

1
2

w2

t x(cid:62)AtA(cid:62)

t x)|Ft−1

(cid:17)

(cid:105)

|Ft−1

= Mt−1(x) exp(−

≤ Mt−1(x) exp(−

= Mt−1(x).

1
2
1
2

w2

t x(cid:62)AtA(cid:62)

t x)E (cid:2)exp(x(cid:62)wtAtηt)|Ft−1

(cid:3)

w2

t x(cid:62)AtA(cid:62)

t x) exp(1/2w2

t (x(cid:62)At)2)

The second equality comes from the fact that St−1 and (cid:101)Vt−1 are Ft−1-measurable. The inequality is
the deﬁnition of the conditional 1-subgaussianity where we also use the Ft−1-measurability of wt.
Using this supermartingale property, we have E[Mt(x)] ≤ 1. The convergence theorem for
non-negative supermartingales ensures that M∞(x) = limt→∞ Mt(x) is almost surely well de-
ﬁned. By introducing the stopped supermartingale Mt(x) = Mmin(t,τ )(x), we have Mτ (x) =
limt→∞ Mt(x). Knowing that Mt(x) is also a supermartingale, we have

E[Mt(x)] = E[Mmin(t,τ )(x)] ≤ E[Mmin(0,τ )(x)] = E[M0(x)] = 1.

By using Fatou’s lemma:

E[Mτ (x)] = E[lim inf
t→∞

Mt(x)] ≤ lim inf
t→∞

E[Mt(x)] ≤ 1.

In the next lemma, we will integrate Mt(x) with respect to a time-dependent probability measure.
This is the key for allowing sequential regularizations in the concentration inequality stated in
Theorem 1. This lemma is inspired by the method of mixtures ﬁrst presented in [25]. The idea of

11

using time-varying probability measures is inspired from the proof of Theorem 11 in [18]. The two
following lemmas are included in the appendix so that the article is self-contained. There are not a
mere consequence of the results in [1] because of the time-dependent regularization parameters. As
explained in Section 3, this is unavoidable when using exponential weights to avoid the vanishing
effect of the regularization.
Lemma 2. Let (ht)t be a sequence of probability measures on Rd. We deﬁne (cid:102)Mt =
(cid:82)
Rd Mt(x)dht(x). Then,

∀t, E[ (cid:102)Mt] ≤ 1

Proof.

(cid:90)

E[ (cid:102)Mt] =

(cid:102)Mt dP =

(cid:90) (cid:18)(cid:90)

(cid:19)

Mt(x)dht(x)

dP

(cid:19)

Mt(x)dP

dht(x)

(Fubini’s theorem)

Rd
(cid:18)(cid:90)

=

=

≤

(cid:90)

Rd

(cid:90)

Rd

(cid:90)

Rd

E[Mt(x)]dht(x)

dht(x)

(Lemma 1)

≤ 1.

(ht probability measure.)

Lemma 2 is a warm-up for the next lemma and is helpful for understanding why Lemma 3 holds. It is
valid for any ﬁxed time t. The next step is to give its equivalent in a stopped version in the speciﬁc
case of gaussian random vectors.
Lemma 3. Let (µt)t be a deterministic sequence of regularization parameters. Let F∞ = σ (∪∞
t=1Ft)
be the tail σ-algebra of the ﬁltration (Ft)t. Let X = (Xt)t≥1 be an independent sequence of gaussian
random vectors such that Xt ∼ N (0, 1
µt

Id) = ht with X independent of F∞. We deﬁne

¯Mt(µt) = E[Mt(Xt)|F∞] =

(cid:90)

Rd

Mt(x)fµt(x)dx,

where fµt is the probability density function associated with ht deﬁned as,

fµt(x) =

1
(cid:112)(2π)d det(1/µtId)

exp(−

µtx(cid:62)x
2

).

Let τ be a stopping time with respect to the ﬁltration (Ft)t then,

E[ ¯Mτ (µτ )] ≤ 1.

Proof. We can use the result of Lemma 1 which gives ∀x ∈ Rd, E[Mτ (x)] ≤ 1.
We have,

E[ ¯Mτ (µτ )] = E[E[Mτ (Xτ )|F∞]] = E[E[E[Mτ (Xτ )|F∞]|(Xt)t≥1]]

= E[E[E[Mτ (Xτ )|(Xt)t≥1]|F∞]] ≤ 1.

The inequality is a consequence of Lemma 1 as, conditionally to the sequence (Xt)t, Mτ (Xτ ) is of
the form Mτ (x) with a ﬁxed x.

We ﬁnally state the main result needed to obtain Theorem 1.
Proposition 1. For (ws)s≥1 a sequence of predictable and positive weights, ∀δ > 0, the following
deviation inequality holds



P

∃t ≥ 0, (cid:107)St(cid:107)

(cid:101)V −1
t

≥ σ

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

(cid:19)

(cid:18) 1
δ

+ log

(cid:32)

det( (cid:101)Vt)
µd
t

(cid:33)

 ≤ δ.

12

Proof. For a ﬁxed t,

¯Mt(µt) =

(cid:90)

Rd

Mt(x)fµt(x)dx

=

=

=

=

=

1
(cid:112)(2π)d det(1/µtId)
1
(cid:112)(2π)d det(1/µtId)
1
(cid:112)(2π)d det(1/µtId)
(cid:17)
2 (cid:107)St(cid:107)2
exp
(cid:112)(2π)d det(1/µtId)
(cid:17)
2 (cid:107)St(cid:107)2
exp
(cid:112)(2π)d det(1/µtId)
(cid:19) (cid:115)

(cid:101)V −1
t

(cid:101)V −1
t

(cid:16) 1

(cid:16) 1

(cid:18) 1
2

(cid:107)St(cid:107)2

(cid:101)V −1
t

= exp

(cid:90)

Rd

(cid:90)

Rd

(cid:90)

Rd

(cid:90)

Rd

(cid:114)

(cid:17)

(cid:16)

(cid:101)V −1
t

(2π)d det

det(µtId)
det( (cid:101)Vt)

.

exp

exp

exp

(cid:18)

(cid:18)

x(cid:62)St −

x(cid:62)St −

1
2

1
2

(cid:107)x(cid:107)2

µtId

−

(cid:19)

dx

(cid:107)x(cid:107)2
(cid:101)Vt

1
2

(cid:107)x(cid:107)2

(cid:101)Vt(0)

(cid:19)

dx

(cid:107)St(cid:107)2

(cid:101)V −1
t

−

1
2

(cid:107)x − (cid:101)V −1

t St(cid:107)2
(cid:101)Vt

(cid:19)

dx

(cid:18) 1
2

(cid:18)

exp

−

1
2

(cid:107)x − (cid:101)V −1

t St(cid:107)2
(cid:101)Vt

(cid:19)

dx

We introduce the particular stopping time,

(cid:26)

τ = min

t ≥ 0, (cid:107)St(cid:107)

(cid:101)V −1
t

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

≥

(cid:19)

(cid:18) 1
δ

+ log

(cid:32)

(cid:33)(cid:27)

.

det( (cid:101)Vt)
det(µtId)

Thus,



P

∃t ≥ 0, (cid:107)St(cid:107)

(cid:101)V −1
t

≥

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

(cid:19)

(cid:18) 1
δ

(cid:32)

+ log

det( (cid:101)Vt)
det(µtId)

(cid:33)

 = P(τ < ∞)



= P

τ < ∞, (cid:107)Sτ (cid:107)

(cid:101)V −1
τ

≥

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

(cid:19)

(cid:18) 1
δ

(cid:32)

+ log

(cid:33)


det( (cid:101)Vτ )
det(µτ Id)



≤ P

(cid:107)Sτ (cid:107)

(cid:101)V −1
τ

≥

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

(cid:19)

(cid:18) 1
δ

(cid:32)

+ log

(cid:33)


det( (cid:101)Vτ )
det(µτ Id)

(cid:32)

= P

exp

(cid:18) 1
2

(cid:107)Sτ (cid:107)2

(cid:101)V −1
τ

(cid:19) (cid:115)

(cid:33)

det(µτ Id)
det( (cid:101)Vτ )

≥

1
δ

≤ δE[ ¯Mτ (µτ )] (Markov’s inequality) ≤ δ (Lemma 3).

A.2 Proof of Theorem 1

We recall that Theorem 1 is established in a stationary environment where ∀t ≥ 1, θ(cid:63)

t = θ(cid:63).

Proof. First note that,

ˆθt = V −1

t

= V −1
t

t
(cid:88)

s=1
t
(cid:88)

s=1

wsAsXs

wsAs(A(cid:62)

s θ(cid:63) + ηs)

(Equation 1)

13

= V −1
t

(cid:32) t

(cid:88)

s=1

wsAsA(cid:62)

s θ(cid:63) + λtθ(cid:63) − λtθ(cid:63)

(cid:33)

Thus,

+ V −1

t St = θ(cid:63) − λtV −1

t

θ(cid:63) + V −1

t St.

ˆθt − θ(cid:63) = V −1

t St − λtV −1

t

θ(cid:63).

(7)

∀x ∈ Rd, ∀t > 0, we have

|x(cid:62)(ˆθt − θ(cid:63))| ≤ (cid:107)x(cid:107)V −1

t

(cid:101)VtV −1
t

(cid:16)

(cid:16)

(cid:107)V −1

t St(cid:107)Vt (cid:101)V −1

t Vt
+ λt(cid:107)θ(cid:63)(cid:107)

+ (cid:107)λtV −1
(cid:17)

t

.

(cid:107)St(cid:107)

(cid:17)

θ(cid:63)(cid:107)Vt (cid:101)V −1

t Vt

≤ (cid:107)x(cid:107)V −1

t

(cid:101)VtV −1
t

(cid:101)V −1
t
t Vt(ˆθt − θ(cid:63)), we have
By applying the previous inequality with x = Vt (cid:101)V −1
.

∀t, (cid:107)ˆθt − θ(cid:63)(cid:107)Vt (cid:101)V −1
t Vt
Knowing that (cid:101)Vt ≥ µtId and that (cid:101)Vt is positive deﬁnite, we have (cid:107)θ(cid:63)(cid:107)

+ λt(cid:107)θ(cid:63)(cid:107)

≤ (cid:107)St(cid:107)

(cid:101)V −1
t

(cid:101)V −1
t

(cid:101)V −1
t

≤ 1√
µt

(cid:107)θ(cid:63)(cid:107)2.

(cid:101)V −1
t

Finally,

∀t, (cid:107)ˆθt − θ(cid:63)(cid:107)Vt (cid:101)V −1
t Vt

≤ (cid:107)St(cid:107)

(cid:101)V −1
t

+

λt√
µt

(cid:107)θ(cid:63)(cid:107)2.

(8)

From Proposition 1, we obtain the following any time high probability upper bound for (cid:107)St(cid:107)

(cid:101)V −1
t

,



P

∀t ≥ 0, (cid:107)St(cid:107)

(cid:101)V −1
t

≤ σ

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

(cid:19)

(cid:18) 1
δ

+ log

(cid:32)

det( (cid:101)Vt)
µd
t

(cid:33)

 ≥ 1 − δ.

Therefore by using inequality 8,


∀t ≥ 0, (cid:107)ˆθt − θ(cid:63)(cid:107)

(cid:101)V −1
t

P

≤

λt√
µt

S + σ

(cid:118)
(cid:117)
(cid:117)
(cid:116)2 log

(cid:19)

(cid:18) 1
δ

+ log

(cid:32)

det( (cid:101)Vt)
µd
t

(cid:33)

 ≥ 1 − δ.

We obtain the exact formula of Theorem 1 by upper bounding det( (cid:101)Vt) as proposed in Proposition
2

B D-LinUCB Analysis

In this section, the environment is non-stationary, which means that the unknown parameter θ(cid:63)
may evolve over time and is denoted θ(cid:63)
t . The reward generation process in the one presented in
Equation (1).

B.1 Preliminary results

In this section, Vt and (cid:101)Vt are deﬁned by

Vt =

t
(cid:88)

s=1

γ−sAsA(cid:62)

s + λγ−tId,

(cid:101)Vt =

We recall the deﬁnition of βt:

t
(cid:88)

s=1

(cid:18)

√

βt =

(cid:115)

λS + σ

2 log(1/δ) + d log

1 +

L2(1 − γ2t)
λd(1 − γ2)

(cid:19)

.

γ−2sAsA(cid:62)

s + λγ−2tId.

With ˆθt deﬁned in equation (3), the conﬁdence ellipsoid we consider is deﬁned by

(cid:26)

Ct =

θ ∈ Rd : (cid:107)θ − ˆθt−1(cid:107)Vt−1 (cid:101)V −1

t−1Vt−1

(cid:27)

≤ βt−1

.

(9)

Theorem 1 can be applied with this choice of weights and regularization. We combine it with an
upper bound for det( (cid:101)Vt) given below.

14

Proposition 2 (Determinant inequality for the weighted design matrix). Let (λt)t be a deterministic
sequence of regularization parameters. Let Vt = (cid:80)t
s=1 wsAsA(cid:62)
s + λtId be the weighted design
matrix. Under the assumption ∀t, (cid:107)At(cid:107)2 ≤ L, the following holds

(cid:32)

det(Vt) ≤

λt +

L2 (cid:80)t
s=1 ws
d

(cid:33)d

.

Proof.

det(Vt) =

≤

≤

≤

d
(cid:89)

i=1
(cid:32)

1
d

(cid:18) 1
d

li

(li are the eigenvalues)

(cid:33)d

d
(cid:88)

i=1

li

(AM-GM inequality)

(cid:19)d

(cid:32)

trace(Vt)

≤

wstrace(AsA(cid:62)

s ) + λt

(cid:33)d

1
d

t
(cid:88)

s=1
(cid:33)d

(cid:32)

(cid:32)

1
d

t
(cid:88)

s=1

ws(cid:107)As(cid:107)2

2 + λt

≤

λt +

(cid:33)d

ws

.

L2
d

t
(cid:88)

s=1

Corollary 2. In the speciﬁc case where the weights are given by wt = γ−t with 0 < γ < 1.
Proposition 2 can be rewritten

(cid:18)

det(Vt) ≤

λt +

L2(γ−t − 1)
d(1 − γ)

(cid:19)d

(cid:18)

=

λγ−t +

L2(γ−t − 1)
d(1 − γ)

(cid:19)d

.

We also have,

(cid:18)

det( (cid:101)Vt) ≤

µt +

L2(γ−2t − 1)
d(1 − γ2)

(cid:19)d

(cid:18)

=

λγ−2t +

L2(γ−2t − 1)
d(1 − γ2)

(cid:19)d

.

Proof. Apply Proposition 2 and use (cid:80)t

s=1 γ−s = γ−t−1

1−γ and (cid:80)t

s=1 γ−2s = γ−2t−1

1−γ2

.

Corollary 2 and Proposition 1 yield the following result.
Corollary 3. ∀δ > 0, with the weights wt = γ−t and 0 < γ < 1, we have

(cid:32)

(cid:115)

P

∃t ≥ 0, (cid:107)St(cid:107)

≥ σ

2 log

(cid:101)V −1
t

(cid:19)

(cid:18) 1
δ

(cid:18)

+ d log

1 +

L2(1 − γ2t)
λd(1 − γ2)

(cid:19)(cid:33)

≤ δ.

Thanks to this corollary we are now ready to show that ¯θt belongs to Ct−1 with high probability.

Proposition 3. Let Ct =

ellipsoid. Let ¯θt = V −1
t−1

(cid:26)

(cid:16)(cid:80)t−1

θ ∈ Rd : (cid:107)θ − ˆθt−1(cid:107)Vt−1 (cid:101)V −1
s θ(cid:63)
s + λγ−(t−1)θ(cid:63)
s=1 γ−sAsA(cid:62)
t
P (cid:0)∀t ≥ 1, ¯θt ∈ Ct

(cid:1) ≥ 1 − δ.

t−1Vt−1
(cid:17)

. Then, ∀δ > 0,

(cid:27)

≤ βt−1

denote the conﬁdence

Proof.

¯θt − ˆθt−1 = V −1
t−1

(cid:32)t−1
(cid:88)

s=1

γ−sAsA(cid:62)

s θ(cid:63)

s + λγ−(t−1)θ(cid:63)

t −

(cid:33)

γ−sAsXs

t−1
(cid:88)

s=1

15

= V −1
t−1

(cid:32)t−1
(cid:88)

s=1

γ−sAsA(cid:62)

s θ(cid:63)

s + λγ−(t−1)θ(cid:63)

t −

t−1
(cid:88)

s=1

γ−sAsA(cid:62)

s θ(cid:63)

s −

(cid:33)

γ−sAsηs

t−1
(cid:88)

s=1

= −V −1

t−1St−1 + λγ−(t−1)V −1

t−1θ(cid:63)
t .

Therefore,

(cid:107)¯θt − ˆθt−1(cid:107)Vt−1 (cid:101)V −1

t−1Vt−1

≤ (cid:107)St−1(cid:107)

(cid:101)V −1
t−1

+ λγ−(t−1)(cid:107)θ(cid:63)
t (cid:107)
λS ( (cid:101)V −1

√

+

(cid:101)V −1
t−1

≤ (cid:107)St−1(cid:107)

≤ βt−1

(cid:101)V −1
t−1
(Corollary 3).

t−1 ≤ 1/(γ−2(t−1)λ)Id and (cid:107)θ(cid:63)

t (cid:107)2 ≤ S)

B.2 Control of the norm of actions
Lemma 4. Let Vt = (cid:80)t
0 < γ < 1. We have

s=1 γ−sAsA(cid:62)

s + λγ−tId and (cid:101)Vt = (cid:80)t

s=1 γ−2sAsA(cid:62)

s + λγ−2tId and

∀t, V −1
t

(cid:101)Vt V −1

t ≤ γ−t V −1

t

.

Proof.

(cid:101)Vt =

t
(cid:88)

s=1

Consequently,

γ−2sAsA(cid:62)

s + λγ−2tId ≤ γ−t

t
(cid:88)

s=1

γ−sAsA(cid:62)

s + λγ−2tId = γ−tVt.

V −1
t

(cid:101)VtV −1

t ≤ γ−tV −1

t VtV −1

t ≤ γ−tV −1

t

.

Thanks to Lemma 4 we establish the following proposition,
Proposition 4.

(cid:16)

min

T
(cid:88)

t=1

1, (cid:107)At(cid:107)2

t−1 (cid:101)Vt−1V −1
V −1

t−1

(cid:17)

≤ 2

(cid:16)

log

T
(cid:88)

t=1

1 + γ−t(cid:107)At(cid:107)2

V −1
t−1

(cid:17)

≤ 2 log

(cid:18) det(VT )
λd

(cid:19)

.

Proof. We ﬁrst use the fact that: ∀x ≥ 0, min(1, x) ≤ 2 log(1 + x).

1, (cid:107)At(cid:107)2

t−1 (cid:101)Vt−1V −1
V −1

t−1

(cid:17)

≤ 2 log

≤ 2 log

≤ 2 log

(cid:16)

(cid:16)

(cid:16)

(cid:16)

min

Furthermore,

1 + (cid:107)At(cid:107)2

t−1 (cid:101)Vt−1V −1
V −1

t−1

(cid:17)

1 + γ−(t−1)(cid:107)At(cid:107)2
V −1
t−1
(cid:17)

1 + γ−t(cid:107)At(cid:107)2

V −1
t−1

(cid:17)

(Lemma 4)

(γ ≤ 1).

Vt ≥ γ−tAtA(cid:62)

t + Vt−1 ≥ V 1/2

t−1 (Id + γ−tV −1/2

t−1 AtA(cid:62)

t V −1/2

t−1 )V 1/2
t−1 .

Given that all those matrices are symmetric positive deﬁnite, the previous inequality implies that

det(Vt) ≥ det(Vt−1) det(1 + (γ−t/2V −1/2
(cid:16)

≥ det(Vt−1)

1 + γ−t(cid:107)At(cid:107)2

t−1 At)(γ−t/2V −1/2
t−1 At)(cid:62))
(cid:17) (cid:0)Using det(Id + xx(cid:62)) = 1 + (cid:107)x(cid:107)2

(cid:1) .

2

V −1
t−1

Therefore,

det(VT )
det(V0)

=

T
(cid:89)

t=1

det(Vt)
det(Vt−1)

T
(cid:89)

≥

t=1

(1 + γ−t(cid:107)At(cid:107)2

).

V −1
t−1

16

Finally by applying the log function to the previous inequality,

(cid:16)

min

T
(cid:88)

t=1

1, (cid:107)At(cid:107)2

V −1
t−1 (cid:101)Vt−1V −1

t−1

(cid:17)

≤ 2

(cid:16)

log

T
(cid:88)

t=1

1 + γ−t(cid:107)At(cid:107)2

V −1
t−1

(cid:17)

≤ 2 log

(cid:18) det(VT )
det(V0)

(cid:19)

.

Corollary 4.

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:18)

min

1, (cid:107)At(cid:107)2

V −1
t−1 (cid:101)Vt−1V −1

t−1

(cid:19)

√

≤

(cid:115)

2d

T log

(cid:19)

(cid:18) 1
γ

(cid:18)

+ log

1 +

L2
dλ(1 − γ)

(cid:19)
.

Proof. The proof of this corollary is based on the previous lemma and on Corollary 2. We have

log

(cid:19)

(cid:18) det(VT )
det(V0)

(cid:32)

≤ log

λγ−T +

L2(γ−T − 1)
d(1 − γ)

(cid:19)d(cid:33)

(Corollary 2)

(cid:18)

1
λd
(cid:18) 1
γ

≤ dT log

(cid:19)

(cid:18)

+ d log

1 +

L2
dλ(1 − γ)

(cid:19)

.

B.3 Proof of Theorem 2

In this subsection we give the proof of Theorem 2 for the high probability upper-bound of the regret
for D-LinUCB.

Proof.

First step: Upper bound for the instantaneous regret.

Let A(cid:63)

t = arg max a∈At(cid:104)a, θ(cid:63)

t (cid:105) and θt = arg max θ∈Ct(cid:104)At, θ(cid:105). We have,
t − At, θ(cid:63)
t (cid:105)
t − ¯θt(cid:105).

t (cid:105) − (cid:104)At, θ(cid:63)
t − At, ¯θt(cid:105) + (cid:104)A(cid:63)

rt = max
a∈At
= (cid:104)A(cid:63)

t − At, θ(cid:63)

t (cid:105) = (cid:104)A(cid:63)

(cid:104)a, θ(cid:63)

Under the event {∀t > 0, ¯θt ∈ Ct}, that occurs with probability at least 1 − δ thanks to Proposition
3, we have,

(cid:104)A(cid:63)

t , ¯θt(cid:105) ≤ arg max

θ∈Ct

(cid:104)A(cid:63)

t , θ(cid:105) = UCBt(A(cid:63)

t ) ≤ UCBt(At) = arg max

(cid:104)At, θ(cid:105) = (cid:104)At, θt(cid:105).

(10)

θ∈Ct

rt ≤ (cid:104)At, θt − ¯θt(cid:105) + (cid:104)A(cid:63)

Then, with probability at least 1 − δ, ∀t > 0,
t − ¯θt(cid:105)
t − At, θ(cid:63)
(cid:107)θt − ¯θt(cid:107)Vt−1 (cid:101)V −1
(cid:107)θt − ¯θt(cid:107)Vt−1 (cid:101)V −1

≤ (cid:107)At(cid:107)V −1
≤ (cid:107)At(cid:107)V −1

t−1 (cid:101)Vt−1V −1

t−1 (cid:101)Vt−1V −1

t−1

t−1

t−1Vt−1

t−1Vt−1

+ (cid:107)A(cid:63)

t − At(cid:107)2(cid:107)θ(cid:63)
t − ¯θt(cid:107)2

t − ¯θt(cid:107)2
(∀a ∈ At(cid:107)a(cid:107)2 ≤ L).

(Cauchy-Schwarz)

+ 2L(cid:107)θ(cid:63)

As discussed in Section 3.2, the two terms are upper bounded using different techniques. The ﬁrst
term is handled with the equivalent in a non-stationary environment of the deviation inequality of
Theorem 1 and the second term is the equivalent of the bias.
Second step: Upper bound for (cid:107)θt − ¯θt(cid:107)Vt−1 (cid:101)V −1
We have,

t−1Vt−1

.

(cid:107)θt − ¯θt(cid:107)Vt−1 (cid:101)V −1

t−1Vt−1

≤ (cid:107)θt − ˆθt−1(cid:107)Vt−1 (cid:101)V −1

t−1Vt−1

+ (cid:107)¯θt − ˆθt−1(cid:107)Vt−1 (cid:101)V −1

t−1Vt−1

≤ 2βt−1,

where the last inequality holds because under our assumption ¯θt ∈ Ct with high probability and by
deﬁnition θt ∈ Ct.

17

Third step: Upper bound for the bias.

Let D > 0,

(cid:107)θ(cid:63)

t − ¯θt(cid:107)2 = (cid:107)V −1

t−1

t−1
(cid:88)

s=1

γ−sAsA(cid:62)

s (θ(cid:63)

s − θ(cid:63)

t )(cid:107)2

t−1
(cid:88)

≤ (cid:107)

s=t−D

t−1
(cid:88)

≤ (cid:107)

s=t−D

t−1
(cid:88)

≤ (cid:107)

p=t−D

V −1
t−1γ−sAsA(cid:62)

s (θ(cid:63)

s − θ(cid:63)

t )(cid:107)2 + (cid:107)V −1
t−1

t−D−1
(cid:88)

s=1

γ−sAsA(cid:62)

s (θ(cid:63)

s − θ(cid:63)

t )(cid:107)2

V −1
t−1γ−sAsA(cid:62)
s

t−1
(cid:88)

p=s

(θ(cid:63)

p − θ(cid:63)

p+1)(cid:107)2 + (cid:107)

t−D−1
(cid:88)

V −1
t−1γ−sAsA(cid:62)
s

p
(cid:88)

s=t−D

(θ(cid:63)

p − θ(cid:63)

p+1)(cid:107)2 +

s=1

1
λ

t−D−1
(cid:88)

s=1

γ−sAsA(cid:62)

s (θ(cid:63)

s − θ(cid:63)

t )(cid:107)V −2

t−1

γt−1−s(cid:107)AsA(cid:62)

s (θ(cid:63)

s − θ(cid:63)

t )(cid:107)2

≤

≤

t−1
(cid:88)

p=t−D

t−1
(cid:88)

p=t−D

(cid:107)V −1
t−1

λmax

p
(cid:88)

s=t−D
(cid:32)

V −1
t−1

γ−sAsA(cid:62)

s (θ(cid:63)

p − θ(cid:63)

p+1)(cid:107)2 +

2L2S
λ

t−D−1
(cid:88)

s=1

γt−1−s

(cid:33)

γ−sAsA(cid:62)
s

p
(cid:88)

s=t−D

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 +

2L2S
λ

γD
1 − γ

.

The ﬁrst inequality is a consequence of the triangular inequality. The third inequality uses that
t−1 ≤ ( γt−1
V −2
In the last inequality, we have used the fact that for a symmetric matrix
M ∈ Md(R) and a vector x ∈ Rd, (cid:107)M x(cid:107)2 ≤ λmax(M )(cid:107)x(cid:107)2.
Furthermore, for x such that (cid:107)x(cid:107)2 ≤ 1, we have that for t − D ≤ p ≤ t − 1,

λ )2Id.

x(cid:62)V −1
t−1

p
(cid:88)

s=t−D

γ−sAsA(cid:62)

s x ≤ x(cid:62)V −1
t−1

t−1
(cid:88)

s=1

γ−sAsA(cid:62)

s x + λγ−(t−1)x(cid:62)V −1

t−1x

t−1
(cid:88)

≤ x(cid:62)V −1
t−1(

s=1

γ−sAsA(cid:62)

s + λγ−(t−1)Id)x = x(cid:62)x ≤ 1.

Therefore, for all p such that t − D ≤ p ≤ t − 1, λmax

(cid:0)V −1

t−1

(cid:80)p

s=t−D γ−sAsA(cid:62)
s

(cid:1) ≤ 1.

By combining the second and the third step, with probability at least 1 − δ:

rt ≤ 2L

t−1
(cid:88)

p=t−D

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 +

4L3S
λ

γD
1 − γ

+ 2βt−1(cid:107)At(cid:107)V −1

t−1 (cid:101)Vt−1V −1

t−1

.

The assumption |(cid:104)At, θ(cid:63)
t−1
(cid:88)

rt ≤ 2L

p=t−D

t (cid:105)| ≤ 1 also implies rt ≤ 2. Hence, with probability at least 1 − δ:

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 + 4L3S

γD
1 − γ

+ 2βt−1 min(1, (cid:107)At(cid:107)V −1

t−1 (cid:101)Vt−1V −1

t−1

).

(11)

To conclude the proof we use the results of Subsection B.2.

Final step:

RT =

T
(cid:88)

t=1

rt

T
(cid:88)

t−1
(cid:88)

≤ 2L

t=1

p=t−D

T
(cid:88)

t−1
(cid:88)

≤ 2L

t=1

p=t−D

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 +

4L3S
λ

γD
1 − γ

T + 2βT

(cid:16)

min

T
(cid:88)

t=1

1, (cid:107)At(cid:107)V −1

t−1 (cid:101)Vt−1V −1

t−1

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 +

4L3S
λ

γD
1 − γ

T + 2βT

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

√

T

(cid:18)

min

t=1

18

(cid:17)

(cid:19)

1, (cid:107)At(cid:107)2

t−1 (cid:101)Vt−1V −1
V −1

t−1

≤ 2LBT D +

4L3S
λ

γD
1 − γ

T + 2

√

√

2βT

(cid:115)

(cid:18)

dT

T log(1/γ) + log

1 +

L2
dλ(1 − γ)

(cid:19)

.

In the ﬁrst inequality, we use that t (cid:55)→ βt is increasing. The second inequality is an application of the
Cauchy-Schwarz inequality to the third term and the last inequality is an application of Corollary
4.

B.4 Proof of Corollary 1

Proof. Let γ be deﬁned as γ = 1 − ( BT
to d2/3B−2/3
In addition,

T

T 2/3 log(T ). Thus, DBT is equivalent to d2/3B1/3

T T 2/3 log(T ).

dT )2/3 and D = log(T )

(1−γ) . With this choice of γ, D is equivalent

γD = exp(D log(γ)) = exp

(cid:18) log(γ)
1 − γ

(cid:19)

log(T )

∼ 1/T.

Hence, T γD 1

1−γ behaves as d2/3T 2/3B−2/3

T

.

Furthermore, log(1/γ) ∼ d−2/3B2/3

T T −2/3, implying that T log(1/γ) ∼ d−2/3B2/3

T T 1/3.

√

that, βT

(cid:114)

dT

T log(1/γ) + log

(cid:16)

1 + L2

dλ(1−γ)

(cid:17)

is equivalent

to

As a result,

it holds
(cid:113)

dT 1/2(cid:112)log(T /BT )
By adding those three terms and neglecting the log factors, we obtain the desired result.

T T 2/3(cid:112)log(T /BT ).

T T 1/3 = d2/3B1/3

d−2/3B2/3

C A new analysis of the SW-LinUCB algorithm

In this section we propose a new analysis of the SW-LinUCB algorithm. This is useful as the proof
provided in [11] has several gaps. First, Lemma 2 of [11] is presented as a speciﬁc case of the
analysis of [1]. It would hold in the case of a growing window, where the argument developed in
[1] could be used, but not with a sliding window, where past actions are removed from the design
matrix. Furthermore, Theorem 2 of [11] that bounds |(cid:104)x, ˆθt−1 − θ(cid:63)
t (cid:105)| for any ﬁxed direction x with
high probability is used in equation (42) with x replaced by At, whereas At is a random variable
strongly related to ˆθt−1.
We only mention this analysis in the Appendix because the deviation inequalities established for the
weighted model can not be used. Nevertheless, we believe that this analysis gives new insights on the
problem with a sliding window.

C.1 Deviation inequality

Let us introduce some notations to clarify the model. We suppose that there is a sliding window of
length l, such that the estimate of the unknown parameter at time t is based on the l last observations.
The optimization program solved is

ˆθt = arg min
θ∈Rd





t
(cid:88)

(Xs − (cid:104)As, θ(cid:105))2 + λ/2(cid:107)θ(cid:107)2
2)

 .



s=max(1,t−l+1)

One has

ˆθt = V −1

t

t
(cid:88)

AsXs, where Vt =

t
(cid:88)

AsA(cid:62)

s + λId.

(12)

s=max(1,t−l+1)

s=max(1,t−l+1)

The expression linking the matrices Vt and Vt−1 is the following
t − At−lA(cid:62)

Vt = Vt−1 + AtA(cid:62)

t−l.

19

The speciﬁcity of the sliding window model is that at time t, to update the design matrix, a new action
vector At is added but the oldest term At−l is also removed . When considering the equivalent of the
quantity Mt(x) deﬁned in the Appendix A, the property of supermartingale does not hold anymore
because of this loss of information. For this reason, all the reasoning that was done in [1] can not be
applied directly.

The reward generation process we consider is still the one presented in Equation 1. As for the
D-LinUCB model, the results are stated with σ-subgaussian random noises but the proofs are done
with σ = 1. Let St = (cid:80)t
s=max(1,t−l+1) Asηs. We start by giving the proof of the analogue of
Lemma 2 presented in [11]. We give an instantaneous deviation inequality.
Proposition 5 (Instantaneous deviation inequality with a sliding window). Let t be a ﬁxed time
instant. For all δ > 0,

(cid:32)

(cid:115)

P

(cid:107)St(cid:107)V −1

t

≥ σ

2 log

(cid:19)

(cid:18) 1
δ

+ log

(cid:19)(cid:33)

(cid:18) det(Vt)
λd

≤ δ.

Proof. We present an interesting trick in this proof for avoiding the loss of information due to the
sliding window that is only usable for instantaneous deviation inequalities.
Let t be the time instant of interest. We assume that t ≥ l. We know that the estimate ˆθt is only based
on observations between time t − l + 1 to t. The trick is to create a ﬁctive regression model starting
a time t − l and receiving the exact same information as the true model between the time instants
t − l + 1 to t.

To ease the understanding of the proof, the notations with dotted symbols refer to the ﬁc-
tive model. Let u be a time instant in [[t − l, t]]. Let
s + λId,
˙Su = (cid:80)u
˙Vu(0) =
(cid:80)u
s corresponds to the design matrix without the regularization term. By deﬁni-

s=max(1,t−l+1) Asηs and ˙Mu(x) = exp(x(cid:62) ˙Su − x(cid:62) ˙Vu(0)x/2). Once again,

s=max(1,t−l+1) AsA(cid:62)

s=max(1,t−l+1) AsA(cid:62)

˙Vu = (cid:80)u

tion, ∀x ∈ Rd,

˙Mt−l(x) = 1.

Using the 1-subgaussianity and following the lines of the proof of Lemma 1,

E[ ˙Mu(x)|Fu−1] ≤ ˙Mu−1(x).
Therefore, ∀u ∈ [[t − l, t]], E[ ˙Mu(x)] ≤ E[ ˙Mt−l(x)] = 1.
Rd, E[ ˙Mt(x)] ≤ 1. By introducing a measure of probability h = N (0, 1
(cid:104)(cid:82) ˙Mt(x)dh(x)
E
formula for (cid:82) ˙Mt(x)dh(x) with the chosen h. Let us remark that ˙St = St and ˙Vt = Vt.
(cid:19)

In particular for u = t, ∀x ∈
λ Id), we still have
≤ 1 using a similar reasoning than in Lemma 2. We can also give an exact

(cid:18)

(cid:105)

(cid:90)

˙Mt(x)dh(x) =

Rd

exp

(cid:16)

exp

x(cid:62)St −

(cid:107)x(cid:107)2

λId

−

(cid:107)x(cid:107)2

Vt(0)

dx

1
2

1
2

1/2(cid:107)St(cid:107)2

V −1
t

− 1/2(cid:107)x − V −1

t St(cid:107)2
Vt

(cid:17)

dx

(cid:18)

exp

−

1
2

(cid:107)x − V −1

t St(cid:107)2
Vt

(cid:19)

dx

=

=

=

(cid:90)

(cid:90)

(cid:90)

Rd

Rd

Rd

(cid:16) 1

1
(cid:112)(2π)d det(1/λId)
1
(cid:112)(2π)d det(1/λId)
(cid:17)
2 (cid:107)St(cid:107)2
exp
(cid:112)(2π)d det(1/λId)
(cid:17)
2 (cid:107)St(cid:107)2
exp
(cid:112)(2π)d det(1/λId)
(cid:19) (cid:115)

V −1
t

V −1
t

(cid:16) 1

(cid:18) 1
2

(cid:107)St(cid:107)2

V −1
t

(cid:113)

(2π)d det (cid:0)V −1

t

(cid:1)

det(λId)
det(Vt)

.

= exp

For this reason,

(cid:32)

(cid:115)

P

(cid:107)St(cid:107)V −1

t

≥

2 log

(cid:19)

(cid:18) 1
δ

+ log

(cid:19)(cid:33)

(cid:18) det(Vt)
det(λId)

20

(cid:32)

= P

exp

(cid:18) 1
2

(cid:107)St(cid:107)2

V −1
t

(cid:19) (cid:115)

det(λId)
det(Vt)

≥

1
δ

(cid:33)

(cid:20)(cid:90)

Rd

(cid:21)

˙Mt(x)dh(x)

≤ δE

≤ δ.

(Markov’s inequality)

The next step is to upper-bound the quantity det(Vt) similarly as in Proposition 2 for the weighted
model.
Proposition 6 (Determinant inequality for the design matrix with a sliding window). In the speciﬁc
case where Vt is deﬁned as Vt = (cid:80)t
s + λId. Under the assumption ∀t, (cid:107)At(cid:107)2 ≤
L, the following holds,

s=max(1,t−l+1) AsA(cid:62)

(cid:18)

det(Vt) ≤

λ +

L2 min(t, l)
d

(cid:19)d

.

The proof of this proposition is the same as in Proposition 2. By using the previous inequality, we
can obtain the following proposition,
Proposition 7. When using a sliding window model where the last l terms are considered, for all
δ > 0,

(cid:32)

(cid:115)

P

∃t ≤ T, (cid:107)St(cid:107)V −1

t

≥ σ

2 log

Proof.

(cid:19)

(cid:18) T
δ

(cid:18)

+ d log

1 +

L2 min(t, l)
λd

(cid:19)(cid:33)

≤ δ.

(cid:32)

(cid:115)

P

∃t ≤ T, (cid:107)St(cid:107)V −1

t

≥ σ

2 log

(cid:19)

(cid:18) T
δ

(cid:18)

+ d log

1 +

L2 min(t, l)
λd

(cid:19)(cid:33)

(cid:32)

(cid:115)

(cid:107)St(cid:107)V −1

t

≥ σ

2 log

(cid:32)

(cid:115)

(cid:107)St(cid:107)V −1

t

≥ σ

2 log

(cid:19)

(cid:19)

(cid:18) T
δ

(cid:18) T
δ

(Proposition 5) ≤ δ.

T
(cid:88)

P

≤

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

P

δ
T

≤

≤

(cid:19)(cid:33)

L2 min(t, l)
λd
(cid:19)(cid:33)

(cid:18)

+ d log

1 +

+ log

(cid:18) det(Vt)
λd

C.2 Regret analysis

The regret analysis of the SW-LinUCB algorithm is similar to the one proposed for D-LinUCB. We
start by deﬁning the conﬁdence ellipsoid used by the algorithm SW-LinUCB.

With the SW-LinUCB algorithm, the βt term is deﬁned in the following way,
(cid:115)

√

βt =

λS + σ

2 log

+ d log

1 +

(cid:19)

(cid:18) T
δ

(cid:18)

L2 min(t, l)
λd

(cid:19)

(13)

Remark: The cost of loosing some information at each step due to the sliding window when t > l is
the term log (cid:0) T
Note that due to the use of a union bound technique the conﬁdence radius is larger than the one
suggested in [11]. Nevertheless, this was not taken into account in simulations for SW-LinUCB.

(cid:1) in the deﬁnition of βt.

(cid:1) rather than log (cid:0) 1

δ

δ

21

Proposition 8. Let Ct =

(cid:26)

θ ∈ Rd : (cid:107)θ − ˆθt−1(cid:107)V −1

t−1

≤ βt−1

(cid:27)

denote the conﬁdence ellipsoid. Let

¯θt = V −1
t−1

(cid:16)(cid:80)t−1

s=max(1,t−l) AsA(cid:62)

s θ(cid:63)

(cid:17)

. Then, ∀δ > 0,

s + λθ(cid:63)
t
P (cid:0)∀t ≥ 1, ¯θt ∈ Ct

Proof.

¯θt − ˆθt−1 = V −1
t−1





t−1
(cid:88)

AsA(cid:62)

s θ(cid:63)

s + λθ(cid:63)

t −

s=max(1,t−l)
t−1St−1 + λV −1

t−1θ(cid:63)
t .

= −V −1

Therefore,

(cid:107)¯θt − ˆθt−1(cid:107)V −1

t−1

≤ (cid:107)St−1(cid:107)V −1

t−1

(cid:1) ≥ 1 − δ.

t−1
(cid:88)

AsA(cid:62)

s θ(cid:63)

s −

t−1
(cid:88)



Asηs



s=max(1,t−l)

s=max(1,t−l)

+ λ(cid:107)θ(cid:63)
√

t (cid:107)V −1

t−1

≤ (cid:107)St−1(cid:107)V −1
≤ βt−1

+

λS (V −1

1
λ
(with probability ≥ 1 − δ thanks to Proposition 7).

t−1 ≤

Id)

t−1

t=1 min(cid:0)1, (cid:107)At(cid:107)2
We need to bound the quantity (cid:80)T
proved in [11]. Nevertheless, we provide a simpler analysis in the following proposition.
Proposition 9. With the sliding window model, the following upper bound holds,

(cid:1). An analysis of this quantity is already

V −1
t−1

T
(cid:88)

t=1

min

(cid:16)
1, (cid:107)At(cid:107)2

V −1
t−1

(cid:17)

≤ 2d(cid:100)T /l(cid:101) log

1 +

(cid:18)

(cid:19)

.

lL2
λd

Proof. We start by rewriting the sum as follows.

T
(cid:88)

t=1

(cid:16)

min

1, (cid:107)At(cid:107)2

V −1
t−1

(cid:17)

=

(cid:100)T /l(cid:101)−1
(cid:88)

(k+1)l
(cid:88)

k=0

t=kl+1

(cid:16)

min

(cid:17)

1, (cid:107)At(cid:107)2

V −1
t−1

For the k-th block of length l we deﬁne the matrix W (k)
as every term in W (k)
∀t ∈ [[kl, (k + 1)l]], Vt ≥ W (k)
correspond to positive deﬁnite matrices. The matrices are deﬁnite positive, thus V −1
and consequently,

t = (cid:80)t
s + λId. We also have
is contained in Vt and the extra-terms in Vt
)−1

t ≤ (W (k)

s=kl+1 AsA(cid:62)

t

t

t

(cid:100)T /l(cid:101)−1
(cid:88)

(k+1)l
(cid:88)

k=0

t=kl+1

(cid:16)

min

1, (cid:107)At(cid:107)2

V −1
t−1

(cid:17)

≤

(cid:100)T /l(cid:101)−1
(cid:88)

(k+1)l
(cid:88)

(cid:18)

min

k=0

t=kl+1

(cid:19)

1, (cid:107)At(cid:107)2

(W (k)

t−1)−1

Furthermore, ∀t ∈ [[kl, (k + 1)l]] we have,

det(W (k)

t

) = det(W (k)
t−1)

(cid:18)

1 + (cid:107)At(cid:107)2

(W (k)

t−1)−1

(cid:19)

.

With positive deﬁnitive matrices whose determinants are strictly positive, this implies that

det(W (k)
(k+1)l)
det(W (k)
kl )

(k+1)l
(cid:89)

=

t=kl+1

)

det(W (k)
t
det(W (k)
t−1)

=

(k+1)l
(cid:89)

t=kl+1

(cid:18)

1 + (cid:107)At(cid:107)2

(W (k)

t−1)−1

By deﬁnition we have W (k)

kl = λId and ∀x ≥ 0, min(1, x) ≤ 2 log(1 + x). So,

(cid:19)

.

(cid:19)

T
(cid:88)

t=1

(cid:16)

min

1, (cid:107)At(cid:107)2

V −1
t−1

(cid:17)

≤ 2

(cid:100)T /l(cid:101)−1
(cid:88)

(k+1)l
(cid:88)

(cid:18)

log

k=0

t=kl+1

22

1 + (cid:107)At(cid:107)2

(W (k)

t−1)−1

(cid:100)T /l(cid:101)−1
(cid:88)

≤ 2

k=0



log



det(W (k)

(k+1)l)
λd



 .

Knowing that W (k)
the proof of Proposition 2),

(k+1)l contains exactly l terms allows us to give the following bound (by following

det(W (k)

(k+1)l) ≤

(cid:18)

λ +

(cid:19)d

.

L2l
d

Finally,

T
(cid:88)

t=1

min

(cid:16)
1, (cid:107)At(cid:107)2

V −1
t−1

(cid:17)

≤ 2d(cid:100)T /l(cid:101) log

1 +

(cid:18)

(cid:19)

.

L2l
λd

With those results we can give a high probability upper bound for the cumulative dynamic regret of
the SW-LinUCB algorithm.
Theorem 3. Assuming that (cid:80)T −1
be bounded for all l > 0, with probability at least 1 − δ, by

s+1(cid:107)2 ≤ BT , the regret of the SW-LinUCB algorithm may

s=1 (cid:107)θ(cid:63)

s − θ(cid:63)

RT ≤ 2LBT l + 2

√

√

2βT

dT (cid:112)(cid:100)T /l(cid:101)

(cid:115)

(cid:18)

log

1 +

(cid:19)
,

L2l
λd

where βT is deﬁned in Equation (13).

Proof.

1rst step: Upper bound for the instantaneous regret

Deﬁning A(cid:63)

t = arg max a∈At(cid:104)a, θ(cid:63)

t (cid:105) and θt = arg max θ∈Ct(cid:104)At, θ(cid:105). We have,
t (cid:105) = (cid:104)A(cid:63)

(cid:104)a, θ(cid:63)

rt = max
a∈At
= (cid:104)A(cid:63)

t (cid:105) − (cid:104)At, θ(cid:63)
t − At, ¯θt(cid:105) + (cid:104)A(cid:63)

t − At, θ(cid:63)

t − At, θ(cid:63)
t (cid:105)
t − ¯θt(cid:105)

Under the event {∀t > 0, ¯θt ∈ Ct}, that occurs with probability at least 1 − δ thanks to Proposition 8,

(cid:104)A(cid:63)

t , ¯θt(cid:105) ≤ arg max

θ∈Ct

(cid:104)A(cid:63)

t , θ(cid:105) = UCBt(A(cid:63)

t ) ≤ UCBt(At) = arg max

(cid:104)At, θ(cid:105) = (cid:104)At, θt(cid:105)

(14)

θ∈Ct

Using Inequality (14), with probability larger than 1 − δ, ∀t > 0,

rt ≤ (cid:104)At, θt − ¯θt(cid:105) + (cid:104)A(cid:63)

t − At, θ(cid:63)
(cid:107)θt − ¯θt(cid:107)Vt−1 + (cid:107)A(cid:63)
(cid:107)θt − ¯θt(cid:107)Vt−1 + 2L(cid:107)θ(cid:63)

t − ¯θt(cid:105)
t − At(cid:107)2(cid:107)θ(cid:63)
t − ¯θt(cid:107)2

≤ (cid:107)At(cid:107)V −1
≤ (cid:107)At(cid:107)V −1

t−1

t−1

(Cauchy-Schwarz)

t − ¯θt(cid:107)2
(Bounded action assumption).

As for the analysis of the regret for the D-LinUCB algorithm, the two terms are upper bounded using
different techniques. The ﬁrst term is handled with the deviation inequality of Proposition 8.
2nd step: Upper bound for (cid:107)θt − ¯θt(cid:107)Vt−1
We have,

(cid:107)θt − ¯θt(cid:107)Vt−1 ≤ (cid:107)θt − ˆθt−1(cid:107)Vt−1 + (cid:107)¯θt − ˆθt−1(cid:107)Vt−1 ≤ 2βt−1.
Where the last inequality holds because under our assumption ¯θt ∈ Ct with probability at least 1 − δ
and by deﬁnition θt ∈ Ct.

3rd step: Upper bound for the bias.

23

This step is similar to the proof proposed in [11] for Lemma 1.





t−1
(cid:88)

s=max(1,t−l)

AsA(cid:62)

s (θ(cid:63)

s − θ(cid:63)
t )





V −1
t−1AsA(cid:62)
s

(θ(cid:63)

p − θ(cid:63)

p+1)

t−1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

V −1
t−1

AsA(cid:62)

s (θ(cid:63)

p − θ(cid:63)

(cid:107)θ(cid:63)

t − ¯θt(cid:107)2 =

≤

≤

≤

≤

(cid:13)
(cid:13)
(cid:13)
V −1
(cid:13)
t−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

t−1
(cid:88)

s=max(1,t−l)

p=max(1,t−l)
(cid:13)
(cid:13)
(cid:13)
V −1
(cid:13)
t−1
(cid:13)
(cid:13)

t−1
(cid:88)

p=max(1,t−l)

t−1
(cid:88)

λmax

p=s

p
(cid:88)

s=max(1,t−l)

p
(cid:88)

s=max(1,t−l)

V −1
t−1

p
(cid:88)

AsA(cid:62)

s (θ(cid:63)

p − θ(cid:63)

(cid:13)
(cid:13)
(cid:13)
p+1)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
(cid:13)
(cid:13)
p+1)
(cid:13)
(cid:13)
(cid:13)2


 (cid:107)θ(cid:63)

AsA(cid:62)
s

p − θ(cid:63)

p+1(cid:107)2.

Furthermore, for x ∈ Rd such that (cid:107)x(cid:107)2 ≤ 1, we have that for max(1, t − l) ≤ p ≤ t − 1,

p=max(1,t−l)

s=max(1,t−l)

x(cid:62)V −1
t−1

p
(cid:88)

s=max(1,t−l)

AsA(cid:62)

s x ≤ x(cid:62)V −1
t−1

t−1
(cid:88)

AsA(cid:62)

s x + λx(cid:62)V −1

t−1x

s=max(1,t−l)


t−1
(cid:88)

≤ x(cid:62)V −1
t−1



AsA(cid:62)

s + λId


 x = x(cid:62)x ≤ 1.

By combining the second and the third step,

s=max(1,t−l)

rt ≤ 2L

t−1
(cid:88)

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 + 2βt−1(cid:107)At(cid:107)V −1

.

t−1

p=max(1,t−l)
By using the assumption ∀a ∈ At, |(cid:104)At, θ(cid:63)
than 1 − δ,

t (cid:105)| ≤ 1, we also have rt ≤ 2. So, with probability greater

rt ≤ 2L

t−1
(cid:88)

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 + 2βt−1 min

To conclude the proof, we use the results of Proposition 9.

p=max(1,t−l)

Final step:

RT =

T
(cid:88)

t=1

T
(cid:88)

t−1
(cid:88)

rt ≤ 2L

t=1

p=max(1,t−l)

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 + 2βT

(cid:16)

1, (cid:107)At(cid:107)V −1

t−1

(cid:17)

.

(15)

(cid:16)

min

T
(cid:88)

t=1

(cid:17)

1, (cid:107)At(cid:107)V −1

t−1

T
(cid:88)

t−1
(cid:88)

≤ 2L

(cid:107)θ(cid:63)

p − θ(cid:63)

p+1(cid:107)2 + 2βT

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

√

T

(cid:18)

min

t=1

p=max(1,t−l)

≤ 2LBT l + 2

√

√

2βT

dT (cid:112)(cid:100)T /l(cid:101)

(cid:115)

(cid:18)

log

1 +

t=1

(cid:19)

.

lL2
λd

(cid:19)

1, (cid:107)At(cid:107)2

V −1
t−1

In the ﬁrst inequality, we use the fact that t (cid:55)→ βt is increasing. The second inequality is an
application of the Cauchy-Schwarz inequality to the second term. The last inequality is an application
of Proposition 9

By denoting ˜O the function growth when omitting the logarithmic terms, we have the following
Corollary.

24

Corollary 5 (Asymptotic regret bound for SW-LinUCB). If BT is known, by choosing l = ( dT
)2/3,
BT
the regret of the SW-LinUCB algorithm is asymptotically upper bounded with high probability by a
term ˜O(d2/3B1/3

T T 2/3) when T → ∞.

If BT is unknown, by choosing l = d2/3T 2/3, the regret of the SW-LinUCB algorithm is asymptotically
upper bounded with high probability by a term ˜O(d2/3BT T 2/3) when T → ∞.

Proof. With this particular choice of l, we have:

lBT ∼ d2/3T 2/3B1/3
T .
βT as deﬁned by equation (13) is equivalent to (cid:112)d log(T ).
√

T (cid:112)(cid:100)T /l(cid:101) has a similar behavior than d−1/3T 1−1/3B1/3
√

(cid:113)

βT

dT (cid:112)(cid:100)T /l(cid:101)

log (cid:0)1 + lL2

λd

(cid:1) is similar to d2/3B1/3

T T 2/3(cid:112)log(T )(cid:112)log(T /BT ).

T , consequently the behavior of

By neglecting the logarithmic term, we have with high probability,

RT = ˜OT →∞(d2/3B1/3

T T 2/3).

25

