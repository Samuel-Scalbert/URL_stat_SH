On the link between emotion, attention and content in
virtual immersive environments
Quentin Guimard, Florent Robert, Camille Bauce, Aldric Ducreux, Lucile

Sassatelli, Hui-Yin Wu, Marco Winckler, Auriane Gros

To cite this version:

Quentin Guimard, Florent Robert, Camille Bauce, Aldric Ducreux, Lucile Sassatelli, et al..
On the link between emotion, attention and content in virtual
ICIP
2022 -
IEEE International Conference on Image Processing, Oct 2022, Bordeaux, France.
￿10.1109/ICIP46576.2022.9897903￿. ￿hal-03825059￿

immersive environments.

HAL Id: hal-03825059

https://hal.science/hal-03825059

Submitted on 21 Oct 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

ON THE LINK BETWEEN EMOTION, ATTENTION AND CONTENT IN VIRTUAL
IMMERSIVE ENVIRONMENTS

Quentin Guimard(cid:63), Florent Robert(cid:63)†, Camille Bauce(cid:63), Aldric Ducreux(cid:63), Lucile Sassatelli(cid:63)§,
Hui-Yin Wu†, Marco Winckler(cid:63)†, Auriane Gros‡

(cid:63)Universit´e Cˆote d’Azur, CNRS, I3S; †Universit´e Cˆote d’Azur, Inria;
‡Universit´e Cˆote d’Azur, CHU de Nice, CoBTeK; §Institut Universitaire de France

ABSTRACT

While immersive media have been shown to generate more
intense emotions, saliency information has been shown to be
a key component for the assessment of their quality, owing to
the various portions of the sphere (viewports) a user can at-
tend. In this article, we investigate the tri-partite connection
between user attention, user emotion and visual content in
immersive environments. To do so, we present a new dataset
enabling the analysis of different types of saliency, both low-
level and high-level, in connection with the user’s state in
360◦ videos. Head and gaze movements are recorded along
with self-reports and continuous physiological measurements
of emotions. We then study how the accuracy of saliency es-
timators in predicting user attention depends on user-reported
and physiologically-sensed emotional perceptions. Our re-
sults show that high-level saliency better predicts user atten-
tion for higher levels of arousal. We discuss how this work
serves as a ﬁrst step to understand and predict user attention
and intents in immersive interactive environments.

Index Terms— 360◦ videos, saliency maps, emotions,

physiological signals, gaze

1. INTRODUCTION

Immersive media and environments are on the rise with in-
creased affordability of virtual reality (VR) equipment and
the deployment of popular platforms for 360◦ streaming or
advanced interaction in the Metaverse1. This new type of
content questions the design of compelling immersive experi-
ences, as well as the technical choices for storage and distri-
bution. Visual quality assessment by humans is key to enable
efﬁcient storage and distribution of content with high percep-
tual quality [1]. To apply quality assessment-driven processes

This work has been partly supported by the French government, through
the UCA JEDI and EUR DS4H Investments in the Future projects ANR-
15-IDEX-0001 and ANR-17-EURE-0004. This work was partly supported
by EU Horizon 2020 project AI4Media, under contract no. 951911 (https:
//ai4media.eu/).

1https://www.cnbc.com/2021/12/27/metas-oculus-virtual-reality-

headsets-were-a-popular-holiday-gift.html

(such as compression, streaming decisions, etc.) to any video
content, it is crucial to automate quality assessment with qual-
ity estimators associated to the content. For immersive media
speciﬁcally, quality assessment depends on the sequence of
viewports attended by the human rater, which can be signiﬁ-
cantly different from one rater to the other. For example, Xu
et al. [2] consider content saliency and viewport preference to
extend the PSNR and SSIM metrics to 360◦ content. The ac-
curacy of quality estimators for immersive content therefore
strongly depends on the accuracy of content-based saliency
estimators predicting patterns of human attention. Saliency
is hence a key component of quality assessment of immer-
sive media. On the other hand, immersive content have been
shown to elicit more intense emotions compared with regular
ﬂat-screen presentations [3, 4, 5].

In this article, we investigate how emotions are associated
with head motion to impact the accuracy of content-based
saliency estimators. To the best of our knowledge, this is the
ﬁrst article to investigate the tri-partite connection between
user attention, user emotion and visual content in immersive
environments. Our contributions are:
• A new dataset from user experiments recording head and
gaze movements, along with self-reports and continuous
physiological measurements of emotions. The stimuli are
360◦ videos selected to enable the analysis of different types
of saliency in connection with the user’s state. We verify the
consistency of our results. The dataset is publicly available2.
• An investigation into how emotions affect the accuracy
of saliency estimators, both with low-level and high-level
saliency, in 360◦ videos. Our results show that high-level
saliency better predicts user attention for higher levels of
arousal.

2. RELATED WORK

Sensing and analyzing emotions in immersive environments
has spurred interest (see, e.g., [3, 4, 5, 6]), more recently cou-
pled with motion recordings and analysis [7, 8, 9, 10, 11].

Human emotions are commonly decomposed along two
main dimensions: valence, representing the negative or posi-

tive nature of an emotion (unpleasant-pleasant), and arousal,
representing the intensity of the perceived emotion (calm-
excited) [12]. The ﬁrst reference database [7] providing emo-
tional ratings and motion recordings of 360◦ videos is made
of 73 VR videos on which 95 users rated valence and arousal
using the self-assessment manikin (SAM) tool [13] after ex-
periencing each video. Their head positions were continu-
ously recorded. A dataset of self-reported emotions of 19
users watching thirty-six 360◦ images is collected by Tang
et al. [8], with eye motion recorded. However, ratings made
in retrospect cannot represent the variety of states a user goes
through during the experience [6], limiting potential analyses
and interpretations. Recent works have therefore proposed
tools enabling a continuous collection of self-reports inside
the immersive environment [9, 10]. The data collected in
these recent works also comprise physiological measurements
of heart rate and electrodermal activity (EDA, as skin conduc-
tance), which has been shown to reliably represent user in-
stantaneous arousal [14]. Understanding how different types
and levels of emotions correspond to speciﬁc types of motion
has already been investigated [7, 8, 11]. Results from Li et
al. [7] show some level of correlation between (time) average
arousal and average pitch angle, and between yaw angle stan-
dard deviation and valence, while results from Tang et al. [8]
show a signiﬁcant impact of negative images on eye behavior.
While above works have focused on the analysis of user
emotion and motion based on coarse-grained categorization
of the entire content (high/low positive/negative valence and
high/low arousal), other works have focused on the impact of
speciﬁc regions on the user’s attention, described with low-
level (LL) saliency or emotional aspects. LL saliency refers
to pixel-level features (e.g., edges, luminance, motion). Cerf
et al. [15] showed that human eye movements are inﬂuenced
both by LL and high-level (HL) saliency (related to higher
semantic concepts such as objects and faces). Chaabouni et
al. [16] showed that normalizing ﬁxations density with LL
saliency signiﬁcantly improves the interest estimators based
on gaze data. Hedger et al. [17] re-examined previous results
suggesting that emotional faces in an image attract more user
attention/ﬁxations outside awareness. They showed that facial
expressions had no effect on attentional allocation, which can
instead be explained by the higher LL saliency.

In this article, we present a ﬁrst step towards understand-
ing the connection between user emotion and predictability
of motion from content saliency. Speciﬁcally,we analyze how
the accuracy of LL and HL saliency estimators depend on
the user’s self-reported and physiologically-sensed emotional
perceptions.

3. MATERIAL AND METHODS

Dataset We present the dataset we have collected to ana-
lyze saliency accuracy in Sec. 4. This dataset is publicly

available2. It is composed of user head and eye movements
recorded while watching 360◦ videos in a VR headset, along
with users’ EDA and heart rate (HR) during viewing, and
valence and arousal ratings collected at the end of every clip.
The user experiment has been approved by the university
ethics committee.
Stimuli In order to investigate the differences in accuracy
of LL and HL saliencies depending on user emotions, we
select seven videos from the database provided by Li et al.
[7] that meet two criteria. First, the videos must span an as
broad as possible range of (valence, arousal). Video details
are provided here2. Second, HL and LL saliencies must not
always overlap. LL and HL saliencies are computed on 100
“patches” made of overlapping projections centered on points
uniformly sampled from each frame of the video. LL saliency
is computed using the Itti model [18] combined with optical
ﬂow between consecutive frames on these individual patches.
[19], HL saliency is obtained
Inspiring from Chopra et al.
from YOLOv4 object detector on these same patches, object
bounding boxes are used as binary saliency maps. These
patches are then back-projected on the equirectangular frame,
by addition of the overlapping patches. To select videos
where the HL and LL saliencies do not overlap systemati-
cally but are rather balanced inside and outside objects, we
compare (i) the number of pixels inside and outside objects,
and (ii) the per-pixel LL saliency (ranging between 0 and
255), computed as the total LL saliency inside and outside
objects normalized with the corresponding number of pixels.
Fig. 1 exempliﬁes that in video 12. The number of pixels
with such minimum LL saliency inside and outside objects is
equivalent over time, as is the per-pixel LL saliency in both
areas. Fig. 2 shows a frame where regions with high LL
saliency can be seen outside the detected objects.

s
l
e
x
i
p

f
o

r
e
b
m
u
N

1.5

1

0.5

0

·105

Inside object(s)
Outside object(s)

0

20 40 60 80 100 120

Video timestamp (sec.)

l
e
x
i
p

r
e
p

y
c
n
e
i
l
a
s
L
L

180

160

140

120

100

Inside object(s)
Outside object(s)

0

20 40 60 80 100 120

Video timestamp (sec.)

Fig. 1: HL and LL saliency characterization of Video 12.
Left: number of pixels inside and outside objects. Right: av-
erage LL saliency per pixel inside and outside objects.

Methodology Recordings of head and eye movements have
been made with a FOVE headset, equipped with an eye-
tracker with a 120Hz acquisition rate, and tethered to a
desktop computer. The video is played in an application

2https://gitlab.com/PEM360/PEM360

Fig. 2: HL and LL saliency visualization for frame 1545 of
video 12. Left:
the frame. Center: HL saliency (detected
objects are elephants). Right: LL saliency.

developed in Unity with the FOVE SDK. Recordings of EDA
and optical pulse have been made with a Shimmer3 GSR+
unit with a frequency range of 15.9Hz. All of the mea-
surements were resampled to 100Hz for analysis. The lab
experiment involved 31 users (10f, 20m, 1nb; 18-29 years
old, M=24, SD=3.26). First, participants were presented a
pre-questionnaire to assess their background with VR and
checking for visual deﬁciencies. Next, the VR experiment
systematically started with a low-arousal (relaxing) video
(ID 32) to bring EDA and HR levels to a user-relative base-
line. Then, the remaining six VR videos were experienced
in a random order by every user. Finally, after each video,
the headset was taken off and the SAM scale presented for
arousal and valence rating. An at least 1-min break outside
the headset was observed between every video. All the videos
were played without sound.
Preliminary analysis After visual inspection to remove er-
roneous EDA data, the phasic component is extracted using
cvxEDA as implemented by Neurokit [20]. The skin conduc-
tance response (SCR) is ﬁnally obtained as the absolute value
of the ﬁrst derivative of the phasic component [10]. The reli-
ability of arousal and valence ratings is assessed by the intra-
class correlation coefﬁcient (ICC), where a class of measure-
ments corresponds to a stimulus (360◦ video). ICC estimates
based on mean ratings with a two-way mixed effects model
are 0.96 (95% CI 0.87-0.99) for arousal and 0.88 (95% CI
0.72-0.98) for valence. The median root square difference of
valence-arousal ratings with the corresponding values avail-
able in the original dataset [7] is 1.17 (range: 1-9), showing
the agreement between both. Finally, we look at the corre-
spondence between SCR and arousal ratings. Similar to Toet
et al. [9], for each video, we compute the mean (resp. median)
of SCR across users. The results show that the video rankings
according to mean arousal and to mean SCR are very close
(not shown here due to space limitation).

4. ANALYSIS

Our objective is to compare the accuracy of both types of
saliency maps, HL and LL, to match the users’ ﬁxations over
every frame of the 360◦ video. To do so, we compute the
normalized scanpath saliency (NSS), which measures the
amount of saliency around ﬁxations [21]. We consider seg-
to average the saliency maps of all frames
ments of 5 sec.

and aggregate the user’s ﬁxations in this interval, hence ob-
taining an NSS value for both saliency types NSSHL
u,v,i and
NSSLL
u,v,i for every user u, video v, and interval i. The av-
erages over intervals (resp. users) are denoted by NSSu,v
and NSSv, respectively. We analyze the association be-
tween NSSHL
u,v with mean centered SCR denoted
cSCRu,v and graded arousal GAu,v. SCR is centered per
user with cSCRu,v = SCRu,v − Ev[SCRu,v] because the
preliminary analysis has shown that the absolute levels of
SCR vary signiﬁcantly across users, but intra-user variations
across videos are consistent with the ordering of each user’s
arousal ratings.

u,v and NSSLL

f
f
i
D
S
S
N

2

1

0

−1

−2

−1 −0.5

0
Centered SCR

0.5 ·10−3

12
13
17
23
27
32
73

10

0

2

4

6
Graded arousal

8

Fig. 3: NSSDif f
against cSCRu,v and GAu,v for every user u
and video v. The black line shows a linear regression model
ﬁtted on the data.

u,v

u,v −NSSLL

u,v = NSSHL

To analyze the difference in accuracy of both types of
saliency depending on the user’s arousal, we consider in Fig.
3 the difference NSSDif f
u,v plotted against
cSCRu,v (left) and graded arousal GAu,v (right) for all u, v,
the points being colored per video. The major ﬁnding is the
increasing trend of NSSDif f with EDA and graded arousal.
Speciﬁcally, the PCC between NSSDif f and EDA cSCR is
0.25 (p < 10−3), and the PCC between NSSDif f and graded
arousal GAu,v is 0.41 (p < 10−9). These estimates are ob-
tained over 217 (u, v) samples. According to Walline [22,
Appendix 6C, page 79], such levels of correlation are signiﬁ-
cant for 123 and 44 samples, respectively (see [23]).

We then analyze the same associations averaged per video
in Fig. 4, where the x-axis of the ﬁrst row is cSCRv and that of
the second row is GAv, with v in the set of video indices. The
columns are numbered from the left. We ﬁrst conﬁrm from
the leftmost column that ordering and appearance of NSSDif f
against EDA or graded arousal are close. Second, we observe
a clear increasing trend conﬁrming the above positive signiﬁ-
cant correlation results.

v

To investigate the reasons for this trend, we decompose
NSSDif f
and NSSLL
into its individual components NSSHL
v
depicted in columns 2 and 3. Owing to the similarity of trends
against EDA and graded arousal, we conduct the analysis only
on the latter. We ﬁrst observe an increasing trend of NSSHL
.
It could have been even clearer considering that underwater
objects in video 12 (brown dot) are often missed by the ob-

v

v

v

1

0.5

0

−0.5

−1

f
f
i
D
S
S
N

12
13
17
23
27
32
73

−1 −0.5

0

Centered SCR

1

0.5

0

−0.5

−1

f
f
i
D
S
S
N

0.5

1
·10−4

−1 −0.5

0

Centered SCR

0.5

1
·10−4

L
H
S
S
N

L
H
S
S
N

1.5

1

0.5

0

1.5

1

0.5

0

L
L
S
S
N

L
L
S
S
N

1.4

1.2

1

0.8

0.6

1.4

1.2

1

0.8

0.6

−1 −0.5

0

Centered SCR

0.5

1
·10−4

e
z
i
s

)
s
(
t
c
e
j
b
o

e
g
a
r
e
v
A

e
z
i
s

)
s
(
t
c
e
j
b
o

e
g
a
r
e
v
A

3

2

1

3

2

1

·104

−1 −0.5

0

Centered SCR

·104

0.5

1
·10−4

3

4
5
Graded arousal

6

3

5
4
Graded arousal

6

3

4
5
Graded arousal

6

3

5
4
Graded arousal

6

Fig. 4: From left to right: NSSDif f
GAv (bottom) for all videos v.

v

, NSSHL

v

, NSSLL

v

and average number of pixels inside objects against cSCRv (top) and

ject detector (large shark), hence under-estimating NSSHL
12 .
We can then question whether this increase is due to users
focusing more when more aroused, or to intrinsic features
of the videos, where larger objects would appear in higher
arousal videos. We verify in the last column that the increase
in NSSHL with arousal cannot be entirely attributed to a rel-
atively larger area occupied by objects. Second, column 3
shows no clear trend. The variation in NSSLL does not ap-
pear to be related to EDA or graded arousal.

We can therefore conclude that the increasing trend of
NSSDif f with arousal is mainly due to higher NSSHL for
higher-arousal videos. A ﬁrst conclusion we may draw is that
the relative weight of HL saliency should vary in a saliency
model depending on the user’s arousal state. Sensing the
user’s state may hence help predict their attention.

5. DISCUSSION

We cannot claim causation on whether users focus because
they are more aroused by the content, or if they are more
aroused because they focus on objects. A ﬁrst question is
whether signiﬁcantly different levels of arousal occur for
users on the same video. This is part of future work. This
would mean that the video content alone is not informative
enough to adapt the relative weights of HL and LL saliency
to users. On the contrary, if the video content is sufﬁcient,
then one can think of leveraging arousal (physiological or
subjective) measurements in quality assessment sessions to
serve as an auxiliary loss to train (deep) saliency models.

While arousal and valence are major dimensions to de-
scribe user emotion of a given content like a video, the richer
experience of an immersive and possibly interactive environ-
ment is described over various additional dimensions, partic-

ularly presence, immersion, agency, engagement, ﬂow, us-
ability, skill or judgement [24]. Recently, valence, arousal
and agency have been shown to interact in non-trivial ways to
produce presence [25]. In 6DoF environments, which we are
currently investigating for rehabilitation scenarios and where
engagement, skills and judgments are major outcomes, it is
crucial to adapt the environment’s content to provide proper
adaptive guidance to the user. This requires an understanding
and the prediction of the user’s attention and intents, which
depend on the user’s emotional state. This work in 3DoF im-
mersive low-interaction environment hence serves as a base-
line for immersive interactive environments.

6. CONCLUSION

In this article, we have ﬁrst introduced a new dataset of user
head and gaze movements in 360◦ videos with valence and
arousal ratings, and continuous physiological measurements
of skin conductance and heart rate. The stimuli have been
speciﬁcally selected to enable a spatio-temporal analysis in
relation to content, user motion and emotions. We have pre-
sented ﬁrst results comparing HL and LL saliency accuracy
depending on user arousal, showing that the accuracy of HL
saliency increases when user arousal increases.

Next steps will consist

in investigating ﬁner tempo-
ral associations of saliency and attention locations with
arousal/EDA, as well as structural modeling of possible inter-
acting factors (e.g., valence, fear, agency) in the production
of head and gaze patterns. Also, the accuracy of more reﬁned
saliency models such as deep neural networks explicitly or
implicitly combining saliency levels will be assessed to bet-
ter understand motion predictability depending on the user’s
emotional state.

7. REFERENCES

[1] Kjell Brunnstr¨om, Sergio Ariel Beker, Katrien De Moor, Ann Dooms,
Sebastian Egger, Marie-Neige Garcia, Tobias Hossfeld, Satu Jumisko-
Pyykk¨o, Christian Keimel, Mohamed-Chaker Larabi, Bob Lawlor,
Patrick Le Callet, Sebastian M¨oller, Fernando Pereira, Manuela
Pereira, Andrew Perkis, Jesenka Pibernik, Antonio Pinheiro, Alexander
Raake, Peter Reichl, Ulrich Reiter, Raimund Schatz, Peter Schelkens,
Lea Skorin-Kapov, Dominik Strohmeier, Christian Timmerer, Martin
Varela, Ina Wechsung, Junyong You, and Andrej Zgank, “Qualinet
White Paper on Deﬁnitions of Quality of Experience,” Mar. 2013, Qua-
linet White Paper on Deﬁnitions of Quality of Experience Output from
the ﬁfth Qualinet meeting, Novi Sad, March 12, 2013.

[2] Mai Xu, Chen Li, Zhenzhong Chen, Zulin Wang, and Zhenyu Guan,
“Assessing visual quality of omnidirectional videos,” IEEE Transac-
tions on Circuits and Systems for Video Technology, vol. 29, no. 12, pp.
3516–3530, 2019.

[3] Rosa Mar´ıa Ba˜nos, Cristina Botella, Isabel Rubi´o, Soledad Quero, Azu-
cena Garc´ıa-Palacios, and Mariano Luis Alca˜niz Raya, “Presence and
emotions in virtual environments: The inﬂuence of stereoscopy,” Cy-
berpsychology & behavior : the impact of the Internet, multimedia and
virtual reality on behavior and society, vol. 11 1, pp. 1–8, 2008.

[4] Anna Felnhofer, Oswald D. Kothgassner, Mareike Schmidt, Anna-
Katharina Heinzle, Leon Beutl, Helmut Hlavacs, and Ilse Kryspin-
Exner, “Is virtual reality emotionally arousing? investigating ﬁve emo-
tion inducing virtual park scenarios,” Int. J. Hum.-Comput. Stud., vol.
82, no. C, pp. 48–56, oct 2015.

[5] Federica Pallavicini, Alessandro Pepe, and Maria Eleonora Minissi,
“Gaming in virtual reality: What changes in terms of usability, emo-
tional response and sense of presence compared to non-immersive
video games?,” Simulation & Gaming, vol. 50, no. 2, pp. 136–159,
2019.

[6] Jan-Niklas Voigt-Antons, Eero Lehtonen, Andres Pinilla Palacios, Dan-
ish Ali, Tanja Kojic, and Sebastian M¨oller, “Comparing Emotional
States Induced by 360◦ Videos Via Head-Mounted Display and Com-
puter Screen,” in 2020 Twelfth International Conference on Quality of
Multimedia Experience (QoMEX), 2020, pp. 1–6.

[7] Benjamin J. Li, Jeremy N. Bailenson, Adam Pines, Walter J. Green-
leaf, and Leanne M. Williams, “A Public Database of Immersive VR
Videos with Corresponding Ratings of Arousal, Valence, and Correla-
tions between Head Movements and Self Report Measures,” Frontiers
in Psychology, vol. 8, pp. 2116, Dec. 2017.

[13] Margaret M. Bradley and Peter J. Lang, “Measuring emotion: The self-
assessment manikin and the semantic differential,” Journal of Behavior
Therapy and Experimental Psychiatry, vol. 25, no. 1, pp. 49–59, 1994.

[14] Wolfram Boucsein, Electrodermal activity, 2nd ed., Electrodermal
activity, 2nd ed. Springer Science + Business Media, New York, NY,
US, 2012, Pages: xviii, 618.

[15] Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, and Christof Koch,
“Predicting human gaze using low-level saliency combined with face
detection,” in Advances in Neural Information Processing Systems,
J. Platt, D. Koller, Y. Singer, and S. Roweis, Eds. 2007, vol. 20, Curran
Associates, Inc.

[16] Souad Chaabouni and Frederic Precioso, “Impact of Saliency and Gaze
Features on Visual Control: Gaze-Saliency Interest Estimator,” in Pro-
ceedings of the 27th ACM International Conference on Multimedia,
Nice France, Oct. 2019, pp. 1367–1374, ACM.

[17] Nicholas Hedger, Matthew Garner, and Wendy J. Adams, “Do emo-
tional faces capture attention, and does this depend on awareness? Evi-
dence from the visual probe paradigm.,” Journal of Experimental Psy-
chology: Human Perception and Performance, vol. 45, no. 6, pp. 790–
802, June 2019.

[18] Laurent Itti, Christof Koch, and Ernst Niebur, “A model of saliency-
based visual attention for rapid scene analysis,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 20, no. 11, pp. 1254–
1259, 1998.

[19] Lovish Chopra, Sarthak Chakraborty, Abhijit Mondal, and Sandip
Chakraborty,
“PARIMA: Viewport Adaptive 360-Degree Video
Streaming,” in Proceedings of the Web Conference 2021. 2021, pp.
2379–2391, ACM.

[20] Dominique Makowski, Tam Pham, Zen J. Lau, Jan C. Bram-
mer, Franc¸ois Lespinasse, Hung Pham, Christopher Sch¨olzel, and
S. H. Annabel Chen, “NeuroKit2: A python toolbox for neurophys-
iological signal processing,” Behavior Research Methods, vol. 53, no.
4, pp. 1689–1696, feb 2021.

[21] Olivier Le Meur and Thierry Baccino, “Methods for comparing scan-
paths and saliency maps: strengths and weaknesses,” Behavior Re-
search Methods, vol. 45, no. 1, pp. 251–266, Mar. 2013.

[22] Jeffrey J. Walline, “Designing Clinical Research: an Epidemiologic
Approach, 2nd Ed.,” Optometry and Vision Science, vol. 78, no. 8,
2001.

[23] UCSF, “Sample size calculators for designing clinical research,” https:

//sample-size.net/correlation-sample-size/, 2021.

[8] Wei Tang, Shiyi Wu, Toinon Vigier, and Matthieu Perreira Da Silva,
“Inﬂuence of Emotions on Eye Behavior in Omnidirectional Content,”
in 2020 Twelfth International Conference on Quality of Multimedia Ex-
perience (QoMEX), Athlone, Ireland, May 2020, pp. 1–6, IEEE.

[24] Katy Tcha-Tokey, Olivier Christmann, Emilie Loup-Escande, and Si-
mon Richir, “Proposition and Validation of a Questionnaire to Measure
the User Experience in Immersive Virtual Environments,” International
Journal of Virtual Reality, vol. 16, no. 1, pp. 33–48, Jan. 2016.

[25] Crescent Jicol, Chun Hin Wan, Benjamin Doling, Caitlin H Illingworth,
Jinha Yoon, Charlotte Headey, Christof Lutteroth, Michael J Proulx,
Karin Petrini, and Eamonn O’Neill, “Effects of Emotion and Agency
on Presence in Virtual Reality,” in Proceedings of the 2021 CHI Con-
ference on Human Factors in Computing Systems, Yokohama Japan,
May 2021, pp. 1–13, ACM.

[9] Alexander Toet, Fabienne Heijn, Anne-Marie Brouwer, Tina Mioch,
and Jan B. F. van Erp, “An Immersive Self-Report Tool for the Affective
Appraisal of 360◦ VR Videos,” Frontiers in Virtual Reality, vol. 1, pp.
552587, Sept. 2020.

[10] Tong Xue, Abdallah El Ali, Tianyi Zhang, Gangyi Ding, and Pablo
Cesar, “CEAP-360VR: A Continuous Physiological and Behavioral
Emotion Annotation Dataset for 360 VR Videos,” IEEE Transactions
on Multimedia, pp. 1–1, 2021.

[11] Tong Xue, Abdallah El Ali, Gangyi Ding, and Pablo Cesar, “Investi-
gating the Relationship between Momentary Emotion Self-reports and
Head and Eye Movements in HMD-based 360◦ VR Video Watching,”
in Extended Abstracts of the 2021 CHI Conference on Human Factors
in Computing Systems, Yokohama Japan, May 2021, pp. 1–8, ACM.

[12] Lisa Feldman Barrett, “Discrete emotions or dimensions? The role of
valence focus and arousal focus.,” Cognition and Emotion, vol. 12, no.
4, pp. 579–599, 1998, Place: United Kingdom Publisher: Taylor &
Francis.

