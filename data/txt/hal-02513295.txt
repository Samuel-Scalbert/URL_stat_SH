Localized Random Shapelets
Maël Guilleme, Simon Malinowski, Romain Tavenard, Xavier Renard

To cite this version:

Maël Guilleme, Simon Malinowski, Romain Tavenard, Xavier Renard. Localized Random Shapelets.
International Workshop on Advanced Analysis and Learning on Temporal Data, 2019, Wurzburg,
Germany. pp.85-97, ￿10.1007/978-3-030-39098-3_7￿. ￿hal-02513295￿

HAL Id: hal-02513295

https://hal.science/hal-02513295

Submitted on 18 May 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Localized Random Shapelets

Mael Guillem´e1,2, Simon Malinowski2, Romain Tavenard3, and Xavier Renard4

1 Energiency
2 Univ Rennes, Inria, CNRS, IRISA
forname.lastname@irisa.fr
3 Univ Rennes, CNRS, LETG, IRISA
4 AXA

Abstract. Shapelet models have attracted a lot of attention from re-
searchers in the time series community, due in particular to its good
classiﬁcation performance. However, such models only inform about the
presence / absence of local temporal patterns. Structural information
about the localization of these patterns is ignored. In addition, end-
to-end learning shapelet models tend to generate meaningless shapelets,
leading to poorly interpretable models. In this paper, we aim at designing
an interpretable shapelet model that takes into account the localization
of the shapelets in the time series. Time series are transformed into fea-
ture vectors composed of both a distance and a localization information.
Then, we design a hierarchical feature selection process using regular-
ization. This process can be tuned to select, for each shapelet, either
only its distance information or both distance and localization informa-
tion. It is hence possible for every selected shapelet to analyze whether
only the presence or the presence and the localization contributed to the
decision process improving interpretability of the decision. Experiments
show that this feature selection process has competitive performance
compared to state-of-the-art shapelet-based classiﬁers, while providing
better interpretability.

Keywords: time series · machine learning · shapelets

1

Introduction

Time series classiﬁcation has recently gained an increasing attention from re-
searchers, due in particular to its possible application in various domains such
as economics, agriculture, and health for instance. There are two main fami-
lies of methods for that task: some deal with raw time series and use or design
dedicated (dis-)similarity measures, while others rely on feature extraction to
embed time series in metric spaces in which standard machine learning tools can
be considered. The work presented in this paper is part of this latter category.
Feature-based methods include works relying on hand-crafted features [3,12] as
well as learning-based approaches, among which the shapelet model plays an
important role. Shapelets have ﬁrst been introduced in [19]. They correspond
to subsequences that are able to discriminate classes. The concept of shapelet

2

M. Guillem´e et al.

(a) Learning Time-Series Shapelets (LS)

(b) Localized Random Shapelets (LRS)

Fig. 1: Comparison between shapelets extracted by the Learning Time-Series
Shapelets (LS) algorithm and our Localized Random Shapelets (LRS) approach.
This Figure has been generated using tslearn implementation of LS [14].

transform has then been proposed in [7]. It consists in transforming time se-
ries into a vector whose components represent the similarity between the time
series and shapelets that have been selected beforehand (or learned, as in [5]).
After this transformation, time series are embedded in a Euclidean space where
classiﬁers like Multi-Layer Perceptron or Support Vector Machines can be used.
Techniques based on the shapelet transform are amongst the most accurate ones
for time series classiﬁcation (an interesting survey and comparison of time se-
ries classiﬁcation methods can be found in [1]). However, they have three main
drawbacks. First, the step of generating (or learning) shapelets that lead to ac-
curate classiﬁcation is computationally demanding (especially when dealing with
large time series datasets). Some works have been proposed in order to fasten
this step, by relying on time series approximation [8] or by drawing random
shapelets [18,10]. Second, no information about the localization of the shapelets
in the time series is available after the transformation. Classical shapelet trans-
form only makes use of the similarity between a shapelet and a time series.
Information about when the shapelet occurs in the time series might be of im-
portance to discriminate classes. This can be related to previous works that have
shown that localization of extracted features in time series improve classiﬁca-
tion accuracy [15]. To better understand why, let us consider, for example, a
sign language sentence recognition task. In this use case, being able to recognize
salient patterns (gesture atoms) is key, but localizing them in the sentence is

0101Localized Random Shapelets

3

also important in order to understand the meaning of the sentence. Third, most
shapelet-based models suﬀer from a lack of interpretability in the sense that (i)
it is diﬃcult to understand the impact of each shapelet on the ﬁnal classiﬁcation
decision and/or (ii) extracted shapelets might not always be related to original
time series. Figure 1 compares shapelets learned by the Learning Shapelet (LS)
algorithm [5] on the EarthQuakes dataset and those extracted (at random) by
our algorithm. Time series are rescaled to lie in the [0, 1] range before learning
the shapelets. However, we can see that LS-extracted shapelets have very little
in common with the time series and do not even ﬁt in the [0, 1] range. In other
words, shapelets cannot be seen as realistic time series patterns. For methods
based on the shapelet transform [7], shapelets are extracted from original time
series. But they then feed an ensemble of classiﬁers, which makes it diﬃcult to
understand relationship between shapelets and class belongings.

In this paper, we propose a novel shapelet model that tackles these draw-
backs. First, in order to reduce the computing cost of selecting the most discrim-
inant shapelets, our model selects shapelets randomly from training data. Hence,
extracted shapelets are, by deﬁnition, closely related to the time series. Moreover,
this random shapelet framework allows us to easily take the shapelet localiza-
tion information into account. Second, we propose a dedicated feature selection
method called Semi-Sparse Group Lasso (SSGL) capable of either ignoring a
shapelet, keeping only its distance information or using both the distance and
localization information. Third, we show that obtained shapelets are meaning-
ful and the resulting model can be easily analyzed to get insights about which
are the important features (for the classiﬁcation task) in the dataset and for
each of these features, what kind of information (presence only or presence and
localization) contributed to the decision. Overall, we show that we are able to
reach competitive performance w.r.t. Learning Shapelets [5] (even outperform-
ing this baseline when larger datasets are considered) with a more interpretable
model. The rest of this paper is organized as follows. Our Localized Random
Shapelet model is detailed in Section 2 and its interpretability is discussed in
Section 3. Section 4 evaluates the beneﬁt of the proposed model on time series
classiﬁcation.

2 Localized Random Shapelet Model

2.1 Background on shapelets and shapelet transform

A shapelet S = s1, . . . , sl is a temporal sequence (that can be extracted from
existing time series or not). Given a time series T = t1, . . . , tL, the distance
between s and T is deﬁned as :

d(T, S) =

min
1≤j≤L−l+1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

l
(cid:88)

i=1

(si

ti+j−1)2.

−

(1)

In other words, euclidean distances between s and every subsequence of T (of
length l) are computed and only the best match (minimum distance) is kept.

4

M. Guillem´e et al.

}

{

S

=

S1, . . . , SK

Given a set
of K shapelets, the shapelet transform of T is
deﬁned as the vector v1, . . . , vK such that vk = d(T, Sk) for all k. The original
way to select a set of shapelets for a classiﬁcation task is to evaluate the dis-
criminatory power of a shapelet candidate by using for instance the information
gain [19,7], and to keep the ones with the higher gain. A strategy based on learn-
ing the shapelets that minimize an objective function was proposed in [5]. Other
works consider the use of random shapelets and then rely on classical feature
selection algorithms together with the classiﬁcation step. In [10], it has been
shown that with this idea, a few thousands subsequences are enough to reach
state-of-the-art classiﬁcation performance on a standard benchmark [2].

2.2 Localized Random Shapelet model

In this framework, no information about the localization of the shapelets in the
time series is used, while it has been shown that this kind of information helps
improving classiﬁcation performance [15]. In this section, we explain how we can
integrate such information in the shapelet transform framework and derive a
feature selection algorithm that keeps localization information only when needed.
In our Localized Random Shapelet (LRS) model, each shapelet S is drawn
uniformly at random from the set of all training time series snippets. Each
shapelet leads to two features for each time series T . The ﬁrst feature is the
same as in the classical shapelet transform, i.e. the shapelet distance1 d(T, S)
between T and s as deﬁned in Equation (1). The second feature corresponds to
the ﬁrst time instant at which this distance is reached. It is computed as

l(T, S) = argmin

(cid:118)
(cid:117)
(cid:117)
(cid:116)

l
(cid:88)

1≤j≤L−l+1

i=1

(si

−

ti+j−1)2.

(2)

= T1, . . . , TN be a dataset of N time series. This set is transformed by

Let

T

the localized random shapelet model into a feature matrix X, such that:






X =

d(T1, S1) l(T1, S1)

...

...

d(TN , S1) l(TN , S1)

d(T1, SK) l(T1, SK)

...

...

d(TN , SK) l(TN , SK)




 .

· · ·

· · ·

(3)

Once this feature matrix computed, we feed it to a standard multi-layer percep-
K-dimensional
tron classiﬁer as shown in Figure 2. This model takes as input a 2
vector and outputs probabilities for the diﬀerent classes at stake. The number
of hidden layers can be adapted to application needs. We denote the whole set
of parameters of this model by θ, and inside this set of parameters, we isolate
the parameters of the ﬁrst layer in the model and denote them β.

×

1 Note that the term distance is an abuse of notation since d(T, S) is not a distance,

mathematically speaking.

Localized Random Shapelets

5

Fig. 2: Overview of our localized random shapelet model. Blue circles indicate
distance features while orange ones correspond to location features. For each
shapelet, a group is formed whose weights are denoted β(k) (where k is the
shapelet index). Note that the number of hidden layers may vary from one
application to the other.

2.3 Structured feature selection

As explained above, the proposed shapelet model relies on random shapelets
(taken from the original time series) in order to fasten the shapelet generation
step. When dealing with such shapelets, a feature selection strategy should be
applied before (or jointly with) the classiﬁcation step, in order to simplify the
resulting representation, which tends to improve overall accuracy [9]. It can be
seen in Equation (3) that the extracted features are structured. Distance features
tell how well a shapelet matches a time series, while localization features inform
about the location of the match. Classical feature selection strategies are hence
not adapted to this kind of feature matrix. Indeed, it does not seem reasonable
to exclude a distance feature related to a shapelet while keeping the associated
localization feature. However, removing a localization feature and keeping the
associated distance feature might be meaningful in cases where the localization
of a shapelet does not impact the class belonging of the times series. In the
following, we design a feature selection strategy adapted to the kind of features
extracted from the localized random shapelet model. This strategy, based on
regularization, is described below.

InputlayerHiddenlayer1Hiddenlayer2Hiddenlayer3OutputlayerGroup1...Groupk...GroupK............β(k)β(k)6

M. Guillem´e et al.

Regularization strategy In the following, we assume that we have a prediction
to be minimized. This loss function is computed
problem with a loss function
over a dataset X of N observations associated with a target vector y. A standard
approach to bias the learning process towards sparse solutions is to derive a
regularized loss function, as done for Lasso regression [16]:

L

Lasso(X, y, θ) =

L

(X, y, θ) + λ
(cid:107)

β

1

(cid:107)

L

(4)

Simon et al. [13] introduced Sparse-Group Lasso (SGL), a more structured
regularization scheme that could take feature group information into account.
The resulting regularized loss function for a K-group problem is:

SGL(X, y, θ) =

L

(X, y, θ) + αλ
β
(cid:107)

L

1 + (1
(cid:107)

−

α)λ

K
(cid:88)

k=1

√pk

β(k)
(cid:107)

2

(cid:107)

(5)

where pk is the number of features in group k and β(k) is the sub-vector of β
1 term enforces per-feature sparsity
made of features from group k. Here, the
(cid:107)
while
2 pushes towards sparsity at the group level. The α parameter hence
acts as a trade-oﬀ between these two regularization terms.

β(k)
(cid:107)

β
(cid:107)

(cid:107)

We consider a slightly diﬀerent setting in which, inside a group, only part of
the features can be dropped. In our case, each group corresponds to a shapelet.
For each shapelet, two features are available: one for the distance and one for
localization of the match. We aim at designing a strategy that can, for each
shapelet (or group), either:

– drop all information related to that shapelet (if the shapelet is useless for

prediction),

– keep only the distance information (if the location of the shapelet does not

help for prediction),

– keep both the distance and localization information.

In order to meet these constraints, we introduce the Semi-Sparse-Group-Lasso
(SSGL) framework, which consists in minimizing the following loss function:

SSGL(X, y, θ) =

L

(X, y, θ) + αλ
(cid:107)

L

Mindθ

1 + (1
(cid:107)

−

α)λ

K
(cid:88)

k=1

√pk

β(k)
(cid:107)

2

(cid:107)

(6)

where Mind is an indicator diagonal matrix made of ones and zeros, the latter
corresponding to variables that should be kept as soon as the group is not zeroed-
out (i.e. variables that will not be considered for (cid:96)1 regularization). In the case
of our localized shapelet model, Mind has a diagonal that alternates between
zeros for dimensions corresponding to distances and ones for those related to the
localization information.

Localized Random Shapelets

7

Fig. 3: Coeﬃcients learned using diﬀerent regularization schemes for a linear
regression problem. Ground-truth coeﬃcients are reported in blue.

Optimization in practice In practice, minimization of such a regularized
loss function can be tackled by several means. First, building on [13], one can
derive a block-wise procedure that considers feature groups one at a time and
starts by deciding on whether it should be zeroed-out or not. In the case of an
ordinary least square regression setting, it gives the following suﬃcient condition
for zeroing-out group k (i.e. dropping all the information from the corresponding
shapelet):

(cid:13)
(cid:13)
(cid:13)
(cid:13)

MindS

(cid:18) X(k) (cid:62)r−k
N

, αλ

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)2 ≤

√pk(1

α)λ,

−

(7)

where X(k) is the submatrix of X in which only variables from group k are kept,
r−k is the partial residual of y, substituting all group ﬁts other than group k
,
and S(
·

) is the coordinate-wise soft thresholding operator:

·

(S(z, αλ))j = sign(zj)(

zj
|

| −

αλ)+.

(8)

If this condition is satisﬁed, all β(k) coeﬃcients are set to zero and the process
goes on to the next group. Otherwise, optimization of the coeﬃcients inside group
k should be performed, either using subgradient equations in a coordinate-wise
algorithm or by performing gradient descent steps inside group k.

In all the experiments presented in this paper, we rather use full gradient
descent on the regularized loss, in order not to face known limitations of block-
wise gradient descent algorithms such as slow convergence and low parallelism
capabilities. This strategy was already successfully applied in [11].

Toy example: structured linear regression Figure 3 presents a comparison
of the three regularization schemes described in this section. For this comparison,
we used the following model to draw samples:

y = X

β + ε,

·

(9)

β(1)1β(1)2β(2)1β(2)2β(3)1β(3)210−410−2100BetavaluesGroup1Group2Group3Ground-truthSSGL(MSE=0.18)Lasso(MSE=0.29)SGL(MSE=0.29)8

M. Guillem´e et al.

(a) Class 1 (A-A)

(b) Class 2 (B-A)

(c) Class 3 (A-B)

(d) Class 4 (B-B)

Fig. 4: An example of each class of TwoPatterns dataset.

where X and ε are drawn from centered normal distributions of standard devi-
(cid:104)
ation 1 and 0.01 respectively and β =
has only
1 , β(3)
three non-zero components: β(2)

β(1)
1 , β(1)
and β(3)
We assume to have a problem similar to the shapelet setting, i.e. we have
groups of two variables among which only β(k)
is concerned by (cid:96)1-norm regu-
larization (for group k). This structural information is used for SGL and SSGL
variants in the experiments, while Lasso is blind to such structure.

1 , β(2)
2 , β(2)
(cf. Figure 3).

2 , β(3)

1 , β(3)

(cid:105)

2

2

2

1

For this example, we use a simple model without any hidden layer. First,
SSGL outperforms both SGL and Lasso in terms of mean squared error (MSE),
showing the beneﬁt of taking variable structure into account in the model. Sec-
ond, from a more qualitative perspective, the structure of the ground truth
coeﬃcients is better preserved with SSGL (all three null coeﬃcients have low
value estimators and β(1)
is better estimated thanks to the intra-group speciﬁc
2
regularization scheme).

3 Model interpretability

In this section, we illustrate the interpretability of our method through a simple
use-case. We consider the TwoPatterns dataset from the UCR & UEA time
series classiﬁcation repository. TwoPatterns is a synthetic dataset in which
each time series is made of two pattern occurrences surrounded by noise. There
are two diﬀerent patterns (named A and B in the following) in the dataset and the
classiﬁcation problem is hence made of four classes, corresponding to all possible
permutations of patterns A and B, as shown in Fig. 4. For this illustration, we
rely on a simple variant of our model in which we have no hidden layer and draw
K = 2, 000 shapelets to build our feature space. We show that, once trained, our
model can be easily analyzed by scrutinizing its weights.

020406080100120−1.5−1.0−0.50.00.51.01.5020406080100120−1.5−1.0−0.50.00.51.01.5020406080100120−1.5−1.0−0.50.00.51.01.5020406080100120−1.5−1.0−0.50.00.51.01.5Localized Random Shapelets

9

(a) 1st shapelet

(b) 2nd shapelet

(c) 3rd shapelet

(d) 4th shapelet

Fig. 5: Four most important shapelets (in red) extracted by our method from
the TwoPatterns dataset.

More precisely, in the following, we aim at answering the following questions:

– Which shapelets are the most important for classiﬁcation?
– For each of these shapelets, is the localization information important for the

decision ?

Note that, in this simple setting where no hidden layer is used, this infor-
mation comes in a straight-forward manner by analyzing weights, but the same
analysis could be performed in the multi-layer case through gradient descent in
the shapelet transform space.

First, we can extract most important shapelets (in terms of classiﬁcation
power) by ranking them with respect to the (cid:96)2-norm of their associated weight
matrix β(k). Top-4 shapelets for dataset TwoPatterns are presented in Fig. 5.
They fully match expectations since they focus on discriminative parts of the
time series and cover both patterns A and B, as well as successions of these.

Then, for a shapelet that is considered discriminant, we can wonder whether
its localization and distance are both used by the model or not. To assess the
importance of the distance feature for a shapelet, we compute the (cid:96)2-norm of
the coeﬃcients of β(k) that are associated with its distance feature. We call this
value distance coeﬃcient. Similarly, to assess the importance of the localization
feature for a shapelet, we compute the (cid:96)2-norm of the coeﬃcients of β(k) that are
associated with its localization feature. We call this value localization coeﬃcient.
The ﬁrst shapelet of Figure 5 has the 4th highest localization coeﬃcient
amongst the 2,000 shapelets, together with the 14th highest distance coeﬃcient.
This means that for this shapelet both the distance and the localization are im-
portant. It is coherent as this shapelet corresponds to a B pattern. The distance
feature enables to discriminate class 1 from the others, while the localization
features enables to discriminate class 2 and class 3. This is conﬁrmed by the
histograms of Figure 6. The second shapelet of Figure 5 has the 2nd highest
distance coeﬃcient amongst the 2,000 shapelets. However, its localization coef-
ﬁcient is not as important as its distance coeﬃcient (217th out of 2,000). This
means that for this shapelet the distance feature is important but not the lo-
calization. This is coherent as this shapelet corresponds to the end of a pattern

20406080100120−10120406080100120−10120406080100120−10120406080100120−10110

M. Guillem´e et al.

Fig. 6: The distribution of the localization (left) and distance (right) for the
most important (ﬁrst row) and second most important (second row) shapelet in
TwoPatterns dataset.

A followed by the beginning of a pattern B, which only occurs in class 3, hence
distance feature is enough to discriminate class 3 from the others, as conﬁrmed
by Figure 6. Similar conclusions can be drawn for shapelets 3 and 4.

We have seen in this section that our model allows easy extraction of the
shapelets that have more contributed to discriminate classes. One can also easily
analyze whether the localization of these shapelets was used by the model. This
kind of information is very important as it brings a richer interpretability in the
decision process. Conversely, state-of-the-art shapelet-based algorithms tend to
suﬀer from lack of interpretability. Shapelet Transform is a weighted ensemble
of several standard classiﬁers, which makes it diﬃcult to assess the importance
of shapelets in the decison process. In the Learning Shapelet (LS) framework,
obtained shapelets are not constrained to be similar to training time series (cf.
Figure 1), which limits the interpretability of such framework.

4 Experiments

In this section, we present experimental results to assess the performance of the
proposed Localized Random Shapelet (LRS) model and compare it to state-of-
the-art shapelet-based methods.

4.1 Experimental setup

The proposed LRS is compared to the following state-of-the-art baselines: Shapelet
Transform (ST) [7], the Learning Shapelet (LS) model [5] and the Fast Shapelet
(FS) one [8]. Experiments are based on the datasets from the UCR & UEA time
series classiﬁcation repository [2], a classical benchmark in the time series classi-
ﬁcation ﬁeld. It is composed of 85 datasets with diverse time series classiﬁcation

020406080100120localization0102030405060numberoftimeseries0510152025303540shapeletdistance0255075100125150numberoftimeseriesclass1(A-A)2(B-A)3(A-B)4(B-B)020406080100120localization0102030405060numberoftimeseries0510152025303540shapeletdistance020406080numberoftimeseriesclass1(A-A)2(B-A)3(A-B)4(B-B)Localized Random Shapelets

11

problems. A description of the datasets can be found in [1]. Classiﬁcation per-
formance for baseline methods is retrieved from the website associated to this
dataset.

For each dataset, we draw K = 2, 000 shapelets uniformly at random from
the time series training set. For the length of the shapelets, we followed the
procedure described in [5]. All time series are represented by a feature vector
which size is twice the number of shapelets (cf. Eq. (3)). This feature vector
embeds information about shapelet distance and localization.

We use a 3-hidden-layer model with 256, 128 and 64 units in the hidden
layers and each internal layer is followed by a batch normalization layer as sug-
gested in [6]. We add some dropout in the second and third layer. We use ReLU
activations [4] for all internal layers and softmax activation for the ﬁnal layer.
The regularization in LRS requires two more hyper-parameters: λ that controls
the regularization strength, and α that controls the trade-oﬀ between lasso and
group-lasso terms. We cross-validated the dropout from the set
,
}
λ from the set
.
}
Models are learned using RMSProp optimizer [17] with a learning rate of 0.001.

0.0, 0.3, 0.5, 0.7
0.1, 0.3, 0.5, 0.7, 0.9
{

10−1, 10−2, . . . 10−7
{

and α from the set

}

{

4.2 Runtime and memory cost

Experiments are run on a Laptop with a Fedora 24 system, 16 GB DDR4 RAM
and dual core CPU (i7-6600U 2.6 GHz). Python code used for these experiments
is publicly available2. As an indication of the low computing and memory cost
of our method, it requires 66 seconds and 1.5 GB of memory to learn a model
(for a given set of hyper-parameters) on the Electric Devices dataset that is
the largest in the UCR & UEA archive.

4.3 Evaluation of the impact of the SSGL regularization

Figure 7 shows the error rates on 85 datasets for two diﬀerent regularization
strategies: Lasso [16] and SSGL. It can be seen that the SSGL regularization
strategy (that allows selection for each shapelet of either distance feature, or
distance and localization features, or none of these) slightly improves classiﬁca-
tion performance over Lasso (that makes the selection for each features indepen-
dently).

4.4 Performance comparison against baselines

Critical diagrams show the average ranks of the classiﬁers in order, and cliques,
represented by a solid bar. A clique represents a group of classiﬁers within which
there is no signiﬁcant pairwise diﬀerence. Figure 8 presents critical diagrams of
the performance against the baselines when considering all datasets, and when
considering only datasets with more than 300 training instances.

2 https://github.com/rtavenar/localized_random_shapelets

12

M. Guillem´e et al.

Fig. 7: Error rates comparison on 85 UCR datasets between LRS with ssgl reg-
ularization against LRS with lasso regularization.

(a) Performance on all the datasets

(b) Performance on large datasets

Fig. 8: Critical diagrams of the performance against the baselines.

Overall, in terms of classiﬁcation performance, only Shapelet Transform out-
performs our approach. As already stated, ST is a weighted ensemble of sev-
eral standard classiﬁers, hence suﬀering from lack of interpretability about the
decision process. The performance of LRS is slightly better than LS (but not
signiﬁcantly). When we only consider large datasets (the 42 datasets with more
than 300 training instances, which are better suited to the characteristics of our
method), performance of LRS is better than LS and FS and gets closer to that of
ST. This seems to indicate that taking the localization information into account
enables to improve classiﬁcation performance provided that suﬃcient training
data is available to correctly ﬁt the model.

5 Conclusion

In this paper, we have proposed a novel shapelet model that has low computing
cost, competitive performance and better interpretability compared to tradi-

0.00.10.20.30.40.50.60.70.80.91.0ErrorrateforLRSwithlassowithlocalization0.00.10.20.30.40.50.60.70.80.91.0ErrorrateforLRSwithssglwithlocalizationW/T/L=44/5/361234CDSTLRSLSFS1234CDSTLRSLSFSLocalized Random Shapelets

13

tional shapelet-based methods. This model uses shapelet localization in addi-
tion to the traditional shapelet transform representation in a random shapelet
framework. As such a framework needs many shapelets as input, we have de-
signed a dedicated hierarchical regularization framework (SSGL) to ﬁt our appli-
cation needs. Experiments show that our model produces a ranking of realistic
shapelets, and is able to explain their importance both in terms of distance and
localization.This enables an easy and rich interpretability of the classiﬁcation
process. Moreover our classiﬁcation performance is competitive with state-of-
the-art shapelet-based classiﬁers.

References

1. Bagnall, A., Lines, J., Bostrom, A., Large, J., Keogh, E.: The great time series
classiﬁcation bake oﬀ: a review and experimental evaluation of recent algorithmic
advances. Data Mining and Knowledge Discovery pp. 1–55 (2016)

2. Bagnall, A., Lines, J., Vickers, W., Keogh, E.: The uea & ucr time series classiﬁ-

cation repository, www.timeseriesclassification.com

3. Bailly, A., Malinowski, S., Tavenard, R., Chapel, L., Guyet, T.: Dense Bag-of-
Temporal-SIFT-Words for Time Series Classiﬁcation. In: Advanced Analysis and
Learning on Temporal Data. Springer (2016). https://doi.org/10.1007/978-3-319-
44412-3 2, https://hal.archives-ouvertes.fr/hal-01252726

4. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiﬁer neural networks. In: Pro-
ceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and
Statistics. pp. 315–323 (2011)

5. Grabocka, J., Schilling, N., Wistuba, M., Schmidt-Thieme, L.: Learning time-
series shapelets. In: Proceedings of the ACM SIGKDD International Conference
on Knowledge Discovery and Data mining. pp. 392–401 (2014)

6. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)

7. Lines, J., Davis, L.M., Hills, J., Bagnall, A.: A shapelet transform for time series
classiﬁcation. In: Proceedings of the ACM SIGKDD International Conference on
Knowledge Discovery and Data mining. pp. 289–297 (2012)

8. Rakthanmanon, T., Keogh, E.: Fast shapelets: A scalable algorithm for discovering

time series shapelets. pp. 668–676 (05 2013)

9. Renard, X., Rifqi, M., Erray, W., Detyniecki, M.: Random-shapelet : an algorithm
for fast shapelet discovery. IEEE International Conference on Data Science and
Advanced Analytics pp. 1–10 (2015)

10. Renard, X., Rifqi, M., Fricout, G., Detyniecki, M.: EAST Representation: Fast Dis-
criminant Temporal Patterns Discovery From Time Series. ECML/PKDD Work-
shop on Advanced Analytics and Learning on Temporal Data (2016)

11. Scardapane, S., Comminiello, D., Hussain, A., Uncini, A.: Group sparse regular-

ization for deep neural networks. Neurocomputing 241, 81–89 (2017)

12. Sch¨afer, P.: The BOSS is concerned with time series classiﬁcation in the presence

of noise. Data Mining and Knowledge Discovery 29(6), 1505–1530 (2015)

13. Simon, N., Friedman, J., Hastie, T., Tibshirani, R.: A sparse-group lasso. Journal

of Computational and Graphical Statistics 22(2), 231–245 (2013)

14. Tavenard, R.: tslearn: A machine learning toolkit dedicated to time-series data

(2017), https://github.com/rtavenar/tslearn

14

M. Guillem´e et al.

15. Tavenard, R., Malinowski, S., Chapel, L., Bailly, A., Sanchez, H., Bustos, B.:
Eﬃcient Temporal Kernels between Feature Sets for Time Series Classiﬁcation.
In: Proceedings of the European Conference on Machine Learning and Prin-
ciples and Practice of Knowledge Discovery. pp. 528–543 (Sep 2017), https:
//halshs.archives-ouvertes.fr/halshs-01561461

16. Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the

Royal Statistical Society. Series B (Methodological) pp. 267–288 (1996)

17. Tieleman, T., Hinton, G.: Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learn-
ing 4(2), 26–31 (2012)

18. Wistuba, M., Grabocka, J., Schmidt-Thieme, L.: Ultra-fast shapelets for time se-
ries classiﬁcation. CoRR abs/1503.05018 (2015), http://arxiv.org/abs/1503.
05018

19. Ye, L., Keogh, E.: Time series shapelets: a new primitive for data mining. In: Pro-
ceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data mining. pp. 947–956 (2009)

