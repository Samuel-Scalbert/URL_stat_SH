Learning Rich Event Representations and Interactions
for Temporal Relation Classification
Onkar Pandit, Pascal Denis, Liva Ralaivola

To cite this version:

Onkar Pandit, Pascal Denis, Liva Ralaivola. Learning Rich Event Representations and Interactions for
Temporal Relation Classification. ESANN 2019 - 27th European Symposium on Artificial Neural Net-
works, Computational Intelligence and Machine Learning, Apr 2019, Bruges, Belgium. ￿hal-02265061￿

HAL Id: hal-02265061

https://hal.science/hal-02265061

Submitted on 8 Aug 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Learning Rich Event Representations and Interactions
for Temporal Relation Classiﬁcation

Onkar Pandit1, Pascal Denis1 and Liva Ralaivola2

1- MAGNET, Inria Lille - Nord Europe, Villeneuve d’Ascq, France
onkar.pandit@inria.fr, pascal.denis@inria.fr

2- QARMA, IUF, LIS, Aix-Marseille University, CNRS, Marseille, France,
Criteo AI Labs, Paris, France. liva.ralaivola@lif.univ-mrs.fr

Abstract. Most existing systems for identifying temporal relations between events
heavily rely on hand-crafted features derived from event words and explicit tempo-
ral markers. Besides, less attention has been given to automatically learning con-
textualized event representations or to ﬁnding complex interactions between events.
This paper ﬁlls this gap in showing that a combination of rich event representations
and interaction learning is essential to more accurate temporal relation classiﬁca-
tion. Speciﬁcally, we propose a method in which i) Recurrent Neural Networks
(RNN) extract contextual information ii) character embeddings capture morpho-
semantic features (e.g.
tense, mood, aspect), and iii) a deep Convolutional Neu-
ral Network (CNN) ﬁnds out intricate interactions between events. We show that
the proposed approach outperforms most existing systems on the commonly used
dataset while using fully automatic feature extraction and simple local inference.

1

Introduction

Recovering temporal information from texts is a crucial part of language understanding,
and it has applications such as question answering, summarization, etc.

Temporal relation identiﬁcation is divided into two main tasks, as identiﬁed by Tem-
pEval campaigns [1]: i) the identiﬁcation of EVENTs and other time expressions (the
so-called TIMEX’s), and ii) the classiﬁcation of temporal relations (or TLINKs) among
and across events and time expressions [2, 3, 4, 5]. Possible relations for this latter task
include temporal precedence (i.e., BEFORE or AFTER), inclusion (i.e., INCLUDES or
IS INCLUDED), and others inspired from Allen’s algebra. In this work, we concentrate
on temporal relation classiﬁcation, speciﬁcally EVENT-EVENT relations.

The problem becomes harder in the absence of explicit temporal connectives (e.g.,
before, during), determining temporal relations depends on numerous factors, rang-
ing from tense and aspect to lexical semantics and even world knowledge. To address
this issue, most state-of-the-art systems for EVENT-EVENT classiﬁcation [2, 3, 5] rely
on manually-crafted feature sets directly extracted from human annotations, comple-
mented with syntactic features, and semantic features extracted from static knowledge
bases like WordNet or VerbOcean. Such an approach is tedious, error-prone and the
semantics of events is poorly modelled due to lack of coverage of existing lexical re-
sources and blindness to event contexts.

We here propose a radically different approach where we altogether dispense with
hand-designed features and instead learn task-speciﬁc event representations. These rep-
resentations include information both from the event words and its surrounding context,
thus giving access to the events’ arguments and modiﬁers. Furthermore, character based

model capture another type of information captured by morphology such as tense and
aspect of event. Plus, we also attempt to learn the potentially rich interactions between
events. Concretely, our learning framework, as depicted in Fig.1, is based on a neu-
ral network architecture, wherein: i) Recurrent Neural Network (RNN) is employed to
learn contextualized event representations ii) character embeddings is used to capture
morphological information, hence encoding tense, mood and aspect information, and
iii) a deep Convolutional Neural Network (CNN) architecture is then used to acquire
complex, non-linear interactions between these representations.

WA

OA

RNN

WB

OB

RNN

SA

C
o
n
c
a
t
e

C
o
n
c
a
t
e

EA

Conv.
+
ReLu

EB

P
o
o
l
i
n
g

Conv.
+
ReLu

P
o
o
l
i
n
g

Conv.
+
ReLu

P
o
o
l
i
n
g

This is one important step up
from the recent work of Mirza and
Tonelli [4], which simply uses pre-
trained word embeddings for event
words and still have to resort to ad-
ditional hand-engineered features to
achieve good temporal classiﬁcation
accuracy. We show our system based
on fully automatic feature extraction
and interaction learning outperforms
other local classiﬁer systems.

D
e
n
s
e

L
a
y
e
r

Pr

s
o
f
t
m
a
x

SB

Representation
Learning

Interaction Learning

Relation
Classiﬁcation

2 Related Work

Fig. 1: Architecture of our proposed model.

Recent temporal classiﬁcation sys-
tems use machine learning tech-
niques due to the availability of annotated datasets. Earlier work [2, 6] studied local
models (i.e., making pairwise decisions on pairs of events) and used gold-standard
features extracted from TimeML annotations. State-of-the-art local models such as
ClearTK [7] relied on an enlarged set of predicted features, and classiﬁers. These lo-
cal models may generate globally incoherent temporal relations, in the sense that the
symmetry and transitivity holding between relations are not followed at the document
level. This has led to development of global models [3]. The state-of-the-art method
[5] proposes a structured prediction approach.

These methods all rely on manually engineered features, which fail to model the
semantics of events. To address this issue, [4] have evaluated the effectiveness of pre-
trained word embeddings of event head-word. They also demonstrated the potency
of basic vector combination schemes. However, representing events with word embed-
dings of only its head-word is not effective and important contextual information is lost.
Recently proposed LSTM-based neural network architectures [8, 9] learn event repre-
sentation with use of event head word as well as context. Also, newly proposed method
[10] have shown efﬁcacy of context with use of gated RNN-attention based neural ar-
chitecture. However, by using only word embeddings they fail to capture inﬂectional
morphology of event head word, which includes crucial linguistic information such as
tense, aspect, and mood. Also they lacked in ﬁnding complicated interaction between
events and relied only on concatenation of event features. Moreover they [9] used syn-
tactically parsed trees as inputs to the LSTM which adds burden of pre-processing.

3 Method

Our proposed neural architecture (Fig.1), consists of two main components: Repre-
sentation Learning and Interaction Learning. In the Representation Learning part, a
bag-of-words in a ﬁxed size window centred on each event word is fetched and fed
into a RNN to get more expressive and compact representation of events. Output of
the RNN is concatenated with character embedding of event head-word to get the ﬁnal
vector for each event. This vector representation is then used at the Interaction Learning
stage: the vector representation of each event is fed to a convolution layer and the ﬁnal
pooling layer outputs an interaction vector between the events. A dense layer is used to
combine the interaction vector before obtaining a probability score for each relation.

3.1 Representation Learning
Context-aware Representation Each word is encoded in the event-word win-
dow with word embeddings [11]. As a result, each word is assigned a ﬁxed d-dimensional
vector representation. Let cl be the context length for each event head word wt. Thus
we consider a window of 2cl + 1 words as input to the RNN. We represent this as ma-
trix W = [wt−cl · · · wt · · · wt+cl ] ∈ R(2cl+1)×d. Note that while considering event
context we stop at sentence boundary, also special symbols are padded if context is
less than cl. The relation between input and output of RNN at each time t is given as
follows,

(1)

ht = σh(Qhwt + Uhht−1 + bh)
ot = σo(Qoht + bo)

(2)
where, wt is the word embedding vector provided at each time step, ht is hidden layer
vector and ot is output vector. Q, U , and b are weight matrices and vector; σh and
σo are activation ReLU functions. For a given event, the ot+cl output vector captures
a complete information about the whole sequence. The outputs of the RNN networks
give compact representations OA and OB of the events.
Morphological Representation Semantics and arguments of events are captured
with context-aware representation but it doesn’t capture morphological information
such as tense and aspect. For instance, tense information is captured from context
as well with auxiliary verbs (will, is). However in the absence of these words, tense
information of event is expressed in inﬂectional sufﬁxes such as -ed, -ing. To obtain
this information, event head word wt is represented as a sequence of character n-grams
c1, · · · , cm where m is number of n-grams in word. Fixed dc dimension vector is
learned for each n-gram. Eventually morphological representation of word is obtained
by adding vectors of n-grams present in the word. Standard fasttext [13] method is used
to get this representation.
Context-aware vector OA and character embedding vector SA are concatenated to ob-
tain ﬁnal event representation as EA, similarly EB also obtained for event B.

Interaction Learning

3.2
A deep Convolution Neural Network (CNN) is employed to learn nonlinear interac-
tions from EA and EB. It is comprised of three convolution and pooling layers placed
alternatively. We feed concatenated learned event representations

EAB = EA ⊕ EB

(3)

to the ﬁrst convolution layer, where ⊕ is the concatenation operation. Each convolution
layer i use ﬁlters fi, after what we compute a feature map

mk

∀k ∈ {1, 2, 3},

i = σ(fi.EAB + bi)

(4)
where fi, bi are ﬁlters and bias matrices respectively and σ is the ReLU activation. The
output is down-sampled with a max-pooling layer to keep prominent features intact.
The output of the last layer gives the interaction between A and B (ρ is max-pooling).
Ecomb = ρ(m3
i )
(5)
The combined Ecomb vector is fed to a fully connected dense layer, followed by a
softmax function to get a probability score for each temporal relation class.

4 Experiments

4.1 Datasets and Evaluation
Relations Following recent work [5], reduced set of temporal relations: after, before,
includes, is included, equal, vague are considered for classiﬁcation.
Evaluation Complying with common practice, system’s performance is measured
over gold event pairs (pairs for which relation is known). Our main evaluation measure
is the Temporal Awareness metric [12], adopted in recent TempEval campaigns. We
also used standard precision, recall, and F1-score.
Datasets We used TimeBank (TB) and AQUAINT (AQ) dataset for training, TimeBank-
Dense(TD) for development and Platinum (TE3-PT) dataset for testing. These are the
most popular datasets used for the task which have been provided at TempEval3 [1].

4.2 Training Details
We used pre-trained Word2Vec vectors from Google1. Each word in the context window
of event is represented with this 300-dimension vector. Character embeddings for event
head word are obtained from fasttext [13] which is also 300-dimension vector. Note that
only one RNN is trained with weight sharing to learn representation. Hyperparameters
were tuned on the development set using a simple grid search. Considered values are:
window size (cl: 3,4,5), number of neurons at RNN (#RNN: 64,128,256,512), number
of ﬁlters for CNN (#ﬁlters: 32,64,128,256), dropout at input (0.1,0.2,0.3,0.4). We also
explored a number of optimization algorithms such as AdaDelta, Adam, RMSProp and
Stochastic Gradient Descent(SGD). Optimal hyper-parameter values are cl = 4,#RNN
=256, #ﬁlters = 64, dropout = 0.4 and Adam optimizer.2 Once we got the optimal pa-
rameter values from the validation set, we re-trained multiple models with 50 different
seed values on the combined training and development data and report the averaged test
performances.

4.3 Results
We ﬁrst compare our RNN-Deep CNN approach to various baseline systems to assess
the effectiveness of the learned event representations. We also want to disentangle the
respective role of the representation learning and interaction learning.
Baseline systems As Mirza and Tonelli [4] reported results only with pairwise f-
score on different dataset, we re-implemented their system with scikit-learn Logistic

1https://code.google.com/archive/p/word2vec/
2We tried different unidirectional and bidirectional variations of RNN-LSTM and GRU, but RNN gave

the best development results.

Systems

WA ⊕ WB
OA ⊕ OB
EA ⊕ EB

M LP (EA, EB)
CN N (EA, EB)
DCN N (EA, EB)

ClearTK
LSTM
SP

(a)

(b)

(c)

Pair Classiﬁcation

Temporal Awareness

P
39.3
35.7
37.6

40.2
38.2
39.4

-
38.7
-

R
34.2
38.9
44.5

46.3
53.7
58.9

-
43.1
-

F1
35.5
37.2
40.8

43.0
44.6
47.2

-
40.5
-

P
27.1
36.5
41.2

51.8
41.7
43.2

33.1
34.6
69.1

R
45.8
35.9
45.9

42.5
60.1
70.1

35.0
51.7
65.5

F1
34.1
36.2
43.4

46.7
49.2
53.4

34.1
41.4
67.2

Table 1: Results of baseline and state-of-the-art systems
regression module, using l2 regularization (noted WA ⊕ WB). Word embeddings WA
and WB of events A and B obtained from Word2Vec were simply concatenated (as this
is their best performing system). We conducted number of experiments to obtain opti-
mal parameters and reported best performing system’s results. We got lower than the
originally reported results as the dataset is sparsely annotated compared to the one in the
paper (TimebankDense). As additional baselines, we used our Representation Model
to learn OA and OB, subsequently EA, EB, but combined these vectors with simple
concatenation (OA ⊕ OB and EA ⊕ EB). In another setting learned representation is
combined with multi-layer perceptron (MLP) and single-layer convolution (CNN).
Comparison to Baseline Systems Sections a and b of Table 1 summarize the
performance of these different systems in terms of pairwise classiﬁcation accuracy and
temporal awareness scores. Looking at the ﬁrst two rows of the table, we see that,
as hypothesized, contextually rich features outperform pre-trained event head word em-
beddings when combined with simple concatenation, both in pairwise classiﬁcation and
in temporal awareness. A further gain in the performance with character embeddings
shows the effectiveness of morphological features. Results from the section establish
the effectiveness of our rich representation learning. In section b of the table, we present
results of the system with different interaction learning.The system with MLP inter-
action learning outperforms simple concatenation (EA ⊕ EB) system, demonstrating
importance of interaction learning. Leveraging both rich event representations learning
and non-linear interaction learning yield the best scores overall, cf. CN N (EA, EB)
and DCN N (EA, EB), which shows their complementarity. There, the Deep CNN
outperforms the single-layer CNN, with F1 scores of 53.4 and 49.2, respectively. This
conﬁrms the importance of non-linear interaction learning in the task.
Comparison with State-of-the-art Finally, in the section c, we compare the perfor-
mance of our best system, DCN N (EA, EB), with recently proposed systems : WA ⊕
WB [4], ClearTK [7], which was the winner of the TempEval 2013 campaign, the
structured prediction (SP) approach[5], which is the best system to date and recently
proposed LSTM [14]. Our system delivers substantial improvements over WA ⊕ WB,
ClearTK, and LSTM based system. However our system lags in comparison with SP as
it relies only on simple local inference opposed to global inference at learning step, also
totally dispenses with hand-crafted features. It is important to observe that our system
closes the performance gap between SP system and automatic feature learned systems.

5 Conclusion and Future Work

In this work, we proposed RNN based neural architecture to learn event representation
and CNN model to get interaction between events. A new perspective towards the
combination of events proved to be effective in getting compound interactions. We
compared result of our system with multiple baselines and state-of-the art systems and
shown effectiveness. We now plan to learn features and interaction while considering
global consistency in the relations of event pairs.

Acknowledgement

This work was supported by ANR Grant GRASP No. ANR-16-CE33-0011-01, as well
as by a grant from CPER Nord-Pas de Calais/FEDER DATA Advanced data science
and technologies 2015-2020.

References

[1] N. UzZaman, H. Llorens, L. Derczynski, J. Allen, M. Verhagen, and J. Puste-
jovsky. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events,
and temporal relations. ACL, 2013.

[2] I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky. Machine learn-

ing of temporal relations. ACL, 2006.

[3] N. Chambers, T. Cassidy, B. McDowell, and S. Bethard. Dense event ordering

with a multi-pass architecture. TACL, 2014.

[4] P. Mirza and S. Tonelli. On the contribution of word embeddings to temporal

relation classiﬁcation. COLING, 2016.

[5] Q. Ning, Z. Feng, and D. Roth. A structured learning approach to temporal relation

extraction. EMNLP, 2017.

[6] Nathanael Chambers, Shan Wang, and Dan Jurafsky. Classifying temporal rela-

tions between events. ACL, 2007.

[7] S. Bethard. ClearTK-TimeML: A minimalist approach to tempeval 2013. ACL,

2013.

[8] D. Dligach, T. Miller, C. Lin, S. Bethard, and G. Savova. Neural temporal relation

extraction. ACL, 2017.

[9] F. Cheng and Y. Miyao. Classifying temporal relations by bidirectional lstm over

dependency paths. ACL, 2017.

[10] Y. Meng and A. Rumshisky. Context-aware neural model for temporal information

extraction. ACL, 2018.

[11] T. Mikolov, I. Sutskever, K. Chen, G. S Corrado, and J. Dean. Distributed repre-

sentations of words and phrases and their compositionality. NIPS, 2013.

[12] N. UzZaman and J. F. Allen. Temporal evaluation. ACL, 2011.
[13] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin. Advances in

pre-training distributed word representations. In LREC, 2018.

[14] Y. Meng, A. Rumshisky, and A. Romanov. Temporal information extraction for
question answering using syntactic dependencies in an lstm-based architecture.
CoRR, abs/1703.05851, 2017.

