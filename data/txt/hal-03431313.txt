An End-to-End Approach for Full Bridging Resolution
Joseph Renner, Priyansh Trivedi, Gaurav Maheshwari, Rémi Gilleron, Pascal

Denis

To cite this version:

Joseph Renner, Priyansh Trivedi, Gaurav Maheshwari, Rémi Gilleron, Pascal Denis. An End-to-End
Approach for Full Bridging Resolution. CODI-CRAC 2021 - Shared-Task: Anaphora Resolution in
Dialogues, Nov 2021, Punta Cana, Dominican Republic. pp.48-54. ￿hal-03431313￿

HAL Id: hal-03431313

https://hal.science/hal-03431313

Submitted on 16 Nov 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

An End-to-End Approach for Full Bridging Resolution

Joseph Renner, Priyansh Trivedi, Gaurav Maheshwari, Rémi Gilleron, Pascal Denis
Magnet, INRIA Lille, France
{joseph.renner, priyansh.trivedi, gaurav.maheshwari,
remi.gilleron, pascal.denis}@inria.fr

Abstract

In this article, we describe our submission
to the CODI-CRAC 2021 Shared Task on
Anaphora Resolution in Dialogues – Track BR
(Gold)1. We demonstrate the performance of
an end-to-end transformer-based higher-order
coreference model ﬁnetuned for the task of full
bridging. We ﬁnd that while our approach is
not effective at modeling the complexities of
the task, it performs well on bridging resolu-
tion, suggesting a need for investigations into a
robust anaphor identiﬁcation model for future
improvements.

1

Introduction

Anaphora is a discourse level phenomenon wherein
a linguistic entity (referred to as anaphor) is associ-
ated with some other linguistic entity (referred to as
an antecedent) within a document (Tognini-Bonelli,
2001). Broadly, the phenomenon is divided into
coreference and bridging anaphora depending on
whether the anaphoric references are linked to as-
sociated antecedents with an identical (is-a), or a
non-identical (¬ is-a) relation, respectively. Fol-
lowing is an example of bridging anaphora – the
focus of this article:

Starbucks has a new take on the unicorn
cappuccino. One employee accidentally
leaked a picture of the secret new drink.

Here, the noun phrase “One employee” (bridging
anaphor) is anaphorically linked to the antecedent –
“Starbucks”. In this instance, bridging anaphor can
be thought of as an expression with an implicit ar-
gument, i.e., one employee (of Starbucks) (Rösiger
et al., 2018).

The task of full bridging is that of identifying
anaphors from a given set of linguistic entities in

1Task home page

- https://competitions.

codalab.org/competitions/30312

the document and linking them with their non-
identical associated antecedent. This task is ar-
guably more difﬁcult and relatively understudied
than that of entity coreference resolution – which
involves identifying coreferent linguistic entities.
This difﬁculty can be seen from the fact that Bridg-
ing anaphors are less likely to conform to syntac-
tic or surface clues (Kobayashi and Ng, 2020),
and the low inter-annotator agreement for bridg-
ing annotations (Markert et al., 2012). Hindrances
to rapid progress on the task, however, also in-
clude lack of ample gold labeled data (Rösiger,
2018; Hou, 2020), and standardised evaluation
schemes (Kobayashi and Ng, 2020). The CODI-
CRAC 2021 shared task (Khosla et al., 2021)
is thus a welcome addition to the existing set
of datasets in the ﬁeld as it provides a consis-
tent benchmark across multiple gold-annotated
datasets.

However, unlike the task variant that the ma-
jority of existing approaches tackle, the shared
task also includes identiﬁcation of the bridging
anaphors as a part of the task. That is, most ex-
isting approaches (Lassalle and Denis, 2011; Hou
et al., 2013; Hou, 2018a,b) assume the anaphor
(here, “one employee”) to be given, and are limited
to selecting the correct antecedent for this phrase,
amongst a predeﬁned list of linguistic entities. The
identiﬁcation of anaphors compounds the difﬁculty
of the task, owing to, amongst other things, the
number of markables (linguistic entities) in a doc-
ument (See Table 1), and low recall of anaphoric
noun phrases in existing datasets (Rösiger et al.,
2018).

That said, coreference resolution is a more com-
plex task in terms of the decisions to be taken
for correct predictions. Coreference chains are
variadic, whereas relations representing bridging
anaphors are binary. Further, contemporary ap-
proaches treat mention detection as a part of the
task. The neural architectures thus proposed for

ProceedingsoftheCODI-CRAC2021SharedTaskonAnaphora,Bridging,andDiscourseDeixisinDialogue,pages48–54PuntaCana,DominicanRepublic,November10,2021.©2021AssociationforComputationalLinguistics48coreference resolution are often more expressive
than the task of full bridging (with gold markables)
requires. Based on this observation, we aim to ﬁnd
whether the aforementioned neural architectures
can be adapted for solving a linguistically com-
plex but simpler (in the above stated terms) task
of full bridging. To that end, we experiment with
the independent variant of the transformer based
higher-order coreference model (Joshi et al., 2019).
We empirically ﬁnd that the approach is inade-
quate in solving the task, and posit that the currently
available amount of gold-labeled data is insufﬁ-
cient for this family of approaches. This suggests
a need for more data, or the use of external in-
formation in solving the full bridging task when
using this family of approaches. However, we ob-
serve that when tasked with only the resolution of
bridging anaphors (i.e., the anaphoric markables
are provided as a part of the task), this approach
performs signiﬁcantly better, suggesting that a two-
step identiﬁcation and resolution approach might
be beneﬁcial for the task.

2 Task Description

In this section, we introduce domain speciﬁc terms
used throughout the article, and formalise the differ-
ent variants of tasks corresponding to identiﬁcation
and association of bridging anaphors. We use the
nomenclature used in (Rösiger et al., 2018).

Markables: A set of linguistic entities in a doc-
ument of which anaphors and antecedents are both
subsets.

Bridging Anaphor: A markable whose inter-
pretation depends upon an antecedent, or more gen-
erally, which is implicitly linked to an antecedent
with a non-identity relation. We refer to them as
simply anaphors in the rest of this article.

Antecedent: The markable which is related to
the anaphor with an implicit non-identity relation.

2.1 Task Variants

There are two primary variants of the task, deﬁned
as follows:

• Bridging Resolution: Given a document, and
bridging anaphor markables, the task is that of
ﬁnding the associated antecedent correspond-
ing to each given bridging anaphors, from one
of the markables preceding it.

ated antecedent from one of the markables
preceding it.

If the markables are provided alongwith the cor-
pus annotations, we call them gold markables. Oth-
erwise, the approaches solving the task are ex-
pected to predict these markables. We refer to the
latter as predicted markables.

The CODI-CRAC 2021 Shared-Task: Anaphora
Resolution in Dialogues - Track BR (Gold) that
we target is thus that of Full Bridging with Gold
Markables2.

3 Approach

3.1 Full Bridging with Higher-Order

coreference model

Our full bridging system is based on the indepen-
dent version (Joshi et al., 2019) of the higher-order
coreference resolution model described in (Lee
et al., 2018). As their model is designed for the
coreference resolution with predicted markables,
we adapt it for the full bridging task with gold
markables. In this sub-section, we provide a brief
overview of our augmented model and the associ-
ated problem formulation.

Following the footsteps of (Lee et al., 2018), we
formulate the problem as selecting an antecedent
yi, from the set γ(i), for each markable mi in the
document. The set includes a dummy antecedent (cid:15)
and all the markables in the document before mi,
that is γ(i) = {(cid:15), m1, ..., mi−1}. A non-dummy
assignment represents an anaphor-antecedent link
between mi and yi, while a dummy assignment
means that the markable has no antecedent in
the document i.e., the markable is not a bridging
anaphor.

For each markable mi, the model learns a dis-
tribution P (yi) over all the previous markable set
γ(i):

P (yi) =

es(mi,yi)
y(cid:48)∈γ(i) es(mi,y(cid:48))

(cid:80)

(1)

The s(x,y) is the scoring function consisting of

three parts deﬁned as:

s(x, y) = sm(x) + sm(y) + sp(x, y)
sm(x) = FFNNm(x)

sp(x, y) = FFNNp(x, y, φ(x, y))

(2)

(3)

(4)

• Full Bridging: Given a document, identify
the bridging anaphors and ﬁnd their associ-

2For the purposes of further analysis, we also consider
Bridging Resolution with Gold Markables over provided
train sets.

49Here x and y are the encoded representation of
the two markables. These encodings are obtained
by concatenating the transformer’s output at the
start and end of the span along with the attention
vector computed over the output representation of
the tokens in the span. φ(x, y) refers to the hand
crafted features (the genre indicating the dataset to
which this document belongs , speaker-ids, length
of the two markables, and the number of tokens be-
tween them), while FFNNm and FFNNp represents
feed forward network.

Recall that the approach in (Lee et al., 2018) is
factored into a two-staged beam search. The ﬁrst
stage is responsible for predicting markables: a
beam of up to M potential markables is computed
based on the spans with the highest markable scores
sm(x) out of all possible text spans, up to a cer-
tain width, in a document. In the second stage, the
pairwise scores sp(x, y) are then only computed
between the top markables, in a coarse-to-ﬁne man-
ner. Since we have access to the gold markables,
we repurpose the markable scorer sm(x) to score
markables as possible antecedents or anaphors. The
number of markables can be large in documents,
thus the beam search is necessary to keep memory
costs down, otherwise the pairwise scoring would
not be feasible.

We refer interested readers to (Lee et al., 2018)
for a more detailed explanation of the model, in-
cluding the coarse-to-ﬁne pairwise scorer. The
model is trained by the marginal log-likelihood
of the possible correct antecedents. We further
add a binary cross entropy based supervision over
the outputs of the markable scorer sm(i), labeling
markables as 0 if its neither an antecedent or an
anaphor, and 1 if it is either of the two.

3.2 Bridging Resolution with Higher-Order

coreference model

We adapt the model explained above to also solve
the Bridging Resolution task (See Sec. 2.1). That
is, since the set of anaphor markables is given, we
do not intend for the model to identity anaphors, as
well as resolve their antecedents, but only do the
latter. This allows simpliﬁcations to the model and
training setting: ﬁrst, we can pass each anaphor,
together with the document (up to the end of the
anaphor sentence), into the model at a time, predict-
ing one antecedent for the input anaphor given the
relevant part of the document, instead of passing
the entire document into the model and predicting

all the anaphors and their antecedents at the same
time.

This change in setting considerably alleviates
memory constraints, since the pairwise scorer
sp(x, y) is only computed between the one given
anaphor and the possible antecedent markables, as
opposed to an n by n pairwise scorer in the full
bridging setting. This eliminates the need for both
the mention scorer sm(x) and the coarse part of
the coarse to ﬁne pairwise scorer, leaving just the
higher order, "ﬁne" pairwise scorer described in
(Lee et al., 2018). Also, this allows the use of a
cross entropy loss over all possible antecedents (ex-
cluding the dummy class, as we know each labeled
anaphor has an antecedent) for each anaphor. Fi-
nally, we remove the auxiliary supervision over the
markable scorer outputs, as the scorer is no longer
used.

4 Experiments

In this ﬁrst experiment, we perform full bridging
(with gold mentions) over the provided datasets
(Sec. 4.1).

4.1 Datasets

The shared task is comprised of conversational doc-
uments from four domains, annotated with bridg-
ing anaphors using the Universal Anaphora for-
mat (Poesio et al., 1999). The ﬁve domains are:
Switchboard: A subset of the Switchboard Dialog
Act Corpus (Godfrey et al., 1992), this dataset con-
sists of transcribed phone conversations between
two participants about varied topics including child
care, recycling and news media. We ﬁlter out tran-
scribed speech disﬂuencies (such as "emm", "ahh",
"uh", etc) based on a hand-crafted list of bi-grams
as a pre-processing step.
Light: Light is a collection of “character driven,
human-human crowdworker interaction involving
action, emotes and dialogue” (Urbanek et al., 2019)
in the context of a fantasy text adventure game.
Persuasion: A collection of crowdsourced online
conversations where a persuader tries to convince
the persuadee to donate to a charity were intro-
duced in (Wang et al., 2019). An annotated subset
of these conversations are a part of this shared task.
AMI: Some of the transcripts of multi-speaker of-
ﬁce meetings (Carletta, 2006) were annotated with
bridging anaphors. Generally, these conversations
are the longest of the four.

In all four cases, the test set is held out but

50Dataset Anaphors Documents

Avg.
Words

Avg.
Markables

Avg.
Words Between

Switchboard
Light
Persuasion
AMI
Trains-91
Trains-93

603
381
245
851
67
610

11
20
21
7
15
94

1362 ± 339
575 ± 79
474 ± 98
4820 ± 2258
956 ± 691
726 ± 408

366 ± 100
194 ± 28
131 ± 30
1276 ± 595
190 ± 132
148 ± 83

131 ± 228
71 ± 100
41 ± 28
197 ± 560
99 ± 165
66 ± 100

Table 1: Some statistics about the train sets of the datasets used in the shared task. From left to right, we report (i)
the number of anaphors across all documents, (ii) number of documents, (iii) avg. length of each document, (iv)
avg. number of markables in a document, and, (v) avg. number of tokens between an anaphor and its antecedent.

documents from the train set are annotated with
markables, bridging anaphors and their antecedents.
Apart from these, annotated instances from Trains-
1993 (Allen, James and Heeman, Peter A., 1995),
and Trains-1991 (Gross et al., 1993) Spoken Dialog
Corpus, a subset of the ARRAU corpus (Uryupina
et al., 2020) were used for training the models as
well. Table 1 contains further statistics on the train
set of these datasets.

4.2 Experimental Setup

We use Entity-F1 (Pradhan et al., 2012) as our met-
ric for this experiment. We initialize our model, as
outlined in Sec. 3.1 with a transformer based en-
coder with bert-base-uncased weights (De-
vlin et al., 2019) provided on the HuggingFace
Model Hub (Wolf et al., 2020), and freeze it be-
fore subsequent ﬁne-tuning. We use a two layer
network with its hidden dimension and dropouts
speciﬁed below, and a ReLU activation in the feed
forward layers indicated in Eqn. 4.

During training, we set the batch size as 1, vary
the hidden dimension of feed forward subnetworks
between {256, 512}, and their dropout between
{0.0, 0.3, 0.5}. We also experiment with multiple
class weights for auxiliary supervision over the
mention scorer’s outputs, to compensate for the
imbalance between anaphoric and non-anaphoric
markables. We vary the inclusion of hand-crafted
features (see φ(x, y) in Eq. 4 and the description
of hand crafted features in Sec. 3.1) as a part of the
grid search.

This experiment also represents our submission
to the shared task. Corresponding to each of the
aforementioned datasets, we submit a separate
model. The hyperparameters for these models are
found by running a 5-fold cross validation based
grid search where, in each fold, the models are
trained on 80% of instances of the correspond-

ing dataset, and 100% of train instances of the
remaining datasets3, and is evaluated on the held
out instances from the corresponding dataset. Once
the hyperparameters are ﬁxed corresponding to a
dataset, we retrain the model from scratch on all
training instances of all datasets for up to 20 epochs.
The performance of the approach can be found in
Table 2.

4.3 Results

We ﬁnd that our end-to-end approach performs sub-
optimally on all four datasets. Upon closer inspec-
tion, this performance is indicative of the challenge
arisen by the amount of markables in a document.
For instance, a document in Persuasion (F1: 16.28)
contains only 134 markables on average, whereas
a document in AMI (F1: 6.00) contains 1381 mark-
ables. This alludes to an inverse correlation be-
tween the average number of markables in doc-
ument of a dataset and the entity F1 score on it.
Also, the “Avg. words between” column in Ta-
ble 1 indicates that anaphors and antecedents lie
closer to each other in Persuasion when compared
to other datasets. We hypothesise that this corre-
lation, modeled as a part of hand-crafted features
is actively exploited by our approach to increase a
higher relative performance on the task.

Moreover, full bridging can be thought of as a
combination of anaphor identiﬁcation and bridg-
ing resolution. In order to ascertain whether our
approach falters disproportionately on either of the
two tasks, we perform a subsequent bridging resolu-
tion experiment over the train sets of these datasets
(as they contain identiﬁed anaphors).

4.4 Bridging Resolution

We make the changes to the model and training
procedure as outlined in Sec. 3.2. We keep the

3Including Trains-91 and Trains-93 datasets

51Test Set

Ent F1

Track

Setting Baselines

Learning
Framework

Markable
Identiﬁcation Model

Training
Set

Development
Set

Switchboard
Light
Persuasion
AMI

7.79
9.35
16.28
6.00

Bridging
Bridging
Bridging
Bridging

Gold
Gold
Gold
Gold

-
-
-
-

-
-
-
-

-
-
-
-

Alltrain
Alltrain
Alltrain
Alltrain

5CVdev
5CVdev
5CVdev
5CVdev

Table 2: Results, and settings of our submission to the shared task as detailed in Sec. 4.2. As mentioned, we use
the Entity-F1 metric to report the performance. Alltrain refers to the collection of all instances from the training
set of the datasets mentioned in Sec. 4.1, and 5CVdev refer to the development subsets, as they occur in each fold
of the aforementioned 5-fold cross-validation based grid search in Sec. 4.2.

Approach

Random
Skip-Gram
Unnamed

Switchboard
Acc

3.96
21.44
34.00

Light
Acc

6.8
25.21
33.08

Persuasion
Acc

6.94
36.33
55.51

AMI
Acc

7.29
18.10
31.02

Table 3: Results of the experiment outlined in Sec. 4.4.

same hyperparameters (and grid search based hy-
perparameter search) as above, and train one model
to make the predictions. However, unlike before,
this experiment is performed in a 5-fold cross val-
idation setup. Speciﬁcally, in each fold, we treat
20% of instances from each dataset as the test set.
Another 10% are reserved for hyperparameter opti-
misation, and the remaining 70% of the instances
are used for training the model. The performance
reported in Table 3 is averaged over the ﬁve folds.

4.4.1 Baselines
We also report the performance of two baselines,
outlined below.

Random: We select one of the markables at
random which appears either in the ﬁrst sentence
of the document, or up to two sentences behind the
anaphor. This sentence based strategy seemed to
perform better than the one used in (Rösiger et al.,
2018; Poesio et al., 2004).

Skip-Gram: We take the mean of pretrained
Skip-Gram (Mikolov et al., 2013) embeddings of
every token in a markable to create its vector rep-
resentation. Then, using the cosine distance as
a measure of anaphora, we select the antecedent
which lies closest to the anaphor.

4.4.2 Results
We ﬁnd that while the task is far from solved, our
approach is signiﬁcantly better at bridging resolu-
tion, compared to full bridging. That is, our model
is unable to perform anaphor identiﬁcation with a
reasonable accuracy leading to a much worse per-

formance on the full bridging task. Interestingly,
the performance gap between the random and skip-
gram baseline suggests that anaphor and antecedent
markables often lie close in the vector spaces in a
non-trivial amount of cases.

Across different datasets, we observe a similar
trend as in the previous experiment. Our approach
(as well as the skip-gram baseline) achieve its high-
est score on Persuasion, followed by Switchboard,
Light and AMI. The performance gap between
AMI and the next best approach is not as stark
here (-9.3% here; -23% in Exp. 1). This can be
explained by observing the random baseline. Its
performance suggests that the sentences around
the anaphor in AMI conversations have the least
amount of markables to choose from, thereby mak-
ing the task slightly easier.

5 Conclusion

In this work, we experiment with a higher-order
coreference model based end-to-end approach for
the full bridging task over conversational docu-
ments. We ﬁnd that it is unable to model the task’s
complexities, however, its performance on bridging
resolution is signiﬁcantly better. This suggests a
different approach to anaphor detection is needed,
whether it be a stand alone anaphor detection model
or a more guided adaptation of the higher-order
coreference model (Joshi et al., 2019) that better
suits the bridging task. We leave investigations
along this line for the future. We also aim to ex-
periment with approaches that can prime a model
towards conversational documents, such as includ-
ing the SpanBERT (Joshi et al., 2020) pretraining
objective as a pre-ﬁnetuning step.

References

Allen, James and Heeman, Peter A. 1995. Trains spo-

ken dialog corpus.

52Jean Carletta. 2006. Announcing the ami meeting cor-

pus. The ELRA Newsletter, 11(1):3–5.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers), pages 4171–4186. Association for Computa-
tional Linguistics.

John J. Godfrey, Edward Holliman, and Jane McDaniel.
1992. SWITCHBOARD: telephone speech corpus
for research and development. In 1992 IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP ’92, San Francisco, Califor-
nia, USA, March 23-26, 1992, pages 517–520. IEEE
Computer Society.

Derek Gross, James Allen, and David Traum. 1993.

The trains 91 dialogues.

Yufang Hou. 2018a. A deterministic algorithm for
bridging anaphora resolution. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October
31 - November 4, 2018, pages 1938–1948. Associ-
ation for Computational Linguistics.

Yufang Hou. 2018b. Enhanced word representations
for bridging anaphora resolution. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT, New
Orleans, Louisiana, USA, June 1-6, 2018, Volume 2
(Short Papers), pages 1–7. Association for Computa-
tional Linguistics.

Yufang Hou. 2020. Bridging anaphora resolution as
question answering. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020,
pages 1428–1438. Association for Computational
Linguistics.

Yufang Hou, Katja Markert, and Michael Strube. 2013.
Global inference for bridging anaphora resolution.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 907–917.

Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.
Weld, Luke Zettlemoyer, and Omer Levy. 2020.
Improving pre-training by representing
Spanbert:
and predicting spans. Trans. Assoc. Comput. Lin-
guistics, 8:64–77.

Mandar Joshi, Omer Levy, Luke Zettlemoyer, and
Daniel S. Weld. 2019. BERT for coreference reso-
In Proceedings of
lution: Baselines and analysis.
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International

Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, Novem-
ber 3-7, 2019, pages 5802–5807. Association for
Computational Linguistics.

Sopan Khosla, Juntao Yu, Ramesh Manuvinakurike,
Vincent Ng, Massimo Poesio, Michael Strube, and
Carolyn Rosé. 2021. The codi-crac 2021 shared
task on anaphora, bridging, and discourse deixis in
dialogue. In Proceedings of the CODI-CRAC 2021
Shared Task on Anaphora, Bridging, and Discourse
Deixis in Dialogue, Association for Computational
Linguistics.

Hideo Kobayashi and Vincent Ng. 2020. Bridging res-
olution: A survey of the state of the art. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics, COLING 2020, Barcelona,
Spain (Online), December 8-13, 2020, pages 3708–
3721. International Committee on Computational
Linguistics.

Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridg-
In Anaphora Process-
ing resolution in french.
ing and Applications - 8th Discourse Anaphora
and Anaphor Resolution Colloquium, DAARC 2011,
Faro, Portugal, October 6-7, 2011. Revised Selected
Papers, volume 7099 of Lecture Notes in Computer
Science, pages 35–46. Springer.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
ﬁne inference. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers), pages
687–692, New Orleans, Louisiana. Association for
Computational Linguistics.

Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classiﬁcation for ﬁne-grained information
status. In The 50th Annual Meeting of the Associa-
tion for Computational Linguistics, Proceedings of
the Conference, July 8-14, 2012, Jeju Island, Korea
- Volume 1: Long Papers, pages 795–804. The Asso-
ciation for Computer Linguistics.

Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
In 1st International Con-
tations in vector space.
ference on Learning Representations, ICLR 2013,
Scottsdale, Arizona, USA, May 2-4, 2013, Workshop
Track Proceedings.

Massimo Poesio, Florence Bruneseaux, and Laurent
Romary. 1999. The mate meta-scheme for corefer-
ence in dialogues in multiple languages. In ACL’99
Workshop Towards Standards and Tools for Dis-
course Tagging, pages 65–74.

Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,

53Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online. Asso-
ciation for Computational Linguistics.

21-26 July, 2004, Barcelona, Spain, pages 143–150.
ACL.

Sameer Pradhan, Alessandro Moschitti, and Nianwen
Joint Conference on Empiri-
Xue, editors. 2012.
cal Methods in Natural Language Processing and
Computational Natural Language Learning - Pro-
ceedings of the Shared Task: Modeling Multilingual
Unrestricted Coreference in OntoNotes, EMNLP-
CoNLL 2012, July 13, 2012, Jeju Island, Korea.
ACL.

Ina Rösiger. 2018. BASHI: A corpus of wall street
journal articles annotated with bridging links.
In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation, LREC
2018, Miyazaki, Japan, May 7-12, 2018. European
Language Resources Association (ELRA).

Ina Rösiger, Arndt Riester, and Jonas Kuhn. 2018.
Bridging resolution: Task deﬁnition, corpus re-
sources and rule-based experiments. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics, COLING 2018, Santa Fe, New
Mexico, USA, August 20-26, 2018, pages 3516–
3528. Association for Computational Linguistics.

Elena Tognini-Bonelli. 2001. Corpus linguistics at

work, volume 6. John Benjamins Publishing.

Jack Urbanek, Angela Fan, Siddharth Karamcheti,
Saachi Jain, Samuel Humeau, Emily Dinan, Tim
Rocktäschel, Douwe Kiela, Arthur Szlam, and Ja-
son Weston. 2019. Learning to speak and act in
In Proceedings of
a fantasy text adventure game.
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, Novem-
ber 3-7, 2019, pages 673–683. Association for Com-
putational Linguistics.

Olga Uryupina, Ron Artstein, Antonella Bristot, Feder-
ica Cavicchio, Francesca Delogu, Kepa Joseba Ro-
dríguez, and Massimo Poesio. 2020. Annotating a
broad range of anaphoric phenomena, in a variety
the ARRAU corpus. Nat. Lang. Eng.,
of genres:
26(1):95–128.

Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,
Sijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-
suasion for good: Towards a personalized persuasive
dialogue system for social good. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages
5635–5649. Association for Computational Linguis-
tics.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,

54