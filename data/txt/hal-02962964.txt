Asteroid: the PyTorch-based audio source separation
toolkit for researchers
Manuel Pariente, Samuele Cornell, Joris Cosentino, Sunit Sivasankaran,

Efthymios Tzinis, Jens Heitkaemper, Michel Olvera, Fabian-Robert Stöter,

Mathieu Hu, Juan M. Martín-Doñas, et al.

To cite this version:

Manuel Pariente, Samuele Cornell, Joris Cosentino, Sunit Sivasankaran, Efthymios Tzinis, et al..
Asteroid: the PyTorch-based audio source separation toolkit for researchers. Interspeech 2020, Oct
2020, Shanghai, China. ￿hal-02962964￿

HAL Id: hal-02962964

https://inria.hal.science/hal-02962964

Submitted on 9 Oct 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Asteroid: the PyTorch-based audio source separation toolkit for researchers

Manuel Pariente1, Samuele Cornell2, Joris Cosentino1, Sunit Sivasankaran1, Efthymios Tzinis3,
Jens Heitkaemper4, Michel Olvera1, Fabian-Robert St¨oter5, Mathieu Hu1, Juan M. Mart´ın-Do˜nas6,
David Ditter7, Ariel Frank8, Antoine Deleforge1, Emmanuel Vincent1

1Universit´e de Lorraine, CNRS, Inria, LORIA, France 2Universit`a Politecnica delle Marche, Italy
3University of Illinois at Urbana-Champaign, USA 4Universit¨at Paderborn, Germany
5Inria and LIRMM, University of Montpellier, France 6Universidad de Granada, Spain
7Universit¨at Hamburg, Germany 8Technion - Israel Institute of Technology, Israel
https://github.com/mpariente/asteroid

Abstract

This paper describes Asteroid,
the PyTorch-based audio
source separation toolkit for researchers. Inspired by the most
successful neural source separation systems, it provides all neu-
ral building blocks required to build such a system. To im-
prove reproducibility, Kaldi-style recipes on common audio
source separation datasets are also provided. This paper de-
scribes the software architecture of Asteroid and its most im-
portant features. By showing experimental results obtained with
Asteroid’s recipes, we show that our implementations are at
least on par with most results reported in reference papers. The
toolkit is publicly available at github.com/mpariente/asteroid.

Index Terms: source separation, speech enhancement, open-
source software, end-to-end

1. Introduction

Audio source separation, which aims to separate a mixture sig-
nal into individual source signals, is essential to robust speech
processing in real-world acoustic environments [1]. Classical
open-source toolkits such as FASST [2], HARK [3], ManyEars
[4] and openBliSSART [5] which are based on probabilistic
modelling, non-negative matrix factorization, sound source lo-
calization and/or beamforming have been successful in the past
decade. However, they are now largely outperformed by deep
learning-based approaches, at least on the task of single-channel
source separation [6–10].

Several open-source toolkits have emerged for deep
learning-based source separation. These include nussl (North-
western University Source Separation Library) [11], ONSSEN
(An Open-source Speech Separation and Enhancement Library)
[12], Open-Unmix [13], and countless isolated implementations
replicating some important papers 1.

We would like to thank Herv´e Bredin for fruitful discussions about
software design and Kaituo Xu for open-sourcing his Conv-Tasnet im-
plementation.

Experiments presented in this paper were partially carried out using
the Grid’5000 testbed, supported by a scientiﬁc interest group hosted by
Inria and including CNRS, RENATER and several Universities as well
as other organizations (see https://www.grid5000.fr).

High Performance Computing resources were partially provided by

the EXPLOR centre hosted by the University de Lorraine

1kaituoxu/TasNet,
yluo42/TAC,
JusperLee/Conv-TasNet,
JusperLee/Dual-Path-RNN-Pytorch,
tky1117/DNN-based source separation ShiZiqiang/dual-path-RNNs-
DPRNNs-based-speech-separation

kaituoxu/Conv-TasNet,

Both nussl and ONSSEN are written in PyTorch [14] and
provide training and evaluation scripts for several state-of-the
art methods. However, data preparation steps are not provided
and experiments are not easily conﬁgurable from the com-
mand line. Open-Unmix does provide a complete pipeline from
data preparation until evaluation, but only for the Open-Unmix
model on the music source separation task. Regarding the iso-
lated implementations, some of them only contain the model,
while others provide training scripts but assume that training
data has been generated. Finally, very few provide the complete
pipeline. Among the ones providing evaluation scripts, differ-
ences can often be found, e.g., discarding short utterances or
splitting utterances in chunks and discarding the last one.

This paper describes Asteroid (Audio source separation
on Steroids), a new open-source toolkit for deep learning-based
audio source separation and speech enhancement, designed for
researchers and practitioners. Based on PyTorch, one of the
most widely used dynamic neural network toolkits, Asteroid
is meant to be user-friendly, easily extensible, to promote repro-
ducible research, and to enable easy experimentation. As such,
it supports a wide range of datasets and architectures, and comes
with recipes reproducing some important papers. Asteroid is
built on the following principles:

1. Abstract only where necessary, i.e., use as much native

PyTorch code as possible.

2. Allow importing third-party code with minimal changes.

3. Provide all steps from data preparation to evaluation.

4. Enable recipes to be conﬁgurable from the command

line.

We present the audio source separation framework in Section 2.
We describe Asteroid’s main features in Section 3 and their
implementation in Section 4. We provide example experimental
results in Section 5 and conclude in Section 6.

2. General framework

While Asteroid is not limited to a single task, single-channel
source separation is currently its main focus. Hence, we will
only consider this task in the rest of the paper. Let x be a single
channel recording of J sources in noise:

x(t) =

J
(cid:88)

j=1

sj(t) + n(t),

(1)

where {sj}j=1..J are the source signals and n is an additive
noise signal. The goal of source separation is to obtain source
estimates {(cid:98)sj}j=1..J given x.

Most state-of-the-art neural source separation systems fol-
low the encoder-masker-decoder approach depicted in Fig. 1
[8, 9, 15, 16]. The encoder computes a short-time Fourier trans-
form (STFT)-like representation X by convolving the time-
domain signal x with an analysis ﬁlterbank. The representa-
tion X is fed to the masker network that estimates a mask for
each source. The masks are then multiplied entrywise with X to
obtain sources estimates {(cid:98)Sj}j=1..J in the STFT-like domain.
The time-domain source estimates {(cid:98)sj}j=1..J are ﬁnally ob-
tained by applying transposed convolutions to {(cid:98)Sj}j=1..J with
a synthesis ﬁlterbank. The three networks are jointly trained
using a loss function computed on the masks or their embed-
dings [6, 17, 18], on the STFT-like domain estimates [7, 15, 19],
or directly on the time-domain estimates [8–10, 16, 20].

Figure 1: Typical encoder-masker-decoder architecture.

3. Functionality

Asteroid follows the encoder-masker-decoder approach, and
provides various choices of ﬁlterbanks, masker networks, and
loss functions.
It also provides training and evaluation tools
and recipes for several datasets. We detail each of these below.

3.1. Analysis and synthesis ﬁlterbanks

As shown in [20–23], various ﬁlterbanks can be used to train
end-to-end source separation systems. A natural abstraction is
to separate the ﬁlterbank object from the encoder and decoder
objects. This is what we do in Asteroid. All ﬁlterbanks in-
herit from the Filterbank class. Each Filterbank can be
combined with an Encoder or a Decoder, which respectively
follow the nn.Conv1d and nn.ConvTranspose1d interfaces
from PyTorch for consistency and ease of use. Notably, the
STFTFB ﬁlterbank computes the STFT using simple convolu-
tions, and the default ﬁlterbank matrix is orthogonal.

Asteroid supports free ﬁlters [8,9], discrete Fourier trans-
form (DFT) ﬁlters [19, 21], analytic free ﬁlters [22], improved
parameterized sinc ﬁlters [22, 24] and the multi-phase Gam-
matone ﬁlterbank [23]. Automatic pseudo-inverse computa-
tion and dynamic ﬁlters (computed at runtime) are also sup-
ported. Because some of the ﬁlterbanks are complex-valued, we
provide functions to compute magnitude and phase, and apply
magnitude or complex-valued masks. We also provide inter-
faces to NumPy [25] and torchaudio2. Additionally, Grifﬁn-

2github.com/pytorch/audio

Lim [26,27] and multi-input spectrogram inversion (MISI) [28]
algorithms are provided.

3.2. Masker network

Asteroid provides implementations of widely used masker
networks: TasNet’s stacked long short-term memory (LSTM)
network [8], Conv-Tasnet’s temporal convolutional network
(with or without skip connections) [9], and the dual-path re-
current neural network (DPRNN) in [16]. Open-Unmix [13] is
also supported for music source separation.

3.3. Loss functions — Permutation invariance

Asteroid supports several loss functions: mean squared er-
ror, scale-invariant signal-to-distortion ratio (SI-SDR) [9, 29],
scale-dependent SDR [29], signal-to-noise ratio (SNR), percep-
tual evaluation of speech quality (PESQ) [30], and afﬁnity loss
for deep clustering [6].

Whenever the sources are of the same nature, a permuta-
tion-invariant (PIT) loss shall be used [7, 31]. Asteroid pro-
vides an optimized, versatile implementation of PIT losses. Let
s = [sj(t)]t=0...T
j=1...J be the matrices of
true and estimated source signals, respectively. We denote as
(cid:98)sσ = [(cid:98)sσ(j)(t)]t=0...T
j=1...J a permutation of s by σ ∈ SJ , where
SJ is the set of permutations of [1, ..., J]. A PIT loss LPIT is
deﬁned as

j=1...J and (cid:98)s = [(cid:98)sj(t)]t=0...T

LPIT(θ) = min
σ∈SJ
where L is a classical (permutation-dependent) loss function,
which depends on the network’s parameters θ through (cid:98)sσ.

L((cid:98)sσ, s),

(2)

We assume that, for a given permutation hypothesis σ, the

loss L((cid:98)sσ, s) can be written as

L((cid:98)sσ, s) = G(cid:0)F((cid:98)sσ(1), s1), ..., F((cid:98)sσ(J), sJ )(cid:1)
(3)
where sj = [sj(0), . . . , sj(T )], (cid:98)sj = [(cid:98)sj(0), . . . , (cid:98)sj(T )], F
computes the pairwise loss between a single true source and its
hypothesized estimate, and G is the reduce function, usually a
simple mean operation. Denoting by F the J × J pairwise loss
matrix with entries F((cid:98)si, sj), we can rewrite (2) as
(cid:1)
G(cid:0)Fσ(1)1, ..., Fσ(J)J

(4)

LPIT(θ) = min
σ∈SJ

and reduce the computational complexity from J! to J 2 by pre-
computing F’s terms. Taking advantage of this, Asteroid pro-
vides PITLossWrapper, a simple yet powerful class that can
efﬁciently turn any pairwise loss F or permutation-dependent
loss L into a PIT loss.

3.4. Datasets

Asteroid provides baseline recipes for the following datasets:
wsj0-2mix and wsj0-3mix [6], WHAM [32], WHAMR [33],
LibriMix [34] FUSS [35], Microsoft’s Deep Noise Suppres-
sion challenge dataset (DNS) [36], SMS-WSJ [37], Kinect-
WSJ [38], and MUSDB18 [39]. Their characteristics are sum-
marized and compared in Table 1. wsj0-2mix and MUSDB18
are today’s reference datasets for speech and music separa-
tion, respectively. WHAM, WHAMR, LibriMix, SMS-WSJ
and Kinect-WSJ are recently released datasets which address
some shortcomings of wsj0-2mix. FUSS is the ﬁrst open-source
dataset to tackle the separation of arbitrary sounds. Note that
wsj0-2mix is a subset of WHAM which is a subset of WHAMR.

Mixture waveformSeparatedwaveformsEncoderDecoderSTFT-likerep.Maskedrep.Maskerwsj0-mix WHAM WHAMR Librimix

DNS

SMS-WSJ Kinect-WSJ MUSDB18

FUSS

Source types
# sources
Noise
Reverb
# channels
Sampling rate
Hours
Release year

speech
2 or 3

1
16k
30
2015

speech
2
(cid:33)

1
16k
30
2019

speech
2
(cid:33)
(cid:33)
1
16k
30
2019

speech
2 or 3
(cid:33)

speech
1
(cid:33)

1
16k
210
2020

1
16k
100 (+aug.)
2020

speech
2
*
(cid:33)
6
16k
85
2019

speech
2
(cid:33)
(cid:33)
4
16k
30
2019

music
4

(cid:33)
2
16k
10
2017

sounds
0 to 4
(cid:33)**
(cid:33)
1
16k
55 (+aug.)
2020

Table 1: Datasets currently supported by Asteroid. * White sensor noise. ** Background environmental scenes.

3.5. Training

For training source separation systems, Asteroid offers a
thin wrapper around PyTorch-Lightning [40] that seam-
lessly enables distributed training, experiment
logging and
more, without sacriﬁcing ﬂexibility. Regarding the optimiz-
ers, we also rely on native PyTorch and torch-optimizer 3.
PyTorch provides basic optimizers such as SGD and Adam and
torch-optimizer provides state-of-the art optimizers such as
RAdam, Ranger or Yogi.

3.6. Evaluation

Evaluation is performed using pb bss eval4, a sub-toolkit of
pb bss5 [41] written for evaluation. It natively supports most
metrics used in source separation: SDR, signal-to-interference
ratio (SIR), signal-to-artifacts ratio (SAR) [42], SI-SDR [29],
PESQ [43], and short-time objective intelligibility (STOI) [44].

4. Implementation

Asteroid follows Kaldi-style recipes [45], which involve sev-
eral stages as depicted in Fig. 2. These recipes implement the
entire pipeline from data download and preparation to model
training and evaluation. We show the typical organization of a
recipe’s directory in Fig. 3. The entry point of a recipe is the
run.sh script which will execute the following stages:

• Stage 0: Download data that is needed for the recipe.
• Stage 1: Generate mixtures with the ofﬁcial scripts, op-

tionally perform data augmentation.

• Stage 2: Gather data information into text ﬁles expected

by the corresponding DataLoader.

• Stage 3: Train the source separation system.
• Stage 4: Separate test mixtures and evaluate.

In the ﬁrst stage, necessary data is downloaded (if available)
into a storage directory speciﬁed by the user. We use the ofﬁcial
scripts provided by the dataset’s authors to generate the data,
and optionally perform data augmentation. All the information
required by the dataset’s DataLoader such as ﬁlenames and
paths, utterance lengths, speaker IDs, etc., is then gathered into
text ﬁles under data/. The training stage is ﬁnally followed by
the evaluation stage. Throughout the recipe, log ﬁles are saved
under logs/ and generated data is saved under exp/.

3github.com/jettify/pytorch-optimizer
4pypi.org/project/pb bss eval
5github.com/fgnt/pb bss

Figure 2: Typical recipe ﬂow in Asteroid.

data/
exp/
logs/
local/

# Output of stage 2
# Store experiments
# Store exp logs

conf.yml
other_scripts.py

# Training config
# Dataset specific

utils/

parse_options.sh
other_scripts.sh

run.sh
model.py
train.py
eval.py

# Kaldi bash parser
# Package-level utils
# Entry point
# Model definition
# Training scripts
# Evaluation script

Figure 3: Typical directory structure of a recipe.

As can be seen in Fig. 4, the model class, which is a direct
subclass of PyTorch’s nn.Module, is deﬁned in model.py.
It is imported in both training and evaluation scripts. Instead
of deﬁning constants in model.py and train.py, most of
them are gathered in a YAML conﬁguration ﬁle conf.yml.
An argument parser is created from this conﬁguration ﬁle to al-
low modiﬁcation of these values from the command line, with
run.sh passing arguments to train.py. The resulting mod-
iﬁed conﬁguration is saved in exp/ to enable future reuse.
Other arguments such as the experiment name, the number of
GPUs, etc., are directly passed to run.sh.

5. Example results

To illustrate the potential of Asteroid, we compare the perfor-
mance of state-of-the-art methods as reported in the correspond-
ing papers with our implementation. We do so on two common
source separation datasets: wsj0-2mix [6] and WHAMR [33].
wsj0-2mix consists of a 30 h training set, a 10 h validation
set, and a 5 h test set of single-channel two-speaker mixtures
without noise and reverberation. Utterances taken from the
Wall Street Journal (WSJ) dataset are mixed together at random
SNRs between −5 dB and 5 dB. Speakers in the test set are dif-
ferent from those in the training and validation sets. WHAMR

Figure 4: Simpliﬁed code example.

[33] is a noisy and reverberant extension of wsj0-2mix. Experi-
ments are conducted on the 8 kHz min version of both datasets.
Note that we use wsj0-2mix separation, WHAM’s clean sep-
aration, and WHAMR’s anechoic clean separation tasks inter-
changeably as the datasets only differ by a global scale.

Table 2 reports SI-SDR improvements (SI-SDRi) on the test
set of wsj0-2mix for several well-known source separation sys-
tems. In Table 3, we reproduce Table 2 from [33] which reports
the performance of an improved TasNet architecture (more re-
current units, overlap-add for synthesis) on the four main tasks
of WHAMR: anechoic separation, noisy anechoic separation,
reverberant separation, and noisy reverberant separation. On
all four tasks, Asteroid’s recipes achieved better results than
originally reported, by up to 2.6 dB.

Reported Using Asteroid

Deep Clustering [46]
TasNet [8]
Conv-TasNet [9]
TwoStep [15]
DPRNN (ks = 16) [16]
DPRNN (ks = 2) [16]
Wavesplit [10]

9.6
10.8
15.2
16.1
16.0
18.8
20.4

9.8
15.0
16.2
15.2
17.7
19.3
-

Table 2: SI-SDRi (dB) on the wsj0-2mix test set for several ar-
chitectures. ks stands for for kernel size, i.e., the length of the
encoder and decoder ﬁlters.

Reported Using Asteroid

Noise Reverb

(cid:33)

(cid:33)

(cid:33)
(cid:33)

[33]

14.2
12.0
8.9
9.2

16.8
13.7
10.6
11.0

Table 3: SI-SDRi (dB) on the four WHAMR tasks using the im-
proved TasNet architecture in [33].

In both Tables 2 and 3, we can see that our implementations
outperform the original ones in most cases. Most often, the

aforementioned architectures are trained on 4-second segments.
For the architectures requiring a large amount of memory (e.g.,
Conv-TasNet and DPRNN), we reduce the length of the training
segments in order to increase the batch size and stabilize gradi-
ents. This, as well as using a weight decay of 10−5 for recurrent
architectures increased the ﬁnal performance of our systems.

Asteroid was designed such that writing new code is very
simple and results can be quickly obtained. For instance, start-
ing from stage 2, writing the TasNet recipe used in Table 3 took
less than a day and the results were simply generated with the
command in Fig. 5, where the GPU ID is speciﬁed with the
--id argument.

n=0
for task in clean noisy reverb reverb_noisy

do
./run.sh --stage 3 --task $task --id $n
n=$(($n+1))

done

Figure 5: Example command line usage.

6. Conclusion

In this paper, we have introduced Asteroid, a new open-source
audio source separation toolkit designed for researchers and
practitioners. Comparative experiments show that results ob-
tained with Asteroid are competitive on several datasets and
for several architectures. The toolkit was designed such that it
can quickly be extended with new network architectures or new
benchmark datasets. In the near future, pre-trained models will
be made available and we intend to interface with ESPNet to
enable end-to-end multi-speaker speech recognition.

7. References

[1] E. Vincent, T. Virtanen, and S. Gannot, Audio Source Separation

and Speech Enhancement, 1st ed. Wiley, 2018.

[2] Y. Sala¨un, E. Vincent, N. Bertin, N. Souvira`a-Labastie, X. Jau-
reguiberry, D. T. Tran, and F. Bimbot, “The Flexible Audio Source
Separation Toolbox Version 2.0,” ICASSP Show & Tell, 2014.

conf.ymltrain.pymodel.py[3] K. Nakadai, H. G. Okuno, H. Nakajima, Y. Hasegawa, and H. Tsu-
jino, “An open source software system for robot audition HARK
and its evaluation,” in Humanoids, 2008, pp. 561–566.

[4] F. Grondin, D. L´etourneau, F. Ferland, V. Rousseau, and
F. Michaud, “The ManyEars open framework,” Autonomous
Robots, vol. 34, pp. 217–232, 2013.

[5] B. Schuller, A. Lehmann, F. Weninger, F. Eyben, and G. Rigoll,
“Blind enhancement of the rhythmic and harmonic sections by
nmf: Does it help?” in ICA, 2009, pp. 361–364.

[6] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clus-
tering: discriminative embeddings for segmentation and separa-
tion,” in ICASSP, 2016, pp. 31–35.

[7] D. Yu, M. Kolbæk, Z. Tan, and J. Jensen, “Permutation invari-
ant training of deep models for speaker-independent multi-talker
speech separation,” in ICASSP, 2017, pp. 241–245.

[8] Y. Luo and N. Mesgarani, “TasNet: Time-domain audio separa-
tion network for real-time, single-channel speech separation,” in
ICASSP, 2018, pp. 696–700.

[9] ——, “Conv-TasNet: Surpassing ideal time–frequency magnitude
masking for speech separation,” IEEE/ACM Trans. Audio, Speech,
Lang. Process., vol. 27, no. 8, pp. 1256–1266, 2019.

[10] N. Zeghidour and D. Grangier,

“Wavesplit:

speech separation by speaker clustering,”
arXiv:2002.08933, 2020.

End-to-end
arXiv preprint

[11] E. Manilow, P. Seetharaman, and B. Pardo, “The Northwestern
University Source Separation Library,” in ISMIR, 2018, pp. 297–
305.

[12] Z. Ni and M. I. Mandel, “Onssen: an open-source speech separa-
tion and enhancement library,” arXiv preprint arXiv:1911.00982,
2019.

[13] F.-R. St¨oter, S. Uhlich, A. Liutkus, and Y. Mitsufuji, “Open-
Unmix - a reference implementation for music source separation,”
J. Open Source Soft., vol. 4, no. 41, p. 1667, 2019.

[14] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury et al., “Py-
Torch: An imperative style, high-performance deep learning li-
brary,” arXiv preprint arXiv:1912.01703, 2019.

[15] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and
P. Smaragdis, “Two-step sound source separation: Training on
learned latent targets,” in ICASSP, 2020, pp. 31–35.

[16] Y. Luo, Z. Chen, and T. Yoshioka, “Dual-path RNN: Efﬁcient long
sequence modeling for time-domain single-channel speech sepa-
ration,” in ICASSP, 2020, pp. 46–50.

[17] Y. Isik, J. Le Roux, Z. Chen, S. Watanabe, and J. R. Hershey,
“Single-channel multi-speaker separation using deep clustering,”
in Interspeech, 2016, pp. 545–549.

[18] Z. Chen, Y. Luo, and N. Mesgarani, “Deep attractor network
for single-microphone speaker separation,” in ICASSP, 2017, pp.
246–250.

[19] J. Heitkaemper, D. Jakobeit, C. Boeddeker, L. Drude, and
R. Haeb-Umbach, “Demystifying TasNet: A dissecting ap-
proach,” in ICASSP, 2020, pp. 6359–6363.

[20] F. Bahmaninezhad, J. Wu, R. Gu, S.-X. Zhang, Y. Xu, M. Yu, and
D. Yu, “A comprehensive study of speech separation: Spectro-
gram vs waveform separation,” in Interspeech, 2019, pp. 4574–
4578.

[21] I. Kavalerov, S. Wisdom, H. Erdogan, B. Patton, K. Wilson,
J. Le Roux, and J. R. Hershey, “Universal sound separation,” in
WASPAA, 2019, pp. 175–179.

[22] M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, “Filterbank
design for end-to-end speech separation,” in ICASSP, 2020, pp.
6364–6368.

[23] D. Ditter and T. Gerkmann, “A multi-phase gammatone ﬁlterbank

for speech separation via TasNet,” in ICASSP, 2020, pp. 36–40.

[24] M. Ravanelli and Y. Bengio, “Speaker recognition from raw wave-

form with SincNet,” in SLT, 2018, pp. 1021–1028.

[25] S. van der Walt, S. C. Colbert, and G. Varoquaux, “The NumPy ar-
ray: A structure for efﬁcient numerical computation,” Computing
in Science and Engineering, vol. 13, no. 2, pp. 22–30, 2011.
[26] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time Fourier transform,” IEEE Trans. Acoust., Speech, Signal Pro-
cess., vol. 32, no. 2, pp. 236–243, 1984.

[27] N. Perraudin, P. Balazs, and P. Søndergaard, “A fast Grifﬁn-Lim

algorithm,” in WASPAA, 2013, pp. 1–4.

[28] D. Gunawan and D. Sen, “Iterative phase estimation for the syn-
thesis of separated sources from single-channel mixtures,” IEEE
Signal Process. Letters, vol. 17, no. 5, pp. 421–424, 2010.
[29] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “SDR —
half-baked or well done?” in ICASSP, 2019, pp. 626–630.
[30] J. M. Mart´ın-Do˜nas, A. M. Gomez, J. A. Gonzalez, and A. M.
Peinado, “A deep learning loss function based on the perceptual
evaluation of the speech quality,” IEEE Signal Process. Letters,
vol. 25, no. 11, pp. 1680–1684, 2018.

[31] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multitalker
speech separation with utterance-level permutation invariant train-
ing of deep recurrent neural networks,” IEEE/ACM Trans. Audio,
Speech, Lang. Process., vol. 25, no. 10, pp. 1901–1913, 2017.
[32] G. Wichern, J. Antognini, M. Flynn, L. R. Zhu, E. McQuinn,
D. Crow, E. Manilow, and J. Le Roux, “WHAM!: extending
speech separation to noisy environments,” in Interspeech, 2019,
pp. 1368–1372.

[33] M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux,
“WHAMR!: Noisy and reverberant single-channel speech sepa-
ration,” in ICASSP, 2020, pp. 696–700.

[34] J. Cosentino, S. Cornell, M. Pariente, A. Deleforge, and E. Vin-
cent, “LibriMix: An open-source dataset for generalizable speech
separation,” arXiv preprint arXiv:2005.11262, 2020.

[35] S. Wisdom, H. Erdogan, D. P. W. Ellis, R. Serizel, N. Tur-
pault, E. Fonseca, J. Salamon, P. Seetharaman, and J. R. Hershey,
“What’s all the fuss about free universal sound separation data?”
in preparation, 2020.

[36] C. K. A. Reddy, E. Beyrami, H. Dubey, V. Gopal, R. Cheng
et al., “The Interspeech 2020 deep noise suppression challenge:
Datasets, subjective speech quality and testing framework,” arXiv
preprint arXiv:2001.08662, 2020.

[37] L. Drude, J. Heitkaemper, C. Boeddeker, and R. Haeb-Umbach,
“SMS-WSJ: Database, performance measures, and baseline
recipe for multi-channel source separation and recognition,” arXiv
preprint arXiv:1910.13934, 2019.

[38] S. Sivasankaran, E. Vincent, and D. Fohr, “Analyzing the impact
of speaker localization errors on speech separation for automatic
speech recognition,” 2020.

[39] Z. Raﬁi, A. Liutkus, F.-R. St¨oter, S. I. Mimilakis, and R. Bittner,

“The MUSDB18 corpus for music separation,” 2017.

[40] W. Falcon et al.,

“Pytorch lightning,” https://github.com/

PytorchLightning/pytorch-lightning, 2019.

[41] L. Drude and R. Haeb-Umbach, “Tight integration of spatial and
spectral features for BSS with deep clustering embeddings,” in
Interspeech, 2017, pp. 2650–2654.

[42] E. Vincent, R. Gribonval, and C. F´evotte, “Performance measure-
ment in blind audio source separation,” IEEE/ACM Trans. Audio,
Speech, Lang. Process., vol. 14, no. 4, pp. 1462–1469, 2006.
[43] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Per-
ceptual evaluation of speech quality (PESQ) — a new method for
speech quality assessment of telephone networks and codecs,” in
ICASSP, vol. 2, 2001, pp. 749–752.

[44] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “An al-
gorithm for intelligibility prediction of time–frequency weighted
noisy speech,” IEEE/ACM Trans. Audio, Speech, Lang. Process.,
vol. 19, no. 7, pp. 2125–2136, 2011.

[45] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek et al.,

“The Kaldi speech recognition toolkit,” in ASRU, 2011.

[46] Z. Wang, J. Le Roux, and J. R. Hershey, “Alternative objective
functions for deep clustering,” in ICASSP, 2018, pp. 686–690.

