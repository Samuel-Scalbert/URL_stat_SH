TAG: Learning Timed Automata from Logs
Lénaïg Cornanguer, Christine Largouët, Laurence Rozé, Alexandre Termier

To cite this version:

Lénaïg Cornanguer, Christine Largouët, Laurence Rozé, Alexandre Termier. TAG: Learning Timed
Automata from Logs. AAAI 2022 - 36th AAAI Conference on Artificial Intelligence, Feb 2022, Virtual,
Canada. pp.1-9. ￿hal-03564455￿

HAL Id: hal-03564455

https://inria.hal.science/hal-03564455

Submitted on 11 Feb 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

TAG: Learning Timed Automata from Logs

L´ena¨ıg Cornanguer1, Christine Largou¨et2, Laurence Roz´e3, Alexandre Termier4
1Inria, Univ Rennes, CNRS, IRISA
2Institut Agro, Univ Rennes, Inria, CNRS, IRISA
3Univ Rennes, INSA Rennes, CNRS, Inria, IRISA
4Univ Rennes, Inria, CNRS, IRISA
lenaig.cornanguer@irisa.fr

Abstract

Event logs are often one of the main sources of information
to understand the behavior of a system. While numerous ap-
proaches have extracted partial information from event logs,
in this work, we aim at inferring a global model of a system
from its event logs.
We consider real-time systems, which can be modeled with
Timed Automata: our approach is thus a Timed Automata
learner. There is a handful of related work, however, they
might require a lot of parameters or produce Timed Automata
that either are undeterministic or lack precision. In contrast,
our proposed approach, called TAG, requires only one pa-
rameter and learns a deterministic Timed Automaton hav-
ing a good tradeoff between accuracy and complexity of the
automata. This allows getting an interpretable and accurate
global model of the real-time system considered. Our experi-
ments compare our approach to the related work and demon-
strate its merits.

Introduction
The classic way of understanding a complex physical, or
software system is to perform a manual analysis of this sys-
tem, in order to produce a model, that should be as accu-
rate as possible. This process is tedious and extremely time-
consuming for the human(s) performing it, especially if the
system is large and with many possible states. Due to this
complexity, it is not done in practice for most running sys-
tems. Instead, the systems are equipped to produce detailed
logs, that can help to detect odd behaviors on the ﬂy, or at
least allow to perform a post-mortem analysis after a fail-
ure happened. Data mining techniques can help analyzing
these logs to discover human-understandable local regulari-
ties in the log, for example in the form of episodes or chron-
icles (Zou et al. 2010; Tatti and Vreeken 2012; Sahugu`ede,
Le Corronc, and Le Lann 2018), that can directly correspond
to parts of the global model. Process mining techniques
(van der Aalst 2016) are dedicated to logs exploration, with
notably one part dedicated to the construction of a global
untimed model in the form of Petri net or BPMN (Business
Process Model and Notation) to realize performance analy-
sis or conformance checking. There are recent attempts to
include time in the construction of the model and not only
in the posterior model analysis (Tax et al. 2019) but time
is still considered in a qualitative way. One can also men-

tion the speciﬁcation mining ﬁeld which focuses on program
logs to learn speciﬁcations in the form of untimed state ma-
chines (Ammons, Bod´ık, and Larus 2002) or Timed Regular
Expressions (given its untimed template) (Narayan and Fis-
chmeister 2019). However, none of those approaches con-
siders the problem of discovering a human-understandable
global temporal model of the real-time system having gen-
erated the log. This is the problem tackled in this paper.

Including time quantitatively in models is crucial for
many applications such as self-driving cars or security pro-
tocols. To model real-time systems, the formalism of Timed
Automata (TA) has been deﬁned by Alur and Dill (Alur
and Dill 1994). TA extends the formalism of automata by
adding clocks expressing explicit timing constraints on the
model. TA are well studied, both in theory and practice.
The success of TA comes from a powerful formalism with
high expressiveness associated with efﬁcient algorithms and
tool support. TA have been applied for the analysis of many
real-time systems for solving problems as diverse as opti-
mal planning, scheduling, or controlled synthesis (Clarke
et al. 2018). They have proved to be relevant outside the
ﬁeld of real-time systems, for biological ecosystems, home
care plans, agricultural processes, or software systems. In
the wide range of applications possible with TA, we can cite
optimization of animal waste allocation to crops (H´elias,
Guerrin, and Steyer 2008), synthetic data generation from
real data for privacy matter (Connes, De La Higuera, and
Le Capitaine 2021), or anomaly detection in water treatment
plant (Xu, Ali, and Yue 2021). Moreover, the graphical rep-
resentation and explicit timing constraints between events
can offer a human-understandable view of the model, as long
as the size remains limited.

Our objective is thus to learn a timed automaton con-
sistent with the logs of the observed system. There is a
large body of work on learning (non-timed) automata and
state machines stemming from the grammatical inference
ﬁeld (Angluin 1987; de la Higuera 2010) and extended for
various classes of models (e.g., DFAs, mealy machines,
Moore machines). Several algorithms exist (e.g., EDSM,
Alergia, MDI) and efﬁcient implementations can be found
in LearnLib (Isberner, Howar, and Steffen 2015).

However, learning a model where time constraints play a
key role is much more complicated and only a few works
have tackled this problem. Learning timed automata is def-

initely a new and promising ﬁeld of research. While some
of the existing approaches rely on interactions with a user
(active learning setting) (An et al. 2020; Henry, J´eron, and
Markey 2020; Grinchtein, Jonsson, and Pettersson 2006),
we focus our contribution on the classical “passive learn-
ing” setting, where the learner has only access to the input
logs. In this setting, three main works have been proposed:
RTI+ (Verwer, de Weerdt, and Witteveen 2012), GenProgTA
(Tappler et al. 2018), and Timed k-Tail (Pastore, Micucci,
and Mariani 2017). These works can achieve the discovery
of TA from logs, however, they have several drawbacks, ei-
ther in the number of parameters, the lack of precision of the
TA learned, or not outputting a deterministic TA.

In this paper we propose a new algorithm, called Timed
Automata Generator (TAG). TAG learns a timed automaton
to describe and understand a time-dependent system only
from its generated logs, without any a priori knowledge.
Compared to other works dedicated to TA learning, our con-
tributions are the following :

• TAG, a new algorithm that easily automates the TA learn-
ing process from logs since it only requires one param-
eter. This parameter controls the level of generalization
of the learned TA, and thus the level of interpretability
of the generated model. The learned TA is deterministic
which contributes to the model understanding.

• An original pipeline that allows to apply model-checking
techniques to query the most complex learned models
that are not visually understandable.

• The ﬁrst extensive experimental study and comparison
with state-of-the-art algorithms for learning TA. These
experiments show that TAG provide the best perfor-
mances both for scalability and for the precision/recall
trade-off. We also provide an experimentation on real-
world data from logs of TV programs.

Background
We start by introducing formally the notion of Timed Au-
tomata, ﬁrst through a simple example and then with the
deﬁnitions and notations used throughout the paper.

Our example considers modeling the behavior of a simple
light in a meeting room with a timed automaton, depicted
in Figure 1. A single sensor sends a press event when the
switch is pressed. The bulb can be off or on with low or high
lighting. When the bulb is off, a single press on the switch
turns the low light on, while a double press (in less than 2
sec.) makes the light bright. In case the delay is too long
between the ﬁrst and second press, the light turns off.

The TA deﬁnes the behavior of the light by three states
(off, light, bright) and the event press tags the transitions. A
single clock c measures the time between each event. In TA,
transitions are instantaneous and allow the reset of clocks.
When a press event occurs, a transition is triggered between
the states off and light, and the clock c is reset. If the next
press event occurs after 3 time units, the system moves back
to off.

In this illustrative example, we notice that time is of
paramount importance. Without knowledge about the value
of the clock, the light state cannot be inferred.

press
c := 0

press
c ∈ [0, 2]

OFF

light

bright

press
c ∈ [3, +∞]

press

Figure 1: Timed Automaton of a light.

We now deﬁne formally the notions of clock and timed
automata, followed by the semantics of the TA and the lan-
guage it entails.

Clocks. Let T be a set of time domains. Let C be a ﬁnite
set of variables called clocks. A clock valuation is a mapping
v : C → T which assigns to each clock a time value. Let
T C be the set of all clock valuations over C. The set of clock
constraints Const(C, T ) is the set of constraints c ∼ t with
c ∈ C, ∼ ∈ {<, ≤, =, ≥, >}, and t ∈ T .

Timed Automaton. A non-deterministic timed automa-
ton A ∈ N T A is a tuple A = (Q, Σ, C, E, q0, F) where
Q is a ﬁnite set of locations, Σ is a ﬁnite set of events or
symbols, C is a ﬁnite set of clocks, E is a ﬁnite set of tran-
sitions, and q0 ∈ Q is the initial location. E ⊆ Q × Σ ×
Const(C, T ), ×P(C) × Q is a ﬁnite set of transitions of
the form (q, a, g, r, q(cid:48)) where q and q(cid:48) are respectively the
source location and destination location, g is a guard i.e., a
constraint on the value of a clock, and r is the set of clocks
being reset on the transition.

Semantics. The semantics of A ∈ N T A is given by
the timed transition system (S, s0, T, Σ, E) where S =
{(q, v) ∈ Q × T C} is the set of states with s0 = (q0, 0).
E ⊆ S × (Σ ∪ T ) × S is the transition relation. The set E
contains two kinds of transitions: timed transitions (delays)
when the clocks valuations evolve in a location, and discrete
transitions (jumps) denoted (q, v) →a (q(cid:48), v(cid:48)) with a ∈ Σ,
expressing that there exists a transition (q, a, g, r, q(cid:48)) ∈ E
such that the valuation v satisﬁes the guard g and ∀c ∈ C if
c ∈ r then v(cid:48)(c) = 0 otherwise v(cid:48)(c) = v(c).

Language of Timed Automaton. An untimed word w ∈
Σ∗ is a ﬁnite sequence of input symbols (elements from Σ,
called alphabet). A timed word tw ∈ (Σ × T )∗ is a ﬁnite se-
quence of input symbols and timestamps (non-decreasing).
In the following, the set of all possible timed words over Σ
and T is called T S(Σ, T ). A timed word tw induces a set of
runs over a timed automaton. A word tw is consistent with
an automaton A if there exists a run ending in an accept-
ing location. For a timed automaton A, the language L(A)
denotes the set of timed words accepted by A.

Sub-classes of Timed Automata. To learn efﬁciently
timed automata from logs, timed-automata formalism has
been restricted to sub-classes (Verwer, de Weerdt, and Wit-
teveen 2012). Deterministic timed automaton (DT A) does
not allow, from a location, two transitions having the same
symbol and overlapping guards towards two different tar-
get locations. 1DT A is a DT A restricted to one clock.
DRT A (Deterministic Real-Time Automata) is a sub-class
of 1DT A where the clock represents the time delay between
two consecutive events. The guard is then deﬁned as a closed
interval. Each transition resets the clock, and to be triggered

the time value of the event should satisfy the guard transi-
tion. A PDRTA (Probabilistic Deterministic Real-Time Au-
tomata) adds probabilities on the DRTA events.

State-of-the-Art
Learning a TA from logs is a complex problem and it has
already been proved that the identiﬁcation of an untimed
automaton is NP-complete (Gold 1978). All the algorithms
presented in this section focus on TA sub-classes and take as
input a set of timestamped event sequences, or timed words.
In our context, we don’t need to distinguish the notions of
source and location, and we will use the term “state” for
both.

GenProgTA. GenProgTA (Tappler et al. 2018) is an al-
gorithm to learn TAs which are deterministic. It is based
on genetic programming. At each generation, three opera-
tions can be performed (randomly chosen): a mutation (state
merging, transition splitting, clock reset addition, state ad-
dition), a cross-over (exchange of a part of the automata)
between two TA of the same population, or a cross-over be-
tween TAs from each population. These operations aren’t
based on the observations and thus can introduce inconsis-
tencies. The evaluation step scores the individuals with sev-
eral criteria such as the automaton size, the ﬁtness with the
observations, or the determinism. This algorithm has numer-
ous parameters such as the probability of each operation, the
population size, or the weights on the evaluation criteria. It
also requires information about the timed properties of the
system (number of clocks, approximate largest constant in
clock constraints). Therefore, this algorithm is more suit-
able for expert users as it requires a lot of prior knowledge
to set the parameters and to choose the ﬁnal model. Due to
the stochastic nature of the approach, different local optima
may be found.

RTI+. RTI+ (Verwer, de Weerdt, and Witteveen 2010)
learns the PDRTA sub-class of TA where transitions are la-
beled with their probability of occurrence. RTI+ is based
on the EDSM (Evidence Driven State Merging) algo-
rithm (Lang, Pearlmutter, and Price 1998) used in grammat-
ical inference to learn untimed automata. The ﬁrst step is to
create an automaton where there exists only one path lead-
ing to each state, and consistent with the input sample. Every
transition is assigned to the same interval guard bounded by
the minimal and maximal observed time value in the whole
sample. Thereafter, this tree-shaped automaton is modiﬁed
in a red-blue framework, i.e. progressively from the initial
state to the extremities. Two modiﬁcation operations can be
realized: merging two states and splitting a transition. To
decide whether two states should be merged or a transition
should be split, a likelihood ratio test is computed. The oper-
ation among all the possible operations increasing the most
the likelihood of the data with the model is selected. Due
to the red-blue framework, the split operation is only ap-
plied to transitions which are followed by parts of the au-
tomaton not modiﬁed (factorized by the merges) yet. RTI+
algorithm has been used to model driving behaviors in the
ﬁeld of autonomous vehicles (Lin et al. 2019), however, its
management of temporal constraints leads to TAs that lack
sensitivity.

Timed k-Tail. Timed k-Tail (TkT) (Pastore, Micucci, and
Mariani 2017) is based on the k-Tail algorithm (Biermann
and Feldman 1972) used to learn untimed automata. In these
two algorithms, the states from which the same future sym-
bol sequences of length k are possible are merged. The se-
quences are compared on the transition symbols and the set
of sequences is called the k-future of the state. Timed k-Tail
learns a sub-class of TA that has clock resets, local clocks,
and a global clock that measures the time since the beginning
of a run. It is designed for systems with nested operations,
i.e., an operation (B) can occur while another operation (A)
is in progress, as long as B ends before the end of operation
A. In the absence of nested operations, the local clocks can
be resumed to a unique clock that measures the delay be-
tween two operations, as in a RTA. In the automaton learned
by Timed k-Tail, the choice of transition at each state is de-
terminist only if we consider the k next symbols.

TAG Algorithm
State-of-the-art algorithms have drawbacks in terms of num-
ber of parameters and the determinism or precision of their
output. Based on this observation, our objective was to de-
velop a new algorithm to overcome these limitations and
combine their strengths. The outcome is TAG, which stands
for Timed Automata Generator, a novel algorithm to learn
DRTA from positive timestamped event sequences. In addi-
tion to the previously presented DRTA characteristics, the
transitions of DRTA learned by TAG are enriched with an
indicative probability of occurrence, and a guard on a global
clock never reinitialized which measures the time since the
beginning of a run. The idea of the algorithm is to ﬁrst
produce an automaton which is basically a graphical rep-
resentation of the input sample with all its redundancies.
The automaton will then be factorized on these structurally
redundant parts to obtain a more compact TA. After the
size reduction, the temporal values are recomputed, and it
may be necessary to reﬁne the automaton in function of the
time. The three main operations are the automaton initial-
ization, the states merging, and the transition splitting. Al-
gorithm 1 summarizes the learning process. After a tree-
shaped automaton initialization (initTA function), the ﬁrst
step of the TAG Algorithm consists in reducing the automa-
ton size by merging all the states that can be merged to-
gether (all possible merge), disregarding the tempo-
ral values. Algorithm 2 describes the merge operation. Two
states can be merged if, from both states, the same events
sequences can happen to the system within the k next transi-
tions (the k-future). When no more states can be merged, the
algorithm attempts to capture the temporal logic of the sys-
tem with the split operation (described in algorithm 3).
TAG’s splits, unlike in RTI+, are applied after the merging
step because the factorization has gathered the paths consid-
ering the events, so a split that should have been done multi-
ple times only needs to be done once, and only the event
sequences that remain speciﬁc to a time window are iso-
lated. The split of transitions creates a temporal determinism
where time inﬂuences the system’s evolution. During this
step, merges can also be realized but only if no more transi-
tion split is needed and if the merge won’t be cancelled by

a split. Splits and merges are realized in a breadth-ﬁrst or-
der fashion. TAG ceases when no more split or merge can
be done. Each transition is associated with a probability cor-
responding to the proportion of time the transition has been
taken in the input sample. These probabilities are initialized
in the automaton initialization and updated after each merge
or split with respect to the new distribution. All the sub-
functions of the algorithm as well as the consistency proof
are given in the supplementary material and an implementa-
tion of the algorithm is available1.

Algorithm 1: TAG : P(T S(Σ, T )) → A

Require: an input S = {S+}
Return: a DRTA consistent with S meaning that ∀s ∈

S, s ∈ L(A)
A = initTA(S)
A=all possible merge(A)
repeat

A=all possible split(A)
SAVE=A
A=merge(A)
until SAVE=A
return A

Algorithm 2: merge : A → A

Require: a DRTA A
Return: a DRTA merge(A)

such that L(A)

⊆

L(merge(A)) and |merge(A)| < |A|.
(q1, q2) = choice locations to merge(A)
for all t ∈ Ein(q2) do

replace t = (q, a, g, q2) with (q, a, g, q1)

end for
for all t ∈ Eout(q2) do

replace t = (q2, a, g, q) with (q1, a, g, q).

end for
Q = Q − {q2}
if q2 = q0 then q0 = q1
transf orm(A, q1)
return A

We now illustrate the three major steps of the algorithm

through simple examples.

Automaton Initialization. The ﬁrst step is to create an
automaton consistent with the input sequences. In such pre-
liminary automaton, there exists one unique path leading to
each state. Each sequence is represented by one of these
paths and two sequences share a portion of path only if they
have the same symbolic preﬁx. As an example, let’s consider
a set of two sequences: {(cid:104)r:2 p:6 t:5(cid:105), (cid:104)r:3 s:1(cid:105)}. Each ele-
ment of a sequence is a pair of symbol and delay. Figure 2
presents the resulting preliminary automaton for these input
sequences after the initialization step. First, an initial state

1TAG source code can be found here:

https://gitlab.inria.fr/lcornang/tag/. The remainder of the
supplementary materials can be found here:
https://gitlab.inria.fr/lcornang/aaai22 tag supplementary materials.

Algorithm 3: split : A × P(T S(Σ, T )) → A

Require: a DRTA A = (Q, Σ, E, q0) and S

∈

P(T S(Σ, T ))

Return: a DRTA consistent with S

(t, g1, Q1) = choice transition to split(A) with t =
(q1, a, g, q2)
add a new location q2,split in Q
for (q2, e, g, q3) ∈ E with q3 ∈ Q1 do
replace in the transition q2 by q2,split

end for
In the transition q1 →a
g2 ∪ g1 = g and g2 ∩ g1 = ∅.
Add the transition q1 →a
g1
return A

q2,split in E.

g q2 replace the guard g by g2 where

S0 is created as the starting state for each sequence. The ﬁrst
pair of the ﬁrst sequence is r:2. Starting from S0, there is no
transition labeled with the symbol r, therefore, a transition
is created towards a new state, S1, and labeled with the sym-
bol r and the guard [2, 2]. Now considering the second pair
p:6, a transition must be created from S1 to a new state S2,
and this transition is labeled with the symbol p and the guard
[6,6]. This process is repeated until the end of the sequence.
The evaluation of the next sequence restarts from the ini-
tial state. Since the ﬁrst pair r:3 displays a symbol already
carried by an outgoing transition of S0, there is no need to
create a new transition, and the guard of the transition la-
beled with r is enlarged to [2, 3] to accept this new temporal
value. Then, a new transition is created from S1 for the last
pair s:1. Unlike the RTI+ approach, each transition is asso-
ciated with a speciﬁc guard characterizing the observed tem-
poral constraints in the sequences. Finally, the transitions are
given a probability corresponding to the learning sample se-
quences distribution, and an interval guard corresponding to
the time elapsed since the beginning of the sequences when
they use the given transition. The guards on the global clock
aren’t shown in the section’s ﬁgures since they are not used
for the model construction.

r [2, 3]
p=1.0

S0

S1

t [5, 5]
p=1.0

S3

p [6, 6]
p=0.5

s [1, 1]
p=0.5

S2

S4

Figure 2: A TA initialized with a set of events sequences.

States Merging. As in k-Tail and Timed k-Tail, two states
are considered for merging if their k-futures are identi-
cal. Let’s consider the automaton of Figure 2, and k =
2, the number of future events to consider per sequence.
The k-future of the state S0 is a set of two sequences:
{(cid:104)r, p(cid:105), (cid:104)r, s(cid:105)}, with (cid:104)r, p(cid:105) corresponding to the path going
through S0, S1 and S2, and (cid:104)r, s(cid:105) corresponding to the path
going through S0, S1, and S4. In this automaton, there is
no other state having the same k-future. Otherwise, the two

S3

S7

a [2, 3]

b [6, 6]

c [0, 5]

S4

S8

S5

S9

d [1, 4]

S6

S10

b [0, 5]

a [4, 6]

(a) Unmerged parts

a [2, 6]

b [0, 6]

c [0, 5]

S3

S4

S5

S6

d [1, 4]

S10

(b) Merged parts

Figure 3: Two parts of the automaton that would have been
successively merged and then split in the absence of the
guard overlap requirement.

states would have been merged following the procedure pre-
sented in Algorithm 2. Merging two states consists in ac-
cumulating their outgoing and incoming transitions on the
ﬁrst and to delete the second. Due to our determinism re-
quirement, if this operation induces a situation where two
transitions have the same symbol and the same source, these
transitions and their destination states will also be merged.
Merging two transitions leads to the guard enlargement to in-
clude both former guards. This is called the determinization
process. After a merge, the probabilities are updated with
the new word’s distribution. In TAG’s last step (splits and
merges succession), an overlapping-guards requirement is
also added to ensure convergence so a merge doesn’t can-
cel the result of splits (Figure 3). k is the only parameter
of TAG. By controlling the length of the event sequences
to compare, it allows tuning the trade-off between general-
ization and over-ﬁtting of the model. If the input sample is
exhaustive or if detecting wrong behavior is more important
than having a small and easily interpretable model, k should
be increased. Its default value is set to 2.

Transitions Splitting. The split procedure relies on both
automaton analysis and input sequences. Let’s consider a
transition (cid:15) and guard [τmin, τmax] from state q to q(cid:48), q(cid:48) lead-
ing to different parts of the automaton: part1 and part2.
Given the analysis of the input sequences, if we observe
that some timed words moving through (cid:15) from q to q(cid:48)
are always going to part1 with a delay in [τmin, τ ], with
τ ∈ [τmin, τmax], while others lead to part2 with a delay
in [τ + 1, τmax], the transition labeled (cid:15) can be split. A new
state q” reached from q is created, such that q(cid:48) leads to part1
and q” to part2. Both transitions outgoing from q are la-
beled by the symbol of (cid:15) but associated to different guards
[τmin, τ ] and [τ + 1, τmax]. Given the automaton displayed
in Figure 4 (left), the traces (not shown here) inform us that
some events b at the beginning of the sequences, associated
with a delay lower than 5, are followed by an event a. Some
other events b at the beginning, associated with a greater de-
lay, are followed by an event c. These events b correspond
to the transition between S0 and S3, which will be split as
presented in Figure 4 (right).

b
[5, 8]

S1

S2

a
[0, 10]

b
[5, 8]

S1

S2

a
[0, 10]

b
[2, 7]

S0

S3

a
[0, 4]

c
[1, 3]

S5

S4

S0

b
[2, 4]

b
[5, 7]

S6

a
[0, 4]

S3

S4

c
[1, 3]

S5

Figure 4: A transition division (left: before, right: after).

Experiments
Following the presentation of the state-of-the-art algorithms
and our contribution, this section will tackle the evaluation
and comparison of the algorithms on synthetic and real-
world data. Full data, scripts and results can be found in the
supplementary materials.

Method
To evaluate TAG, we addressed four questions:

Q1. How does TAG scale with the complexity of the data?
Q2. How does its parameter k impact the result?
Q3. What is the contribution of each operation to the result?
Q4. How does TAG cope with real-world data?

We studied the ﬁrst concern by challenging RTI+, Timed k-
Tail, and our algorithm TAG on the quality of their models
and their runtime in various situations using synthetic data
(we couldn’t test GenProgTA because its interface doesn’t
allow the addition of new data and the source code isn’t
available). These same data were used to investigate Q2 and
Q3. We also carried out a real-data experiment to demon-
strate TAG’s ability to deal with real-world data and to pro-
duce interpretable models, using the logs of the programs
of a Canadian TV channel and querying TAG’s output with
model-checking.

Experiment on synthetic data. To study how TAG and
the state-of-the-art algorithms scale with the complexity of
the input data, we identiﬁed multiple factors possibly infer-
ring the runtime and model quality. There are two types of
factors. The ﬁrst factors are inherent to the expected model:

• The alphabet size (number of different events or sym-

bols),

• The number of states of the system,
• The outdegree (average number of outgoing transitions

per state),

• The proportion of twinned-transitions (transitions from
the same state labeled with the same symbol but having
non-overlapping guards), these twinned-transitions cre-
ate an untimed underminism.

In real life, these factors are not controllable as the expected
model depends on the studied system. The second type of
factor is related to the learning process and these can be, to
some extent, adjusted by the user:

• The size of the input sample i.e., the number of timed

words the algorithms will have to process,

• TAG’s parameter k, only studied for our algorithm.

We deﬁned a set of values to test for each factor (Table 1). To
evaluate the impact of the factors independently from each
other, we varied the factors one after the other. When one
factor varies, the others have a ﬁxed value (in bold in Ta-
ble 1). We then ranked these factors according to the es-
timated impact they would have on the model quality and
the runtime. We tested the factors in order of increasing im-
pact (average impact on model quality and runtime), which
corresponds to the order of the factors in Table 1. To con-

Factor

Alphabet size
Outdegree

State number

Tested values

2, 4, 5, 6, 8, 10, 15
1, 1.25, 1.5, 1.75, 2, 2.25, 2.5
4, 6, 8, 10, 15, 25, 50, 75,
100, 500

Twinned transitions
proportion
Number of timed words
TAG parameter k

0, 0.10, 0.25, 0.50, 0.7

50, 100, 500, 1000, 2500
2, 3, 4, 5, 6

Table 1: Factors order and values (default value in bold).

trol the factors related to the expected model and to have a
ground truth, we started from synthetic model TAs, gener-
ated to have a given value for each factor. We generated 200
TAs per combination to gain conﬁdence in the results. The
datasets are composed of runs of these model TAs, each run
being a timed word. For the evaluation of the learned TA,
we also generated 100 other timed words consistent with the
model and 100 words inconsistent with the model. We used
the same protocol for the ablation study. 200 TA were gen-
erated with the default value for every factor.

The runtime was measured on a MacBook Pro with an
Intel Core i9 processor clocked at 2,4 GHz and a memory of
16 Go 2667 MHz DDR4. TAG is implemented in Python,
Timed k-Tail’s author implementation (where k is hard
coded to 2) is in Java and RTI+ in C++. RTI+ was executed
with a signiﬁcance value of 0.05 for the likelihood ratio test
(default value). TkT was executed with no nested events
considerations (absent in our data) and no enlargement of
the guards (default value). To evaluate the learned TAs,
two accuracy scores were selected. The True Positive Rate
(TPR), also called recall, is the probability for a word
consistent with the model to be recognized by the learned
automaton. The Positive Predictive Value (PPV), also
called precision, is the probability for a word recognized
by the learned automaton to be consistent with the model
automaton. A high recall and precision are both desired but
are in practice rather impossible to have simultaneously. As
we haven’t a speciﬁc ﬁeld of application, we consider them
equally important. The F1-score is the harmonic mean of
these two measures. Aiming for a good F1-score leads us to
a good trade-off between recall and precision.

Experiment on real-world data. To assess TAG’s abil-
ity to learn an interpretable and exploitable model from real
data, we used the logs of the programs of the Canadian
TV channel CBC Windsor (Canadian Radio-television and

Telecommunications Commission 2015). We ﬁrst used the
data of the Friday mornings of August 2020 (from 6:00 AM
to 12:00 AM, one word per day). Then, we used the data
of every day of July and August 2020 (Canadian summer
school vacations months). The entries of the logs were sum-
marized by their class (commercial message, promotion for
a program ...) or category (program for children, news...) in
case of a program. A word consists of the sequence of entries
class/category and their duration for a day.

To query the automaton learned by TAG with the sum-
mer data, we can take advantage of both the classical ex-
pressiveness of TA and the probabilities associated with the
TAG’s TA transitions. We used UPPAAL SMC (Bulychev
et al. 2012), a model-checking tool for Timed Automata
with stochastic properties. UPPAAL SMC extends the basic
query language of UPPAAL (Bengtsson et al. 1996), which
is a subset of Timed Computation Tree Logic (TCTL), with
queries related to the stochastic behavior of systems. The
result of UPPAAL SCM queries is obtained by monitoring
simulations of the system and by statistical hypothesis test-
ing. The answer is an interval of probability with a conﬁ-
dence of 0.95 of being within. We used both classical UP-
PAAL and UPPAAL SMC queries and for the latter, the
number of simulations was ﬁxed at 10000.

Results
Experiment on synthetic data. The whole results of the
comparison of the algorithms for every factor are in the sup-
plementary materials. We only present here the most signiﬁ-
cant results. For all the statistical tests, the signiﬁcance level
is of α = 0.05. On the ﬁgures, one asterisk indicates a sig-
niﬁcant difference with a p-value of at least α.

Q1. How does TAG scale with the complexity of the data?

In the ﬁrst place, we focus on the runtime. Globally, the
runtime of the three algorithms is comparable, with RTI+
(median ˜x = 0.22s, interquartile range IQR = 0.25s) fol-
lowed by TAG (˜x = 0.46s, IQR = 0.21s) and Timed k-
Tail (˜x = 0.64s, IQR = 0.06s). The factors positively
correlated with TAG’s runtime are the following: the out-
degree as more merges are necessary; the number of states
since TAG process the pair of states in a breadth-ﬁrst or-
der to ﬁnd split candidates; the parameter k as the states k-
future is constructed recursively; and obviously the number
of timed words (Figure 6b). The twinned transition propor-
tion is slightly negatively correlated to the runtime.

Turning now on the quality of the obtained models, eval-
uated with the recall, the precision, and the F1-score, Fig-
ure 5 presents these scores for the three algorithms and for
all the tested combinations. Globally, TAG’s precision al-
ways stays good (0.92 on average) and signiﬁcantly better
than the other algorithms (rate of increase of 0.48 with RTI+
and 0.34 with Timed k-Tail). Due to the trade-off, it leads to
a slight loss of recall in comparison to Timed k-Tails (rate
loss of 0.02, giving a recall of 0.97 on average) but the gain
in accuracy is signiﬁcant with an F1-score above the others
on average. Timed k-Tail has the best recall overall but tends
to produce models too generalized because of the absence
of splits. RTI+ produces models not precise enough because

Figure 5: Scores per algorithm.

of its bad management of the time constraints that leads to
guards too wide. Let’s now look at the effect of the differ-
ent factors. As shown in Figure 7, TAG is less sensitive to
the alphabet size than Timed k-Tail, which produces mod-
els too compact and thus not much precise when there are
few different symbols, and RTI+, which doesn’t generalize
enough in presence of many symbols and therefore doesn’t
recognize well new words of the language. TAG is sensi-
tive to the complexity of the model automaton (outdegree,
number of states, and twinned transitions proportion). As it
becomes more complex, the recall decreases since the rep-
resented part of the language in the sample data becomes
smaller, and thus the new words will be less probably rec-
ognized. However, its precision stays good (0.79 on aver-
age for the worst case corresponding to an outdegree of 2.5)
and systematically above the other algorithms, as well as the
F1-score. This means that TAG automata are accurate and
not too generalized. Finally, the recall (and F1-score) of the
TA of the three algorithms unsurprisingly increases with the
number of timed words in the input data sample (Figure 6a),
with a stabilization of the score at 500 timed words for our
parameter’s combination.

In summary, these results show that TAG offers a better
trade-off between recall and precision than the state-of-the-
art algorithms with a comparable runtime. Globally, TAG
automata are accurate and not too generalized. The factor
impacting the most TAG is the number of timed words in the
input sample, which increases the runtime but also the qual-
ity of the obtained TA. The complexity factors penalizing
TAG’s quality are the number of states and the outdegree.

Q2. How does TAG’s parameter k impact the result?

As k increases, the precision tends to increase while the
recall decreases (Figure 8). However, the gain in precision is
inferior to the loss of recall. Therefore, the default value is
set to k = 2, letting the user freedom to tune it in function
to its application and its needs. Particularly, there can be a
visual impact and thus an interpretability impact if the model
is meant to be human-understandable.

Q3. What is the contribution of each operation to the result?
We performed an ablation study to conﬁrm the impor-
tance of each operation of TAG. Figure 9 compares the

(a) Evolution of the recall.

(b) Evolution of the runtime.

Figure 6: Evolution of the recall and the runtime w.r.t.the
number of timed words.

Figure 7: Precision and recall when the alphabet size varies.

precision, the recall, and the F1-score obtained for the TA
produced with only the tree-shaped automaton initialization,
then with the initialization and the merges, and ﬁnally with
the initialization, the merges, and the splits. With only the
initialization, the precision is maximal since there is no
generalization, the automaton language corresponds to the
input traces (plus the values inside the guard intervals). For
the same reason, the recall is lower than with the merges
and the splits. Merges induce a signiﬁcant augmentation
of the recall by generalizing the model. Lastly, TAG recall
and precision are improved by the splits leading to a better
F1-score. Beyond these positive results on synthetic and
random data, the importance of the splits becomes more
evident in the case of systems where some events would
happen after a ﬁrst event only within a limited time window
and never otherwise. In an TA learned with splits, the timed
condition would be necessary to access this part of the TA.

Experiment on real-world data.

*********F1−scorePrecisionRecallTAGTimedk−TailRTI+TAGTimedk−TailRTI+TAGTimedk−TailRTI+0.250.500.751.00AlgorithmScore0.750.800.850.900.951.0005001000150020002500Timed words numberRecallAlgorithmRTI+TAGTkT0.06250.50004.000005001000150020002500Timed words numberRuntime (s) (in log scale)AlgorithmRTI+TAGTkT0.60.70.80.91.00.960.970.980.991.002468101214Alphabet sizePrecisionRecallScorePrecisionRecallAlgorithmRTI+TAGTkTFigure 8: TAG’s Precision and recall when k varies.

Figure 9: Ablation study on recall, precision and F1-score.

Q4. How does TAG cope with real-world data?

The TA learned by TAG with the TV logs of the Friday
mornings (329 events) is shown in Figure 10. The guards
are originally in seconds and have been formatted to make
the ﬁgure more apprenhensible. The ﬁrst guard in bracket
limits the delay of occurrence after the last event. The sec-
ond guard in bracket preceded by the letter ”t” corresponds
to the value of a global clock started at 6:00 AM in the initial
state and never reset.

A

S0

S1

News
[0s, 0s]
t[6h00, 6h00]
p=1.0

Program promotion
[59m30, 59m30]
t[7h00, 7h00]
p=0.25

Ads
[59m30, 59m30]
t[7h00, 7h00]
p=0.75

B

S2

Interstitials
[30s, 26m25]
t[7h00, 10h59]
p=0.93

Children’s
programs
[5s, 7m23]
t[7h00, 11h00]
p=0.62

S3
Children’s programs
[25s, 15m09]
t[11h13, 11h52]
p=0.61

Films
[5s, 2m13]
t[7h44, 10h33]
p=0.38

Program promotion
[8m55, 12m48]
t[11h09, 11h13]
p=0.07

Ads
[15s, 11m09]
t[11h09, 11h48]
p=0.39

S4

C

S5

Program promotion
[30s, 10m03]
t[11h12, 11h51]
p=1.00

Figure 10: Learned TA from the Friday morning logs of a
Canadian TV channel.

Query
What is the probability to watch one
hour of children’s program without
interruption?
If we watch this channel all the day
long, are we sure to have, at some
point, a children’s program?
Between 10h and 11h, what is the
probability to watch a children’s pro-
gram?
Globally, is it more probable to have
a ﬁlm than a children’s program?

Result

Time (s)

[0.07,0.08]

0.722

No

0.003

[0.46,0.56]

0.049

No

0.011

Table 2: Queries, result, and execution time.

low one another until 11:00 AM (B on the ﬁgure). Chil-
dren’s programs are more probable and these programs are
frequently cut by interstitials. Then, the ads and program’s
teasers are back and cut children’s programs until the 12:00
AM news (C on the ﬁgure). The interest of the probabili-
ties and the guards on the global clock is demonstrated here
since they provide substantial information for the system
comprehension.

The TA learned with the whole data of July and August
(54065 events) is naturally much bigger since the time slot
is wider and the day types differ. 15240 merges and 12 splits
were necessary. It has 65 states, 125 transitions, and 14 dif-
ferent symbols. Table 2 presents the queries submitted to the
model-checker, the results, and the execution time. In less
time than a human would take to analyze even the ﬁrst TV
programs automaton, the responses to useful questions can
be obtained with model-checking queries.

Conclusion
This paper introduces a new algorithm, TAG, to learn Timed
Automata from logs of real-time systems. This learned au-
tomaton can be used to model a time-dependent system
to understand its behavior without any a-priori knowledge
about it. The unique parameter k offers a trade-off between
precision and recall of the learned model, depending on the
application domain or the desired level of interpretability.
On this relatively new subject, this study is the only one
that compares the existing approaches. Experiments have
shown that TAG is fully capable of inferring a TA describing
the system including realistic time constraints while remain-
ing interpretable, visually if the model is small, and thanks
to model-checking otherwise. The model can then be used
to perform anomaly detection, data generation, prediction,
or else veriﬁcation. An interesting perspective would be to
learn a model composed of various interacting subsystems
to limit the global model size. As more and more physical
systems are being equipped with sensors producing time se-
ries, it could also be useful to develop a method to obtain a
TA from this kind of data.

This TA has 6 states, 10 transitions, and 6 different sym-
bols. This TA shows that after the morning news, which lasts
about one hour (A on the ﬁgure), and a session of ads or
program promotions, ﬁlms and/or children’s programs fol-

References
Alur, R.; and Dill, D. L. 1994. A theory of timed automata.
Theoretical Computer Science, 126(2): 183–235.

0.940.950.960.970.980.940.950.960.970.9823456kPrecisionRecallScorePrecisionRecall0.70.80.91.0F1 scorePrecisionRecallComponentsInitializationInitialization +mergesInitialization +merges +splitsAmmons, G.; Bod´ık, R.; and Larus, J. R. 2002. Mining
speciﬁcations. In Proceedings of the 29th ACM SIGPLAN-
SIGACT symposium on Principles of programming lan-
guages - POPL ’02, 4–16. Portland, Oregon: ACM Press.
ISBN 978-1-58113-450-6.
An, J.; Chen, M.; Zhan, B.; Zhan, N.; and Zhang, M.
2020. Learning One-Clock Timed Automata. In Tools and
Algorithms for the Construction and Analysis of Systems
(TACAS), 444–462.
Angluin, D. 1987. Learning regular sets from queries and
counterexamples. Information and Computation, 75(2): 87–
106.
Bengtsson, J.; Larsen, K.; Larsson, F.; Pettersson, P.; and Yi,
W. 1996. UPPAAL — a tool suite for automatic veriﬁca-
tion of real-time systems. In Goos, G.; Hartmanis, J.; van
Leeuwen, J.; Alur, R.; Henzinger, T. A.; and Sontag, E. D.,
eds., Hybrid Systems III, volume 1066, 232–243. Berlin,
Heidelberg: Springer Berlin Heidelberg. ISBN 978-3-540-
61155-4 978-3-540-68334-6. Series Title: Lecture Notes in
Computer Science.
Biermann, A. W.; and Feldman, J. A. 1972. On the Synthesis
of Finite-State Machines from Samples of Their Behavior.
IEEE Transactions on Computers, C-21(6): 592–597.
Bulychev, P.; David, A.; Larsen, K. G.; Mikuˇcionis, M.;
Bøgsted Poulsen, D.; Legay, A.; and Wang, Z. 2012.
UPPAAL-SMC: Statistical Model Checking for Priced
Timed Automata. Electronic Proceedings in Theoretical
Computer Science, 85: 1–16.
Canadian
tions Commission. 2015.
https://open.canada.ca/data/en/dataset/800106c1-0b08-
401e-8be2-ac45d62e662e. Accessed Aug. 23, 2021.
Clarke, E. M.; Henzinger, T. A.; Veith, H.; and Bloem, R.
2018. Handbook of Model Checking. Springer.
Connes, V.; De La Higuera, C.; and Le Capitaine, H. 2021.
Using Grammatical Inference to Build Privacy Preserving
In International Conference on
Data-sets of User Logs.
Grammatical Inference. Nantes, France.
de la Higuera, C. 2010. Grammatical Inference: Learning
Automata and Grammars. Cambridge University Press.
Gold, E. M. 1978. Complexity of automaton identiﬁcation
from given data. Information and Control, 37(3): 302–320.
Grinchtein, O.; Jonsson, B.; and Pettersson, P. 2006. Infer-
ence of Event-Recording Automata Using Timed Decision
Trees. In CONCUR 2006 – Concurrency Theory, 435–449.
Henry, L.; J´eron, T.; and Markey, N. 2020. Active Learning
of Timed Automata with Unobservable Resets. In Formal
Modeling and Analysis of Timed Systems FORMATS, 144–
160.
H´elias, A.; Guerrin, F.; and Steyer, J.-P. 2008. Using timed
automata and model-checking to simulate material ﬂow
in agricultural production systems—Application to animal
waste management. Computers and Electronics in Agricul-
ture, 63(2): 183–192.
Isberner, M.; Howar, F.; and Steffen, B. 2015. The Open-
Source LearnLib - A Framework for Active Automata
Learning. In Computer Aided Veriﬁcation (CAV), 487–495.

Telecommunica-
Television program logs.

Radio-television

and

Lang, K. J.; Pearlmutter, B. A.; and Price, R. A. 1998. Re-
sults of the Abbadingo one DFA learning competition and a
new evidence-driven state merging algorithm. In Grammat-
ical Inference LNCS, volume 1433, 1–12.
Lin, Q.; Zhang, Y.; Verwer, S.; and Wang, J. 2019. MOHA:
A Multi-Mode Hybrid Automaton Model for Learning Car-
IEEE Transactions on Intelligent
Following Behaviors.
Transportation Systems, 20(2): 790–796.
Narayan, A.; and Fischmeister, S. 2019. Mining Time for
Timed Regular Speciﬁcations. In 2019 IEEE International
Conference on Systems, Man and Cybernetics (SMC), 63–
69. Bari, Italy: IEEE. ISBN 978-1-72814-569-3.
Pastore, F.; Micucci, D.; and Mariani, L. 2017. Timed k-
In IEEE In-
tail: Automatic inference of timed automata.
ternational conference on software testing, veriﬁcation and
validation (ICST), 401–411.
Sahugu`ede, A.; Le Corronc, E.; and Le Lann, M.-V. V. 2018.
Chronicle Discovery for Diagnosis from Raw Data: A Clus-
tering Approach. In 10th IFAC Symposium on Fault Detec-
tion, Supervision and Safety for Technical Processes, SAFE-
PROCESS 2018, 8p. Varsaw, Poland.
Tappler, M.; Aichernig, B. K.; Larsen, K. G.; and Lorber, F.
2018. Learning Timed Automata via Genetic Programming.
arXiv:1808.07744 [cs].
Tatti, N.; and Vreeken, J. 2012. The long and the short of it:
summarising event sequences with serial episodes. In Yang,
Q.; Agarwal, D.; and Pei, J., eds., ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Min-
ing, KDD, 462–470.
Tax, N.; Alasgarov, E.; Sidorova, N.; Haakma, R.; and
van der Aalst, W. M. P. 2019. Generating time-based la-
bel reﬁnements to discover more precise process models. J.
Ambient Intell. Smart Environ., 11(2): 165–182.
van der Aalst, W. M. P. 2016. Process Mining: Data Science
in Action. Berlin, Heidelberg: Springer Berlin Heidelberg :
Imprint: Springer, 2nd ed. 2016 edition. ISBN 978-3-662-
49851-4.
Verwer, S.; de Weerdt, M.; and Witteveen, C. 2010. A
likelihood-ratio test for identifying probabilistic determin-
istic real-time automata from positive data. In International
Colloquium on Grammatical Inference, 203–216. Springer.
Verwer, S.; de Weerdt, M.; and Witteveen, C. 2012. Efﬁ-
ciently identifying deterministic real-time automata from la-
beled data. Machine Learning, 86(3): 295–333.
Xu, Q.; Ali, S.; and Yue, T. 2021. Digital Twin-based
In 2021
Anomaly Detection in Cyber-physical Systems.
14th IEEE Conference on Software Testing, Veriﬁcation and
Validation (ICST), 205–216.
Zou, J.; Xiao, J.; Hou, R.; and Wang, Y. 2010. Frequent
Instruction Sequential Pattern Mining in Hardware Sample
Data.
In Webb, G. I.; Liu, B.; Zhang, C.; Gunopulos, D.;
and Wu, X., eds., IEEE International Conference on Data
Mining (ICDM), 1205–1210.

