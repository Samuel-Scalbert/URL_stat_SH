Linked Data Ground Truth for Quantitative and
Qualitative Evaluation of Explanations for Relational
Graph Convolutional Network Link Prediction on
Knowledge Graphs
Nicholas Halliwell, Fabien Gandon, Freddy Lecue

To cite this version:

Nicholas Halliwell, Fabien Gandon, Freddy Lecue. Linked Data Ground Truth for Quantitative and
Qualitative Evaluation of Explanations for Relational Graph Convolutional Network Link Prediction
on Knowledge Graphs. WI-IAT 2021 - 20th IEEE/WIC/ACM International Conference on Web Intel-
ligence and Intelligent Agent Technology, Dec 2021, Melbourne, Australia. Ôøø10.1145/3486622.3493921Ôøø.
Ôøøhal-03430113v2Ôøø

HAL Id: hal-03430113

https://hal.science/hal-03430113v2

Submitted on 23 Nov 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L‚Äôarchive ouverte pluridisciplinaire HAL, est
destin√©e au d√©p√¥t et √† la diffusion de documents
scientifiques de niveau recherche, publi√©s ou non,
√©manant des √©tablissements d‚Äôenseignement et de
recherche fran√ßais ou √©trangers, des laboratoires
publics ou priv√©s.

Linked Data Ground Truth for Quantitative and Qualitative
Evaluation of Explanations for Relational Graph Convolutional
Network Link Prediction on Knowledge Graphs

Nicholas Halliwell
Inria, UCA, CNRS, I3S
Sophia Antipolis, France
nicholas.halliwell@inria.fr

Fabien Gandon
Inria, UCA, CNRS, I3S
Sophia Antipolis, France
fabien.gandon@inria.fr

Freddy Lecue
CortAIx, Thales
Montreal, Canada
freddy.lecue@thalesgroup.com

ABSTRACT
Relational Graph Convolutional Networks (RGCNs) identify rela-
tionships within a Knowledge Graph to learn real-valued embed-
dings for each node and edge. Recently, researchers have proposed
explanation methods to interpret the predictions of these black-
box models. However, comparisons across explanation methods for
link prediction remains difficult, as there is neither a method nor
dataset to compare explanations against. Furthermore, there exists
no standard evaluation metric to identify when one explanation
method is preferable to the other. In this paper, we leverage linked
data to propose a method, including two datasets (Royalty-20k,
and Royalty-30k), to benchmark explanation methods on the task
of explainable link prediction using Graph Neural Networks. In
particular, we rely on the Semantic Web to construct explanations,
ensuring that each predictable triple has an associated set of triples
providing a ground truth explanation. Additionally, we propose the
use of a scoring metric for empirically evaluating explanation meth-
ods, allowing for a quantitative comparison. We benchmark these
datasets on state-of-the-art link prediction explanation methods
using the defined scoring metric, and quantify the different types
of errors made with respect to both data and semantics.

CCS CONCEPTS
‚Ä¢ Mathematics of computing ‚Üí Computing most probable
explanation; ‚Ä¢ Computing methodologies ‚Üí Knowledge rep-
resentation and reasoning; Neural networks.

KEYWORDS
link prediction, Explainable AI, knowledge graphs, graph neural
networks.

ACM Reference Format:
Nicholas Halliwell, Fabien Gandon, and Freddy Lecue. 2021. Linked Data
Ground Truth for Quantitative and Qualitative Evaluation of Explanations
for Relational Graph Convolutional Network Link Prediction on Knowledge
Graphs. In IEEE/WIC/ACM International Conference on Web Intelligence (WI-
IAT ‚Äô21), December 14‚Äì17, 2021, ESSENDON, VIC, Australia. ACM, New York,
NY, USA, 8 pages. https://doi.org/10.1145/3486622.3493921

Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or affiliate of a national govern-
ment. As such, the Government retains a nonexclusive, royalty-free right to publish or
reproduce this article, or to allow others to do so, for Government purposes only.
WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia
¬© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9115-3/21/12. . . $15.00
https://doi.org/10.1145/3486622.3493921

1 INTRODUCTION
Knowledge Graphs [3] are used on tasks such as search engine
enhancement, question answering, and product recommendation.
Knowledge Graphs often represent facts as triples in the form (sub-
ject, predicate, object), where a subject and object represent an entity,
linked by some predicate.

Knowledge Graphs can be incomplete or evolving, hence they
often do not explicitly contain every available fact. Link prediction
on Knowledge Graphs [13] is used to identify unknown facts from
existing ones. A typical way to perform link prediction on Knowl-
edge Graphs involves the use of graph embeddings algorithms.
Such algorithms learn a function mapping each subject, object, and
predicate to a low dimensional space. A scoring function is defined
to quantify if a link (relation) should exist between two nodes (en-
tities). DistMult [14] learns a diagonal matrix for each relation. A
Relational Graph Convolutional Network (RGCN) [12] leverages
Graph Convolutional Networks [8] with the scoring function from
DistMult as an output layer, returning a probability of the input
triple being a fact.

Often the prediction alone is not enough to support decision-
making. Users must understand why the model arrived at a partic-
ular decision [2]. Therefore, there has been a push to explain the
predictions of these algorithms, creating the task of explainable
link prediction. The term explanation is commonly defined as a
statement that makes something clear. Throughout this paper, we
use the term explanation to refer to a set of observations (triples)
that provides an understanding of the black-box model‚Äôs predic-
tions. In other words, we define ground truth explanations as a set
of triples that cannot be ignored when justifying the suggestion of
adding a targeted link to the graph.

This paper focuses on explainable link prediction specifically
on Knowledge Graphs, in particular: ExplaiNE [7] quantifies how
the predicted probability of a link changes when weakening or
removing a link with a neighboring node, while GNNExplainer [15]
explains the predictions of any Graph Neural Network, learning a
mask over the adjacency matrix to identify the most informative
subgraph. ExplaiNE and GNNExplainer share the idea of providing
a selection of existing triples to the user as an explanation.

These state-of-the-art explanation methods have no common
datasets used as benchmarks, and have no standard evaluation
metrics to measure explanation quality. This prevents quantitative
evaluation and comparisons across explanation methods. In this
paper, we propose a method, along with two datasets, Royalty-20k,
and Royalty-30k, to quantitatively evaluate explanation methods
on the task of link prediction using Graph Neural Networks. These

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

Nicholas Halliwell, Fabien Gandon, and Freddy Lecue

datasets includes ground truth explanations, allowing for compar-
isons with predicted explanations. Additionally, we propose the
use of an evaluation metric, leveraging a similarity between the
predicted and ground truth explanation to measure the quality
of explanation. Lastly, we benchmark state-of-the-art explanation
methods using the proposed dataset and evaluation metric, and
quantify the different types of errors made in terms of both data
and semantics.

This paper is organized as follows: Section 2 provides an overview
of related work on the task of Knowledge Graph embeddings, and
explainable link prediction, along with their shortcomings. Section 3
describes a generic approach to generate datasets with ground truth
explanations, and a metric for empirical evaluation. Section 4 ap-
plies this approach to construct two datasets, outlining the rules
that define each dataset. Section 5 details the benchmark performed
on the Royalty datasets, and reports the results. Lastly, Section 6
concludes this work, outlining opportunities for future work. All
the resources used and produced in this work are available online
including the download link for the reasoner, code for this paper
and datasets.1

2 RELATED WORK: LOOKING FOR A

GROUND TRUTH

Knowledge Graph embeddings. Knowledge Graph embedding
algorithms learn continuous vectors for each subject, predicate and
object. The loss functions are often designed to capture specific
algebraic properties of predicates (symmetric, reflexive, transitive,
etc). A scoring function is defined to assign a value to each triple
based on if the subject, predicate, and object form a valid fact.
Typically, the scoring function is included in the loss function. We
refer the reader to a recent survey [6] for further details.

A Relational Graph Convolutional Networks (RGCN) [12] can be
used to learn embeddings and perform link prediction on Knowl-
edge Graphs. The RGCN performs embedding updates for a given
entity by multiplying the neighboring entities with a weight matrix
for each relation in the dataset, and summing across each neighbor
and relation. A weight matrix for self connections is also learned,
and added to the neighbor embedding summation.

Indeed there are other approaches for link prediction i.e. rule
based, this work however focuses on link prediction on Knowledge
Graphs using Graph Neural Networks.

Explainable link prediction. Few algorithms exist to under-
stand the predictions of Knowledge Graph embedding algorithms.
For a given embedding model and some scoring function ùëî, Ex-
plaiNE [7] computes the gradient of the scoring function with
respect to the adjacency matrix. Indeed this measures the change
in score due to a small perturbation in the adjacency matrix, that
is, how much will the score change if a link is added or removed
between two given nodes. Formally, given two nodes ùëñ, ùëó serving
as prediction candidates, and two nodes ùëò, ùëô serving as a candidate
explanation, the score assigned to node pair ùëò, ùëô is given by:

ùúïùëîùëñ ùëó
ùúïùëéùëòùëô

(A) = ‚àáXùëîùëñ ùëó (X‚àó)ùëá ¬∑

ùúïX‚àó
ùúïùëéùëòùëô

(A),

(1)

1https://github.com/halliwelln/Explain-KG

where X‚àó is the optimal embedding matrix, and ùëéùëòùëô is an element

of the adjacency matrix A.

To explain the predictions of any Graph Neural Network, GN-
NExplainer [15] learns a mask over the input adjacency matrix to
identify the most relevant subgraph. This is achieved by minimizing
the cross entropy between the predicted label using the input adja-
cency matrix, and the predicted label using the masked adjacency
matrix. The objective function minimized by GNNExplainer is:

‚àí

min
M

ùê∂
(cid:213)

ùëê=1

1[ùë¶ = ùëê] ùëôùëúùëîùëÉŒ¶ (ùëå = ùë¶|Aùëê ‚äô ùúé (M), Xùëê ),

(2)

where M is the mask learned and ‚äô denotes element-wise multi-

plication.

Explanation quality. The weak point of the empirical evalua-
tion of these explanation methods is often explanation quality. The
authors of ExplaiNE acknowledge the difficulty in measuring the
quality of explanation generated and a lack of available datasets
with ground truth explanations [7]. Moreover, they rely on the
assumption that the explanation can be found using one of the 1ùë†ùë°
degree neighbors. On the task of movie recommendation, ExplaiNE
measures the quality of explanations using the average Jaccard
similarity between the genres for a given recommended movie, and
the set of genres from the top 5 ranked explanations computed. A
ùëù-value is computed to estimate the significance of the average. It
is unclear how this evaluation method generalizes to tasks outside
of movie recommendation. Ideally, a performance metric would not
have to rely on such assumptions and would generalize to other
tasks.

Ground truth. In general, ground truth does not exist for expla-
nations. For the task of node classification, GNNExplainer uses sim-
ulated data with ground truth explanations in the form of connected
subgraphs. The explanation accuracy of each node‚Äôs predicted label
is then computed. However, no insight is provided on how to simu-
late ground truth data for the task of link prediction. Furthermore,
GNNExplainer has not been benchmarked by its authors on the
task of explainable link prediction on Knowledge Graphs.

Datasets. Datasets with explanations are not available for the
previously mentioned approaches to measure the quality of expla-
nations. The authors of ExplaiNE benchmark their approach with 4
datasets: Karate, DBLP, MovieLens, and Game of Thrones networks.
These datasets do not include ground truth explanations. Addition-
ally, it is non-trivial to define ground truth explanations on these
networks. Without a dataset containing ground truth explanations,
it is difficult to recognize if explanation methods such as ExplaiNE
and GNNExplainer, are generating high quality explanations. Fur-
thermore, these algorithms use different approaches to evaluating
explanations. There is no standard quantitative metric to measure
the quality of explanations generated, making comparisons of the
methods difficult.

Contributions. Our contributions include a method to quanti-
tatively evaluate explanation methods on the task of link predic-
tion on Knowledge Graphs. Additionally, we propose two datasets,
Royalty-20k, and Royalty-30k, that includes ground truth expla-
nations for each observation. Furthermore, we propose the use of

Linked Data Ground Truth for Quantitative and Qualitative Evaluation of Explanations

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

a scoring metric leveraging the similarity between predicted and
ground truth explanations, allowing for quantitative comparisons
across explanation methods. Lastly, we benchmark state-of-the-art
explanation methods, using the proposed dataset and metrics, and
quantify the different types of errors made in terms of both data
and semantics.

3 GENERATING GROUND TRUTH

EXPLANATIONS FOR EVALUATION

3.1 Inference Traces as Explanations
We introduce a generic approach to generate datasets with ground
truth explanations. We propose to view the ground truth generation
as equivalent to computing a single justification for an entailment.
We selected the single-all-axis glass-box category of algorithms [4]
that computes a single justification for a triple we will then try to
predict instead of inferring. A small and exact set of explanations
are needed, that of which must be precisely controlled and selected.
Therefore, we select an open-source semantic reasoner with rule-
tracing capabilities [1] to generate ground truth explanations for
chosen rules, without needing manual annotations. In essence, this
tracing pinpoints the input triples that caused the generation of a
triple we will then try to predict and explain.

We rely on a set of rules equivalent to strict Horns clauses i.e.
disjunctions of literals with exactly one positive literal ùëôùëê , all the
other ùëôùëñ being negated: ¬¨ùëô1 ‚à® ... ‚à® ¬¨ùëôùëõ ‚à® ùëôùëê . The implication form
of the clause can be seen as an inference rule assuming that, if all
ùëôùëñ hold (the antecedent of the rule), then the consequent ùëôùëê also
holds, denoted ùëôùëê ‚Üê ùëô1 ‚àß ... ‚àß ùëôùëõ. In our case, each literal is a binary
predicate capturing a triple pattern of the Knowledge Graph with
variables universally quantified for the whole clause. For instance,
‚Ñéùëéùë†ùê∫ùëüùëéùëõùëëùëùùëéùëüùëíùëõùë° (ùëã, ùëç ) ‚Üê ‚Ñéùëéùë†ùëÉùëéùëüùëíùëõùë° (ùëã, ùëå ) ‚àß ‚Ñéùëéùë†ùëÉùëéùëüùëíùëõùë° (ùëå, ùëç ).

For a given Knowledge Graph and a given set of rules, the se-
mantic reasoner performs a forward chaining materialization of
all inferences that can be made. Each time the engine finds a map-
ping of triples ùëá1, . . . ,ùëáùëõ making the antecedent of a rule true, it
materializes the consequent triple ùëáùëê , and records the explanations
in the form ùëáùëê ‚Üê (ùëá1, . . . ,ùëáùëõ), where ùëáùëê is a generated triple, and
triples ùëá1, . . . ,ùëáùëõ are its explanation. Note that using reasoning to
generate explanations is independent of the algorithm used on the
link prediction task.

Indeed this forms an intuitive explanation for graph data, a re-
cent study shows users prefer example based explanations [5]. This
generic approach to generating ground truth explanations can be
applied to many Knowledge Graphs and many sets of rules. In
this work, we focus on non-ambiguous explanations i.e. logical
rules that are carefully constructed to give only one ground truth
explanation. These rules and datasets were designed to construct
explanations containing triples that cannot be ignored when jus-
tifying the suggestion of adding a targeted link to the graph. To
our knowledge, this approach to generate ground truth explana-
tions has not been previously applied to the task of explainable link
prediction on Knowledge Graphs using Graph Neural Networks.

3.2 Explanation Evaluation Metric
To our knowledge, there is no standard evaluation metric to measure
the quality of explanations generated by link prediction explanation

methods. A standard evaluation metric is needed to identify when
one explanation method is preferable to the other. This metric must
compare the predicted explanation set to a ground truth explanation
set, and assign a similarity score to these two sets.

One way to measure the similarity between a predicted and
ground truth explanation set would be to use the Jaccard similarity
between a ground truth explanation set ùê∏ and a predicted set of
explanations ÀÜùê∏ is:

ùêΩ (ùê∏, ÀÜùê∏) =

|ùê∏ ‚à© ÀÜùê∏|
|ùê∏ ‚à™ ÀÜùê∏|

=

|ùê∏ ‚à© ÀÜùê∏|
|ùê∏| + | ÀÜùê∏| ‚àí |ùê∏ ‚à© ÀÜùê∏|

.

(3)

In this context, a Jaccard similarity of 1 means the predicted set
of the explanation method ÀÜùê∏ exactly matches the ground truth set ùê∏.
Similarly, when ùê∏ and ÀÜùê∏ have no elements in common, the Jaccard
similarity is 0. We feel this metric is appropriate, as both ExplaiNE
and GNNExplainer are asked to only identify existing triples in the
graph to serve as an explanation, therefore only set similarity need
be considered.

As an example, let ùê∏ ={(Abel, King of Denmark, hasParent, Beren-
garia of Portugal), (Berengaria of Portugal, hasParent, Sancho I of
Portugal)} and ÀÜùê∏ ={(Abel, King of Denmark, hasParent, Berengaria
of Portugal), (Valdemar II of Denmark, hasParent, Sophia of Minsk)}.
Hence ùêΩ (ùê∏, ÀÜùê∏) = 0.333, as they share only one triple in common.

This metric has several nice properties; the Jaccard similarity pe-
nalizes a set of candidate explanations when the cardinality differs
from the ground truth explanation set. Additionally, the order of
the explanations is not considered. Metrics like ROUGE-N [10] or
BLEU [11] used in Natural Language Processing (NLP) to compare
translations against multiple references adds complexity with no
immediate benefit in our case.

ùë°ùëù
ùë°ùëù+ùëì ùëù , ùëüùëíùëêùëéùëôùëô =

ùë°ùëù
ùë°ùëù+ùëì ùëõ , and ùêπ1 = 2 ¬∑

A second way to measure explanation quality is to consider the
precision, recall and ùêπ1-Score of each explanation method, where
ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ ¬∑ùëüùëíùëêùëéùëôùëô
ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ =
ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëüùëíùëêùëéùëôùëô . In
this context, a false positive (fp) corresponds to a triple predicted
to be in the explanation set but shouldn‚Äôt be. Similarly, a false
negative (fn) corresponds to a triple that is predicted to not be
in the explanation set but should be. Lastly a true positive (tp)
corresponds to a triple that is correctly predicted to belong in the
explanation set.

The precision answers the following question; given that a triple
is predicted to be in the explanation set, what are the chances that it
actually belongs in the explanation set? Furthermore, the recall can
be interpreted as how many triples the model was able to correctly
identify as belonging in the explanation set. The traditional ùêπ1-
Score computes the harmonic mean between the precision and
recall, incorporating both of these metrics when evaluating the
effectiveness of explanation retrieval. To compare two sets, the
Jaccard similarity forms a more intuitive scoring metric, thus we use
this metric in addition to precision, recall and ùêπ1-Score in comparing
explanation methods.

4 EXTRACTING AND GENERATING THE

ROYALTY DATASETS

Applying the method of Section 3.1, we build two datasets (Royalty-
20k and Royalty-30k), a collection of 20, 080 and 30, 734 triples

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

Nicholas Halliwell, Fabien Gandon, and Freddy Lecue

Figure 1: A triple (Princess Marie Anne of France, hasGrandparent, Anne of Austria) plotted in red with its explanation set in
green {(Princess Marie Anne of France, hasParent, Louis XIV of France), (Louis XIV of France, hasParent, Anne of Austria)}, and
neighboring triples.

respectively, containing royal family members from DBpedia [9].
The triples and explanations are derived from a set of rules intro-
duced later in this section. We use family members to construct
datasets with ground truth explanations, as the logical rules can be
easily understood, and no prior domain knowledge is needed.

An example from the Royalty-30k can be seen in Figure 1. Take
two entities Princess Marie Anne of France, and Anne of Austria,
that we wish to predict the link hasGrandparent between. Anne of
Austria is the grandparent of Princess Marie Anne of France, because
Louis XIV of France is the parent of Princess Marie Anne of France,
and Anne of Austria is the parent of Louis XIV of France.

Each example in the Royalty datasets consists of a triple e.g.
(Princess Marie Anne of France, hasGrandparent, Anne of Austria) and
a set of triples defining its ground truth explanation e.g. {(Princess
Marie Anne of France, hasParent, Louis XIV of France), (Louis XIV of
France, hasParent, Anne of Austria)}.

4.1 Royalty Datasets Rule Generation
In this work we focus on 4 logical rules based on family relation-
ships: hasSpouse, hasSuccessor, hasPredecessor, and hasGrandparent.
The predicate of each triple used on the link prediction task is in
the consequent of one of these rules. The associated explanation
set consists of the triples that triggered the rule.

Indeed there may be several ways to define these logical rules.
For example, hasSuccessor can be defined using its inverse relation
hasPredecessor. However, hasPredecessor in some cases, could be
correlated to the hasParent relation and therefore considered an
explanation. Both explanations could be correct in many cases,
thus the optimal explanation would be ambiguous. Therefore in
this work, we define all rules in both datasets such that there is
one and only one possible explanation set for each predicate. This
prevents an explanation method from having to arbitrarily select
between alternative explanations, and ensures a better evaluation
and understanding of the explanation techniques.

We define the Royalty-20k dataset using rules for hasSpouse, has-
Successor, and hasPredecessor predicates. The Royalty-30k dataset
is defined using rules for hasSpouse and hasGrandparent predicates.
We create two datasets, separating hasSuccessor and hasPredecessor
from hasGrandparent, to avoid having multiple ways to explain a
predicate. Each rule is detailed below.

Spouse. Some entity ùëã is the spouse of ùëå if ùëå is the spouse of
ùëã . i.e. ‚Ñéùëéùë†ùëÜùëùùëúùë¢ùë†ùëí (ùëã, ùëå ) ‚Üê ‚Ñéùëéùë†ùëÜùëùùëúùë¢ùë†ùëí (ùëå, ùëã ). This is a symmetric
relationship. There are 7, 526 triples with the hasSpouse predicate
in each dataset, 3, 763 of which are generated by rules. Note this
rule is the same for both datasets.

Successor and Predecessor. A successor in the context of roy-
alty is one who immediately follows the current holder of the throne.
ùëã is the successor of ùëå if ùëå is the predecessor of ùëã . Equivalently,
‚Ñéùëéùë†ùëÜùë¢ùëêùëêùëíùë†ùë†ùëúùëü (ùëã, ùëå ) ‚Üê ‚Ñéùëéùë†ùëÉùëüùëíùëëùëíùëêùëíùë†ùë†ùëúùëü (ùëå, ùëã ). Likewise, a prede-
cessor is defined as one who held the throne immediately before
the current holder. ùëã is the predecessor of ùëå if ùëå is the successor of
ùëã . Equivalently, ‚Ñéùëéùë†ùëÉùëüùëíùëëùëíùëêùëíùë†ùë†ùëúùëü (ùëã, ùëå ) ‚Üê ‚Ñéùëéùë†ùëÜùë¢ùëêùëêùëíùë†ùë†ùëúùëü (ùëå, ùëã ). In-
deed hasSuccessor and hasPredecessor follow an inverse relationship,
therefore triples with the hasSuccessor predicate are used to explain
the triples with the hasPredecessor predicate and vice-versa. There
are 6, 277 triples with hasSuccessor predicate, 2, 003 of which are
generated by rules. Similarly, there are 6, 277 triples with hasPrede-
cessor predicate, 2, 159 of which are generated by rules.

Grandparent. We define hasGrandparent to use a chain prop-
erty pattern, detailed in Section 4.2. ùëå is the grandparent of ùëã if ùëå is
the parent of ùëã ‚Äôs parent ùëÉ. Equivalently, ‚Ñéùëéùë†ùê∫ùëüùëéùëõùëëùëùùëéùëüùëíùëõùë° (ùëã, ùëå ) ‚Üê
‚Ñéùëéùë†ùëÉùëéùëüùëíùëõùë° (ùëã, ùëÉ) ‚àß ‚Ñéùëéùë†ùëÉùëéùëüùëíùëõùë° (ùëÉ, ùëå ). There are 7, 736 triples with
hasGrandparent predicate, all of which are generated by rules. Note
hasParent is provided by the DBpedia data and not defined by any
external logical rule.

4.2 Dataset Specifics
Many of these rules have similar structures because of the algebraic
properties of the predicate of the triples they generate. A predicate
ùëù is said to be symmetric for some subject ùë† and object ùëú if and only
if (ùë†, ùëù, ùëú) ‚Üê (ùëú, ùëù, ùë†). A predicate ùëù1 is the inverse of ùëù2 if and only
if (ùë†, ùëù1, ùëú) ‚Üê (ùëú, ùëù2, ùë†). Lastly, a predicate ùëù is a chain of predicates
ùëùùëñ if and only if (ùë†, ùëù, ùëú) ‚Üê (ùë†, ùëù1, ùë†2) ‚àß ... ‚àß (ùë†ùëõ, ùëùùëõ, ùëú).

Table 1 gives the details of each predicate. The ‚Äú# of Triples‚Äù
column denotes the total number of triples in the dataset with that
predicate. The ‚Äú# of Rule-Generated Triples‚Äù column denotes the
number of triples that were generated from a triggered rule. These
are triples not listed on DBpedia, and thus generated by the semantic
reasoner. The ‚Äú# Unique Entities‚Äù column denotes the number of
unique nodes in the graph for a given predicate, including the nodes
in the associated explanation triples. Furthermore, the explanation

Linked Data Ground Truth for Quantitative and Qualitative Evaluation of Explanations

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

Dataset

Predicate

# Triples

Royalty-20k

Royalty-30k

hasSpouse
hasSuccessor
hasPredecessor
Full data

hasSpouse
hasGrandparent
hasParent
Full data

7,526
6,277
6,277
20,080

7,526
7,736
15,472
30,734

# Rule
Generated
Triples

3,763
2,003
2,159
7,924

3,763
7,736
0
11,499

# Unique
Entities

Explanation
Cardinality

Predicate
Property

6,114
6,928
6,928
8,861

6,114
4,330
-
11,483

1
1
1
-

1
2
-
-

Symmetric
Inverse
Inverse
-

Symmetric
Chain
-
-

Table 1: Royalty datasets: Breakdown of each predicate in the dataset. # of Triples denotes the total number of triples with
that predicate. Explanation Cardinality denotes the number of triples in the ground truth explanation set.

cardinality (Expl. Cardinality) column gives the number of triples in
the ground truth explanation sets for each triple inferred by a given
rule. This is determined by the definition of the logical rule. Lastly,
the Predicate property column describes the algebraic properties
of the relations generated by the rules.

5 BENCHMARK
5.1 Experiment Details
One way to perform link prediction on Knowledge Graphs is to
learn an embedding for each entity and relation. For this experiment,
we use a Relational Graph Convolutional Network (RGCN) [12]
to learn embeddings. We chose to use this algorithm, as it can be
used with multiple explanation methods without the need for any
further adaptations. GNNExplainer is only defined for Graph Neu-
ral Networks, hence a GNN must be used on the link prediction
task. ExplaiNE requires a model that takes an adjacency matrix as
input. The RGCN meets both of these requirements. Additionally,
the scoring function has a meaningful interpretation, returning the
probability that the input triple is a fact. For a fair comparison of
explanation methods, both approaches must use the same embed-
dings. We fix the number of dimensions to 25, and use a learning
rate of 0.001 for all rules. The number of epochs used to train the
RGCN varied per rule, we use between 50 and 2000, as this gave
the best performance on the task of link prediction.

We use ExplaiNE [7] and GNNExplainer [15] to explain the
predictions of the RGCN. Model performance is reported on the full
dataset, and for each predicate subset. We report the accuracy of
the RGCN as a performance metric on the task of link prediction.
ExplaiNE relies on the assumption that an optimal explanation
can be found using one of the adjacent neighbors. We drop this
assumption on our experiment, and allow ExplaiNE to pick any
observed triple in the graph as a possible explanation candidate.
Note that using the gradient of the scoring function with respect
to the adjacency matrix, ExplaiNE requires no hyper-parameter
tuning.

We train the GNNExplainer using a learning rate of 0.001 for
each rule, which was the best performing learning rate from the set
{0.00001, 0.0001, 0.001}. We use between 10 and 30 iterations for

each observation. We use 3-fold cross validation for both models,
and report results of the best performing fold.

5.2 Link Prediction-Results
The first section of Table 2 reports results on the Royalty-20k dataset.
The topmost row reports the performance of the RGCN on the
task of link prediction. We observe the highest accuracy on the
hasSuccessor predicate, and performance dropping across each of
the other predicates. Overall, we see similar results for hasSuccessor,
and hasPredecessor.

The second section of Table 2 reports results on the Royalty-
30k dataset. From the topmost row we can see the performance of
the RGCN on the task of link prediction. We observe the highest
accuracy on the hasGrandparent predicate, which follows a chain
property. We observe the lowest performance on the hasSpouse
predicate.

5.3 Quantitative Evaluation of Link Prediction

Explanation

GNNExplainer. From Table 2, we can see the performance of
GNNExplainer on the task of explainable link prediction. On both
datasets, we observe its best Jaccard and ùêπ1-Score performance
on the hasSpouse predicate. Note that these predicates, hasSpouse,
hasSuccessor and hasPredecessor all have an explanation cardinality
of 1, meaning the ground truth explanation set contains 1 triple.
On the Royalty-30k dataset, we observe performance drops on the
hasGrandparent predicate. Recall this predicate follows a chain
property and has an explanation set with 2 triples, forming a path.
Note GNNExplainer has a recall of 1 for all rules. Indeed this
means GNNExplainer was able to correctly identify triples that
belong to the explanation. However, the predicted explanation sets
were often too large (up to 20 triples on average, for some rules),
and had many false positives: a recall of 1 can be trivially achieved
by including the entire input graph in the predicted explanation.
This was not the case for the predicted explanations of GNNEx-
plainer, however, the cardinality of the predicted explanation set of
GNNExplainer was often larger than the ground truth cardinality.

ExplaiNE. Lastly, Table 2 reports the performance of ExplaiNE
on the task of explainable link prediction. On both datasets, again

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

Nicholas Halliwell, Fabien Gandon, and Freddy Lecue

Dataset

Models

Metrics

Spouse

Successor

Predecessor Grandparent

Full set

Predicates

RGCN

Accuracy

0.682

GNN
Explainer

ExplaiNE

Precision
Recall
ùêπ1
Jaccard

Precision
Recall
ùêπ1
Jaccard

0.656
1.0
0.792
0.328

0.754
0.571
0.65
0.388

RGCN

Accuracy

0.682

GNN
Explainer

ExplaiNE

Precision
Recall
ùêπ1
Jaccard

Precision
Recall
ùêπ1
Jaccard

0.656
1.0
0.792
0.328

0.754
0.571
0.65
0.388

0.696

0.182
1.0
0.307
0.178

0.319
0.317
0.318
0.314

-

-
-
-
-

-
-
-
-

0.692

0.182
1.0
0.308
0.178

0.368
0.365
0.366
0.363

-

-
-
-
-

-
-
-
-

Royalty-20k

Royalty-30k

-

-
-
-
-

-
-
-
-

0.713

0.067
1.0
0.125
0.133

0.101
0.135
0.115
0.135

0.623

0.277
1.0
0.433
0.184

0.397
0.577
0.47
0.274

0.621

0.261
1.0
0.414
0.174

0.363
0.412
0.386
0.216

Table 2: Benchmark results on Royalty-20k and Royalty-30k: Link prediction results for RGCN, and explanation evaluation
for GNNExplainer and ExplaiNE. Best ùêπ1 and Jaccard scores per predicate denoted in bold.

we observe the best Jaccard and ùêπ1-Score performance on the has-
Spouse predicate. In general, we observe that rules with a similar
explanation structure had similar performance. On the Royalty-30k
dataset, we see lower relative performance on the predicates where
larger explanations need to be predicted, e.g. hasGrandparent.

GNNExplainer vs. ExplaiNE. Overall, we find ExplaiNE out-
performed GNNExplainer in terms of Jaccard score for all rules
across both datasets. Additionally, we find GNNExplainer outper-
forms ExplaiNE in terms of ùêπ1 score on the hasSpouse, and hasGrand-
parent predicate subsets, along with the Royalty-30k full dataset.
This is likely due to the high recall of GNNExplainer‚Äôs predicted
explanations. This is evidence the ùêπ1 score is not a good metric in
that specific configuration.

5.4 Qualitative Evaluation of Link Prediction

Explanation

Table 3 gives a breakdown of each explanation method‚Äôs most
frequent error by subset. Each row of this table can be read as
follows: Under the hasSpouse subset for example, the most com-
mon predicate across ExplaiNE‚Äôs incorrectly predicted explanations
was hasSpouse, and this predicate was observed in 100% of errors.
This error occurs when ExplaiNE predicts the wrong subject or
object in the explanation. For GNNExplainer, hasSpouse was also
the most common predicate amongst incorrectly predicted expla-
nations, also accounting for 100% of errors. Indeed this is possible
on the hasSpouse subset, as under this subset, there is only one
possible predicate to predict (hasSpouse).

As an example of one of ExplaiNE‚Äôs errors, for some triple (Albert
III, Count of Everstein, hasSpouse,Richeza of Poland) and its expla-
nation (Richeza of Poland, hasSpouse,Albert III, Count of Everstein),
ExplaiNE predicted a first degree neighbor (Richeza of Poland, has-
Spouse,Alfonso VIII of Leon and Castile) to be its explanation. Note
the incorrectly predicted triple uses the hasSpouse predicate but in
a wrong way.

Table 4 reports the most frequently missing predicate from Ex-
plaiNE‚Äôs errors. Each row denotes the predicate subset, the ground
truth predicates defining the rule, and the percentage of triples not
containing the ground truth predicate(s). For example, under the
hasSuccessor subset of the Royalty-20k dataset, 78% of ExplaiNE‚Äôs
errors did not contain hasPredecessor. Note we do not report the
most frequently missing predicate for GNNExplainer, as the recall
for each subset was 1, the ground truth triple(s) were always in-
cluded in the predicted explanation. Therefore, the percent missing
is 0 for each subset.

The first row of Figure 2 shows histograms of predicate counts of
ExplaiNE‚Äôs incorrectly predicted explanations. For example, under
the hasSuccessor subset of the Royalty-20k dataset, hasSuccessor
was the most frequently predicted predicate amongst ExplaiNE‚Äôs
incorrect explanations. From this we can conclude on this subset,
ExplaiNE‚Äôs most frequent error occurred by predicting the wrong
predicate. We also observe this phenomenon under the hasPrede-
cessor subset of the Royalty-20k dataset, and the hasGrandparent
subset of the Royalty-30k dataset. ExplaiNE incorrectly predicts ex-
planations to have the same predicate as the input triple (the triple
we want an explanation for). Furthermore, on the Royalty-20k and
Royalty-30k full data, ExplaiNE‚Äôs errors most frequently contained

Linked Data Ground Truth for Quantitative and Qualitative Evaluation of Explanations

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

Most Frequently Predicated Predicate

ExplaiNE

GNNExplainer

Dataset

Predicate

Most Frequent
Predicate

% of Error

Most Frequent
Predicate

% of Error

ùëÖùëúùë¶ùëéùëôùë°ùë¶ ‚àí 20ùëò

hasSpouse
hasSuccessor
hasPredecessor

hasSpouse
hasSuccessor
hasPredecessor

ùëÖùëúùë¶ùëéùëôùë°ùë¶ ‚àí 30ùëò

hasSpouse
hasGrandparent

hasSpouse
hasParent

100%
78%
73%

100%
54%

hasSpouse
hasPredecessor
hasSuccessor

hasSpouse
hasParent

100%
50%
50%

100%
50%

Table 3: Most frequent predicate across incorrectly predicted explanations, along with the percentage of error by subset.

ExplaiNE: Most Frequently Missing Predicate

Dataset

Predicate

Ground Truth % Missing

ùëÖùëúùë¶ùëéùëôùë°ùë¶ ‚àí 20ùëò

ùëÖùëúùë¶ùëéùëôùë°ùë¶ ‚àí 30ùëò

hasSpouse
hasSuccessor
hasPredecessor

hasSpouse
hasPredecessor
hasSuccessor

hasSpouse

hasGrandparent

hasSpouse
hasParent
hasParent

0%
78%
73%

0%
27%
27%

Table 4: ExplaiNE‚Äôs most frequently missing predicate. Each row denotes the predicate subset, the ground truth predicates
defining the rule, and the percentage of triples not containing the ground truth predicate(s)

the hasSpouse predicate. We can conclude that ExplaiNE‚Äôs use of the
gradient of the score with respect to the adjacency matrix assigns
a large gradient to triples with the same predicate as the input, and
to first degree neighbors of the input subject and object.

The second row of Figure 2 shows histograms of predicates
counts of GNNExplainer‚Äôs incorrectly predicted explanations. Un-
der the hasSuccessor, hasPredecessor subsets, GNNExplainer‚Äôs errors
were uniform. Incorrectly predicting an explanation to contain ei-
ther hasSuccessor or hasPredecessor predicates was equally likely.
We can conclude from this that a triple with an incorrect predicate
was equally likely to be predicted by GNNExplainer as a triple with
the correct predicate and incorrect subject and/or object. Similar
to ExplaiNE, the most frequent error on hasGrandparent occurred
by incorrectly predicting the same predicate as the input triple. On
the Royalty-30k full dataset, the majority of GNNExplainer‚Äôs errors
were triples using the hasGrandparent predicate.

5.5 Discussion
From GNNExplainer‚Äôs high recall, we conclude the explanations of
this method identifies all triples belonging in the explanation set.
However, many irrelevant triples are also included in the predicted
explanation set that shouldn‚Äôt be, and often the size of the true
explanation set is overestimated.

More generally, our method allows us to see that GNNExplainer
and ExplaiNE do not always make the same types of mistakes: one
may often choose an irrelevant relation type while the other may
often pick the right type of relation but with the wrong arguments.

This is useful to evaluate the impact of the choices made in Equa-
tions 1 and 2, and to propose and evaluate new methods addressing
the shortcomings.

From this experiment, we can see the importance of the Royalty-
20k and Royalty-30k datasets, along with the method we use to
generate it. This experiment shows that state-of-the-art explana-
tion methods do not always give accurate explanations. There are
many approaches to generating explanations, however, they must
be evaluated with a ground truth dataset and quantitative metric.
Our method, dataset, and metric allow researchers to develop new
explanation methods and quantitatively evaluate their explanations
in a way they were previously unable to.

6 CONCLUSION
On the task of explainable link prediction, there is no standard
dataset available to quantitatively compare explanations, as no
standard method exists to generate datasets with explanations. Ad-
ditionally, there is no standard evaluation metric to determine when
one explanation method is preferable to the other. In this work,
we propose a method, including two datasets (Royalty-20k, and
Royalty-30k), to compare predicted and ground truth explanations.
Furthermore, we propose the use of an evaluation metric, lever-
aging the Jaccard similarity between the predicted and ground
truth explanation for quantitative comparisons across explanation
methods. Lastly, we benchmark two state-of-the-art explanation
methods, ExplaiNE and GNNExplainer, and perform a quantitative
analysis on their predicted explanations using the Royalty datasets
and the aformentioned evaluation metric. As a result, we are able

WI-IAT ‚Äô21, December 14‚Äì17, 2021, ESSENDON, VIC, Australia

Nicholas Halliwell, Fabien Gandon, and Freddy Lecue

(a)

Royalty-20k
ExplaiNE
Successor

(b)

Royalty-20k
ExplaiNE
Predecessor

(c)

Royalty-20k
ExplaiNE
Full Data

(d)

Royalty-30k
ExplaiNE
Grandparent

(e)

Royalty-30k
ExplaiNE
Full Data

(f)

Royalty-20k
GNNExplainer
Successor

(g)

Royalty-20k
GNNExplainer
Predecessor

(h)

Royalty-20k
GNNExplainer
Full Data

(i)

Royalty-30k
GNNExplainer
Grandparent

(j)

Royalty-30k
GNNExplainer
Full Data

Figure 2: Predicate Frequency Count on Incorrectly Predicted Explanations. Note hasSpouse was omitted as only one predicate
could be predicted (hasSpouse).

to identify and quantify the different types of errors they make in
terms of both data and semantics.

This paper provides opportunities for future extensions. This
could involve using the ground truth explanations to distinguish
between model error and an explanation method error, i.e., de-
termining if the RGCN is causing the explanation method (Ex-
plaiNE/GNNExplainer) to produce incorrect explanations, or if the
error is coming from the explanation method itself.

REFERENCES
[1] Olivier Corby, Alban Gaignard, Catherine Faron Zucker, and Johan Montagnat.
2012. KGRAM Versatile Inference and Query Engine for the Web of Linked Data.
In IEEE/WIC/ACM Int. Conference on Web Intelligence.

[2] Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy L√©cu√©, Zeynep Akata,
Simone Stumpf, Peter Kieseberg, and Andreas Holzinger. 2018. Explainable AI:
The New 42?. In Machine Learning and Knowledge Extraction - Second IFIP CD-
MAKE (LNCS, Vol. 11015), Andreas Holzinger, Peter Kieseberg, A Min Tjoa, and
Edgar R. Weippl (Eds.). Springer.

[3] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d‚ÄôAmato, Gerard de Melo,
Claudio Gutierrez, Jos√© Emilio Labra Gayo, Sabrina Kirrane, Sebastian Neumaier,
Axel Polleres, et al. 2020. Knowledge graphs. preprint arXiv:2003.02320 (2020).

[4] Matthew Horridge. 2011. Justification based explanation in ontologies. Ph.D.

Dissertation. University of Manchester, UK.

[5] Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, and Mani B.
Srivastava. 2020. How Can I Explain This to You? An Empirical Study of Deep
Neural Network Explanation Methods. In Advances in Neural Information Pro-
cessing Systems.

[6] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. 2020.
A Survey on Knowledge Graphs: Representation, Acquisition and Applications.
CoRR abs/2002.00388 (2020). arXiv:2002.00388

[7] Bo Kang, Jefrey Lijffijt, and Tijl De Bie. 2019. ExplaiNE: An Approach for Explain-
ing Network Embedding-based Link Predictions. CoRR abs/1904.12694 (2019).
arXiv:1904.12694

[8] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations, ICLR.

[9] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas,
Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef,
S√∂ren Auer, and Christian Bizer. 2015. DBpedia - A large-scale, multilingual
knowledge base extracted from Wikipedia. Semantic Web (2015).

[10] Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic Evaluation of Summaries Us-
ing N-gram Co-occurrence Statistics. In Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics,
HLT-NAACL, Marti A. Hearst and Mari Ostendorf (Eds.). The Association for
Computational Linguistics.

[11] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:
a Method for Automatic Evaluation of Machine Translation. In Meeting of the
Association for Computational Linguistics. ACL.

[12] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,
Ivan Titov, and Max Welling. 2018. Modeling Relational Data with Graph
Convolutional Networks. In European Semantic Web Conference, ESWC, Aldo
Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha√´l Troncy,
Laura Hollink, Anna Tordai, and Mehwish Alam (Eds.).

[13] Benjamin Taskar, Ming Fai Wong, Pieter Abbeel, and Daphne Koller. 2003. Link
Prediction in Relational Data. In Advances in Neural Information Processing Sys-
tems.

[14] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em-
bedding Entities and Relations for Learning and Inference in Knowledge Bases.
In International Conference on Learning Representations, ICLR.

[15] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. GNNExplainer: Generating Explanations for Graph Neural Networks. In
Advances in Neural Information Processing Systems.

successorpredecessor02004006008001000predecessorsuccessor02004006008001000spousesuccessorpredecessor010002000300040005000grandparentparent050010001500200025003000spousegrandparentparent01000200030004000predecessorsuccessor01000200030004000500060007000predecessorsuccessor01000200030004000500060007000predecessorsuccessorspouse025005000750010000125001500017500grandparentparent05000100001500020000250003000035000grandparentparentspouse010000200003000040000