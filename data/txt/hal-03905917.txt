Collaborative Algorithms for Online Personalized Mean
Estimation
Mahsa Asadi, Aurélien Bellet, Odalric-Ambrym Maillard, Marc Tommasi

To cite this version:

Mahsa Asadi, Aurélien Bellet, Odalric-Ambrym Maillard, Marc Tommasi. Collaborative Algorithms
for Online Personalized Mean Estimation. Transactions on Machine Learning Research Journal, 2022.
￿hal-03905917￿

HAL Id: hal-03905917

https://inria.hal.science/hal-03905917

Submitted on 19 Dec 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Collaborative Algorithms for Online Personalized
Mean Estimation

Mahsa Asadi
Univ. Lille, Inria, CNRS, Centrale Lille
UMR 9189 - CRIStAL, F-59000 Lille, France

Aurélien Bellet
Univ. Lille, Inria, CNRS, Centrale Lille
UMR 9189 - CRIStAL, F-59000 Lille, France

Odalric-Ambrym Maillard
Univ. Lille, Inria, CNRS, Centrale Lille
UMR 9189 - CRIStAL, F-59000 Lille, France

Marc Tommasi
Univ. Lille, Inria, CNRS, Centrale Lille
UMR 9189 - CRIStAL, F-59000 Lille, France

llvllahsa@gmail.com

aurelien.bellet@inria.fr

odalric.maillard@inria.fr

marc.tommasi@inria.fr

Abstract

We consider an online estimation problem involving a set of agents. Each agent has access
to a (personal) process that generates samples from a real-valued distribution and seeks to
estimate its mean. We study the case where some of the distributions have the same mean,
and the agents are allowed to actively query information from other agents. The goal is to
design an algorithm that enables each agent to improve its mean estimate thanks to com-
munication with other agents. The means as well as the number of distributions with same
mean are unknown, which makes the task nontrivial. We introduce a novel collaborative
strategy to solve this online personalized mean estimation problem. We analyze its time
complexity and introduce variants that enjoy good performance in numerical experiments.
We also extend our approach to the setting where clusters of agents with similar means seek
to estimate the mean of their cluster.

1 Introduction

With the widespread of personal digital devices, ubiquitous computing and IoT (Internet of Things), the
need for decentralized and collaborative computing has become more pressing. Indeed, devices are first of all
designed to collect data and this data may be sensitive and/or too large to be transmitted. Therefore, it is
often preferable to keep the data on-device, where it has been collected. Local processing on a single device
is a always possible option but learning in isolation suffers from slow convergence time when data arrives
slowly. In that case, collaborative strategies can be investigated in order to increase statistical power and
accelerate learning. In recent years, such collaborative approaches have been broadly referred to as federated
learning (Kairouz et al., 2021).

The data collected at each device reflects the specific usage, production patterns and objective of the associ-
ated agent. Therefore, we must solve a set of personalized tasks over heterogeneous data distributions. Even
though the tasks are personalized, collaboration can play a significant role in reducing the time complexity
and accelerating learning in presence of agents who share similar objectives. An important building block to
design collaborative algorithms is then to identify agents acquiring data from the same (or similar) distri-
bution. This is particularly difficult to do in an online setting, in which data becomes available sequentially
over time.

1

In this work, we explore this challenging objective in the context of a new problem: online personalized mean
estimation. Formally, each agent continuously receives data from a personal σ-sub-Gaussian distribution and
aims to construct an accurate estimation of its mean as fast as possible. At each step, each agent receives a
new sample from its distribution but is also allowed to query the current local average of another agent. To
enable collaboration, we assume the existence of an underlying class structure where agents in the same class
have the same mean value. We also consider a relaxed assumption where the means of agents in a class are
close (but not necessarily equal). Such assumptions are natural in many real-world applications (Adi et al.,
2020). A simple example is that of in different environments, monitoring parameters such as temperature in
order to accurately estimate their mean (see for instance Mateo et al., 2013). Another example is collaborative
filtering, where the goal is to estimate user preferences by leveraging the existence of clusters of users with
similar preferences (Su & Khoshgoftaar, 2009a). Crucially, the number of classes and their cardinality are
unknown to the agents and must be discovered in an online fashion.

We propose collaborative algorithms to solve this problem, where agents identify the class they belong to in
an online fashion so as to better and faster estimate their own mean by assigning weights to other agents’
estimates. Our approach is grounded in Probably Approximately Correct (PAC) theory, allowing agents to
iteratively discard agents in different classes with high confidence. We provide a theoretical analysis of our
approach by bounding the time required by an agent to correctly estimate its class with high probability,
as well as the time required by an agent to estimate its mean to the desired accuracy. Our results highlight
the dependence on the gaps between the true means of agents in different classes, and show that in some
settings our approach achieves nearly the same time complexity as an oracle who would know the classes
beforehand. Our numerical experiments on synthetic data are in line with our theoretical findings and show
that some empirical variants of our approach can further improve the performance in practice.

The paper is organized as follows. Section 2 discusses the related work on federated learning and collaborative
online learning. In Section 3, we formally describe the problem setting and introduce relevant notations.
In Section 4, we introduce our algorithm and its variants. Section 5 presents our theoretical analysis of
the proposed algorithm in terms of class and mean estimation time complexity. Section 6 is devoted to
illustrative numerical experiments. Section 7 extends our approach to the case where classes consist of
agents with similar (but not necessarily equal) means and agents seek to estimate the mean of their class.
We conclude and discuss perspectives for future work in Section 8.

2 Related Work

Over the last few years, collaborative estimation and learning problems involving several agents with local
datasets have been extensively investigated under the broad term of Federated Learning (FL) (Kairouz et al.,
2021). While traditional FL algorithms learn a global estimate for all agents, more personalized approaches
have recently attracted a lot of interest (see for instance Vanhaesebrouck et al., 2017; Smith et al., 2017;
Fallah et al., 2020; Sattler et al., 2020; Hanzely et al., 2020; Marfoq et al., 2021, and references therein).
With the exception of recent work on simple linear regression settings (Cheng et al., 2021), these approaches
typically lack clear statistical assumptions on the relation between local data distributions and do not provide
error guarantees with respect to these underlying distributions. More importantly, the above methods focus
on the batch learning setting and are not suitable for online learning.

In the online setting, the work on collaborative learning has largely focused on multi-armed bandits (MAB).
Most approaches however consider a single MAB instance which is solved collaboratively by multiple agents.
Collaboration between agents can be implemented through broadcast messages to all agents (Hillel et al.,
2013; Tao et al., 2019), via a server (Wang et al., 2020b), or relying only on local message exchanges over a
network graph (Sankararaman et al., 2019; Martínez-Rubio et al., 2019; Wang et al., 2020a; Landgren et al.,
2021; Madhushani et al., 2021). Other approaches do not allow explicit communication but instead consider
a collision model where agents receive no reward if several agents pull the same arm (Boursier & Perchet,
2019; Wang et al., 2020a). In any case, all agents aim at solving the same task.

Some recent work considered collaborative MAB settings where the arm means vary across agents. Extending
their previous work (Boursier & Perchet, 2019), Boursier et al. (2020) consider the case where arm means
can vary among players. Under their collision model, the problem reduces to finding a one-to-one assignment

2

of agents to arms. In Shi & Shen (2021), the local arm means of each agent are IID random realizations
of fixed global means and the goal is to solve the global MAB using only observations from the local arms
with an algorithm inspired from traditional FL. Similarly, Karpov & Zhang (2022) extend the work of Tao
et al. (2019) by considering different local arm means for each agent with the goal to identify the arm with
largest aggregated mean. Shi et al. (2021) introduce a limited amount of personalization by extending the
model of Shi & Shen (2021) to optimize a mixture between the global and local MAB objectives. Réda et al.
(2022) further consider a known weighted combination of the local MAB objectives, and focus on the pure
exploration setting (best arm identification) rather than regret minimization. A crucial difference with our
work is that there is no need to discover relations between local distributions to solve the above problems.

Another related problem is to identify a graph structure on top of the arms in MAB. Kocák & Garivier
(2020; 2021) construct a similarity graph while solving the best arm identification problem, but consider
only a single agent. In contrast, our work considers a multi-agent setting with personalized estimation tasks,
and our approach discovers similarities across agents’ tasks in an online manner.

3 Problem Setting

We consider a mean estimation problem involving A agents. The goal of each agent a ∈ [A] = {1, 2, . . . , A}
is to estimate the mean µa of a personal distribution νa
over R. In this work, we assume that there exists
σ ≥ 0 such that each νa

is σ-sub-Gaussian, i.e.:

∀λ ∈ R,

log Ex∼νa

exp(λ(x − µa)) ≤

1
2

λ2σ2.

This classical assumption captures a property of strong tail decay, and includes in particular Gaussian
distributions (in that case, the smallest possible σ2 corresponds to the variance) as well as any distribution
supported on a bounded interval (e.g., Bernoulli distributions).

We consider an online and collaborative setting where data points are received sequentially and agents can
query each other to learn about their respective distributions. Agents should be thought of as different user
devices which operate in parallel. Therefore, they all receive a new sample and query another agent at each
time step.

′

a

=1 xt

Formally, we assume that time is synchronized between agents and at each time step t, each agent a receives
, which is used to update its local mean
a new sample xt
from its personal distribution νa with mean µa
a
t ∑t
estimate ¯xt
. It also chooses another agent l to query. As a response from querying agent l,
a,a = 1
t′
of agent l (i.e., the average of t independent samples from the personal
agent a receives the local average ¯xt
l,l
distribution νl) and stores it in its memory ¯xt
a,l = t.
a,l
a,A)] of the last local averages (and associated
Each agent a thus keeps a memory [(¯xt
number of samples) that it received from other agents. The information contained in this memory is used
to compute an estimate µt
of µa at each time t. Our goal is to design a query and estimation procedure for
a
each agent.

along with the corresponding number of samples nt

a,1), . . . , (¯xt

a,A, nt

a,1, nt

As described above, note that when an agent queries another agent at time t, it does not receive one sample
from this agent (as e.g. in multi-armed bandits), but receives the full statistics of observations of this agent
up to time t. This is considerably much more information than in typical MAB settings, and naturally
requires specific strategies.

The goal of each agent to find a good estimate of its personal mean as fast as possible, without consideration
for the quality of the estimates in earlier steps (i.e., we do not seek to minimize a notion of regret). In the
online learning terminology, this is referred to as a pure exploration setting (like best arm identification in
multi-armed bandits, see Audibert et al., 2010). Formally, we will measure the performance of an algorithm
using the notion of (ϵ, δ)-convergence in probability (Bertsekas & Tsitsiklis, 2002; Wasserman, 2013), which
we recall below.

Definition 1 (PAC-convergence). An estimation procedure for agent a is called (ϵ, δ)-convergent if there
of agent a is ϵ-distant from the true mean
exists τa ∈ N such that the probability that the mean estimator µt
a

3

for any time t > τa is at least 1 − δ:

P(∀t > τa ∶ ∣µt

a − µa∣ ≤ ϵ) > 1 − δ.

(1)

While it is easy to design (ϵ, δ)-convergent estimation procedure for a single agent taken in isolation, the
goal of this paper is to propose collaborative algorithms where agents benefit from information from other
agents by taking advantage of the relation between the agents’ distributions. This will allow them to build
up more accurate estimations in less time, i.e., with smaller time complexity τa.
Specifically, to foster collaboration between agents, we consider that the set of agents [A] is partitioned into
equivalence classes that correspond to agents with the same mean.1 In real scenarios, these classes may
represent sensors in the same environment, objects with the same technical characteristics, users with the
same behavior, etc. This assumption makes it possible for an agent to design strategies to identify other
agents in the same class and to use their estimates in order to speed up the estimation of his/her own mean.
Formally, we define the class of a as the set of agents who have the same mean as a.
Definition 2 (Similarity class). The similarity class of agent a is given by:

where ∆a,l = ∣µa − µl∣ is the gap between the means of agent a and agent l.

Ca = {l ∈ [A] ∶ ∆a,l = 0},

define the problem structure. We consider that the agents do not know the means,
are completely unknown.

The gaps {∆a,l}a,l∈[A]
the gaps, or even the number of underlying classes. Hence the classes {Ca}a∈[A]
This makes the problem quite challenging.
Remark 1 (Scalability). In large-scale systems, it may be impractical for agents to query all other agents
and/or to maintain a memory size that is linear in the number of agents A. From a practical point of view,
each agent can instead consider a restricted subset of agents of reasonable size. This subset could be picked
uniformly at random, be composed of neighboring nodes in the network or in the physical world, or be based
on prior knowledge on who is more likely to be in the same class (when available).

4 Proposed Approach

In this section, we first introduce some of the key technical components used in our approach, and then
present our proposed algorithm.

4.1 Main Concepts

a,1, . . . , ¯xt

a,A] that it holds in memory at time t. The generic confidence bound βδ(nt

a,l)] for the
a,l)
seen for agent l at time t, and δ corresponds to the risk parameter

In our approach, each agent a computes confidence intervals Ia,l = [¯xt
mean estimators [¯xt
takes as input the number of samples nt
so that with probability at least 1 − δ the true mean µl falls within the confidence interval Ia,l.
Agent a will use these confidence intervals to assess whether another agent l belongs to the class Ca through
the evaluation of an optimistic distance defined below.
Definition 3 (Optimistic distance). The optimistic distance with agent l from the perspective of agent a is
defined as:

a,l − βδ(nt

a,l + βδ(nt

a,l), ¯xt

a,l

a,δ(l) = ∣¯xt
dt

a,a − ¯xt

a,l∣ − βδ(nt

a,a) − βδ(nt

a,l).

(2)

a,δ(l) is, with high probability, a lower bound on
The “optimistic” terminology is justified by the fact that dt
the distance between the true means µa and µl of distributions νa and νl. Recall that two agents belong to
the same class if the distance of their true mean is zero. Since these values are unknown, the idea of the

1In Section 7, we will consider a relaxed version of this assumption where classes consist of agents with similar (not necessarily

equal) means.

4

Algorithm 1 ColME
Parameters: agent a, time horizon H, risk δ, weighting scheme α, and query strategy choose_agent

a,l ← 0, n0

∀l ∈ [A]: ¯x0
a ← {l ∈ [A] ∶ dt
C0
for t = 1, . . . , H do

a,l ← 0
a,δ(l) ≤ 0} = [A]

∀l ∈ [A]: ¯xt
Perceive:

a,l ← ¯xt−1

a,l

, nt

a,l ← nt−1
a,l

a,a ← t

, nt

a × 1
t

a,a × t−1
t

a ∼ νa
+ xt

Receive sample xt
¯xt
a,a ← ¯xt−1
Query:
Ct
a ← {l ∈ [A] ∶ dt
Query agent l = choose_agent(Ct
a,l ← ¯xt
¯xt
a,l ← t
Estimate:

a,δ(l) ≤ 0}

l,l, nt

a) to get ¯xt
l,l

a,δ(l) ≤ 0}

Ct
a ← {l ∈ [A] ∶ dt
µt
a ← ∑l∈Ct
end for
Output: µH
a

a

a,l × ¯xt
αt

a,l

above heuristic is to provide a proxy based on observed data and high probability confidence bounds. In
particular, we will adopt the Optimism in Face of Uncertainty Principle (OFU) (see Auer et al., 2002) and
consider that two agents may be in the same class if the optimistic distance is zero or less. Hence, we define
an optimistic notion of class accordingly.
Definition 4. The optimistic similarity class from the perspective of agent a at time t is defined as:

Having introduced the above concepts, we can now present our algorithm.

a = {l ∈ [A] ∶ dt
Ct

a,δ(l) ≤ 0}.

4.2 Algorithm

The collaborative mean estimation algorithm we propose, called ColME, is given in Algorithm 1 (taking the
perspective of agent a). For conciseness, we consider that βδ(0) = +∞. At each step t, agent a performs
three main steps.

In the Perceive step, the agent receives a sample from its distribution and updates its local average together
with the number of samples.

In the Query step, agent a selects another agent following a query strategy given as a parameter to the
ColME algorithm. Agent a runs the choose_agent function to select another agent l and asks for its current
local estimate to update its memory. We propose two variants for the choose_agent function:

• Round-Robin: cycle over the set [A] of agents one by one in a fixed order.

• Restricted-Round-Robin: like round-robin but ignores agents that are not in the set of optimistically

similar agents Ct
a

.

The focus on round-robin-style strategies is justified by the information structure of our problem setting,
which is very different from classic bandits.
Indeed, querying an agent at time t produces an estimate
computed on the t observations collected by this agent so far. The choice of variant (Round-Robin or
Restricted-Round-Robin) will affect the class identification time complexity, as we shall discuss later.

based on available in-
Finally, in the Estimate step, agent a computes the optimistic similarity class Ct
a
formation, and constructs its mean estimate as a weighted aggregation of the local averages of agents that
belong to Ct
a

. We propose different weighting mechanisms:

5

Simple weighting. This is a natural weighting mechanism for aggregating samples:

αt

a,l =

nt
a,l
∑l∈Ct

a

nt

a,l

.

Soft weighting. This is a heuristic weighting mechanism which leverages the intuition that the more the
confidence intervals of two agents overlap, the more likely that they are in the same class. Moreover, the
smaller the union of the agent means, the more confident we are that the agents are in the same class. In
other words, we are not equally confident about all the agents that are selected for estimation, and this
weighting mechanism incorporates this information:

a,l = nt
αt
a,l

∣Ia,a ∩ Ia,l∣
∣Ia,a ∪ Ia,l∣

×

1

Zsoft

,

where Zsoft = ∑i∈Ct

a

nt

a,i∣Ia,a∪Ia,i∣
∣Ia,a∩Ia,i∣

is a normalization factor.

Aggressive weighting. This is an extension of the previous soft weighting mechanism that is more
selective. Not only does it consider the overlap and intersection of the agents’ confidence intervals,
but it also requires the size of the intersection to be larger than half the size of both confidence in-
tervals from the two agents. Let us denote the binary value associated with this condition by Ea,l =
1
{∣Ia,a∩Ia,l∣>min{βδ(nt

. Then

a,l),βδ(nt

a,a)}}

a,l = nt
αt
a,l

∣Ia,a ∩ Ia,l∣
∣Ia,a ∪ Ia,l∣

×

Ea,l
Zagg

,

where Zagg = ∑i∈Ct

a

nt

a,i∣Ia,a∪Ia,i∣×Ea,i
∣Ia,a∩Ia,i∣

is a normalization factor.

4.3 Baselines

We introduce two baselines that will be used to put the performance of our approach into perspective, both
theoretically and empirically.

Local estimation. Estimates are computed without any collaboration, using only samples received from
the agent’s own distribution, i.e. µt

.

a = ¯xt

a,a

Oracle weighting. The agent knows the true class Ca via an oracle and uses the simple weighting αt

a,l =

a,l

nt
∑l∈Ca nt

a,l

.

5 Theoretical Analysis

In this section, we provide a theoretical analysis of our algorithm ColME for the query strategy Restricted-
Round-Robin and the simple weighting scheme. Specifically, we bound the time complexity in probability
for both class and mean estimation. Below, we explain the key steps involved in our analysis and state our
main results. All proofs can be found in the appendix.

A key aspect of our analysis is to characterize when the optimistic similarity class (Definition 4) coincides
with the true classes. We show that this is the case when two conditions hold. First, for a given agent a, we
need the confidence interval computed by a about agent l to contain the true mean µl for all l ∈ A.
Definition 5. We define the following events:

Et

a = ⋂
l∈[A]

∣¯xt

a,l − µl∣ ≤ βδ(nt

a,l),

Ea = ⋂
t∈N

Et
a.

6

(3)

(4)

We can guarantee that Ea holds with high probability via an appropriate parameterization of confidence
intervals. We use the so-called Laplace method (Maillard, 2019).

Lemma 1. Let δ ∈ (0, 1), a ∈ [A]. Setting βδ(n) = σ
have:

√

√

× (1 + 1
n

) ln(

n + 1/γ(δ)) with γ(δ) = δ
8×A

, we

2 1
n

P(Ea) ≥ 1 −

δ
8

.

(5)

The second condition is that agent’s a memory about the local estimates of other agents should contain
enough samples. Let us denote by ⌈β−1
Definition 6. From the perspective of agent a and at time t, event Gt
a

δ (x)⌉ the smallest integer n such that x > βδ(n).

is defined as:

nt
a,l > n⋆

a,l,

Gt

a = ⋂
l∈[A]
⎧⎪⎪
⌈β−1
⎨
⎪⎪⎩
⌈β−1

a,l =
n⋆

δ ( ∆a,l
)⌉
δ ( ∆a
)⌉

4

4

(6)

if l ∉ Ca,
otherwise,

where

∆a,l.

with ∆a = minl∈[A]/Ca
Note that the required number of samples is inversely proportional to the gaps between the means of agents
in different classes. Having enough samples and knowing that the true means fall within the confidence
bounds, we can show that the class-estimation rule dt
Lemma 2 (Class membership rule). Under Et

a,δ(l) ≤ 0 indicates the membership of l in Ca.
and ∀l ∈ [A] and at time t: dt

a,δ(l) > 0 ⇐⇒ l ∈ [A]/Ca.

a ∧ Gt
a

Using the above lemma, we obtain the following result for the time complexity of class estimation.
Theorem 1 (ColME class estimation time complexity). For any δ ∈ (0, 1), employing Restricted-Round-
Robin query strategy, we have:

P(∃t > ζa ∶ Ct

a ≠ Ca) ≤

δ
8

, with ζa = n⋆

a,a + A − 1 − ∑

1

l∈[A]∖Ca

{n⋆

a,a>n⋆

a,l+A−1}.

(7)

The dominating term in the class estimation time complexity ζa for agent a is equal to the number n⋆
of
samples required to distinguish agent a from the one who has smallest nonzero gap ∆a to a, which is of order
̃O(1/∆2
a).2 There is then an additional term of A − 1 since all others agents that are not in Ca could require
the same number of samples. Finally, the last term in (7) accounts for agents that require less samples and
had thus been eliminated before, which reflects the gain of using Restricted-Round-Robin query strategy
over Round-Robin. When we have enough samples (at least ζa), Theorem 1 guarantees that we correctly
learn the class (Ca = Ct
) with high probability. We build upon this result to quantify the mean estimation
a
time complexity of our approach.
Theorem 2 (ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-
Round-Robin query strategy and simple weighting, the mean estimator µt
)-convergent,
a
that is:

of agent a is (ϵ, δ
4

a,a

P(∀t > τa ∶ ∣µt

a − µa∣ ≤ ϵ) > 1 −

δ
4

, with τa = max(ζa,

⌈β−1

δ (ϵ)⌉
∣Ca∣

+

∣Ca∣ − 1
2

).

(8)

Several comments are in order. First, recall that collaboration induces a bias in mean estimation before class
estimation time. Because the problem structure is unknown, any collaborative algorithm that aggregate
observations from different agents will suffer from this bias, but the bias vanishes as soon as the class is
estimated and we outperform local estimation.

2We use ̃O(⋅) to hide constant and logarithmic terms.

7

Then, to interpret the guarantees provided by Theorem 2, it is useful to compare them with the local
δ (ϵ)⌉ = ̃O(1/ϵ2). Inspecting (8), we see that our approach
estimation baseline, which has time complexity ⌈β−1
has a time complexity of ̃O(max{1/∆2
a, 1/ϵ2∣Ca∣}). In other words, it is faster than local estimation as long
as the time ζa needed to correctly identify the class Ca is smaller than ⌈β−1

δ (ϵ)⌉, that is precisely when:

ϵ < βδ(n⋆

a,a + A − 1 − ∑

1

l∈[A]/{Ca}

{n⋆

a,a>n⋆

a,l+A−1}

).

(9)

This condition, which roughly amounts to ϵ < ∆a, relates the desired precision of the solution ϵ to the problem
(see Definition 6). We
structure captured by the gaps {∆a,l}l∈[A]
will see in our experiments that our theory predicts quite well whether an agent empirically benefits from
collaboration.

between the true means through {n⋆

a,l}l∈[A]

√

Remarkably, our approach can be up to ∣Ca∣ times faster than local estimation: this happens roughly when
∣Ca∣, i.e., for large enough gaps or small enough ϵ. In this regime, the speed-up achieved by our
ϵ < ∆a/
Indeed, the time complexity of the oracle weighting baseline introduced in
approach is nearly optimal.
−1
= ̃O(1/ϵ2∣Ca∣), just like our approach. Note that in a full information
Section 4.3 is precisely ⌈β
δ (ϵ)⌉
+ ∣Ca∣−1
2
∣Ca∣
setting where agent a would know Ca and would also have access to up-to-date samples from all agents at
each step, the time complexity would be only slightly smaller, namely ⌈β

.

−1
δ (ϵ)⌉
∣Ca∣

Remark 2 (Frequency of communication). For simplicity, we consider that agents communicate each time
they collect a new sample, which is standard in the literature of collaborative learning (see for instance the
collaborative MAB approaches discussed in Section 2). However, different trade-offs between communication
and data collection can be considered. In particular, it is straightforward to adapt the setting and our results
to the case where each agent collects m samples between each communication: it amounts to multiplying by
m the number of observations in our confidence intervals and empirical estimates. This provides a way to
reduce communication, as well as to mitigate privacy concerns by ensuring that only sufficiently aggregated
quantities are exchanged (even in early rounds). Extensions to cases where the number of samples between
each communication is random and/or varies across agents are left for future work.

6 Numerical Results

In this section, we provide numerical experiments on synthetic data to illustrate our theoretical results and
assess the practical performance of our proposed algorithms.3

6.1 Experimental Setting

We consider A = 200 agents, a time horizon of 2500 steps and a risk parameter δ = 0.001. The personal
distributions of agents are all Gaussian with variance σ2 = 0.25 and belong to one of 3 classes with means 0.2,
0.4 and 0.8. The class membership of each agent (and thus the value of its true mean) is chosen uniformly
at random among the three classes. We thus obtain roughly balanced class sizes. While the evaluation
presented in this section focuses on this 3-class problem, in the appendix we provide additional results on a
simpler 2-class problem (with means 0.2 and 0.8) where the benefits of our algorithm is even more significant.

We consider several variants of our algorithm ColME: we compare query strategies Round-Robin and
Restricted-Round-Robin with simple weighting, and also evaluate the use of soft and aggressive weight-
ing schemes in the Restricted-Round-Robin case. This gives 4 variants of our algorithm: Round-Robin,
Restricted-Round-Robin, Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-Robin.

Regarding competing approaches, we recall that our setting is novel and we are not aware of existing algo-
rithms addressing the same problem. We can however compare against two baseline strategies. The Local
baseline corresponds to the case of no collaboration. On the other hand, the Oracle baseline represents an
upper bound on the achievable performance by any collaborative algorithm as it is given as input the true
class membership of each agent and thus does not need to perform class estimation.

3The code can be found at https://github.com/llvllahsa/CollaborativePersonalizedMeanEstimation

8

(a) Class estimation precision over all agents

(b) Error in mean estimation over all agents

Figure 1: Results across time on the 3-class problem (Gaussian distributions with true means 0.2, 0.4, 0.8).
Thanks to our collaborative algorithms (Soft-Restricted-Round-Robin, Aggressive-Restricted-Round-Robin,
Restricted-Round-Robin, Round-Robin), agents are able to estimate their true class (Fig. 1(a)) and thereby
obtain accurate mean estimates much more quickly than using purely local estimation (Fig. 1(b)).

Algorithm

Round-Robin (RR)
Restricted-RR

class 0.2

class 0.4

avg/std

max

avg/std

1378 ± 194
1376 ± 211

2150
2163

1382 ± 191
1379 ± 210

class 0.8
avg/std max

407 ± 40
373 ± 55

662
934

max

2159
2245

Theoretical Restricted-RR (ζa)

3878

3878

1085

Table 1: Empirical class estimation times of Round-Robin and Restricted-RR on the 3-class problem (Gaus-
sian distributions with true means 0.2, 0.4, 0.8). We report the average, standard deviation and maximum
across agents and runs. We also report the high-probability class estimation time ζa given by our theory.

All algorithms are compared across 20 random runs corresponding to 20 different samples. In a given run,
at each time step, each agent receives the same sample for all algorithms.

6.2 Class Estimation

We start by investigating the performance in class estimation. In this experiment, only Round-Robin and
Restricted-Round-Robin are shown since the different weighting schemes have no effect on class estimation.
We first look at how well an agent a estimates its true class Ca with its heuristic class Ct
a
measure this, we consider the precision at time t computed as follows:

across time. To

precision

Ct
a

=

∣Ct

a ∩ Ca∣
∣Ct
a∣

.

(10)

We compute the average and standard deviation of (10) across runs, and then average these over all agents.
Figure 1(a) shows how the precision of class estimation varies across time as agents progressively remove
others from their heuristic class and eventually identify their true class. As can be seen clearly in Figure 3
in the appendix, the classes 0.2 and 0.8 are separated very early, quickly followed by 0.4 and 0.8 and finally,
after sufficiently many samples have been collected, the pair with the smallest gap (0.4 and 0.2).

We also observe that Round-Robin and Restricted-Round-Robin only differ slightly in the last time steps
before classes are identified. This is in line with Theorem 1, which shows that class estimation time mainly
, the time needed to eliminate the agent with smallest nonzero gap. This dominant term
depends on n⋆
and the fact that querying an agent at time t yields the full statistics of observations of this agent up to time
t explain that the gain of Restricted-Round-Robin is small compared to vanilla Round-Robin.

a,a

Table 1 shows statistics about the average, standard deviation and maximum time taken by an agent to
correctly identify its class. As expected, agents from class 0.8 identify their class much faster as the gaps

9

05001000150020002500Time0.30.40.50.60.70.80.91.0Precision in class estimationrestricted-round-robinround-robin05001000150020002500Time0.0000.0050.0100.0150.0200.0250.030Error in mean estimationsoft-restricted-round-robinaggressive-restricted-round-robinrestricted-round-robinround-robinoraclelocalAlgorithm

Round-Robin (RR)
Restricted-RR
Soft-Restricted-RR
Aggressive-Restricted-RR

Local

Oracle

conv(0.1)

conv(0.01)

avg/std

max

avg/std

417 ± 230
405 ± 227
82 ± 48
56 ± 39

1239
1175
340
340

916 ± 427
894 ± 438
608 ± 290
335 ± 127

max

2088
1925
1432
958

41 ± 39

289

4494 ± 3945

28616

5 ± 4

19

98 ± 63

303

Theoretical Restricted-RR (τa)
Theoretical Local (τa)

3878‌
885

33406
100216

Table 2: Empirical convergence times (see Eq. 12) of different algorithms on the 3-class problem (Gaussian
distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable regime) and
ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across agents and
runs. We also report the high-probability mean estimation times τa given by our theory for Restricted-
Round-Robin and Local. In line with our theory, we see that our approach largely outperforms the local
estimation baseline in the favorable regime and remains competitive in the unfavorable regime.

with respect to other classes are larger. Table 1 also reports the high-probability class estimation time ζa
of Restricted-Round-Robin given by our theoretical analysis (Theorem 1). This theoretical value is rather
close to the maximum value we observe: although these values are not directly comparable, it suggests that
our analysis captures the correct order of magnitude.

6.3 Mean Estimation

We now turn to our main objective: mean estimation. The error of an agent a at time t is evaluated as the
absolute difference of its mean estimate with its true mean:

errort

a = ∣µt

a − µa∣.

(11)

Similar to above, we compute the average and standard deviation of this quantity across runs, and then
for each time step we report in Figure 1(b) the average of these quantities across all agents for the
different algorithms (Soft-Restricted-Round-Robin, Aggressive-Restricted-Round-Robin, Restricted-Round-
Robin, Round-Robin, Oracle, and Local).

As expected, all variants of ColME suffer from mean estimation bias in the early steps (due to op-
timistic class estimation). However, as the estimated class of each agent gets more precise (see Fig-
ure 1(a)), agents progressively eliminate this bias and eventually learn estimates with similar error and
variance as the Oracle baseline. On the other hand, Local does not have estimation bias (hence achieves
smaller error on average in early rounds) but exhibits much higher variance, and its average error con-
verges very slowly towards zero. These results show the ability of our collaborative algorithms to con-
struct highly accurate mean estimates much faster than without collaboration. We can also see that
Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-Robin converge much quicker to low error
estimates than Restricted-Round-Robin. This shows that our proposed heuristic weighting schemes success-
fully reduce the relative weight given to agents that actually belong to different classes well before they are
identified as such with sufficient confidence. The aggressive weighting scheme is observed to perform best in
practice.

In the appendix, we plot the error in mean estimation for each class separately and observe that agents with
mean 0.8 (i.e., in the class that is easiest to discriminate from others) are the fastest to reach highly accurate
estimates, followed by those with mean 0.4, and finally those with mean 0.2.

Finally, we quantitatively compare the convergence time of different algorithms with an empirical measure
inspired by our theoretical PAC criterion (Definition 1). We define the empirical convergence time of an

10

agent as the earliest time step where the estimation error of the agent always stays lower than some ϵ:

conva(ϵ) = min{τ ∈ N ∶ ∀t ≥ τ, errort

a ≤ ϵ}.

(12)

We denote by conv(ϵ) the average of the above quantity across all runs and all agents.

Table 2 reports the average, standard deviation and maximum empirical convergence time across agents and
runs for two values of ϵ (we also provide per-class tables in the appendix). These values were chosen to
reflect the two different regimes suggested by our theoretical analysis. Indeed, recall that our theory gives
a criterion to predict whether our collaborative algorithms will outperform the Local baseline: this is the
case when the desired accuracy of the solution ϵ is small enough for the given problem instance (see Eq. 9).
For the problem considered here, Eq. (9) gives that Restricted-Round-Robin will outperform Local for all
agents as soon as ϵ is smaller than 0.049. We thus choose ϵ = 0.01 as the favorable regime (where we should
beat Local) and ϵ = 0.1 as the unfavorable regime. We provide the corresponding high-probability mean
estimation times τa of Restricted-Round-Robin and Local given by our theoretical analysis (Theorem 2).
The results in Table 2 are consistent with our theory. All variants of our algorithms largely outperform
Local for ϵ = 0.01,4 while Local is better for ϵ = 0.1 as agents can reach this precision using only their
own samples faster than they can reliably estimate their class. Overall, Restricted-Round-Robin performs
marginally better than Round-Robin, while Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-
Robin significantly outperform Round-Robin and Restricted-Round-Robin in both cases. Note that Soft-
Restricted-Round-Robin and Aggressive-Restricted-Round-Robin perform roughly the same as Local in the
unfavorable regime, and get very close to the performance of the Oracle baseline in the favorable regime.
These results again show the relevance of our collaborative algorithms and heuristic weighting schemes. We
observe that our theoretical results get looser as ϵ → 0, which is somewhat expected.

7 Extension to Imperfect Classes

So far we have assumed that two agents are in the same class if their personal distributions have exactly the
same mean, which can be restrictive for some use-cases. In this section, we show that we can extend the
problem setup and our approach to the case where two agents are considered to be in the same class if their
means are close enough and agents seek to estimate the mean of their class.

Formally, we define a new notion of similarity class parameterized by a radius η, which generalizes our
previous notion introduced in Definition 2.

Definition 7. Given η > 0, the η-similarity class of agent a is given by:

where ∆a,l = ∣µa − µl∣ is the gap between the means of agent a and agent l.

Cη,a = {l ∈ [A] ∶ ∆a,l ≤ η},

This notion of “imperfect” similarity class allows to capture situations where clusters of agents have similar
(but not necessarily equal) means. Such discrepancies between the means of agents in the same class may for
instance stem from the presence of local measurement bias (e.g., due to local variations in the environment,
see Taghavi et al., 2016). They can also be used to model groups of agents with similar preferences, behavior,
or goals, in applications like collaborative filtering (Su & Khoshgoftaar, 2009b),

In this context, it is natural to slightly redefine the estimation objective. Instead of estimating its personal
mean µa as considered so far, each agent a aims to estimate the mean of its class:

µη,a =

1
∣Cη,a∣

µl.

∑
l∈Cη,a

(13)

For instance, in the presence of (centered) local measurement bias, estimating the class mean (instead of the
local mean) allows to debias the estimate.

4We had to run Local for a much larger time horizon of 30000 steps for all runs to converge to accuracy ϵ = 0.01.

11

Remark 3 (Non-separated clusters). We do not formally require that the η-similarity classes form separated
clusters, in the sense that for three distinct agents a, l, i ∈ [A] we may have simultaneously i ∈ Cη,a, i ∈ Cη,l
and Cη,a ≠ Cη,l. This happens when ∆a,i ≤ η, ∆l,i ≤ η and η < ∆a,l ≤ 2η. In this case, the “class” of an agent
simply corresponds to a ball of radius η around its mean, which potentially overlaps with others and thus
violates the transitivity property of equivalence classes. For consistency with the rest of the paper and with
an slight abuse of terminology, we continue to use the term “class”. Although the case of separated clusters
appears more natural, we note that our proposed approach will still work in the non-separated setting, in the
sense that agents will correctly estimate the mean of their class as defined in Eq. 13.

Based on the above, we can adapt the notion of optimistic similarity class (Definition 2) and the condition
on the number of samples required for this optimistic class to coincide with the true class (Definition 6) by
incorporating η.
Definition 8. The η-optimistic similarity class from the perspective of agent a at time t is defined as:

Definition 9. From the perspective of agent a and at time t, event Gt

η,a

is defined as:

η,a = {l ∈ [A] ∶ dt
Ct

a,δ(l) ≤ η}.

Gt

η,a = ⋂
l∈[A]

a,l > nη
nt

a,l,

where

nη
a,l =

⎧⎪⎪
⌈β−1
⎨
⎪⎪⎩
⌈β−1

δ ( ∆a,l−η
δ ( ∆η,a−η

4

4

)⌉
)⌉

(14)

if l ∉ Ca,
otherwise,

with ∆η,a = minl∈[A]/Cη,a
Lemma 3 (Class membership rule). Under Et
[A]/Cη,a.

∆a,l.

a ∧ Gt

η,a

and ∀l ∈ [A] and at time t: dt

a,δ(l) > η ⇐⇒ l ∈

We can see from the above that ruling out an agent l from the optimistic class Cη,a requires more samples
for larger η, which is expected as the size of the confidence interval needs to be smaller to make this decision
reliably.

With these tools in place, we can use our collaborative mean estimation algorithm ColME (Algorithm 1)
presented before, with only minor modifications: we simply need to replace the notion of optimistic similarity
class by the η-version of Definition 8, and compute the estimate µt
at time t using a simple class-uniform
η,a
to match the objective in Eq. 13. We refer to this algorithm as η-ColME. Note
weighting scheme αt
that η becomes a parameter of the algorithm, allowing to choose the desired radius for the class structure.

a,l = 1
∣Ct

η,a∣

We can now state the class and mean estimation complexity of η-ColME. The proofs can be found in the
appendix.
Theorem 3 (η-ColME class estimation time complexity). For any δ ∈ (0, 1), employing Restricted-Round-
Robin query strategy, we have:

P(∃t > ζ η

a ∶ Ct

η,a ≠ Cη,a) ≤

δ
8

, with ζ η

a = nη

a,a + A − 1 − ∑

1

l∈[A]∖Cη,a

{nη

a,a>nη

a,l+A−1}.

(15)

Theorem 4 (η-ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-
Round-Robin query strategy and class-uniform weighting (while employing Cη,a), the mean estimator µt
of
a
agent a is (ϵ, δ
4

)-convergent, that is:

P(∀t > τ η

a ∶ ∣µt

η,a − µη,a∣ ≤ ϵ) > 1 −

δ
4

, with τ η

a = max(ζ η

a , β−1

δ (ϵ) + ∣Cη,a∣ − 1).

(16)

The class estimation result (Theorem 3) is similar to its “perfect” class counterpart (Theorem 1) except
that it involves η-dependent quantities. The mean estimation result (Theorem 4) differs slightly more from
the perfect class case (Theorem 2) because the estimation objective (see Eq. 13) and weighting scheme are
different. But most importantly, we see that for large enough gaps or small enough precision ϵ (similar to
Eq. 9), we again achieve an optimal speed since the time complexity of an oracle weighting baseline that
δ (ϵ) + ∣Cη,a∣ − 1.
would know the true classes beforehand is β−1

12

8 Conclusion

We have presented collaborative online algorithms where each agent learns the set (class) of agents who
shares the same (or similar) objective and uses this knowledge to speed up the estimation of its personalized
mean. We have provided PAC-style guarantees for the class and mean estimation time complexity of our
algorithms. In addition, we have introduced a number of sample weighting mechanisms to decrease the bias
of the estimates in the early rounds of learning, whose benefits are illustrated empirically.

Our work initiates the study of online, collaborative and personalized estimation and learning problems,
which we believe to be a promising area for future work. We outline a few interesting directions below.

Instead of the optimistic approach taken in this work, one could try to
Optimistic vs conservative.
design a more conservative algorithm where an agent incorporates the estimate of another agent only when
it knows (with sufficient probability) that it belongs to the same class. This would avoid introducing bias
in the estimation in early steps. However, a downside of such an approach is that agents would typically
need some knowledge of the gaps between their true means in order to determine when another agent can
be considered to be in the same class, which would be a big practical limitation.

Large-scale variants. While a simple way to scale up our approach to a large number of agents is to
have each agent focus on a restricted subset of other agents (see Remark 1), an interesting direction to allow
more exploration in large-scale systems could rely on the idea of peer sampling (Jelasity et al., 2007), i.e.,
randomly sampling a few agents from time to time to discover potential new members of the class beyond
the initial subset.

Handling data drift. We would like to extend our approach to handle data drift, where the means of
agents can change over time. Here, one could try to adapt ideas from non-stationary bandits, such as sliding-
window UCB (Garivier & Moulines, 2011) or UCB strategies mixed with change-point detection algorithms
(Cao et al., 2019).

In use cases where data is considered sensitive (e.g., personal data), it is important to
Privacy guarantees.
provide strong privacy guarantees to the agents. While our approach only requires agents to share aggregate
quantities (see also Remark 2), these may still leak sensitive information. In future work, we would like
to use tools from differential privacy (Dwork & Roth, 2013), such as the tree aggregation technique for
sharing cumulative sums in an online way (Dwork et al., 2010; Chan et al., 2011), to provide formal privacy
guarantees and analyze the resulting trade-offs between privacy and utility.

Extensions to personalized learning tasks. Finally, the problem could be extended to cases where
each agent aims to solve a personalized machine learning task (Vanhaesebrouck et al., 2017) based on the
data it receives online. In that case, a structure in the distribution conditioned by the outputs of the learned
models can be inferred, introducing an interesting exploration-exploitation dilemma in the learning task.

Acknowledgments

The authors thank the reviewers for their insightful comments that allowed to improve the paper. This work
was funded in part by Métropole Européenne de Lille (MEL), Inria, Université de Lille, and the I-SITE
ULNE through the AI chair Apprenf number R-PILOTE-19-004-APPRENF.

References

Erwin Adi, Adnan Anwar, Zubair Baig, and Sherali Zeadally. Machine learning and data analytics for the

iot. Neural Computing and Applications, 32(20):16205–16233, 2020.

Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. Best arm identification in multi-armed bandits.

In COLT, 2010.

13

Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine learning, 47(2):235–256, 2002.

Dimitri P Bertsekas and John N Tsitsiklis. Introduction to probability, volume 1. Athena Scientific Belmont,

MA, 2002.

Etienne Boursier and Vianney Perchet. Sic - mmab: Synchronisation involves communication in multiplayer

multi-armed bandits. In NeurIPS, 2019.

Etienne Boursier, Emilie Kaufmann, Abbas Mehrabian, and Vianney Perchet. A practical algorithm for

multiplayer bandits when arm means vary among players. In AISTATS, 2020.

Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with change

detection for piecewise-stationary bandit. In ICML, 2019.

T.-H. Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Transac-

tions on Information Systems Security, 14(3):1–26, 2011.

Gary Cheng, Karan Chadha, and John Duchi. Federated asymptotics: a model to compare federated learning

algorithms. arXiv preprint arXiv:2108.07313, 2021.

Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Foundations and

Trends® in Theoretical Computer Science, 9(3-4):211–407, 2013.

Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N. Rothblum. Differential privacy under continual

observation. In STOC, 2010.

Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning

approach. In NeurIPS, 2020.

Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In

ALT, 2011.

Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter Richtárik. Lower bounds and optimal algorithms

for personalized federated learning. In NeurIPS, 2020.

Eshcar Hillel, Zohar Shay Karnin, Tomer Koren, Ronny Lempel, and Oren Somekh. Distributed exploration

in multi-armed bandits. In NIPS, 2013.

Márk Jelasity, Spyros Voulgaris, Rachid Guerraoui, Anne-Marie Kermarrec, and Maarten Van Steen. Gossip-

based peer sampling. ACM Transactions on Computer Systems (TOCS), 25(3):8–es, 2007.

Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hu-
bert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih
Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben
Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný, Aleksan-
dra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar
Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana
Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian
Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu,
and Sen Zhao. Advances and open problems in federated learning. Foundations and Trends® in Machine
Learning, 14(1–2):1–210, 2021.

Nikolai Karpov and Qin Zhang. Collaborative best arm identification with limited communication on non-iid

data. arXiv preprint arXiv:2207.08015, 2022.

Tomáš Kocák and Aurélien Garivier. Best arm identification in spectral bandits. In IJCAI, 2020.

Tomáš Kocák and Aurélien Garivier. Epsilon best arm identification in spectral bandits. In IJCAI, 2021.

14

Peter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. Distributed cooperative decision making

in multi-agent multi-armed bandits. Automatica, 125:109445, 2021.

Udari Madhushani, Abhimanyu Dubey, Naomi Ehrich Leonard, and Alex Pentland. One more step towards

reality: Cooperative bandits with imperfect communication. In NeurIPS, 2021.

Odalric-Ambrym Maillard. Mathematics of statistical sequential decision making. PhD thesis, Université de

Lille, Sciences et Technologies, 2019.

Othmane Marfoq, Giovanni Neglia, Aurélien Bellet, Laetitia Kameni, and Richard Vidal. Federated Multi-

Task Learning under a Mixture of Distributions. In NeurIPS, 2021.

David Martínez-Rubio, Varun Kanade, and Patrick Rebeschini. Decentralized cooperative stochastic bandits.

In Proc. of NIPS, 2019.

Fernando Mateo, Juan José Carrasco, Abderrahim Sellami, Mónica Millán-Giraldo, Manuel Domínguez, and
Emilio Soria-Olivas. Machine learning methods to forecast temperature in buildings. Expert Systems with
Applications, 40(4):1061–1068, 2013.

Clémence Réda, Sattar Vakili, and Emilie Kaufmann. Near-optimal collaborative learning in bandits. In

NeurIPS, 2022.

Abishek Sankararaman, Ayalvadi Ganesh, and Sanjay Shakkottai. Social learning in multi agent multi armed
bandits. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 3(3):1–35, 2019.

Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-agnostic
distributed multitask optimization under privacy constraints. IEEE Transactions on Neural Networks and
Learning Systems, 2020.

Chengshuai Shi and Cong Shen. Federated multi-armed bandits. In AAAI, 2021.

Chengshuai Shi, Cong Shen, and Jing Yang. Federated multi-armed bandits with personalization. In AIS-

TATS, 2021.

Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In

NIPS, 2017.

Xiaoyuan Su and Taghi M. Khoshgoftaar. A survey of collaborative filtering techniques. Advances in Artificial

Intelligence, 2009, 2009a.

Xiaoyuan Su and Taghi M. Khoshgoftaar. A survey of collaborative filtering techniques. Advances in Artificial

Intelligence, 2009b.

Ehsan Taghavi, Ratnasingham Tharmarasa, Thia Kirubarajan, Yaakov Bar-Shalom, and Michael McDon-
ald. A practical bias estimation algorithm for multisensor-multitarget tracking. IEEE Transactions on
Aerospace and Electronic Systems, 52(1):2–19, 2016.

Chao Tao, Qin Zhang, and Yuan Zhou. Collaborative learning with limited interaction: Tight bounds for

distributed exploration in multi-armed bandits. In FOCS, 2019.

Paul Vanhaesebrouck, Aurélien Bellet, and Marc Tommasi. Decentralized collaborative learning of person-

alized models over networks. In Proc. of AISTATS, 2017.

Po-An Wang, Alexandre Proutiere, Kaito Ariu, Yassir Jedra, and Alessio Russo. Optimal algorithms for
multiplayer multi-armed bandits. In International Conference on Artificial Intelligence and Statistics, pp.
4120–4129. PMLR, 2020a.

Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang. Distributed bandit learning: Near-optimal

regret with efficient communication. In ICLR, 2020b.

Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science & Business

Media, 2013.

15

Appendix A Proof of Lemma 1

Lemma 4. Let µt
a
and is σ-sub-gaussian. For all δ ∈ (0, 1), it holds:

be the mean value of t independent real-valued random variables with the true mean µa

√

P(∃t ∈ N, µt

a − µa ≥ σ

√

P(∃t ∈ N, µa − µt

a ≥ σ

2

t
2

t

(1 +

(1 +

1

t
1

t

) ln(

) ln(

√

t + 1/δ)) ≤ δ ,

√

t + 1/δ)) ≤ δ .

(17)

(18)

Proof. The two inequalities are proved in the same way as a direct consequence of Maillard (2019), Lemma
2.7 therein. Let Y1, . . . , Yt be a sequence of independent real-valued random variables where for each s ≤ t,
Ys has mean µs

and is σs-sub-Gaussian, then for all δ ∈ (0, 1), it holds that

P(∃t ∈ N,

t
∑
s=1

(Ys − µs) ≥

s (1 +
σ2

√

) ln(

1

t

t
∑
s=1

t + 1/δ)) ≤ δ .

When all random variables Ys have the same mean µa and variance σ, we have

P(∃t ∈ N,

t
∑
s=1

(Ys − µa) ≥

2tσ2(1 +

1

t

) ln(

√

t + 1/δ)) ≤ δ,

Taking the average rather than the sum, i.e. dividing both sides by t we obtain that:

P(∃t ∈ N,

t
∑
s=1

(

Ys
t

−

µa
t

) ≥

2

t

σ2(1 +

√

) ln(

1

t

t + 1/δ)) ≤ δ,

¿
`
`(cid:192)2

√

√

And denoting µt

a = ∑t

s=1

P(∃t ∈ N,

t
∑
s=1

Ys
t

− µa ≥

, we conclude

Ys
t

√

√

P(∃t ∈ N, µt

a − µa ≥ σ

2

t

2

t

σ2(1 +

√

) ln(

1

t

t + 1/δ)) ≤ δ .

(1 +

√

) ln(

1

t

t + 1/δ)) ≤ δ .

Lemma 1. Let δ ∈ (0, 1), a ∈ [A]. Setting βδ(n) = σ
have:

√

2 1
n

√

× (1 + 1
n

) ln(

n + 1/γ(δ)) with γ(δ) = δ
8×A

, we

P(Ea) ≥ 1 −

δ
8

.

(5)

Proof. Let us recall that Ea = ⋂
t∈N

∣¯xt

a,l − µl∣ ≤ βδ(nt

a,l). Then

⋂
l∈[A]
P(Ea) = 1 − P( ¯Ea),

= 1 − P(∃t ∈ N, ∃l ∈ [A] ∶ ∣¯xt
≥ 1 − ∑
l∈[A]

P(∃t ∈ N ∶ ∣¯xt

a,l − µl∣ > βδ(nt
a,l)).

a,l − µl∣ > βδ(nt

a,l)),

defining γ(δ) = δ
8×A

and using Lemma 4

P(Ea) ≥ 1 − ∑
l∈[A]

P(∃t ∈ N ∶ ∣¯xt

a,l − µl∣ > σ

¿
`
`(cid:192)

2
nt

a,l

× (1 +

1
nt

a,l

) ln(

√

a,l + 1/γ(δ))),
nt

≥ 1 − ∑
l∈[A]

γ(δ) = 1 − ∑
l∈[A]

δ
8A

= 1 −

δ
8

.

16

Appendix B Proof of Theorem 1

In this section, we prove Theorem 1. We first show Lemma 2 using two technical lemmas. We then prove
Lemma 7, which combined with Lemma 2, yields the main result (Theorem 1). Let us first remark that

a,δ(l) = ∣¯xt
dt
= ∣(¯xt

a,l∣ − βδ(nt

a,a − ¯xt
a,a − µa) − (¯xt

a,a) − βδ(nt

a,l),
a,l − µl) + (µa − µl)∣ − βδ(nt

a,a) − βδ(nt

a,l).

As a consequence we have

a,δ(l) ≤ ∆a,l + ∣¯xt
dt
a,δ(l) ≥ ∆a,l − ∣¯xt
dt

a,a − µa∣ + ∣¯xt
a,a − µa∣ − ∣¯xt

a,l − µl∣ − βδ(nt
a,l − µl∣ − βδ(nt

a,a) − βδ(nt
a,a) − βδ(nt

a,l).
a,l).

(19)

(20)

Lemma 5. Under Ea, ∀l ∈ [A], if l /∈ Ca then ∀nt

a,l ≥ n⋆

a,l = ⌈β−1

δ ( ∆a,l

4

)⌉ we have dt

a,δ(l) > 0.

a,a) ≤ βδ(nt

a,l − µl∣ ≤ βδ(nt

a,l). Hence, using (20), dt

Proof. Under Ea, we have ∣¯xt
βδ(nt
∆a,l = 0 and since βδ(nt
a,δ(l) ≥ ∆a,l − 4βδ(nt
dt
Lemma 6. Under Ea, ∀l ∈ [A], ∀t ∈ N, if l ∈ Ca then dt

a,l) > 0, we need that ∆a,l

a,l) and ∣¯xt
a,δ(l) ≥ ∆a,l − 2βδ(nt

> βδ(nt

4

a,l) > 0 we cannot ensure that ∆a,l − 4βδ(nt

a,l) and hence n⋆
a,δ(l) ≤ 0.

a,a − µa∣ ≤ βδ(nt
a,a) − 2βδ(nt

a,a). Since nt
a,a ≥ nt
a,l
a,l) ≥ ∆a,l − 4βδ(nt

, we also have
a,l). If l ∈ Ca then
a,l) > 0. If l /∈ Ca then to ensure that
a,l = ⌈β−1

δ ( ∆a,l

)⌉.5

4

Proof. Again, recall that under Et
a
a,δ(l) ≤ ∆a,l + βδ(nt
dt

a,a) + βδ(nt

, we have ∣¯xt

a,l − µl∣ ≤ βδ(nt

a,l) and ∣¯xt

a,a − µa∣ ≤ βδ(nt

a,a). Hence, using (19),

a,l) − βδ(nt

a,a) − βδ(nt

a,l) = ∆a,l. If l ∈ Ca then ∆a,l = 0 and thus dt

a,δ(l) ≤ 0.

We can now prove Lemma 2, which we restate here for convenience.

Lemma 2 (Class membership rule). Under Et

a ∧ Gt
a

and ∀l ∈ [A] and at time t: dt

a,δ(l) > 0 ⇐⇒ l ∈ [A]/Ca.

Proof. From Lemma 6, we directly have one implication. For the other one, if l /∈ Ca because Gt
a
have ∀l ∈ [A], nt

, therefore we can apply Lemma 5 and we directly have dt

a,l ≥ n⋆
a,l

holds, we

Lemma 7. Under Ea, and using Restricted-Round-Robin algorithm, Gt
a

a,δ(l) > 0.
holds when t > ζa where

ζa = n⋆

a,a − 1 + A − ∑

1

l∈[A]∖Ca

{n⋆

a,a>n⋆

a,l−1+A}.

a,l − ¯xt

a,a∣ − βδ(nt

Proof. According to Algorithm 1, C0
∣¯xt
class Ca is at least n⋆
a,l
robin), we are sure that it will be removed from Ct
a

a = [A] and an agent is eliminated from set Ct
a

a,δ(l) =
a,l) > 0. According to Lemma 5, the time required to eliminate agent l from the
a,l − 1, then using Restricted-Round-Robin (or round

. If agent l is queried at time n⋆

for all t larger than n⋆

at time t if dt

a,a) − βδ(nt

a,l − 1 + A.

Let us consider h being an agent such that n⋆
n⋆

can be denoted by n⋆

.

a,a

a,h

a,h = maxl∈[A]/Ca n⋆

a,l

. By definition, ∆a,h = minl∈[A]/Ca

∆a,l and

In the case of round robin, we are sure that Gt
a
Round-Robin, since the loop ignores agents not in Ct
a

will be true when t ≥ n⋆
, we have that Gt
a

a,a − 1 + A. But using Restricted-
holds when t > ζa where

5In extremely rare cases, the expression β−1

) could be an integer and we should add 1 to get a strict inequality. But

δ ( ∆a,l

4

for conciseness of the expression, we omit the +1 in the definition of n⋆

a,l.

17

ζa = n⋆

a,a − 1 + A − ∑

1

l∈[A]∖Ca

{n⋆

a,a>n⋆

a,l−1+A}.

Finally, we use the above lemmas to prove Theorem 1, which we restate below for convenience.

Theorem 1 (ColME class estimation time complexity). For any δ ∈ (0, 1), employing Restricted-Round-
Robin query strategy, we have:

P(∃t > ζa ∶ Ct

a ≠ Ca) ≤

δ
8

, with ζa = n⋆

a,a + A − 1 − ∑

1

l∈[A]∖Ca

{n⋆

a,a>n⋆

a,l+A−1}.

(7)

Proof. From Lemma 7 and Lemma 2, we deduce that if Ea holds and knowing that Ct
then ∀t > ζa, Ca = Ct
a) ≥ P(Ea) ≥ 1 − δ/8 using Lemma 1.
a

. Hence, P(∀t > ζa, Ca = Ct

a = {l ∈ [A] ∶ dt

a,δ(l) ≤ 0}

Appendix C Proof of Theorem 2

In this section we detail the proof of Theorem 2, about the PAC mean estimation properties of the ColME
strategy. We restate the theorem below for convenience.

Theorem 2 (ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-
)-convergent,
Round-Robin query strategy and simple weighting, the mean estimator µt
a
that is:

of agent a is (ϵ, δ
4

P(∀t > τa ∶ ∣µt

a − µa∣ ≤ ϵ) > 1 −

δ
4

, with τa = max(ζa,

⌈β−1

δ (ϵ)⌉
∣Ca∣

+

∣Ca∣ − 1
2

).

(8)

Proof. Let us assume that at time t we have Ct

a = Ca . Therefore

µt
a = ∑
l∈Ca

¯xt
a,lαt

a,l =

∑l∈Ca

¯xt
a,lnt
a,l
∑l∈Ca nt

a,l

.

¯xt
a,lnt
a,l

is the sum of all nt

Remark that ∑l∈Ca
is the estimation of µa with ∑l∈Ca nt
should have β(∑l∈Ca nt
Algorithm 13 using Restricted-Round-Robin, we know that when Ct
queried. Therefore,

samples received by all agents l in Ca. In other words, µt
a
a − µa∣ ≤ ϵ when Ea holds, we
. With
a = Ca, then only members of Ca are

a,l) ≤ ϵ. Let us see at what time denoted by nϵ,a we have ⌈β−1(ϵ)⌉ = ∑l∈Ca nt

examples. Hence in order to have ∣µt

a,l

a,l

a,l

⌈β−1(ϵ)⌉ = nϵ,a + (nϵ,a − 1) + ⋅ ⋅ ⋅ + (nϵ,a − ∣Ca∣ + 1) = ∣Ca∣nϵ,a −

∣Ca∣ − 1
2

∣Ca∣,

nϵ,a =

⌈β−1(ϵ)⌉
∣Ca∣

+

∣Ca∣ − 1
2

.

As a summary, if Ea holds, then we have ∀t ≥ nϵ,a, Ct
Theorem 1, we have P(∃t > ζa ∶ Ct
δ
8

+ P( ¯Ea) = δ

a ≠ Ca) ≤ δ
8

4

.

. Since τa = max(ζa, nϵ,a) ≥ ζa, then P(∃t > τa ∶ ∣µt

a − µa∣ ≤ ϵ. Now, following
a − µa∣ > ϵ) ≤

a = Ca implies that ∣µt

Appendix D Proof of Theorem 3

The proof of Theorem 3 follows the same step as that of Theorem 1, up to replacing the 0 threshold by
η. We only state the intermediate lemmas (which are adaptations of Lemmas 5-6-7) and omit the detailed
proof.

Lemma 8. Under Ea, ∀l ∈ [A], if l /∈ Cη,a then ∀nt

a,l ≥ nη

a,l = ⌈β−1

δ ( ∆a,l−η

4

)⌉ we have dt

a,δ(l) > η.

18

Lemma 9. Under Ea, ∀l ∈ [A], ∀t ∈ N, if l ∈ Cη,a then dt
Lemma 10. Under Ea, and using Restricted-Round-Robin algorithm, Gt

a,δ(l) ≤ η.

η,a

holds when t > ζ η
a

where

a = nη
ζ η

a,a − 1 + A − ∑

1

l∈[A]∖Cη,a

Appendix E Proof of Theorem 4

{nη

a,a>nη

a,l−1+A}.

In this section we detail the proof of Theorem 4, about the PAC mean estimation properties of the η-ColME
strategy. We restate the theorem below for convenience.
Theorem 4 (η-ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-
Round-Robin query strategy and class-uniform weighting (while employing Cη,a), the mean estimator µt
of
a
agent a is (ϵ, δ
4

)-convergent, that is:

P(∀t > τ η

a ∶ ∣µt

η,a − µη,a∣ ≤ ϵ) > 1 −

δ
4

, with τ η

a = max(ζ η

a , β−1

δ (ϵ) + ∣Cη,a∣ − 1).

(16)

Proof. Since t > τ η

a > ζ η
a

, at time t we have Ct

η,a = Cη,a . Therefore

µt
a = ∑
l∈Cη,a

¯xt
a,lαt

a,l =

¯xt
∑l∈Cη,a
a,l
∣Cη,a∣

.

is not equivalent to the average of all the samples of agents in Cη,a:

Remark that µt
it is the average of
a
the mean values for each agent in Cη,a. Therefore, although some agents may have more samples than the
η,a − µη,a∣ ≤ ϵ. When Ea holds, we can
others, all are assigned uniform weights. We would like to have ∣µt
rewrite this as

∣µt

η,a − µη,a∣ = ∣

Therefore, we need:

1
∣Cη,a∣

¯xt
a,l − µl∣ ≤

∑
l∈Cη,a

1
∣Cη,a∣

∣¯xt

a,l − µl∣ ≤ ϵ

∑
l∈Cη,a

∣¯xt

a,l − µl∣ ≤ ∣Cη,a∣ × ϵ,

∑
l∈Cη,a

A sufficient condition for the above inequality to hold is to ensure that each term is bounded by ϵ:

∀l ∈ Cη,a ∶ ∣¯xt

a,l − µl∣ ≤ ϵ

(21)

a,l) < ϵ for all l ∈ Cη,a. Since we are using Restricted-Round-Robin and also that
This is achieved when βδ(nt
a,1 −∣Cη,a∣+1
η,a = Cη,a, the number of samples required for each agent in Cη,a are nt
Ct
where we consider the one with the maximum number of observations to have index 1 for notation simplicity
(which corresponds to index a). For Eq. 21 to hold, it is thus sufficient to have:

a,1 −2, . . . , nt

a,1 −1, nt

, nt

a,1

δ (ϵ) < nt
β−1

a,a − ∣Cη,a∣ + 1

δ (ϵ) + ∣Cη,a∣ − 1 < nt
β−1

a,a

a = max(ζ η

δ (ϵ) + ∣Cη,a∣ − 1).

Therefore τ η
a , β−1
As a summary, if Ea holds, then we have ∀t ≥ τ η
a
orem 3, we have P(∃t > ζ η
δ
8

η,a ≠ Cη,a) ≤ δ
8

+ P( ¯Ea) = δ

a ∶ Ct

4

.

, Ct
. Since τ η

η,a = Cη,a implies that ∣µt
ϵ,a) ≥ ζ η
a

a = max(ζ η

a , nη

η,a −µη,a∣ ≤ ϵ. Now, following The-
, then P(∃t > τ η
a ∶ ∣µt
a − µη,a∣ > ϵ) ≤

Appendix F Additional Experimental Results

In this section we provide additional illustrative results to better understand different aspects of our ColME
algorithm.

19

(a) First class (mean 0.2)

(b) Second class (mean 0.4)

Figure 2: Error in mean estimation for the 3-class problem (Gaussian distributions with true means 0.2, 0.4,
0.8). Note that the time scale is different for the third class to show the relevant details more clearly.

(c) Third class (mean 0.8)

F.1 Per-Class behavior on the 3-class problem

For the 3-class problem described in the main text, we provide complementary figures to show the error in
mean estimation in each class separately. These plots are shown in Figure 2. We can see that the average
class identification time (represented by yellow dots) is different for different classes. For instance, as the gap
between the class with mean 0.8 and the other two is larger, this class requires less samples to be identified.
Indeed, the average class identification time is less than t = 400 for that class (Figure 2(c)), while it is about
t = 1400 for the other two (Figures 2(a)-2(b)). Therefore, agents from the class with mean 0.8 reach a highly
accurate estimate much faster than agents from other classes.

We also show the class estimation precision across each pair of classes in Figure 3. We see that classes 0.2
and 0.8, who have the biggest gap, are separated first (Figure 3(b)), then 0.4 and 0.8 (Figure 3(c)) and
finally 0.2 and 0.4 (Figure 3(a)).

Finally, for mean estimation, we provide the per-class counterparts of Table 2 in Tables 3-5. In line with
previous results on class estimation, agents of class 0.8 are the ones that converge faster to the desired mean
estimation accuracy. Interestingly, observe that for ϵ = 0.1, agents of class 0.2 converge almost as fast as
those from class 0.8: this is because they do not need to eliminate all agents from class 0.4 to reach this
accuracy. This illustrates how our approach can naturally adapt to the gaps and desired estimation accuracy.

F.2 Results on a 2-class problem

We experiment with a 2-class problem generated in the same way as the 3-class problem considered in the
main text, except that the means are chosen among {0.2, 0.8}. This makes the problem easier since the gap
between the two classes corresponds to the largest gap in the 3-class problem. The results shown in Figure 4
reflect this: agents correctly identify their class and reach highly accurate mean estimates much faster than

20

05001000150020002500Time0.000.050.100.150.200.250.300.350.40Error in mean estimationsoft-restricted-round-robinaggressive-restricted-round-robinrestricted-round-robinround-robinoraclelocal05001000150020002500Time0.000.050.100.150.200.250.300.350.40Error in mean estimationsoft-restricted-round-robinaggressive-restricted-round-robinrestricted-round-robinround-robinoraclelocal02004006008001000Time0.000.050.100.150.200.250.300.350.40Error in mean estimationsoft-restricted-round-robinaggressive-restricted-round-robinrestricted-round-robinround-robinoraclelocal(a) Class estimation precision for 0.2 vs 0.4

(b) Class estimation precision for 0.2 vs 0.8

(c) Class estimation precision for 0.4 vs 0.8

Figure 3: Class estimation precision across time for each pair of classes on the 3-class problem.

Algorithm

Round-Robin (RR)
Restricted-RR
Soft-Restricted-RR
Aggressive-Restricted-RR

Local

Oracle

conv(0.1)
avg/std max

289 ± 92
271 ± 77
111 ± 30
74 ± 40

41 ± 40

6 ± 4

594
568
239
215

288

15

conv(0.01)

avg/std

1207 ± 170
1194 ± 177
761 ± 173
382 ± 95

max

1782
1857
1379
830

4548 ± 4075

28616

100 ± 63

246

Theoretical Restricted-RR (τa)
Theoretical Local (τa)

3878
885

33406
100216

Table 3: Empirical convergence times (see Eq. 12) for class 0.2 of different algorithms on the 3-class problem
(Gaussian distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable
regime) and ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across
agents and runs. We also report the high-probability mean estimation times τa given by our theory for
Restricted-Round-Robin and Local.

in the 3-class problem. Consequently, the improvement compared to Local is even more significant and our
approach almost matches the performance of Oracle. We omit the per-class figures as they are essentially
the same as Figure 4(b).

21

05001000150020002500Time0.50.60.70.80.91.0Precision in class estimation(0.2-0.4)restricted-round-robinround-robin05001000150020002500Time0.50.60.70.80.91.0Precision in class estimation(0.2-0.8)restricted-round-robinround-robin05001000150020002500Time0.50.60.70.80.91.0Precision in class estimation(0.4-0.8)restricted-round-robinround-robin‌
Algorithm

Round-Robin (RR)
Restricted-RR
Soft-Restricted-RR
Aggressive-Restricted-RR

Local

Oracle

conv(0.1)

conv(0.01)

avg/std

max

avg/std

721 ± 160
708 ± 158
31 ± 33
35 ± 42

1239
1175
340
340

1232 ± 181
1216 ± 183
829 ± 188
420 ± 92

max

2088
1925
1432
832

41 ± 39

277

4482 ± 3846

26488

4 ± 3

19

99 ± 70

303

Theoretical Restricted-RR (τa)
Theoretical Local (τa)

3878
885

33406
100216

Table 4: Empirical convergence times (see Eq. 12) for class 0.4 of different algorithms on the 3-class problem
(Gaussian distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable
regime) and ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across
agents and runs. We also report the high-probability mean estimation times τa given by our theory for
Restricted-Round-Robin and Local.

Algorithm

conv(0.1)

conv(0.01)

avg/std

max

avg/std

Round-Robin (RR)
Restricted-RR
Soft-Restricted-RR
Aggressive-Restricted-RR

Local

Oracle

271 ± 35
266 ± 31.25
100 ± 36
57 ± 24

41 ± 39

6 ± 4

401
384
232
172

289

17

379 ± 38
342 ± 38
280 ± 78
222 ± 90

max

538
526
958
958

4434 ± 3883

27470

95 ± 56

208

Theoretical Restricted-RR (τa)
Theoretical Local (τa)

1085‌
885

33406
100216

Table 5: Empirical convergence times (see Eq. 12) for class 0.8 of different algorithms on the 3-class problem
(Gaussian distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable
regime) and ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across
agents and runs. We also report the high-probability mean estimation times τa given by our theory for
Restricted-Round-Robin and Local.

(a) Class estimation precision over all agents

(b) Error in mean estimation over all agents

Figure 4: Results for the 2-class problem (Gaussian distributions with true means 0.2 and 0.8).

22

0100200300400500Time0.50.60.70.80.91.0Precision in class estimationrestricted-round-robinround-robin05001000150020002500Time0.00000.00250.00500.00750.01000.01250.01500.01750.0200Error in mean estimationsoft-restricted-round-robinaggressive-restricted-round-robinrestricted-round-robinround-robinoraclelocal‌
