Escaping the Curse of Dimensionality in Similarity
Learning: Eﬀicient Frank-Wolfe Algorithm and
Generalization Bounds
Kuan Liu, Aurélien Bellet

To cite this version:

Kuan Liu, Aurélien Bellet. Escaping the Curse of Dimensionality in Similarity Learning: Eﬀi-
cient Frank-Wolfe Algorithm and Generalization Bounds. Neurocomputing, 2019, 333, pp.185-199.
￿10.1016/j.neucom.2018.12.060￿. ￿hal-02166425￿

HAL Id: hal-02166425

https://inria.hal.science/hal-02166425

Submitted on 26 Jun 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Escaping the Curse of Dimensionality in Similarity
Learning: Eﬃcient Frank-Wolfe Algorithm and
Generalization Bounds

Kuan Liua,1, Aur´elien Belletb,∗

aGoogle Inc., USA
bINRIA, France

Abstract

Similarity and metric learning provides a principled approach to construct
a task-speciﬁc similarity from weakly supervised data. However, these meth-
ods are subject to the curse of dimensionality: as the number of features grows
large, poor generalization is to be expected and training becomes intractable
due to high computational and memory costs. In this paper, we propose a sim-
ilarity learning method that can eﬃciently deal with high-dimensional sparse
data. This is achieved through a parameterization of similarity functions by
convex combinations of sparse rank-one matrices, together with the use of a
greedy approximate Frank-Wolfe algorithm which provides an eﬃcient way to
control the number of active features. We show that the convergence rate of the
algorithm, as well as its time and memory complexity, are independent of the
data dimension. We further provide a theoretical justiﬁcation of our modeling
choices through an analysis of the generalization error, which depends logarith-
mically on the sparsity of the solution rather than on the number of features.
Our experiments on datasets with up to one million features demonstrate the
ability of our approach to generalize well despite the high dimensionality as well
as its superiority compared to several competing methods.

Keywords: Metric learning, Frank-Wolfe algorithm, Generalization bounds

1. Introduction

High-dimensional and sparse data are commonly encountered in many ap-
plications of machine learning, such as computer vision, bioinformatics, text
mining and behavioral targeting. To classify, cluster or rank data points, it is

∗Corresponding author
Email addresses: liukuan@google.com (Kuan Liu), aurelien.bellet@inria.fr

(Aur´elien Bellet)

1Most of this work was done when the author was aﬃliated with the Information Sciences

Institute, University of Southern California, USA.

Preprint submitted to Neurocomputing

January 6, 2019

5

10

15

20

25

30

35

40

45

50

important to be able to compute semantically meaningful similarities between
them. However, deﬁning an appropriate similarity measure for a given task is
often diﬃcult as only a small and unknown subset of all features are actually
relevant. For instance, in drug discovery studies, chemical compounds are typi-
cally represented by a large number of sparse features describing their 2D and
3D properties, and only a few of them play in role in determining whether the
compound will bind to a particular target receptor (Leach and Gillet, 2007). In
text classiﬁcation and clustering, a document is often represented as a sparse
bag of words, and only a small subset of the dictionary is generally useful to
discriminate between documents about diﬀerent topics. Another example is
targeted advertising, where ads are selected based on ﬁne-grained user history
(Chen et al., 2009).

Similarity and metric learning (Bellet et al., 2015) oﬀers principled ap-
proaches to construct a task-speciﬁc similarity measure by learning it from
weakly supervised data, and has been used in many application domains. The
main theme in these methods is to learn the parameters of a similarity (or dis-
tance) function such that it agrees with task-speciﬁc similarity judgments (e.g.,
of the form “data point x should be more similar to y than to z”). To ac-
count for correlations between features, similarity and metric learning typically
estimates a number of parameters which is quadratic in the data dimension d.
When data are high-dimensional, these methods are thus particularly aﬀected
by the so-called “curse of dimensionality”, which manifests itself at both the
algorithmic and generalization levels. On the one hand, training the similar-
ity quickly becomes infeasible due to a quadratic or cubic complexity in d. In
fact, the O(d2) parameters may not even ﬁt in memory. On the other hand,
putting aside the training phase, learning so many parameters would lead to se-
vere overﬁtting and poor generalization performance (especially for sparse data
where some features are rarely observed). Simple workarounds have been used
to address this limitation, such as projecting the data into a low-dimensional
space before learning the similarity (see e.g. Davis et al., 2007; Weinberger and
Saul, 2009; Guillaumin et al., 2009). However, such heuristics do not provide
satisfactory solutions: they often hurt the performance and make the resulting
similarity function diﬃcult to interpret.

In this paper, we propose a novel method to learn a bilinear similarity func-
tion SM (x, x(cid:48)) = xTM x(cid:48) directly in the original high-dimensional space while
escaping the curse of dimensionality. This is achieved by combining three in-
gredients: the sparsity of the data, the parameterization of M as a convex
combination of rank-one matrices with a special sparsity structure, and an ap-
proximate Frank-Wolfe procedure (Frank and Wolfe, 1956; Jaggi, 2013) to learn
the similarity parameters. The resulting algorithm greedily incorporates one
pair of features at a time into the learned similarity, providing an eﬃcient way
to ﬁlter out irrelevant features as well as to guard against overﬁtting through
early stopping. Remarkably, the convergence rate of the algorithm as well as
its time and memory complexity are all independent of the dimension d. The
resulting similarity functions are extremely sparse, which makes them fast to
compute and easier to interpret.

2

We provide strong theoretical and empirical evidence of the usefulness of
our approach. On the theory part, we perform a generalization analysis of the
solution returned by our algorithm after a given number of iterations. We derive
excess risk bounds with respect to the minimizer of the expected risk which
conﬁrm that our modeling choices as well as our Frank-Wolfe algorithm and early
stopping policy provide eﬀective ways to avoid overﬁtting in high dimensions. A
distinctive feature of the generalization bound we obtain is the adaptivity of its
model class complexity term to the actual sparsity of the approximate solution
found by our algorithm, again removing the dependence on the dimension d. We
also evaluate the proposed approach on several synthetic and real datasets with
up to one million features, some of which have a large proportion of irrelevant
features. To the best of our knowledge, it is the ﬁrst time that a full similarity
or distance metric is learned directly on such high-dimensional datasets without
ﬁrst reducing dimensionality. Our experiments show that our approach is able to
generalize well despite the high dimensionality, and even to recover the ground
truth similarity function when the training similarity judgments are suﬃciently
informative. Furthermore, our approach clearly outperforms both a diagonal
similarity learned in the original space and a full similarity learned in a reduced
space (after PCA or random projections). Finally, we show that our similarity
functions can be extremely sparse (in the order of 0.0001% of nonzero entries),
thereby drastically reducing the dimension while also providing an opportunity
to analyze the importance of the original features and their pairwise interactions
for the problem at hand.

The present work extends a previously published conference paper (Liu et al.,
2015a) by providing additional technical and experimental results. Firstly, we
present a novel generalization analysis which further backs up our approach
from a statistical learning point of view. Secondly, we conduct experiments on
high-dimensional synthetic data showing that our approach generalizes well as
the dimensionality increases and can even accurately recover the ground truth
notion of similarity. Finally, we extend the discussion of the related work and
provide additional details on algorithms and proofs.

The paper is organized as follows. Section 2 introduces some background
and related work on similarity learning and Frank-Wolfe algorithms. Section
3 describes our problem formulation, the proposed algorithm and its analysis.
Generalization bounds are established in Section 4. Finally, Section 5 describes
our experimental results, and we conclude in Section 6.

55

60

65

70

75

80

85

2. Background and Related Work

In this section, we review some background and related work in metric and

similarity learning (Section 2.1) and the Frank-Wolfe algorithm (Section 2.2).

90

2.1. Metric and Similarity Learning

Metric and similarity learning has attracted a lot of interest over the past ten
years. The great majority of work has focused on learning either a Mahalanobis

3

95

100

105

110

115

120

125

130

135

distance dM (x, x(cid:48)) = (x − x(cid:48))TM (x − x(cid:48)) where M is a symmetric positive
semi-deﬁnite (PSD) matrix, or a bilinear similarity SM (x, x(cid:48)) = xTM x(cid:48) where
M is often taken to be an arbitrary d × d matrix. A comprehensive survey
of existing approaches can be found in (Bellet et al., 2013). We focus below
on the two topics most relevant to our work: (i) eﬃcient algorithms for the
high-dimensional setting, and (ii) the derivation of generalization guarantees
for metric and similarity learning.

Metric learning in high dimensions. Both Mahalanobis distance metric learning
and bilinear similarity learning require estimating O(d2) parameters, which is
undesirable in the high-dimensional setting for the reasons mentioned earlier.
In practice, it is thus customary to resort to dimensionality reduction (such as
PCA, SVD or random projections) to preprocess the data when it has more
than a few hundred dimensions (see e.g., Davis et al., 2007; Weinberger and
Saul, 2009; Guillaumin et al., 2009; Ying and Li, 2012; Wang et al., 2012; Lim
et al., 2013; Qian et al., 2014; Liu et al., 2015b; Yao et al., 2018). Although this
strategy can be justiﬁed formally in some cases (Liu et al., 2015b; Qian et al.,
2015), the projection may intertwine useful features and irrelevant/noisy ones
and thus hurt the performance of the resulting similarity function. It also makes
it hard to interpret and use for data exploration, preventing the discovery of
knowledge that can be valuable to domain experts.

There have been very few satisfactory solutions to this essential limita-
tion. The most drastic strategy is to learn a diagonal matrix M (Schultz and
Joachims, 2003; Gao et al., 2014), which is very restrictive as it amounts to a
simple weighting of the features. Instead, some approaches assume an explicit
low-rank decomposition M = LTL and learn L ∈ Rr×d in order to reduce
the number of parameters (Goldberger et al., 2004; Weinberger and Saul, 2009;
Kedem et al., 2012). This results in nonconvex formulations with many local
optima (Kulis, 2012), and requires to tune r carefully. Moreover, the training
complexity still depends on d and can thus remain quite large. Another direc-
tion is to learn M as a combination of rank-one matrices. In particular, Shi
et al. (2014) generate a set of rank-one matrices from the training data and then
learn a metric as a sparse combination. However, as the dimension increases,
a larger dictionary is needed and can be expensive to generate. Some other
work has studied sparse and/or low-rank regularization to reduce overﬁtting in
high dimensions (Rosales and Fung, 2006; Qi et al., 2009; Ying et al., 2009) but
this does not in itself reduce the training complexity of the algorithm. Zhang
and Zhang (2017) proposed a stochastic gradient descent solver together with
low-rank regularization in an attempt to keep the intermediate solutions low-
rank. The complexity per iteration of their approach is linear in d but cubic in
the rank of the current solution, which quickly becomes intractable unless the
regularization is very strong.

Finally, some greedy algorithms for metric learning have been proposed in
the literature to guarantee a tighter bound on the rank of intermediate solutions.
Atzmon et al. (2015) use a block coordinate descent algorithm to update the
metric one feature at a time. Shen et al. (2012) selects rank-one updates in a

4

140

145

150

155

160

165

170

175

boosting manner, while DML-eig (Ying and Li, 2012) and its extension DML-ρ
(Cao et al., 2012b) rely on a greedy Frank-Wolfe algorithm to optimize over the
set of PSD matrices with unit trace. However, these greedy methods still suﬀer
from a computational cost of O(d2) per iteration and are thus unsuitable for the
high-dimensional setting we consider in this work. In contrast, we will propose
an algorithm which is linear in the number of nonzero features and can thus be
eﬃciently applied to high-dimensional sparse data.

Generalization bounds for metric learning. The derivation of generalization guar-
antees for metric and similarity learning has been investigated in the supervised
setting, where the metric or similarity is learned from a labeled dataset of n
points by (regularized) empirical risk minimization. For a given family of loss
functions, the results generally bound the maximal deviation between the ex-
pected risk (where the expectation is taken over the unknown data distribution)
and the empirical risk of the learned metric.2 These bounds are generally of
order O(1/

n).

√

√

Several technical tools have been used to address the challenge of learning
from dependent pairs/triplets, leading to diﬀerent trade-oﬀs in terms of tight-
ness, generality, and dependence on the feature dimension d. The results of
Jin et al. (2009) apply only under Frobenius norm regularization of M and
d factor in the rate. Using an adaptation of algorithmic robustness,
have a
Bellet and Habrard (2015) obtain bounds which hold also for sparsity-inducing
regularizers but with a covering number term that can be exponential in the
dimension. Bian and Tao (2011) rely on assumptions on the data distribution
and do not show an explicit dependence on the dimension. Cao et al. (2012a)
derive bounds based on Rademacher complexity and maximal deviation results
for U -statistics (Cl´emen¸con et al., 2008). Depending on the regularization used,
the dependence on the dimension d ranges from logarithmic to linear. Verma
and Branson (2015) show that the
d factor of Jin et al. (2009) is in fact un-
avoidable in the worst case without some form of regularization (or restriction of
the hypothesis class). They derive bounds which do not depend on the dimen-
sion d but on the Frobenius norm of the optimal parameter M . Note however
that their analysis assumes that the metrics are learned from a set of i.i.d. pairs
or triplets, which is rarely seen in practice.

√

In all the above work, generalization in metric learning is studied indepen-
dently of the algorithm used to solve the empirical risk minimization problem,
and none of the bounds are adaptive to the actual sparsity of the solution. In
contrast, we will show that one can use early stopping in our algorithm to control
the complexity of the hypothesis class so as to make the bounds independent
of the dimension d, eﬀectively balancing the empirical (optimization) error and
the generalization error.

2This is in contrast to a diﬀerent line of work, inspired by the problem of ordinal embedding,
which aims to learn a metric which correctly orders a ﬁxed set of known points (see for instance
Jain et al., 2017)

5

Algorithm 1 Standard Frank-Wolfe algorithm

Input: Initial point M (0) ∈ D
for k = 0, 1, 2, . . . do

S(k) ← arg minS∈D(cid:104)S, ∇f (M (k))(cid:105)
γ(k) ← 2
M (k+1) ← (1 − γ(k))M (k) + γ(k)S(k)

k+2 (or determined by line search)

end for

2.2. Frank-Wolfe Algorithms

The Frank-Wolfe (FW) algorithm was originally introduced by Frank and
Wolfe (1956) and further generalized by Clarkson (2010) and Jaggi (2013). FW
aims at solving constrained optimization problems of the following general form:

min
M ∈D

f (M ),

(1)

180

185

where f is a convex and continuously diﬀerentiable function, and the feasible
domain D is a convex and compact subset of some Hilbert space equipped with
inner product (cid:104)·, ·(cid:105).

Starting from a feasible initial point M (0) ∈ D, the standard FW algorithm
iterates over the following steps. First, it ﬁnds the feasible point S(k) ∈ D which
minimizes the linearization of f at the current point M (k):

S(k) ∈ arg min

(cid:104)S, ∇f (M (k))(cid:105).

S∈D

(2)

The next iterate M (k+1) is then constructed as a convex combination of M (k)
and S(k), where the relative weight of each component is given by a step size
γ(k). The step size can be decreasing with the iteration number k or set by line
search. The overall algorithm is summarized in Algorithm 1. FW is guaranteed
to converge to an optimal solution of (1) at rate O(1/k), see for instance (Jaggi,
2013) for a generic and concise proof.

Unlike projected gradient, FW is a projection-free algorithm: each iterate
M (k) is feasible by construction since it is a convex combination of elements of
D. Instead of computing projections onto the feasible domain D, FW solves the
linear optimization subproblem (2). The linearity of the objective (2) implies
that a solution S(k) always lies at an extremal point of D. This leads to the
interpretation of FW as a greedy algorithm which adds an extremal point to
the current solution at each iteration (Clarkson, 2010). In other words, M (k)
can be written as a sparse convex combination of extremal points:

M (k) =

(cid:88)

α(k)
S(k)S(k),

where

(cid:88)

S(k) = 1 and α(k)
α(k)

S(k) ≥ 0, (3)

S(k)∈S (k)

S(k)∈S (k)

where S (k) denotes the set of “active” extremal points that have been added up
to iteration k. When the extremal points of D have speciﬁc structure (such as
sparsity, or low-rankness), this structure can be leveraged to compute a solution

190

6

Algorithm 2 Frank-Wolfe algorithm with away steps

Input: Initial point M (0) ∈ D
for k = 0, 1, 2, . . . do

S(k)
F ← arg minS∈D(cid:104)S, ∇f (M (k))(cid:105), D(k)
A ← arg maxS∈S (k)(cid:104)S, ∇f (M (k))(cid:105), D(k)
S(k)
if (cid:104)D(k)

F , ∇f (M (k))(cid:105) ≤ (cid:104)D(k)
F and γmax ← 1

D(k) ← D(k)

A , ∇f (M (k))(cid:105) then

else

D(k) ← D(k)

A and γmax ← α(k)
S(k)
A

end if
γ(k) ← 2
M (k+1) ← M (k) + γ(k)D(k)

k+2 (or determined by line search)

F = S(k)

F − M (k) // forward direction
A // away direction

A = M (k) − S(k)

/(1 − α(k)
S(k)
A

)

// choose forward step

// choose away step

end for

195

200

205

210

of (2) much more eﬃciently than the projection operator, see Jaggi (2011, 2013)
for compelling examples.

A drawback of the standard FW algorithm is that “removing” an extremal
point S(k) from the current iterate (or signiﬁcantly reducing its weight α(k)
S(k))
can only be done indirectly by adding (increasing the weight of) other extremal
points. The variant of FW with away steps (Gu´elat and Marcotte, 1986) ad-
dresses this issue by allowing the algorithm to choose between adding a new
extremal point (forward step) or reducing the weight of an existing one (away
step), as shown in Algorithm 2. This can lead to sparser solutions (Gu´elat and
Marcotte, 1986; Clarkson, 2010; Jaggi, 2011) and faster convergence in some
cases (Gu´elat and Marcotte, 1986; Lacoste-Julien and Jaggi, 2015).

In the present work, we will introduce a FW algorithm with away steps to
eﬃciently perform similarity learning for high-dimensional sparse data. One of
our key ingredients will be the design of a feasible domain with appropriate
sparsity structure.

3. Proposed Approach

This section introduces hdsl (High-Dimensional Similarity Learning), the
approach proposed in this paper. We ﬁrst describe our problem formulation
(Section 3.1), then derive and analyze an eﬃcient FW algorithm to solve it in
Section 3.2.

3.1. Problem Formulation

In this work, our goal is to learn a similarity function for high-dimensional
sparse data. We assume the data points lie in some space X ⊆ Rd, where d is
large (d > 104) but points are s-sparse on average (s (cid:28) d). In other words,

7

their number of nonzero entries is typically much smaller than d. We focus on
learning a similarity function SM : X × X → R of the form

SM (x, x(cid:48)) = xTM x(cid:48) = (cid:104)xx(cid:48)(cid:62), M (cid:105),

where M ∈ Rd×d and (cid:104)·, ·(cid:105) denotes the Frobenius inner product. Notice that
for any M , SM can be computed in O(s2) time on average if data points are
stored in a sparse format.

Feasible domain. We will derive an algorithm to learn a very sparse M with
time and memory requirements that depend on s but not on d. To this end,
given a scale λ > 0 which will play the role of a regularization parameter, we
parameterize M as a convex combination of rank-one, 4-sparse d × d bases:

M ∈ Dλ = conv(Bλ),

with Bλ =

(cid:110)

P (ij)
λ

, N (ij)
λ

(cid:111)

,

(cid:91)

ij

where for any pair of features i, j ∈ {1, . . . , d}, i (cid:54)= j,

P (ij)

λ = λ(ei + ej)(ei + ej)T =

N (ij)

λ = λ(ei − ej)(ei − ej)T =











 ,

· · · · ·
· λ · λ ·
· · · · ·
· λ · λ ·
· · · · ·
·
·
·
·
· λ · −λ ·
·
·
·
·
· −λ · λ ·
·
·
·
·

·
·
·



 .

215

220

225

The use of such sparse matrices was ﬁrst suggested by Jaggi (2011). Besides
the fact that they are instrumental to the eﬃciency of our algorithm (see Sec-
tion 3.2), we give some additional motivation for their use in the context of
similarity learning.

First, any M ∈ Dλ is a convex combination of symmetric PSD matrices and
is thus also symmetric PSD. Unlike many metric learning algorithms, we thus
avoid the O(d3) cost of projecting onto the PSD cone. Constraining M to be
symmetric PSD provides useful regularization to prevent overﬁtting (Chechik
et al., 2009) and ensures that SM can be interpreted as a dot product after a
linear transformation of the inputs:

SM (x, x(cid:48)) = xTM x(cid:48) = (Lx)T(Lx(cid:48)),

where M = LLT with L ∈ Rd×k. Because the bases in Bλ are rank-one,
the dimensionality k of the transformed space is at most the number of bases
composing M .

i + xjx(cid:48)

j + xix(cid:48)

Second, each basis operates on two features only. In particular, SP (ij)

(x, x(cid:48)) =
λ(xix(cid:48)
i) assigns a higher similarity score when feature i ap-
pears jointly in x and x(cid:48) (likewise for j), as well as when feature i in x and
feature j in y co-occur (and vice versa). Conversely, SN (ij)
penalizes the cross-
λ
In the context of text data represented as
occurrences of features i and j.

j + xjx(cid:48)

λ

8

230

bags-of-words (or other count data), the semantic behind the bases in Bλ is
quite natural: they can be intuitively thought of as encoding the fact that a
term i or j present in both documents makes them more similar, and that two
terms i and j are associated with the same/diﬀerent class or topic.

Optimizing over the convex hull Dλ of Bλ will allow us to easily control the
number of active features, thereby learning a very compact representation with
eﬃcient similarity computations.

Optimization problem. We now describe the optimization problem to learn the
similarity parameters. Following previous work (see for instance Schultz and
Joachims, 2003; Weinberger and Saul, 2009; Chechik et al., 2009), our training
data consists of weak supervision in the form of triplet constraints:

T = {xt should be more similar to yt than to zt}T

t=1 .

Such constraints can be built from a labeled training sample (see Section 4),
provided directly by domain experts or crowdsourcing campaign, or obtained
through implicit feedback such as clicks on search engine results. For notational
convenience, we denote At = xt(yt − zt)T ∈ Rd×d for each constraint t =
1, . . . , T so that we can concisely write SM (xt, yt) − SM (xt, zt) = (cid:104)At, M (cid:105).
We measure the degree of violation of each constraint t with the smoothed
hinge loss (cid:96) : R → R+ deﬁned as

(cid:96) (cid:0)(cid:104)At, M (cid:105)(cid:1) =






0
1
2 − (cid:104)At, M (cid:105)
2 (1 − (cid:104)At, M (cid:105))2

1

if (cid:104)At, M (cid:105) ≥ 1
if (cid:104)At, M (cid:105) ≤ 0
otherwise

.

This convex loss is a continuously diﬀerentiable version of the standard hinge loss
which tries to enforce a margin constraint of the form SM (xt, yt) ≥ SM (xt, zt)+
1. When this constraint is satisﬁed, the value of the loss is zero. On the other
hand, when the margin is negative, i.e. SM (xt, yt) ≤ SM (xt, zt), the penalty
is linear in the margin violation. A quadratic interpolation is used to bridge
between these two cases to ensure that the loss is diﬀerentiable everywhere.

Remark 1 (Choice of loss). One may use any other convex and continuously
diﬀerentiable loss function in our framework, such as the squared hinge loss, the
logistic loss or the exponential loss.

Given λ > 0, our similarity learning formulation aims at ﬁnding the matrix
M ∈ Dλ that minimizes the average margin penalty (as measured by (cid:96)) over
the triplet constraints in T :

235

240

245

min
M ∈Rd×d

f (M ) =

1
T

T
(cid:88)

t=1

(cid:96) (cid:0)(cid:104)At, M (cid:105)(cid:1)

s.t. M ∈ Dλ.

(4)

Due to the convexity of the smoothed hinge loss, (4) involves minimizing
a convex function over the convex domain Dλ. Note that the gradient of the

9

Algorithm 3 Frank Wolfe algorithm for problem (4)
1: initialize M (0) to an arbitrary B ∈ Bλ
2: for k = 0, 1, 2, . . . do
3: B(k)
4: B(k)
5:

(cid:104)B, ∇f (M (k))(cid:105), D(k)
F ← arg minB∈Bλ
A ← arg maxB∈S (k)(cid:104)B, ∇f (M (k))(cid:105), D(k)

A , ∇f (M (k))(cid:105) then

if (cid:104)D(k)

F , ∇f (M (k))(cid:105) ≤ (cid:104)D(k)
F and γmax ← 1

D(k) ← D(k)

D(k) ← D(k)

A and γmax ← α(k)
B(k)
A

/(1 − α(k)
B(k)
A

end if
γ(k) ← arg minγ∈[0,γmax] f (M (k) + γD(k))

9:
10:
11: M (k+1) ← M (k) + γ(k)D(k)
12: end for

6:
7:
8:

else

F ← B(k)
A ← M (k) − B(k)

F − M (k) // forward dir.
// away dir.

A

// choose forward step

)

// choose away step

// perform line search

// update iterate towards direction

objective is given by

∇f (M ) =

1
T

T
(cid:88)

t=1

Gt(M ),

with Gt(M ) =






0
−At
((cid:104)At, M (cid:105) − 1) At

if (cid:104)At, M (cid:105) ≥ 1
if (cid:104)At, M (cid:105) ≤ 0
otherwise

.

(5)

In the next section, we propose a greedy algorithm to eﬃciently ﬁnd sparse

approximate solutions to this problem.

3.2. Algorithm
3.2.1. Exact Frank-Wolfe Algorithm

We propose to use a Frank-Wolfe algorithm with away steps (see Section 2.2)
to learn the similarity. We will exploit the fact that in our formulation (4), the
extremal points (vertices) of the feasible domain Dλ are the elements of Bλ
and have special structure. Our algorithm is shown in Algorithm 3. During
the course of the algorithm, we explicitly maintain a representation of each
iterate M (k) as a convex combination of basis elements as previously discussed
in Section 2.2:

M (k) =

(cid:88)

B∈Bλ

α(k)

B B,

where

(cid:88)

B∈Bλ

B = 1 and α(k)
α(k)

B ≥ 0.

250

We denote the set of active basis elements in M (k) as S (k) = {B ∈ Bλ :
α(k)
B > 0}. The algorithm goes as follows. We initialize M (0) to a random basis
element. Then, at each iteration, we greedily choose between moving towards
a (possibly) new basis (forward step) or reducing the weight of an active one
(away step). The extent of the step is determined by line search. As a result,

10

255

260

265

Algorithm 3 adds only one basis (at most 2 new features) at each iteration, which
provides a convenient way to control the number of active features and maintains
a compact representation of M (k) for a memory cost of O(k). Furthermore,
away steps provide a way to reduce the importance of a potentially “bad” basis
element added at an earlier iteration (or even remove it completely when γ(k) =
γmax). Recall that throughout the execution of the FW algorithm, all iterates
M (k) remain convex combinations of basis elements and are thus feasible. The
following proposition shows that the iterates of Algorithm 3 converge to an
optimal solution of (4) with a rate of O(1/k).

(cid:80)T

t=1 (cid:107)At(cid:107)2

Proposition 1. Let λ > 0, M ∗ be an optimal solution to (4) and L =
1
F . At any iteration k ≥ 1 of Algorithm 3, the iterate M (k) ∈ Dλ
T
satisﬁes f (M (k)) − f (M ∗) ≤ 16Lλ2/(k + 2). Furthermore, it has at most rank
k + 1 with 4(k + 1) nonzero entries, and uses at most 2(k + 1) distinct features.

Proof. We ﬁrst show that ∇f is L-Lipschitz continuous on Dλ with respect to
the Frobenius norm, i.e. for any M1, M2 ∈ Dλ,

(cid:107)∇f (M1) − ∇f (M2)(cid:107)F ≤ L(cid:107)M1 − M2(cid:107)F

(6)

for some L ≥ 0. Note that

(cid:107)∇f (M1) − ∇f (M2)(cid:107)F =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Gt(M1) −

1
T

T
(cid:88)

t=1

(cid:13)
(cid:13)
Gt(M2)
(cid:13)
(cid:13)
(cid:13)F

≤

1
T

T
(cid:88)

t=1

(cid:13)Gt(M1) − Gt(M2)(cid:13)
(cid:13)

(cid:13)F .

Let t ∈ {1, . . . , T }. We will now bound ∆t = (cid:107)Gt(M1) − Gt(M2)(cid:107)F for any
M1, M2 ∈ Dλ. The form of the gradient (5) requires to consider several cases:

270

(i) If (cid:104)At, M1(cid:105) ≥ 1 and (cid:104)At, M2(cid:105) ≥ 1, we have ∆t = 0.

(ii) If (cid:104)At, M1(cid:105) ≤ 0 and (cid:104)At, M2(cid:105) ≤ 0, we have ∆t = 0.

(iii) If 0 < (cid:104)At, M1(cid:105) < 1 and 0 < (cid:104)At, M2(cid:105) < 1, we have:

∆t = (cid:107)(cid:104)At, M1 − M2(cid:105)At(cid:107)F = (cid:107)At(cid:107)F |(cid:104)At, M1 − M2(cid:105)|

≤ (cid:107)At(cid:107)2

F (cid:107)M1 − M2(cid:107)F .

(iv) If (cid:104)At, M1(cid:105) ≥ 1 and (cid:104)At, M2(cid:105) ≤ 0, we have

∆t = (cid:107)At(cid:107)F ≤ (cid:107)At(cid:107)F |(cid:104)At, M1 − M2(cid:105)| ≤ (cid:107)At(cid:107)2

F (cid:107)M1 − M2(cid:107)F .

(v) If (cid:104)At, M1(cid:105) ≥ 1 and 0 < (cid:104)At, M2(cid:105) < 1, we have:

∆t = (cid:107)((cid:104)At, M2(cid:105) − 1)At(cid:107)F = (cid:107)At(cid:107)F (1 − (cid:104)At, M2(cid:105))
≤ (cid:107)At(cid:107)F (1 − (cid:104)At, M2(cid:105)) + (cid:107)At(cid:107)F ((cid:104)At, M1(cid:105) − 1)
= (cid:107)At(cid:107)F (cid:104)At, M1 − M2(cid:105) ≤ (cid:107)At(cid:107)2
F (cid:107)M1 − M2(cid:107)F .

11

(vi) If (cid:104)At, M1(cid:105) ≤ 0 and 0 < (cid:104)At, M2(cid:105) < 1, we have:

∆t = (cid:107) − At − ((cid:104)At, M2(cid:105) − 1)At(cid:107)F = (cid:107)At(cid:104)At, M2(cid:105)(cid:107)F = (cid:107)At(cid:107)F (cid:104)At, M2(cid:105)

≤ (cid:107)At(cid:107)F (cid:104)At, M2(cid:105) − (cid:107)At(cid:107)F (cid:104)At, M1(cid:105)
= (cid:107)At(cid:107)F (cid:104)At, M2 − M1(cid:105) ≤ (cid:107)At(cid:107)2

F (cid:107)M1 − M2(cid:107)F .

The remaining cases are also bounded by (cid:107)At(cid:107)2
cases (iv)-(v)-(vi). Hence ∇f is L-Lipschitz continuous with L = (cid:107)At(cid:107)2
F .

F (cid:107)M1 − M2(cid:107)F by symmetry to

It is easy to see that diam(cid:107)·(cid:107)F (Dλ) =

8λ. The convergence rate then

√

275

follows from the general analysis of the FW algorithm (Jaggi, 2013).

The second part of the proposition follows directly from the structure of the

bases and the greedy nature of the algorithm.

Note that the optimality gap in Proposition 1 is independent of d. Indeed,
At has O(s2) nonzero entries on average, hence the term (cid:107)At(cid:107)2
F in the Lipschitz
constant L can be bounded by s2(cid:107)At(cid:107)∞, where (cid:107)A(cid:107)∞ = maxd
i,j=1 |Ai,j|. This
means that Algorithm 3 is able to ﬁnd a good approximate solution based on
a small number of features in only a few iterations, which is very appealing in
the high-dimensional setting we consider.

3.2.2. Complexity Analysis

We now analyze the time and memory complexity of Algorithm 3. The
form of the gradient (5) along with the structure of the algorithm’s updates are
crucial to its eﬃciency. Since M (k+1) is a convex combination of M (k) and
a 4-sparse matrix B(k), we can eﬃciently compute most of the quantities of
interest through careful book-keeping.

In particular, storing M (k) at iteration k requires O(k) memory. We can
also recursively compute (cid:104)At, M (k+1)(cid:105) for all constraints in only O(T ) time
and O(T ) memory based on (cid:104)At, M (k)(cid:105) and (cid:104)At, B(k)(cid:105). This allows us, for
instance, to eﬃciently compute the objective value as well as to identify the
set of satisﬁed constraints (those with (cid:104)At, M (k)(cid:105) ≥ 1) which are ignored in
the computation of the gradient. Finding the away direction at iteration k
can be done in O(T k) time. For the line search, we use a bisection algorithm
to ﬁnd a root of the gradient of the 1-dimensional function of γ, which only
depends on (cid:104)At, M (k)(cid:105) and (cid:104)At, B(k)(cid:105), both of which are readily available. Its
time complexity is O(T log 1
(cid:15) ) where (cid:15) is the precision of the line-search, with a
memory cost of O(1).

The bottleneck is to ﬁnd the forward direction. Indeed, sequentially consid-
ering each basis element is intractable as it takes O(T d2) time. A more eﬃcient
strategy is to sequentially consider each constraint, which requires O(T s2) time
and O(T s2) memory. The overall iteration complexity of Algorithm 3 is given
in Table 1.

280

285

290

295

300

305

12

Variant

Time complexity Memory complexity

Exact (Algorithm 3)

Mini-batch

Mini-batch + heuristic

˜O(T s2 + T k)
˜O(M s2 + T k)
˜O(M s + T k)

˜O(T s2 + k)
˜O(T + M s2 + k)
˜O(T + M s + k)

Table 1: Complexity of iteration k (ignoring logarithmic factors) for diﬀerent variants of the
algorithm.

3.2.3. Approximate Forward Step

Finding the forward direction can be expensive when T and s are both large.
We propose two strategies to alleviate this cost by ﬁnding an approximately
optimal basis (see Table 1 for iteration complexity).

Mini-batch approximation. Instead of ﬁnding the forward and away directions
based on the full gradient at each iteration, we can estimate it on a mini-batch
of M (cid:28) T constraints drawn uniformly at random (without replacement). The
complexity of ﬁnding the forward direction is thus reduced to O(M s2) time
and O(M s2) memory. Consider the deviation between the “value” of any basis
element B ∈ Bλ on the full set of constraints and its estimation on the mini-
batch, namely

(cid:12)
(cid:12)
(cid:12)

1
M

(cid:88)

t∈M

(cid:104)B, Gt(cid:105) −

1
T

T
(cid:88)

t=1

(cid:12)
(cid:12)
(cid:12),
(cid:104)B, Gt(cid:105)

(7)

where M is the set of M constraint indices drawn uniformly and without replace-
ment from the set {1, . . . , T }. Under mild assumptions, concentration bounds
such as Hoeﬀding’s inequality for sampling without replacement (Serﬂing, 1974;
Bardenet and Maillard, 2015) can be used to show that the probability of (7)
being larger than some constant decreases exponentially fast with M . The FW
algorithm is known to be robust to inexact gradients, and convergence guaran-
tees similar to Proposition 1 can be obtained directly from (Jaggi, 2013; Freund
and Grigas, 2013).

Fast heuristic. To avoid the quadratic dependence on s, we propose to use
the following heuristic to ﬁnd a good forward basis. We ﬁrst pick a feature
i ∈ [d] uniformly at random, and solve the linear problem over the restricted set
(cid:83)
} and use
the resulting basis for the forward direction. This can be done in only O(M s)
time and O(M s) memory and gives good performance in practice, as we shall
see in Section 5.

λ }. We then solve it again over the set (cid:83)

k{P (kj)

j{P (ij)

, N (kj)
λ

, N (ij)

λ

λ

310

315

320

325

4. Generalization Analysis

In this section, we derive generalization bounds for the proposed method.
Our main goal is to give a theoretical justiﬁcation of our approach, in particular

13

330

335

340

by (i) showing that our choice of feasible domain Dλ helps to reduce overﬁtting
in high dimensions, and (ii) showing that the proposed greedy Frank-Wolfe algo-
rithm provides a simple way to balance between optimization and generalization
errors through early stopping.

4.1. Setup and Notations

As in previous work on generalization bounds for metric learning, we consider
the supervised learning setting where the training sample is a set of labeled
points S = {zi = (xi, yi)}n
i=1 drawn i.i.d. from a probability distribution µ over
the space Z = X × Y, where X ⊆ Rd and Y = {1, . . . , C} is the label set. We
assume that BX = supx,x(cid:48),x(cid:48)(cid:48)∈X (cid:107)x(x(cid:48) − x(cid:48)(cid:48))T (cid:107) is bounded for some convenient
matrix norm (cid:107) · (cid:107).

For simplicity, we assume that the univariate loss function (cid:96) : R → R+ is
1-Lipschitz, which is the case for the smoothed hinge loss used in our algorithm.
Given a triplet (z, z(cid:48), z(cid:48)(cid:48)) ∈ Z 3, we say that it is admissible if y = y(cid:48)
(cid:54)= y(cid:48)(cid:48).
Since we only want to consider admissible triplets, we will use the triplet-wise
loss function LM (z, z(cid:48), z(cid:48)(cid:48)) = I[y = y(cid:48) (cid:54)= y(cid:48)(cid:48)] · (cid:96)((cid:104)x(x(cid:48) − x(cid:48)(cid:48))T, M (cid:105)) indexed by
M ∈ Dλ, which is equal to zero for non-admissible triplets.

Given a matrix M ∈ Dλ, we deﬁne its empirical risk associated on the

training set S as follows:

LS(M ) =

1
n(n − 1)(n − 2)

(cid:88)

i(cid:54)=j(cid:54)=k

LM (zi, zj, zk).

Similarly, its expected risk is deﬁned as

L(M ) =

E
z,z(cid:48),z(cid:48)(cid:48)∼µ

[LM (z, z(cid:48), z(cid:48)(cid:48))] .

(8)

(9)

345

In contrast to the standard supervised classiﬁcation setting, note that the em-
pirical risk (8) takes the form of an average of dependent terms known as a
U -statistic (Lee, 1990).

From our feasible domain Dλ = conv(Bλ), we can deﬁne a sequence of nested

sets as follows:

D(k)

λ =

(cid:40) k

(cid:88)

i=1

αiBi : Bi ∈ Bλ, αi ≥ 0,

(cid:41)

αi = 1

,

k = 1, . . . , 2d(d − 1). (10)

k
(cid:88)

i=1

In other words, D(k)
λ

consists of all d × d matrices which can be decomposed
as a convex combination of at most k elements of the basis set Bλ. Clearly, we
have D(1)
λ ⊂ · · · ⊂ D(2d(d−1))
= Dλ. Note also that since (cid:96) is 1-Lipschitz,
by Holder’s inequality we have ∀k:

λ ⊂ D(2)

λ

sup
z,z(cid:48),z(cid:48)(cid:48)∈Z,M ∈D(k)

λ

|LM (z, z(cid:48), z(cid:48)(cid:48))| ≤

sup
x,x(cid:48),x(cid:48)(cid:48)∈X ,M ∈D(k)

λ

|(cid:96)((cid:104)x(x(cid:48) − x(cid:48)(cid:48))T , M (cid:105))|

≤ BX sup

(cid:107)M (cid:107)∗,

M ∈D(k)

λ

(11)

14

where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107).

In the following, we derive theoretical results that take advantage of the
structural properties of our algorithm, namely that the matrix M (k) returned
after k ≥ 1 iterations of Algorithm 3 belongs to D(k)
λ . We ﬁrst bound the
Rademacher complexity of D(k)
λ and derive bounds on the maximal deviation be-
tween L(M ) and LS(M ) for any M ∈ D(k)
λ . We then use these results to derive
bounds on the excess risk L(M (k)) − L(M ∗), where M ∗ ∈ arg minM ∈Dλ
L(M )
is the expected risk minimizer. All proofs can be found in the appendix.

350

355

4.2. Main Results

We ﬁrst characterize the Rademacher complexity of the loss functions in-
dexed by elements of D(k)
λ . Given k ∈ {1, . . . , 2d(d − 1)}, consider the family
F (k) = {LM : M ∈ D(k)
λ } of functions mapping from Z 3 to R+. We will con-
sider the following deﬁnition of the Rademacher complexity of F (k) with respect
to distribution µ and sample size n ≥ 3, adapted from (Cl´emen¸con et al., 2008;
Cao et al., 2012a):

(cid:16)

F (k)(cid:17)

Rn

= Eσ,S∼µn

(cid:20)

1
(cid:98)n/3(cid:99)

(cid:98)n/3(cid:99)
(cid:88)

i=1

sup
M ∈D(k)

λ

σiLM (zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99))

(cid:21)
,

(12)
where σ = (σ1, . . . , σ(cid:98)n/3(cid:99)) are independent uniform random variables taking
values in {−1, 1}. The following lemma gives a bound on the above Rademacher
complexity.

Lemma 1 (Bounded Rademacher complexity). Let n ≥ 3, λ > 0 and 1 ≤ k ≤
2d(d − 1). We have

Rn(F (k)) ≤ 8λBX

360

Proof. See Appendix B.

(cid:115)

2 log k
(cid:98)n/3(cid:99)

.

365

There are two important consequences to Lemma 1. First, restricting the
set of feasible matrices M to Dλ = D(2d(d−1))
instead of Rd×d leads to a
√
Rademacher complexity with a very mild O(
log d) dependence in the di-
mension. This validates our design choice for the feasible domain in the high-
dimensional setting we consider. Second, the Rademacher complexity can ac-
tually be made independent of d by further restricting the number of bases
k.

λ

Using this result, we derive a bound for the deviation between the expected

risk L(M ) and the empirical risk LS(M ) of any M ∈ D(k)
λ .

Theorem 1 (Maximal deviations). Let S be a set of of n points drawn i.i.d.
from µ, λ > 0 and 1 ≤ k ≤ 2d(d − 1). For any δ > 0, with probability 1 − δ we

15

have

sup
M ∈D(k)

λ

[L(M ) − LS(M )] ≤ 16λBX

(cid:115)

2 log k
(cid:98)n/3(cid:99)

+ 3BX BD(k)

λ

(cid:114)

2 ln (2/δ)
n

,

(13)

370

where BD(k)

λ

= supM ∈D(k)

λ

(cid:107)M (cid:107)∗.

Proof. See Appendix C.

√

n)
The generalization bounds given by Theorem 1 exhibit a standard O(1
rate. They also conﬁrm that restricting the number k of bases is a good strat-
egy to guard against overﬁtting when the feature dimension d is high. Inter-
=
estingly, note that due to the convex hull structure of our basis set, BD(k)
supM ∈D(k)
(cid:107)M (cid:107)∗ can be easily bounded by a quantity independent of d for any
k ≥ 1 and any dual norm (cid:107) · (cid:107)∗. We thus have complete freedom to choose the
primal norm (cid:107)·(cid:107) so as to make BX = supx,x(cid:48),x(cid:48)(cid:48)∈X (cid:107)x(x(cid:48)−x(cid:48)(cid:48))T (cid:107) as small as pos-
sible. A good choice of primal norm is the inﬁnity norm (cid:107)A(cid:107)∞ = maxd
i,j=1 |Ai,j|,
which is independent of d. For instance, if X = [0, 1]d we have BX = 1. The
dual norm of the inﬁnity norm being the L1 norm, we then have for any k ≥ 1:

λ

λ

BD(k)

λ

= sup

(cid:107)M (cid:107)1 = sup

d
(cid:88)

|Mi,j| ≤ 4λ.

(14)

M ∈D(k)

λ

M ∈D(k)

λ

i,j=1

Theorem 1 is directly comparable to the results of Cao et al. (2012a), who
derived generalization bounds for similarity learning under various norm regu-
larizers. Their bounds have a similar form, but exhibit a dependence on the
feature dimension d which is at least logarithmic (sometimes even linear, de-
pending on the norm used to regularize the empirical risk). In contrast, our
bounds depend logarithmically on k (cid:28) d. This oﬀers more ﬂexibility in the
high-dimensional setting because k can be directly controlled by stopping our
algorithm after k (cid:28) d iterations to guarantee that the output is in D(k)
λ . This is
highlighted by the following corollary, which combines the generalization bounds
of Theorem 1 with the O(1/k) convergence rate of our Frank-Wolfe optimization
algorithm (Proposition 1).

375

380

Corollary 1 (Excess risk bound). Let S be a set of n points drawn i.i.d. from
µ, λ > 0. Given k ∈ {1, . . . , 2d(d − 1)}, let M (k) be the solution returned after
k iterations of Algorithm 3 applied to the problem minM ∈Dλ LS(M ), and let
M ∗ ∈ arg minM ∈Dλ
L(M ) be the expected risk minimizer over Dλ. For any
δ > 0, with probability 1 − δ we have

L(M (k)) − L(M ∗) ≤

16Lλ2
k + 2

+ 16λBX

(cid:115)

2 log k
(cid:98)n/3(cid:99)

+ 5BX BD(k)

λ

(cid:114)

ln (4/δ)
n

.

Proof. See Appendix D.

16

385

390

395

400

405

410

Corollary 1 shows that the excess risk with respect to the expected risk min-
imizer M ∗ depends on a trade-oﬀ between optimization error and complexity
of the hypothesis class. Remarkably, this trade-oﬀ is ruled by the number k
of iterations of the algorithm: as k increases, the optimization error term de-
creases but the Rademacher complexity terms gets larger. We thus obtain an
excess risk bound which adapts to the actual sparsity of the solution output
by our algorithm. This is in accordance with our overall goal of reducing over-
ﬁtting by allowing a strict control on the complexity of the learned similarity,
and justiﬁes an early-stopping strategy to achieve a good reduction in empirical
risk by selecting the most useful bases while keeping the solution complexity
small enough. Again, the excess risk is independent of the feature dimension
d, suggesting that in the high-dimensional setting it is possible to ﬁnd sparse
solutions with small excess risk. To the best of our knowledge, this is the ﬁrst
result of this nature for metric or similarity learning.

Remark 2 (Approximation of empirical risk by subsampling). The empirical
risk (8) is a sum of O(n3) term, which can be costly to minimize in the large-
scale setting. To reduce the computational cost, an alternative to the mini-batch
strategy described in Section 3.2.3 is to randomly subsample M terms of the sum
(e.g., uniformly without replacement) and to solve the resulting approximate em-
pirical risk minimization problem. For general problems involving U -statistics,
Cl´emen¸con et al. (2016) showed that sampling only M = O(n) terms is suﬃ-
n) rate. These arguments can be adapted to our
cient to maintain the O(1/
setting to obtain results similar to Theorem 1 and Corollary 1 for this subsam-
pled empirical risk.

√

5. Experiments

In this section, we present experiments to evaluate the performance and ro-
bustness of hdsl. In Section 5.1, we use synthetic data to study the performance
of our approach in terms of similarity recovery and generalization in high dimen-
sions in a controlled environment. Section 5.2 evaluates our algorithm against
competing approaches on classiﬁcation and dimensionality reduction using real-
world datasets.

415

5.1. Experiments on Synthetic Data

We ﬁrst conduct experiments on synthetic datasets in order to address two

questions:

1. Is the algorithm able to recover the ground truth sparse similarity function

from (potentially weak) similarity judgments?

420

2. How well does the algorithm generalize as the dimensionality increases?

17

(a) Feature recovery AUC

(b) Entry recovery AUC

Figure 1: Similarity recovery experiment on synthetic data. Figure 1(a) and Figure 1(b) show
the AUC scores (for feature recovery and entry recovery respectively) along the iterations of
the algorithm for diﬀerent values of α.

425

430

435

440

445

5.1.1. Similarity Recovery

To investigate the algorithm’s ability to recover the underlying similarity,
we generate a ground truth similarity metric M ∈ Rd×d where d = 2000. M is
constructed as a convex combination of 100 randomly selected rank-one 4-sparse
bases as speciﬁed in Section 3.1. The combination coeﬃcients are drawn from a
Dirichlet distribution with shape parameter 9 and scale parameter 0.5. Without
loss of generality, we choose the metric to be block structured by restricting
the basis selection from two blocks. This makes the resulting matrix easier to
visualize, as show in Figure 2(a).

We then generate 5000 training samples from the uniform distribution on
[0, 1] with 2% sparsity. From this sample, we create 30, 000 training triplets
{(x1, x2, x3)} where x1 is randomly picked and x2 (or x3) is sampled among
x1’s top α% similar (or dissimilar) samples as measured by the ground truth
metric M . The parameter α controls the “quality” of the triplet constraints:
a larger α leads to less similar (or dissimilar) samples in the triplets, thereby
providing a weaker signal about the underlying similarity. We experiment with
various α (10%, 20%, 25%, 30%) to investigate the robustness of hdsl to the
quality of the supervision. In all our experiments, we use λ = 100.

Results. We aim to measure how accurately we recover the entries (i.e., pairs of
features) that are active in the ground truth similarity as training proceeds. To
do so, at each iteration k of hdsl, we rank each pair of features by descending
order of the absolute value of the corresponding entry in the current matrix
M (k). We then compute the Area under the ROC Curve (AUC) of the ranking
induced by the similarity with respect to the list of active entries in the ground
truth similarity. The AUC is well-suited to the imbalanced setting (as active
entries in the ground truth are a small subset of all entries). It can be interpreted
as the probability that a random entry that is active in the ground truth is
ranked higher than a random inactive one. Following a similar process, we also
compute an AUC score for individual features: this is done by ranking each

18

(a) True similarity

(b) Learned similarity (α = 20%)

Figure 2: Similarity recovery experiment on synthetic data. Figure 2(a) shows the underlying
ground truth similarity, where blue dots represent positive entries and red dots represent
negative entries (combination coeﬃcients are not displayed). Figure 2(b) shows the similarity
learned by hdsl (α =20%), which is visually very close to the ground truth.

450

feature by the L1 norm of its associated row in the matrix.

455

460

465

470

The AUC scores for feature and entry recovery along the iterations are re-
ported in Figure 1 for diﬀerent values of α. When the quality of the triplet
constraints is high (α =10%,20%), the AUC increases quickly to converge very
close to 1.0, indicating an almost perfect recovery of relevant features/entries.
This conﬁrms that hdsl is able to accurately identify the small number of cor-
rect features and pairs of features. As α increases (i.e., the similarity constraints
become noisy and less informative), the AUC increases at a slower pace and the
ﬁnal value decreases. This is expected as the quality of the information carried
by the similarity judgments is key to recover the ground truth similarity. Yet,
even for α =30%, the ﬁnal AUC score is still very high (above 0.85 for both
feature and entry recovery). This good recovery behavior is conﬁrmed by the
visual representations of the ground truth and learned similarity matrices shown
in Figure 2. We observe that the learned similarity (when α = 20%) clearly re-
covers the block structure of the true similarity, and is able to correctly identify
most individual entries with very few false positives.

5.1.2. Link Prediction

We now investigate the ability of our algorithm to generalize well as the
feature dimensionality increases by conducting a signed link prediction experi-
ment, which is the task of distinguishing positive and negative interactions in a
network (see e.g. Agrawal et al., 2013).

We generate 500 samples with diﬀerent number of features d ranging from
5, 000 to 1, 000, 000. As the dimension d increases, we decrease the average spar-
sity of data (from 0.02 to 0.002) to limit running time. In real high-dimensional
instead, a
datasets, features typically do not appear in a uniform frequency:

19

(a) Feature frequency (d = 50, 000)

(b) AUC scores on the test set

Figure 3: Link prediction experiment on synthetic data. Figure 3(a) shows the feature fre-
quency distribution, which follows a power law as in many real high-dimensional datasets.
Figure 3(b) shows AUC scores on the test set for diﬀerent number of features (in log scale)
and number of training constraints per link.

475

480

485

490

495

500

small portion of features tends to dominate the others. Following this observa-
tion, we generate features whose frequency follow a power law style distribution,
as shown in Figure 3(a). The ground truth similarity is then a convex combina-
tion of randomly selected bases as in the previous experiment, except that we
restrict the selected bases to those involving features that are frequent enough
(a frequency of at least 0.1 was chosen for this experiment). This is needed to
ensure that the features involved in the ground truth similarity will occur at
least a few times in our small dataset, but we emphasize that the algorithm is
exposed to the entire feature set and does not know which features are relevant.
Based on the samples and the ground truth similarity, we generate signed
link observations of the form {xi
(yi ∈ {−1, 1}). We associate the
2, yi}N
i
label yi = 1 (positive link) to pairs for which the similarity between x1 and x2
ranks in the top 5% of x1’s (or x2’s) neighbors according to the ground truth
similarity measure. On the other hand, yi = −1 (negative link) indicates that
the similarity ranks in the bottom 5% of x1’s (or x2’s) neighbors. We split these
link observations into training, validation and test sets of 1, 000 observations
each. Triplets constraints are generated from training links — given a pair
x1, x2, y, we randomly sample x3 as a similar (if y = −1) or dissimilar (if y = 1)
node. The validation set is used to tune the hyperparameter λ and for early
stopping.

1, xi

Results. We measure the generalization ability of hdsl by the AUC score of
link prediction on the test set. Figure 3(b) reports these AUC scores across
diﬀerent dimensions. We also show results for diﬀerent numbers of constraints
per training link. The results are averaged over 5 random runs. As one would
expect, the task becomes increasingly diﬃcult as the dimension becomes larger,
since the size of the training set is ﬁxed (1, 000 training links generated from 500
nodes). However, the performance decreases slowly (roughly logarithmically)
with the dimension, and we achieve very high AUC scores (larger than 0.9)

20

Datasets
dexter
dorothea
rcv1 2
rcv1 4

Dimension
20,000
100,000
47,236
29,992

Sparsity Training size Validation size Test size
0.48%
0.91%
0.16%
0.26%

300
800
12,145
3,850

300
350
4,048
2,888

2,000
800
4,049
2,887

Table 2: Datasets used in the experiments

even for one million features. We also see that training from more constraints
tends to improve the prediction performance.

505

5.2. Experiments on Real Datasets

We now present comparative experiments on several high-dimensional real
datasets, evaluating our approach against several baselines and competing meth-
ods.

5.2.1. Setup
Datasets. We report experimental results on several real-world classiﬁcation
datasets with up to 100,000 features. Dorothea and dexter come from the NIPS
2003 feature selection challenge (Guyon et al., 2004) and are respectively phar-
maceutical and text data with predeﬁned splitting into training, validation and
test sets. They both contain a large proportion of noisy/irrelevant features.
Reuters CV1 is a popular text classiﬁcation dataset with bag-of-words repre-
sentation. We use the binary classiﬁcation version from the LIBSVM dataset
collection3 (with 60%/20%/20% random splits) and the 4-classes version (with
40%/30%/30% random splits) introduced by Cai and He (2012). Detailed infor-
mation on the datasets and splits is given in Table 2. All datasets are normalized
such that each feature takes values in [0, 1].

510

515

520

Competing methods. We compare the proposed approach (hdsl) to several
methods:

• dot: The standard dot product, which is equivalent to setting M = I.

• diag: Diagonal similarity learning (i.e., a weighting of the features), as
done in Gao et al. (2014). We obtain it by minimizing the same loss as in
hdsl with (cid:96)2 and (cid:96)1 regularization, i.e.,

min
w∈Rd

f (w) =

1
T

T
(cid:88)

t=1

(cid:96) (cid:0)(cid:104)At, diag(w)(cid:105)(cid:1) + λΩ(w),

where Ω(w) ∈ {(cid:107)w(cid:107)2
timization was done using (proximal) gradient descent.

2, (cid:107)w(cid:107)1} and λ is the regularization parameter. Op-

525

3http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

21

• rp+oasis: Similarity learning in random projected space. Given r (cid:28) d,
let R ∈ Rd×r be a matrix where each entry rij is randomly drawn from
r RTx ∈ Rr
N (0, 1). For each data instance x ∈ Rd, we generate ˜x = 1√
and use this reduced data in OASIS (Chechik et al., 2009), a fast online
method to learn a bilinear similarity from triplet constraints.

• pca+oasis: Similarity learning in PCA space. Same as rp+oasis, except
that PCA is used instead of random projections to project the data into
Rr.

• svm: Support Vector Machines. We use linear SVM, which is known to
perform well for sparse high-dimensional data (Caruana et al., 2008), with
(cid:96)2 and (cid:96)1 regularization. We also use nonlinear SVM with the polynomial
kernel (2nd and 3rd degree) popular in text classiﬁcation (Chang et al.,
2010). The SVM models are trained using liblinear (Fan et al., 2008) and
libsvm (Chang and Lin, 2011) with 1vs1 paradigm for multiclass.

We have also tried to compare our method with Comet (Atzmon et al.,
2015), which also learns a bilinear similarity in a greedy fashion with rank-1 up-
dates. However, as mentioned in Section 2.1 their coordinate descent algorithm
has a time complexity of O(d2) per iteration, as well as overall memory com-
plexity of O(d2). We run the sparse version of code provided by the authors4
on a machine with a 2.3GHz Intel Core i7 and 16GB memory. On the dexter
dataset (which has the smallest dimensionality in our benchmark), a single pass
over the features took more than 4 hours, while the authors reported that about
10 passes are generally needed for Comet to converge (Atzmon et al., 2015).
On the dorothea dataset, Comet returned a memory error. As a result, we
did not include Comet to our empirical comparison. In contrast, on the same
hardware, our approach hdsl takes less than 1 minute on dexter and less than
1 hour on dorothea.

Training Procedure. For all similarity learning algorithms, we generate 15 train-
ing constraints for each instance by identifying its 3 target neighbors (nearest
neighbors with same label) and 5 impostors (nearest neighbors with diﬀerent
label), following Weinberger and Saul (2009). Due to the very small number
of training instances in dexter, we found that better performance is achieved
across all methods when using 20 training constraints per instance, drawn at
random based on its label. All parameters are tuned using the accuracy on the
validation set. For hdsl, we use the fast heuristic described in Section 3.2.3 and
tune the scale parameter λ ∈ {1, 10, . . . , 109}. The regularization parameters of
diag and svm are tuned in {10−9, . . . , 108} and the “aggressiveness” parameter
of OASIS is tuned in {10−9, . . . , 102}.

530

535

540

545

550

555

560

22

Dataset
dexter
dorothea
rcv1 2
rcv1 4

dot

20.1
9.3
6.9
11.2

rp+oasis

pca+oasis

24.0 [1000]
11.4 [150]
7.0 [2000]
10.6 [1000]

9.3 [50]
9.9 [800]
4.5 [1500]
6.1 [800]

diag-(cid:96)2
8.4
6.8
3.5
6.2

diag-(cid:96)1
8.4 [773]
6.6 [860]
3.7 [5289]
7.2 [3878]

hdsl

6.5 [183]
6.5 [731]
3.4 [2126]
5.7 [1888]

Table 3: k-NN test error (%) of the similarities learned with each method. The number of
features used by each similarity (when smaller than d) is given in brackets. Best accuracy on
each dataset is shown in bold.

Dataset
dexter
dorothea
rcv1 2
rcv1 4

svm-poly-2
9.4
7
3.4
5.7

svm-poly-3
9.2
6.6
3.3
5.7

svm-linear-(cid:96)2
8.9
8.1
3.5
5.1

svm-linear-(cid:96)1
8.9 [281]
6.6 [366]
4.0 [1915]
5.7 [2770]

hdsl

6.5 [183]
6.5 [731]
3.4 [2126]
5.7 [1888]

Table 4: Test error (%) of several SVM variants compared to hdsl. As in Table 3, the number
of features is given in brackets and best accuracies are shown in bold.

565

570

575

580

585

5.2.2. Results
Classiﬁcation Performance. We ﬁrst investigate the performance of each sim-
ilarity learning approach in k-NN classiﬁcation (k was set to 3 for all exper-
iments). For rp+oasis and pca+oasis, we choose the dimension r of the
reduced space based on the accuracy of the learned similarity on the validation
set, limiting our search to r ≤ 2000 because OASIS is extremely slow beyond
this point.5 Similarly, we use the performance on validation data to do early
stopping in hdsl, which also has the eﬀect of restricting the number of features
used by the learned similarity.

Table 3 shows the k-NN classiﬁcation performance. We can ﬁrst observe that
rp+oasis often performs worse than dot, which is consistent with previous ob-
servations showing that a large number of random projections may be needed
to obtain good performance (Fradkin and Madigan, 2003). pca+oasis gives
much better results, but is generally outperformed by a simple diagonal simi-
larity learned directly in the original high-dimensional space. hdsl, however,
outperforms all other algorithms on these datasets, including diag. This shows
the good generalization performance of the proposed approach, even though the
number of training samples is sometimes very small compared to the number
of features, as in dexter and dorothea. It also shows the relevance of encoding
“second order” information (pairwise interactions between the original features)
in the similarity instead of considering a simple weighting of features as in diag.
Table 4 shows the comparison with SVMs. Interestingly, hdsl outperforms
all SVM variants on dexter and dorothea, both of which have a large proportion

4https://github.com/yuvalatzmon/COMET
5Note that the number of PCA dimensions is at most the number of training examples.

Therefore, for dexter and dorothea, r is at most 300 and 800 respectively.

23

(a) dexter dataset

(b) rcv1 4 dataset

Figure 4: Number of active features learned by hdsl as a function of the iteration number.

of irrelevant features. This shows that its greedy strategy and early stopping
mechanism achieves better feature selection and generalization than the (cid:96)1 ver-
sion of linear SVM. On the other two datasets, hdsl is competitive with SVM,
although it is outperformed slightly by one variant (svm-poly-3 on rcv1 2 and
svm-linear-(cid:96)2 on rcv1 4), both of which rely on all features.

Feature selection and sparsity. We now focus on the ability of hdsl to perform
feature selection and more generally to learn sparse similarity functions. To bet-
ter understand the behavior of hdsl, we show in Figure 4 the number of selected
features as a function of the iteration number for two of the datasets. Remember
that at most two new features can be added at each iteration. Figure 4 shows
that hdsl incorporates many features early on but tends to eventually converge
to a modest fraction of features (the same observation holds for the other two
datasets). This may explain why hdsl does not suﬀer much from overﬁtting
even when training data is scarce as in dexter.

Another attractive characteristic of hdsl is its ability to learn a matrix
that is sparse not only on the diagonal but also oﬀ-diagonal (the proportion
of nonzero entries is in the order of 0.0001% for all datasets). In other words,
the learned similarity only relies on a few relevant pairwise interactions between
features. Figure 5 shows two examples, where we can see that hdsl is able to
exploit the product of two features as either a positive or negative contribution
to the similarity score. This opens the door to an analysis of the importance of
pairs of features (for instance, word co-occurrence) for the application at hand.
Finally, the extreme sparsity of the matrices allows very fast similarity com-
putation. Together with the superior accuracy brought by hdsl, it makes our
approach potentially useful in a variety of contexts (k-NN, clustering, ranking,
etc).

Finally, it is also worth noticing that hdsl uses signiﬁcantly less features
than diag-(cid:96)1 (see numbers in brackets in Table 3). We attribute this to the
extra modeling capability brought by the non-diagonal similarity observed in

590

595

600

605

610

615

24

0200040006000800010000050100150200250iterationNumber of Selected Features020004000600080000500100015002000iterationNumber of Selected Features(a) dexter (20, 000 × 20, 000 matrix, 712
nonzeros)

(b) rcv1 4 (29, 992 × 29, 992 matrix, 5263
nonzeros)

Figure 5: Sparsity structure of the matrix M learned by hdsl. Positive and negative entries
are shown in blue and red, respectively (best seen in color).

Figure 5.6

Dimension reduction. We now investigate the potential of hdsl for dimension-
ality reduction. Recall that hdsl learns a sequence of PSD matrices M (k). We
can use the square root of M (k) to project the data into a new space where
the dot product is equivalent to SM (k) in the original space. The dimension of
the projection space is equal to the rank of M (k), which is upper bounded by
k + 1 (see Section 3.1). A single run of hdsl can thus be seen as incrementally
building projection spaces of increasing dimensionality.

To assess the dimensionality reduction quality of hdsl (measured by k-NN
classiﬁcation error on the test set), we plot its performance at various iterations
during the runs that generated the results of Table 3. We compare it to two
standard dimensionality reduction techniques: random projection and PCA. We
also evaluate rp+oasis and pca+oasis, i.e., learn a similarity with OASIS on
top of the RP and PCA features.7 Note that OASIS was tuned separately for
each projection size, making the comparison a bit unfair to hdsl. The results
are shown in Figure 6. As observed earlier, random projection-based approaches
achieve poor performance. When the features are not too noisy (as in rcv1 2
and rcv1 4), PCA-based methods are better than hdsl at compressing the space
into very few dimensions, but hdsl eventually catches up. On the other hand,
PCA suﬀers heavily from the presence of noise (dexter and dorothea), while

620

625

630

635

6Note that hdsl uses roughly the same number of features as svm-linear-(cid:96)1 (Table 4), but
it is diﬃcult to draw any solid conclusion because the objective and training data for each
method are diﬀerent, and SVM is a combination of binary models.

7Again, we were not able to run OASIS beyond a certain dimension due to computational

complexity.

25

(a) dexter dataset

(b) dorothea dataset

(c) rcv1 2 dataset

(d) rcv1 4 dataset

Figure 6: k-NN test error as a function of the dimensionality of the space (in log scale). Best
seen in color.

hdsl is able to quickly improve upon the standard similarity in the original
space. Finally, on all datasets, we observe that hdsl converges to a station-
ary dimension without overﬁtting, unlike pca+oasis which exhibits signs of
overﬁtting on dexter and rcv1 4 especially.

640

6. Concluding Remarks

645

In this work, we proposed an eﬃcient approach to learn similarity functions
from high-dimensional sparse data. This is achieved by forming the similarity
as a combination of simple sparse basis elements that operate on only two fea-
tures and the use of an (approximate) Frank-Wolfe algorithm. Our algorithm is
completed by a novel generalization analysis which validates the design choices
and highlights the robustness of our approach to high dimensions. Experiments
on synthetic and real datasets conﬁrmed the good practical behavior of our
method for classiﬁcation and dimensionality reduction. The learned similarity
may be applied to other algorithms that rely on a similarity function (clustering,

26

1011021031045101520253035404550projection space dimensionError rate  HDSLRPRP+OASISPCAPCA+OASISIdentity101102103104510152025303540projection space dimensionError rate  HDSLRPRP+OASISPCAPCA+OASISIdentity101102103104510152025303540projection space dimensionError rate  HDSLRPRP+OASISPCAPCA+OASISIdentity101102103104510152025303540projection space dimensionError rate  HDSLRPRP+OASISPCAPCA+OASISIdentity650

655

660

665

670

675

ranking), or as a way to preprocess the data before applying another learning al-
gorithm. We also note that St.Amand and Huan (2017) have recently extended
our hdsl algorithm to learn local metrics for diﬀerent regions of the space in
addition to the global metric.

We leave several fundamental questions for future work. In particular, our
framework could be extended to optimize a loss function related to a linear
classiﬁcation objective. We could then attempt to adapt our analysis to obtain
generalization bounds directly for the classiﬁcation error. Such bounds exist in
the literature (see Bellet et al., 2012; Guo and Ying, 2014) but exhibit a classic
dependence on the data dimension that could be avoided with our approach.
Another interesting, though challenging direction is to formally study the condi-
tions under which a sparse ground truth similarity can be accurately recovered
from similarity judgments. Inspiration could be drawn from the related problem
of sparse recovery in the compressed sensing literature (Foucart and Rauhut,
2013).

Acknowledgments. This work was partially supported by a grant from CPER
Nord-Pas de Calais/FEDER DATA Advanced data science and technologies
2015-2020. It was also supported in part by the Intelligence Advanced Research
Projects Activity (IARPA) via Department of Defense U.S. Army Research Lab-
oratory (DoD / ARL) contract number W911NF-12-C-0012, a NSF IIS-1065243,
an Alfred. P. Sloan Research Fellowship, DARPA award D11AP00278, and an
ARO YIP Award (W911NF-12-1-0241). The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. The views and conclusions contained herein
are those of the authors and should not be interpreted as necessarily represent-
ing the oﬃcial policies or endorsements, either expressed or implied, of IARPA,
DoD/ARL, or the U.S. Government.

References

Agrawal, P., Garg, V.K., Narayanam, R., 2013. Link label prediction in signed

social networks., in: IJCAI, pp. 2591–2597.

680

Atzmon, Y., Shalit, U., Chechik, G., 2015. Learning sparse metrics, one feature
at a time, in: NIPS 2015 Workshop on Feature Extraction: Modern Questions
and Challenges.

Bardenet, R., Maillard, O.A., 2015. Concentration inequalities for sampling

without replacement. Bernoulli 21, 1361–1385.

685

Bellet, A., Habrard, A., 2015. Robustness and Generalization for Metric Learn-

ing. Neurocomputing 151, 259–267.

Bellet, A., Habrard, A., Sebban, M., 2012. Similarity Learning for Provably

Accurate Sparse Linear Classiﬁcation, in: ICML, pp. 1871–1878.

27

Bellet, A., Habrard, A., Sebban, M., 2013. A Survey on Metric Learning for
Feature Vectors and Structured Data. Technical Report. arXiv:1306.6709.

690

Bellet, A., Habrard, A., Sebban, M., 2015. Metric Learning. Morgan & Claypool

Publishers.

Bian, W., Tao, D., 2011. Learning a Distance Metric by Empirical Loss Mini-

mization, in: IJCAI, pp. 1186–1191.

695

Cai, D., He, X., 2012. Manifold Adaptive Experimental Design for Text Catego-
rization. IEEE Transactions on Knowledge and Data Engineering 24, 707–719.

Cao, Q., Guo, Z.C., Ying, Y., 2012a. Generalization Bounds for Metric and
Similarity Learning. Technical Report. University of Exeter. ArXiv:1207.5437.

Cao, Q., Ying, Y., Li, P., 2012b. Distance Metric Learning Revisited,

in:

700

ECML/PKDD, pp. 283–298.

Caruana, R., Karampatziakis, N., Yessenalina, A., 2008. An empirical evalua-

tion of supervised learning in high dimensions, in: ICML, pp. 96–103.

Chang, C.C., Lin, C.J., 2011. LIBSVM : a library for support vector machines.

ACM Transactions on Intelligent Systems and Technology 2, 27–27.

705

Chang, Y.W., Hsieh, C.J., Chang, K.W., Ringgaard, M., Lin, C.J., 2010. Train-
ing and Testing Low-degree Polynomial Data Mappings via Linear SVM.
Journal of Machine Learning Research 11, 1471–1490.

Chechik, G., Shalit, U., Sharma, V., Bengio, S., 2009. An online algorithm for

large scale image similarity learning., in: NIPS, pp. 306–314.

710

Chen, Y., Pavlov, D., Canny, J.F., 2009. Large-scale behavioral targeting, in:

KDD.

Clarkson, K.L., 2010. Coresets, sparse greedy approximation, and the Frank-

Wolfe algorithm. ACM Transactions on Algorithms 6, 1–30.

715

Cl´emen¸con, S., Colin, I., Bellet, A., 2016. Scaling-up Empirical Risk Minimiza-
tion: Optimization of Incomplete U-statistics. Journal of Machine Learning
Research 17, 1–36.

Cl´emen¸con, S., Lugosi, G., Vayatis, N., 2008. Ranking and Empirical Minimiza-

tion of U-statistics. Annals of Statistics 36, 844–874.

Davis, J.V., Kulis, B., Jain, P., Sra, S., Dhillon, I.S., 2007. Information-theoretic

720

metric learning, in: ICML, pp. 209–216.

Fan, R.E., Chang, K.W., Hsieh, C.J., Wang, X.R., Lin, C.J., 2008. LIBLIN-
EAR: A Library for Large Linear Classiﬁcation. Journal of Machine Learning
Research 9, 1871–1874.

28

Foucart, S., Rauhut, H., 2013. A Mathematical Introduction to Compressive

725

Sensing. Birkha¨user.

Fradkin, D., Madigan, D., 2003. Experiments with random projections for

machine learning, in: KDD, pp. 517–522.

Frank, M., Wolfe, P., 1956. An algorithm for quadratic programming. Naval

Research Logistics Quarterly 3, 95–110.

730

Freund, R.M., Grigas, P., 2013. New Analysis and Results for the Conditional

Gradient Method. Technical Report. arXiv:1307.0873.

Gao, X., Hoi, S.C., Zhang, Y., Wan, J., Li, J., 2014. SOML: Sparse Online
Metric Learning with Application to Image Retrieval, in: AAAI, pp. 1206–
1212.

735

Goldberger, J., Roweis, S., Hinton, G., Salakhutdinov, R., 2004. Neighbourhood

Components Analysis, in: NIPS, pp. 513–520.

Gu´elat, J., Marcotte, P., 1986. Some comments on Wolfe’s away step. Mathe-

matical Programming 35, 110–119.

Guillaumin, M., Verbeek, J.J., Schmid, C., 2009. Is that you? Metric learning

740

approaches for face identiﬁcation, in: ICCV, pp. 498–505.

Guo, Z.C., Ying, Y., 2014. Guaranteed Classiﬁcation via Regularized Similarity

Learning. Neural Computation 26, 497–522.

Guyon, I., Gunn, S.R., Ben-Hur, A., Dror, G., 2004. Result Analysis of the

NIPS 2003 Feature Selection Challenge, in: NIPS.

745

Hoeﬀding, W., 1948. A Class of Statistics with Asymptotically Normal Distri-

bution. The Annals of Mathematical Statistics 19, 293–325.

Jaggi, M., 2011. Sparse Convex Optimization Methods for Machine Learning.

Ph.D. thesis. ETH Zurich.

Jaggi, M., 2013. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Opti-

750

mization, in: ICML.

Jain, L., Mason, B., Nowak, R., 2017. Learning Low-Dimensional Metrics, in:

NIPS.

Jin, R., Wang, S., Zhou, Y., 2009. Regularized Distance Metric Learning: The-

ory and Algorithm, in: NIPS.

755

Kedem, D., Tyree, S., Weinberger, K., Sha, F., Lanckriet, G., 2012. Non-linear

Metric Learning, in: NIPS, pp. 2582–2590.

Kulis, B., 2012. Metric Learning: A Survey. Foundations and Trends in Machine

Learning 5, 287–364.

29

Lacoste-Julien, S., Jaggi, M., 2015. On the Global Linear Convergence of Frank-

760

Wolfe Optimization Variants, in: NIPS.

Leach, A.R., Gillet, V.J., 2007. An Introduction to Chemoinformatics. Springer.

Lee, A.J., 1990. U-Statistics: Theory and Practice. Marcel Dekker, New York.

Lim, D.K., McFee, B., Lanckriet, G., 2013. Robust Structural Metric Learning,

in: ICML.

765

Liu, K., Bellet, A., Sha, F., 2015a. Similarity Learning for High-Dimensional

Sparse Data, in: AISTATS, pp. 653–662.

Liu, W., Mu, C., Ji, R., Ma, S., Smith, J.R., Chang, S.F., 2015b. Low-Rank

Similarity Metric Learning in High Dimensions, in: AAAI.

McDiarmid, C., 1989. On the method of bounded diﬀerences. Surveys in com-

770

binatorics 141, 148–188.

Qi, G.J., Tang, J., Zha, Z.J., Chua, T.S., Zhang, H.J., 2009. An Eﬃcient Sparse
Metric Learning in High-Dimensional Space via l1-Penalized Log-Determinant
Regularization, in: ICML.

Qian, Q., Jin, R., Zhang, L., Zhu, S., 2015. Towards Making High Dimensional
Distance Metric Learning Practical. Technical Report. arXiv:1509.04355.

775

Qian, Q., Jin, R., Zhu, S., Lin, Y., 2014. An Integrated Framework for High
Dimensional Distance Metric Learning and Its Application to Fine-Grained
Visual Categorization. Technical Report. arXiv:1402.0453.

Rosales, R., Fung, G., 2006. Learning Sparse Metrics via Linear Programming,

780

in: KDD, pp. 367–373.

Schultz, M., Joachims, T., 2003. Learning a Distance Metric from Relative

Comparisons, in: NIPS.

Serﬂing, R.J., 1974. Probability inequalities for the sum in sampling without

replacement. The Annals of Statistics 2, 39–48.

785

Shalev-Shwartz, S., Ben-David, S., 2014. Understanding Machine Learning:

From Theory to Algorithms. Cambridge University Press.

Shen, C., Kim, J., Wang, L., van den Hengel, A., 2012. Positive Semideﬁnite
Metric Learning Using Boosting-like Algorithms. Journal of Machine Learning
Research 13, 1007–1036.

790

Shi, Y., Bellet, A., Sha, F., 2014. Sparse Compositional Metric Learning, in:

AAAI, pp. 2078–2084.

St.Amand, J., Huan, J., 2017. Sparse Compositional Local Metric Learning, in:

KDD.

30

Verma, N., Branson, K., 2015. Sample complexity of learning mahalanobis

795

distance metrics, in: NIPS.

Wang, J., Woznica, A., Kalousis, A., 2012. Parametric Local Metric Learning

for Nearest Neighbor Classiﬁcation, in: NIPS, pp. 1610–1618.

Weinberger, K.Q., Saul, L.K., 2009. Distance Metric Learning for Large Margin
Nearest Neighbor Classiﬁcation. Journal of Machine Learning Research 10,
207–244.

800

Yao, D., Zhao, P., Pham, T.A.N., Cong, G., 2018. High-dimensional Similarity

Learning via Dual-sparse Random Projection, in: IJCAI.

Ying, Y., Huang, K., Campbell, C., 2009. Sparse Metric Learning via Smooth

Optimization, in: NIPS, pp. 2214–2222.

805

Ying, Y., Li, P., 2012. Distance Metric Learning with Eigenvalue Optimization.

Journal of Machine Learning Research 13, 1–26.

Zhang, J., Zhang, L., 2017. Eﬃcient Stochastic Optimization for Low-Rank

Distance Metric Learning, in: AAAI.

Appendix A. Technical Lemmas

810

The following classic result, known as the ﬁrst Hoeﬀding’s decomposition,

allows to represent a U -statistic as a sum of i.i.d. blocks.

Lemma 2 (Hoeﬀding, 1948). Let q : Z × Z × Z → R be a real-valued function.
Given the i.i.d. random variables z1, z2, ..., zn ∈ Z, we have

Un(q) =

1
n(n − 1)(n − 2)

(cid:88)

i(cid:54)=j(cid:54)=k

q(zi, zj, zk)

=

1
n!

(cid:88)

π

1
(cid:98)n/3(cid:99)

(cid:98)n/3(cid:99)
(cid:88)

i=1

q(zπ(i), zπ(i+(cid:98)n/3(cid:99)), zπ(i+2×(cid:98)n/3(cid:99))).

1

Proof. Observe that ∀i (cid:54)= j (cid:54)= k, q(zi, zj, zk) appears once on the left hand side
n(n−1)(n−2) of its value, while on the right hand side it appears
and Un(q) has
(n − 3)! × (cid:98)n/3(cid:99) times, because for each of the (cid:98)n/3(cid:99) positions there are (n − 3)!
n(n−1)(n−2) of its
possible permutations. Thus the right hand side also has
function value. We thus have the equality.

1

815

The next technical lemma is based on the above representation.

31

Lemma 3. Let Q be a set of functions from Z 3 to R. If z1, z2, ..., zn ∈ Z are
i.i.d., then we have

E[sup
q∈Q

1
n(n − 1)(n − 2)

(cid:88)

i(cid:54)=j(cid:54)=k

q(zi, zj, zk)]

≤ E[sup
q∈Q

1
(cid:98)n/3(cid:99)

(cid:98)n/3(cid:99)
(cid:88)

i=1

q(zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99))].

Proof. From Lemma 2, we observe that

E[sup
q∈Q

1
n(n − 1)(n − 2)

(cid:88)

q(z, z(cid:48), z(cid:48)(cid:48))]

z(cid:54)=z(cid:48)(cid:54)=z(cid:48)(cid:48)

= E[sup
q∈Q

1
n!

(cid:88)

π

1
(cid:98)n/3(cid:99)

≤

=

1
n!

1
n!

(cid:88)

E[

π

1
(cid:98)n/3(cid:99)

sup
q∈Q

(cid:88)

π

E[sup
q∈Q

1
(cid:98)n/3(cid:99)

(cid:98)n/3(cid:99)
(cid:88)

i=1

(cid:98)n/3(cid:99)
(cid:88)

i=1

(cid:98)n/3(cid:99)
(cid:88)

i=1

q(zπ(i), zπ(i+(cid:98)n/3(cid:99)), zπ(i+2×(cid:98)n/3(cid:99)))]

q(zπ(i), zπ(i+(cid:98)n/3(cid:99)), zπ(i+2×(cid:98)n/3(cid:99)))]

q(zπ(i), zπ(i+(cid:98)n/3(cid:99)), zπ(i+2×(cid:98)n/3(cid:99)))]

= E[sup
q∈Q

1
(cid:98)n/3(cid:99)

(cid:98)n/3(cid:99)
(cid:88)

i=1

q(zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99))],

which proves the result.

Finally, we recall McDiarmid’s inequality.

Lemma 4 (McDiarmid, 1989). Let Z be some set and let f : Z n → R be a
function of n variables such that for some c > 0, for all i ∈ {1, . . . , n} and for
all z1, . . . , zn, z(cid:48)

i ∈ Z, we have

|f (z1, . . . , zi−1, zi, zi+1, . . . , , zn) − f (z1, . . . , zi−1, z(cid:48)

i, zi+1, . . . , zn)| ≤ c.

Let Z1, . . . , Zn be n independent random variables taking values in Z. Then,
with probability at least 1 − δ, we have

|f (Z1, . . . , Zn) − E[f (Z1, . . . , Zn)]| ≤ c

(cid:114)

n log(2/δ)
2

.

820

Appendix B. Proof of Lemma 1

Proof. Given a training sample S = {zi = (xi, yi) : i ∈ 1, . . . , n} ∼ µn, we
denote the set of admissible triplets involved in the Rademacher complexity by
AS = (cid:8)i : yi = yi+(cid:98)n/3(cid:99) (cid:54)= yi+2×(cid:98)n/3(cid:99), i = 1, . . . , (cid:98)n/3(cid:99)(cid:9) ,

32

and let m = |AS| ≤ (cid:98)n/3(cid:99). We have:

Rn(F (k)) = Eσ,S∼µn

≤ Eσ,S∼µn

sup
M ∈D(k)

λ

sup
M ∈D(k)

λ

1
(cid:98)n/3(cid:99)

1
(cid:98)n/3(cid:99)

(cid:88)

i∈AS
(cid:88)

i∈AS

σi(cid:96)((cid:104)xi(xi+(cid:98)n/3(cid:99) − xi+2×(cid:98)n/3(cid:99))T, M (cid:105))

σi(cid:104)xi(xi+(cid:98)n/3(cid:99) − xi+2×(cid:98)n/3(cid:99))T, M (cid:105)

=

≤

=

≤

m
(cid:98)n/3(cid:99)

m
(cid:98)n/3(cid:99)
1
(cid:98)n/3(cid:99)
1
(cid:98)n/3(cid:99)

Eσ,S∼µn

1
m

(cid:88)

sup
M ∈D(k)
i∈AS
λ
√
2 log k
m
(cid:112)2 log k

max
u∈U

max
u∈U

(cid:107)u − ¯u(cid:107)2

(cid:107)u − ¯u(cid:107)2

√

m(cid:112)2 log k

8λBX

σi(cid:104)xi(xi+(cid:98)n/3(cid:99) − xi+2×(cid:98)n/3(cid:99))T, M (cid:105)

(B.1)

(B.2)

(B.3)

≤ 8λBX

(cid:115)

2 log k
(cid:98)n/3(cid:99)

,

(cid:80)k

where the set U = {uτ ∈ Rm : τ = 1, . . . , k, (uτ )i = (cid:104)xγ(i)(xγ(i)+(cid:98)n/3(cid:99) −
xγ(i)+2×(cid:98)n/3(cid:99))T , Bτ (cid:105), γ : {1, . . . , m} → AS is bijective, Bτ ∈ Bλ}, and ¯u =
1
τ =1 uτ . The inequality (B.1) follows from the contraction property (see
k
Shalev-Shwartz and Ben-David, 2014, Lemma 26.9). The inequality (B.2) fol-
lows from the fact M is a convex combination of set of k bases combined with
the properties in Shalev-Shwartz and Ben-David (2014, Lemma 26.7, 26.8). Fi-
nally, inequality (B.3) follows from the sparsity structure of the bases and the
fact that xi(xj − xk)T has no entries with absolute value greater than BX .

825

Appendix C. Proof of Theorem 1

Proof. Let us consider the function

Φ(S) = sup

[L(M ) − LS(M )].

M ∈D(k)

λ

830

Let S = {z1, . . . , zq−1, zq, zq+1, . . . , zn} and S(cid:48) = {z1, . . . , zq−1, z(cid:48)

q, zq+1, . . . , zn}

be two samples diﬀering by exactly one point. We have:

Φ(S(cid:48)) − Φ(S) ≤ sup

[LS(M ) − LS(cid:48)(M )]

≤

≤

M ∈D(k)

λ

1
n(n − 1)(n − 2)

sup
M ∈D(k)

λ

(cid:88)

i(cid:54)=j(cid:54)=k

|LM (zi, zj, zk) − LM (z(cid:48)

i, z(cid:48)

j, z(cid:48)

k)|

1
n(n − 1)(n − 2)

6(n − 1)(n − 2)BX BD(k)

λ

=

6
n

BX BD(k)

λ

.

33

The ﬁrst inequality comes from the fact that the diﬀerence of suprema does
not exceed the supremum of the diﬀerence. The last inequality makes use of
(11). Similarly, we can obtain Φ(S) − Φ(S(cid:48)) ≤ 6BX BD(k)
/n, thus we have
|Φ(S) − Φ(S(cid:48))| ≤ 6BX BD(k)
/n. We can therefore apply McDiarmid’s inequality
(see Lemma 4 in Appendix A) to Φ(S): for any δ > 0, with probability at least
1 − δ we have:

λ

λ

[L(M )−LS(M )] ≤ ES

sup
M ∈D(k)

λ

sup
M ∈D(k)

λ

[L(M )−LS(M )]+3BX BD(k)

λ

(cid:114)

2 ln (2/δ)
n

.

We thus need to bound ES supM ∈D(k)

λ

3 (see Appendix A) with qM (z, z(cid:48), z(cid:48)(cid:48)) = L(M ) − LM (z, z(cid:48), z(cid:48)(cid:48)) gives
[L(M ) − ¯LS(M )],

[L(M ) − LS(M )] ≤ ES

ES

(C.1)
[L(M ) − LS(M )]. Applying Lemma

sup
M ∈D(k)

λ

i=1 LM (zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99)). Let ¯S = {¯z1, ..., ¯zn}

(cid:80)(cid:98)n/3(cid:99)

where ¯LS(M ) = 1
be an i.i.d. sample independent of S. Then
[L(M ) − ¯LS(M )] = ES

ES

(cid:98)n/3(cid:99)

sup
M ∈D(k)

λ

[E ¯S

¯L ¯S(M ) − ¯LS(M )]

≤ E

S, ¯S

[ ¯L ¯S(M ) − ¯LS(M )].

sup
M ∈D(k)

λ

sup
M ∈D(k)

λ
sup
M ∈D(k)

λ

Let σ1, . . . , σ(cid:98) n

3 (cid:99) ∈ {−1, 1} be a collection of i.i.d. Rademacher variables.

By standard symmetrization techniques, we have that

E

S, ¯S

sup
M ∈D(k)

λ

[ ¯L ¯S(M ) − ¯LS(M )]

(cid:98)n/3(cid:99)
(cid:88)

= E

1
(cid:98)n/3(cid:99)

σ,S, ¯S

sup
M ∈D(k)
− LM (zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99))(cid:3)

σi

i=1

λ

(cid:2)LM ( ¯zi, ¯zi+(cid:98)n/3(cid:99), ¯zi+2×(cid:98)n/3(cid:99))

≤

1
(cid:98)n/3(cid:99)

[E

σ, ¯S

(cid:98)n/3(cid:99)
(cid:88)

i=1

sup
M ∈D(k)

λ

σiLM ( ¯zi, ¯zi+(cid:98)n/3(cid:99), ¯zi+2×(cid:98)n/3(cid:99))

+ Eσ,S

(cid:98)n/3(cid:99)
(cid:88)

i=1

sup
M ∈D(k)

λ

σiLM (zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99))]

= 2Eσ,S

1
(cid:98)n/3(cid:99)

(cid:98)n/3(cid:99)
(cid:88)

i=1

sup
M ∈D(k)

λ

We have thus shown:

σiLM (zi, zi+(cid:98)n/3(cid:99), zi+2×(cid:98)n/3(cid:99)) = 2Rn(F (k)).

ES

sup
M ∈D(k)

λ

[L(M ) − LS(M )] ≤ 2Rn(F (k)).

(C.2)

34

Plugging (C.2) into (C.1) and using Lemma 1, we get the desired result.

835

Appendix D. Proof of Corollary 1

Proof. The excess risk of M (k) with respect to M ∗ can be decomposed as
follows:

L(M (k)) − L(M ∗) = L(M (k)) − LS(M (k)) + LS(M (k)) − LS(MS)

+ LS(MS) − LS(M ∗) + LS(M ∗) − L(M ∗)

≤ L(M (k)) − LS(M (k))
(cid:123)(cid:122)
(cid:125)
generalization error

(cid:124)

+ LS(M (k)) − LS(MS)
(cid:123)(cid:122)
(cid:125)
optimization error

(cid:124)

+LS(M ∗) − L(M ∗),

(D.1)

where MS ∈ arg minM ∈Dλ

LS(M ) is an empirical risk minimizer.

The generalization error term in (D.1) can be bounded using Theorem 1 (re-
calling that M (k) ∈ D(k)
λ by construction), while the optimization error term is
bounded by the convergence rate of our Frank-Wolfe algorithm (Proposition 1).
In the last term, M ∗ does not depend on S, hence we can use Hoeﬀding’s
inequality together with (11) and (14) to obtain that for any δ > 0, with prob-
ability at least 1 − δ/2:

LS(M ∗) − L(M ∗) ≤ BX BD(k)

λ

(cid:114)

log(4/δ)
2n

.

We get the corollary by combining the above results using the union bound.

35

