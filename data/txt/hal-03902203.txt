Differential Privacy has Bounded Impact on Fairness in
Classification
Paul Mangold, Michaël Perrot, Aurélien Bellet, Marc Tommasi

To cite this version:

Paul Mangold, Michaël Perrot, Aurélien Bellet, Marc Tommasi. Differential Privacy has Bounded Im-
pact on Fairness in Classification. International Conference on Machine Learning, Jul 2023, Honolulu,
United States. ￿hal-03902203v2￿

HAL Id: hal-03902203

https://hal.science/hal-03902203v2

Submitted on 18 Sep 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Paul Mangold * 1 Micha¨el Perrot * 1 Aur´elien Bellet 1 Marc Tommasi 1

Abstract
We theoretically study the impact of differential
privacy on fairness in classiﬁcation. We prove
that, given a class of models, popular group fair-
ness measures are pointwise Lipschitz-continuous
with respect to the parameters of the model. This
result is a consequence of a more general state-
ment on accuracy conditioned on an arbitrary
event (such as membership to a sensitive group),
which may be of independent interest. We use
this Lipschitz property to prove a non-asymptotic
bound showing that, as the number of samples
increases, the fairness level of private models gets
closer to the one of their non-private counterparts.
This bound also highlights the importance of the
conﬁdence margin of a model on the disparate
impact of differential privacy.

1. Introduction

The performance of machine learning models have mainly
been evaluated in terms of utility, that is their ability to
solve speciﬁc tasks. However, they can be used in sensi-
tive contexts, and impact people’s lives. It is thus crucial
that users can trust these models. While trustworthiness
encompasses multiple concepts, fairness and privacy have
attracted a lot of interest in the past few years. Fairness
requires models not to unjustly discriminate against spe-
ciﬁc individuals or subgroups of the population, and privacy
preserves individual-level information about the training
data from being inferred from the model. These two no-
tions have been extensively studied in isolation: there exists
numerous approaches to learn fair models (Caton & Haas,
2020; Mehrabi et al., 2021), or to preserve privacy (Dwork
et al., 2014; Liu et al., 2021). However, only few works
studied the interplay between privacy and fairness. In this
paper, we take a step forward in this direction, proposing a

*Equal contribution 1Univ. Lille, Inria, CNRS, Centrale Lille,
UMR 9189 - CRIStAL, F-59000 Lille, France. Correspondence
to: Paul Mangold <paul.mangold@inria.fr>, Micha¨el Perrot
<michael.perrot@inria.fr>.

Proceedings of the 40 th International Conference on Machine
Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).

1

new theoretical bound on the relative impact of privacy on
fairness in classiﬁcation.

Fairness takes various forms (depending on the task and
context), and several deﬁnitions exist. On the one hand, the
goal may be to ensure that similar individuals are treated
similarly. This is captured by individual fairness (Dwork
et al., 2012) and counterfactual fairness (Kusner et al., 2017).
On the other hand, group fairness requires that decisions
made by machine learning models do not unjustly discrimi-
nate against subgroups of the population. In this paper, we
focus on group fairness and consider four popular deﬁni-
tions, namely Equalized Odds (Hardt et al., 2016), Equality
of Opportunity (Hardt et al., 2016), Accuracy Parity (Zafar
et al., 2017), and Demographic Parity (Calders et al., 2009).

Differential privacy (Dwork, 2006) has been widely adopted
for controlling how much information the output of an al-
gorithm may leak about its input data. It allows publishing
machine learning models while preventing an adversary
from guessing too conﬁdently the presence (or absence) of
an individual in the training data. To enforce differential
privacy, one typically releases a noisy estimate of the true
model (Dwork, 2006), so as to conceal any sensitive infor-
mation contained in individual data points. This induces
a trade-off between the strength of the protection and the
utility of the learned model. While this trade-off has been
extensively studied (Chaudhuri et al., 2011; Bassily et al.,
2014; Liu et al., 2021), its implications for fairness are not
yet well understood.

√

Contributions. In this work, we quantify the difference in
fairness levels between private and non-private models in
multi-class classiﬁcation. We derive high probability bounds
showing that this difference shrinks at a rate of (cid:101)O(
p/n),
where n is the number of data records, and p the dimension
of the model. To obtain this result, we ﬁrst prove that the
accuracy of a model conditioned on an arbitrary event (such
as membership to a sensitive group), is pointwise Lipschitz
continuous with respect to the model parameters. This prop-
erty is inherited by many popular group fairness notions,
such as Equalized Odds, Equality of Opportunity, Accuracy
Parity and Demographic Parity. Consequently, two sufﬁ-
ciently close models will have similar fairness levels. We
then upper-bound the distance between the optimal non-
private model and the private models obtained with privacy

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

preserving mechanisms like output perturbation (Chaudhuri
et al., 2011; Lowy & Razaviyayn, 2021) or DP-SGD (Song
et al., 2013; Bassily et al., 2014). These bounds hold for
strongly convex empirical risk minimization formulations,
potentially allowing explicit fairness-promoting convex reg-
ularization terms (Bechavod & Ligett, 2017; Huang & Vish-
noi, 2019; Lohaus et al., 2020; Tran et al., 2021). Combining
these two results, we derive high probability bounds on the
fairness loss due to privacy. They show that, with enough
training examples, (i) given an optimal non-private model,
enforcing privacy will not harm fairness too much, and (ii)
given a private model, the corresponding (unknown) non-
private optimal model cannot be vastly fairer. Our results
also highlight the role of the conﬁdence margin of models
in the disparate impact of differential privacy: notably, if the
non-private model has high per-group conﬁdence, then our
bound on the loss in fairness due to privacy will be smaller.
Our contributions can be summarized as follows:

• We prove that group fairness is pointwise Lipschitz,
with a smaller constant for models with large margins.

• We bound the distance between private and optimal
models, and show that the difference in their fairness
p/n).
levels decreases in (cid:101)O(

√

• We show that this bound can be computed even when
the optimal model is unknown, and numerically demon-
strate that we obtain non-trivial guarantees.

Related work. The joint study of fairness and privacy in
machine learning only goes back a few years, and has been
the focus of a recent survey (Fioretto et al., 2022). One may
identify three main research directions. First, it has been
empirically observed that privacy can exacerbate unfairness
(Bagdasaryan et al., 2019; Pujol et al., 2020; Farrand et al.,
2020; Uniyal et al., 2021; Ganev et al., 2022) and, con-
versely, that enforcing fairness can lead to more privacy
leakage for the unprivileged group (Chang & Shokri, 2020).
These empirical results suggest that some properties of the
dataset (such as group sizes and groupwise input norms)
and the choice of the private training method may affect
the extent of these disparate impacts. Unfortunately, these
observations are not supported by theoretical results, and
it is not clear why and when disparate impact occurs. Sec-
ond, a few approaches have been proposed to learn models
that are both fair and privacy preserving. However, these
works either have limited theoretical guarantees on their
performance (Kilbertus et al., 2018; Xu et al., 2019; 2020;
Tran et al., 2020), or learn stochastic models which might
not be usable in contexts where deterministic decisions are
expected (Jagielski et al., 2019; Mozannar et al., 2020). Fi-
nally, a few works have shown that fairness and privacy are
incompatible in some settings, in the sense that there exists
data distributions where enforcing one prevents the other

from being satisﬁed (Sanyal et al., 2022), or where enforcing
both implies trivial utility (Cummings et al., 2019; Agar-
wal, 2020). While appealing at ﬁrst glance, these results
usually consider unrealistic cases that are hardly encoun-
tered in practice. In this paper, we also study fairness and
privacy jointly but rather than studying whether they may
be achieved simultaneously, we investigate the relative dif-
ference in fairness level between private and non-private
models.

To the best of our knowledge, the work closest to ours is
the one of Tran et al. (2021). They analyze the impact of
privacy on fairness in Empirical Risk Minimization, where
their notion of fairness is deﬁned as the difference between
the excess risk computed on the overall population and the
excess risk computed on a subgroup of the population. They
study the expected behavior over the possible private mod-
els while our results are model-speciﬁc. In line with our
work, their results suggest that the distance to the decision
boundary plays a key role in the disparate impact of differen-
tial privacy. However, the quantity appearing in their result
is based on a second-order Taylor approximations which
is loose for popular classiﬁcation loss functions. In con-
trast, the quantity appearing in our bounds is precisely the
conﬁdence margin considered in prior work on multi-class
margin-based classiﬁcation (Cortes et al., 2013). Finally
and most importantly, loss-based fairness does not necessar-
ily imply that the actual decisions taken by the model are
fair with respect to standard group-fairness notions (Lohaus
et al., 2020). In contrast, our work provides guarantees in
terms of these widely-accepted group fairness deﬁnitions.

2. Preliminaries

S

X

D

, and a ﬁnite set

be a distribution over

In this section, we present the fairness and privacy notions
that will be used throughout the paper. We consider a multi-
class classiﬁcation setting with a feature space
, a ﬁnite
of values for the sensitive
set of labels
Y
attribute. Let
, and
X × S × Y
be a training set of n
D =
examples drawn i.i.d. from
be a space of real-
R equipped with a norm
valued functions h :
(cid:107)·(cid:107)H. For an example x
, the predicted label is the one
with the highest value, that is H(x) = arg maxy∈Y h(x, y).
In case of a tie, a random label among the most likely ones
is predicted.

(x1, s1, y1), . . . , (xn, sn, yn)
}
. Let

X × Y →
∈ X

H

D

{

The conﬁdence margin (Cortes et al., 2013) of a model h
for an example-label pair (x, y) is deﬁned as

ρ(h, x, y) = h(x, y)

max
y(cid:48)(cid:54)=y

−

h(x, y(cid:48)) .

This conﬁdence margin is positive when the example x is
classiﬁed as y by h and negative otherwise. In this paper, we
make the assumption that the margin is Lipschitz-continuous
in the model h.

2

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Assumption 2.1 (Lipschitzness of the margin). We assume
that ρ is Lipschitz-continuous in its ﬁrst argument, that is
for all h, h(cid:48)

and (x, y)

,

∈ H
ρ(h, x, y)

|

∈ X × Y
Lx,y

−

ρ(h(cid:48), x, y)

(cid:107)H ,
may depend on the example (x, y).

h
(cid:107)

| ≤

h(cid:48)

−

where Lx,y < +

∞

Note that this assumption is not very restrictive. Typically,
it holds for any class of differentiable models with either (i)
bounded gradients, or (ii) continuous gradients on a compact
parameter space (e.g., generalized linear models or smooth
deep neural networks (Hastie et al., 2009)). We stress that
Lx,y does not need to hold uniformly on the data (although
). Indeed, we will see in our
it must hold uniformly on
H
results (Section 3.2) that a large Lx,y can be compensated
for by the small probability of x, y in the data distribution.

To illustrate Assumption 2.1, consider linear models of the
form h(x, y) = W T
y x where W is a real-valued matrix
where each line is a vector Wy of label-speciﬁc parame-
W (cid:48)
h
(cid:107)2. Then, remark
ters. Deﬁne
(cid:107)
−
h(cid:48)(x, y)
h(x, y)
ρ(h, x, y)
that
+
−
|
|
−
h(cid:48)
h(x, y(cid:48))
(cid:107)H. We
h
x
maxy(cid:48)(cid:54)=y
(cid:107)2 (cid:107)
−
|
thus have Lx,y = 2

h(cid:48)
W
(cid:107)H =
(cid:107)
ρ(h(cid:48), x, y)
| ≤ |
h(cid:48)(x, y(cid:48))
2
−
| ≤
x
(cid:107)2.
(cid:107)

−

(cid:107)

A

: (

→ H

X × S × Y

The goal of a learning algorithm
is
to ﬁnd the best possible model to solve the task. In this work,
the quality of a model h is evaluated through its accuracy
Acc(h) = P (H(X) = Y ) but also its fairness level (as
deﬁned in Section 2.1). Furthermore, given a non-private
, our goal will be to compare the quality of its
algorithm
that guarantees
output to that of a private version
differential privacy.

priv of

A

A

A

)n

2.1. Fairness

S

In this paper, we focus on group fairness. These deﬁnitions
are based on the idea that a group of individuals should not
be discriminated against, compared to the overall population.
Usually, these groups are deﬁned by the sensitive attribute
. However, in some cases, it is necessary to consider
from
more ﬁne grained partitions. This is for example the case in
Equalized Odds (Hardt et al., 2016), where a model is fair
if its performance is the same on the overall population and
on subgroups of individuals that share the same sensitive
group and the same label. Thus, for the sake of generality,
we assume that the data can be partitioned into K disjoint
groups denoted by D1, . . . , Dk, . . . , DK. As in Mahesh-
wari & Perrot (2022), we consider fairness deﬁnitions that,
for each group k, can be written as:

Fk(h, D) = C 0

k +

K
(cid:88)

k(cid:48)=1

C k(cid:48)
k

P (H(X) = Y

Dk(cid:48)) ,

(1)

|

where the C k(cid:48)
k ’s are group speciﬁc values independent of
h, that typically depend on the size of the groups. In Ap-

3

pendix A, we show that usual group fairness notions such
as Demographic Parity (with binary labels) (Calders et al.,
2009), Equality of Opportunity (Hardt et al., 2016), Equal-
ized Odds (Hardt et al., 2016), and Accuracy Parity (Za-
far et al., 2017) can all be expressed in the form of (1).
By convention, we consider that Fk(h, D) > 0 when the
group k is advantaged by h compared to the overall popu-
lation, Fk(h, D) < 0 when the group is disadvantaged and
Fk(h, D) = 0 when h is fair for group k.

In some cases, rather than measuring fairness for each group
k independently, it is interesting to summarize the informa-
tion with an aggregate value. For example, we will use the
mean of the absolute fairness level of each group:

Fair(h, D) =

1
K

K
(cid:88)

|

k=1

Fk(h, D)

,

|

(2)

which is 0 when h is fair and positive when it is unfair.

2.2. Differential Privacy

We measure the privacy of machine learning models with
differential privacy (see Deﬁnition 2.2 below). Differential
privacy (DP) guarantees that the outcomes of a randomized
algorithm are similar when run on datasets that differ in
at most one data point. It effectively preserves privacy by
preventing an adversary observing the trained model from
inferring the presence of an individual in the training set. A
key property of differential privacy is that it still holds after
post-processing of the algorithm’s outcome (Dwork, 2006),
as long as this post-processing is independent of the data.
)n be two datasets of n elements.
Let D, D(cid:48)
D(cid:48)) if
We say that they are neighboring (denoted by D
they differ in at most one element.

(
X × S × Y

≈

∈

Deﬁnition 2.2 (Differential Privacy – Dwork (2006)). Let
be a randomized algorithm.
priv is ((cid:15), δ)-differentially private if, for all
)n and all subsets

priv : (
A
We say that
A
neighboring datasets D, D(cid:48)
of hypotheses

X × S × Y

X ×S ×Y

→ H

)n

∈

(

,

(cid:48)

P(

priv(D)

A

H

⊆ H
(cid:48))

∈ H

≤

exp((cid:15)) P(

priv(D(cid:48))

A

(cid:48)) + δ .

∈ H

: (

To design differentially private algorithms to estimate a
Rp, we need to quantify
)n
function
how much changing one point in a dataset can impact the
output of
. This is typically measured by (an upper bound
A
on) the (cid:96)2-sensitivity of

X × S × Y

, deﬁned as

→

A

A

(D(cid:48))

A

∆(

(D)

− A

) = sup

D≈D(cid:48) (cid:107)A

(cid:107)2 .
)n can then be
The value of
released privately using the Gaussian mechanism (Dwork
et al., 2014). Formally, to guarantee ((cid:15), δ)-differential pri-
(D), calibrated to its
vacy, we add Gaussian noise to

(
X × S × Y

on a dataset D

A

∈

A

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

sensitivity and the desired level of privacy:

priv(D) =

A

(D) +

A

N

2∆(

(cid:16)

0,

A

(cid:15)2

)2 log(1.25/δ)

(cid:17)

Ip

,

A

N

(0, σ2Ip) is a sample from the normal distribu-
where
tion with mean zero and variance σ2Ip.
In many cases
priv is computed
(e.g., when the dataset D is large),
priv is ((cid:15), δ)-
on a random subsample of D. Assuming
A
priv to a randomly selected
differentially private, applying
fraction q of D satisﬁes (O(q(cid:15)), qδ)-differential privacy,
thereby amplifying privacy guarantees (Kasiviswanathan
et al., 2011; Beimel et al., 2014). This privacy ampliﬁcation
by subsampling phenomenon, together with the Gaussian
mechanism, serve as building blocks in more complex algo-
rithms. In particular, they can be composed (Dwork et al.,
2014), allowing the design of iterative private algorithms
such as DP-SGD (Bassily et al., 2014; Abadi et al., 2016).

A

3. Pointwise Lipschitzness and Group Fairness

Here, we show that several group fairness notions are point-
wise Lipschitz with respect to the model. To this end, we ﬁrst
prove a more general result on the pointwise Lipschitzness
of accuracy conditionally on an arbitrary event.

3.1. Pointwise Lipschitzness of Conditional Accuracy

We ﬁrst relate the difference of conditional accuracy of
two models to the distance that separates them. This is
summarized in the next theorem.

Theorem 3.1 (Pointwise Lipschitzness of Conditional
Accuracy). Let
be a set of real-valued functions with
LX,Y the Lipschitz constants deﬁned in Assumption 2.1. Let
h, h(cid:48)
be two models, (X, Y, S) be a triple of random
, and E be an arbitrary event.
variables with distribution
Assume that E (LX,Y /(cid:12)

D
(cid:12)ρ(h(cid:48), X, Y )(cid:12)
(cid:12)

E) < +

, then

∈ H

H

P(H(X) = Y
|

E)
|
−
(cid:18) LX,Y

E

≤

ρ(h, X, Y )
|
|

|
P(H (cid:48)(X) = Y
(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

h

(cid:107)

−

∞
E)
|
(cid:107)H .

|
h(cid:48)

(Lip)

Proof. (Sketch) The proof of this theorem is in two steps.
First, we use the Lipschitzness of the margin (Assump-
tion 2.1), the triangle inequality, and the union bound to
P (H(X) = Y
E)
show that
| ≤
−
|
P (LX,Y /|ρ(h, X, Y )|
(cid:13)h − h(cid:48)(cid:13)
1/(cid:13)
E). Then, applying
(cid:13)H |
Markov’s inequality gives the desired result. The complete
proof can be found in Appendix B.

P (H (cid:48)(X) = Y

E)

≥

|

|

y. This implies that, when the probability (given E) that
a point has a small margin (relatively to Lx,y) is small,
E
is also small. This is notably the case

(cid:16) LX,Y

(cid:17)

|ρ(h,X,Y )|

(cid:12)
(cid:12)
(cid:12) E

for large margin classiﬁers.

Additionally, we note that data records that are unlikely do
not affect the result too much. Indeed, even if Lx,y/|ρ(h, x, y)|
is large, this can be compensated for by the small probability
of observing x, y so that the value of E
is
not signiﬁcantly affected.

(cid:16) LX,Y

(cid:12)
(cid:12)
(cid:12) E

|ρ(h,X,Y )|

(cid:17)

|

|

−

−

h(cid:48)

, if

h
(cid:107)

ρ(h, x, y)

and the distance

h(cid:48)
h
(cid:107)
ρ(h, x, y)
|

It is worth noting that the bound presented in Theorem 3.1
can be tightened (at the expense of readability) without
affecting the quantities that need to be controlled, that
is the margin
(cid:107)H.
Hence, note that given (x, y)
| ≥
∈ X × Y
(cid:107)H, then it means that h’s margin is large
Lx,y
enough to ensure that h and h(cid:48) have the same prediction
on x. The corresponding term in the expectation may then
be accounted for as zero, improving the upper bound (Re-
mark B.2). Interestingly, if all the examples are classiﬁed
with such a large margin, our bound becomes 0, further
hinting toward the importance of large margin classiﬁers.
This result may be further tightened by using a Chernoff
bound instead of Markov’s inequality (Remark B.1), yield-
P(H (cid:48)(X) = Y
βX,Y (h), with
ing
(cid:16)

P(H(X) = Y

E)
| ≤
|
− t|ρ(h,X,Y )|
LX,Y

E)
|

(cid:17)(cid:111)

(cid:110)

|

−
et

h−h(cid:48)
(cid:107)

(cid:107)H E

e

(cid:12)
(cid:12)
(cid:12)E

.

βX,Y (h) = inf
t≥0

In the subsequent theoretical developments, we use the
bound derived in Theorem 3.1 for the sake of readability. In
the numerical experiments (Section 5), we use the version
of the bound that yields the tightest results by combining
both of the aforementioned techniques.

3.2. Pointwise Lipschitzness of Group Fairness Notions

We now use Theorem 3.1’s general result to relate the fair-
ness levels of two classiﬁers, based on their distance. In
Theorem 3.2, we show that fairness notions in the form
of (1) are pointwise Lipschitz.
Theorem 3.2 (Pointwise Lipschitzness of Fairness). Let
h, h(cid:48)
, and LX,Y deﬁned as in Assumption 2.1. For any
[K],
fairness notion of the form of (1), we have, for all k

∈ H

−

Fk(h, D)
|

Fk(h(cid:48), D)
| ≤
(cid:12)
(cid:12)
with χk(h, D) = (cid:80)K
(cid:12)C k(cid:48)
(cid:12)
(cid:12)
(cid:12)
k
larly, for the aggregate measure of fairness deﬁned in (2),

χk(h, D)
(cid:107)
(cid:16) LX,Y

−
(cid:12)
(cid:12)
(cid:12) Dk(cid:48)

|ρ(h,X,Y )|

k(cid:48)=1

h(cid:48)

E

h

. Simi-

∈
(cid:107)H .
(cid:17)

P (H(X) = Y

Theorem 3.1 shows the pointwise lipschitzness of the func-
E) and underlines the impor-
tion h
tance of the conﬁdence margin ρ(h, x, y) of the model h.
Note that Lx,y/|ρ(h, x, y)| is small when the model h is conﬁ-
dent (relatively to Lx,y) in its prediction for the true label

(cid:55)→

|

Fair(h, D)

|

−

Fair(h(cid:48), D)

1
K

K
(cid:88)

k=1

| ≤

χk(h, D)

h

(cid:107)

−

h(cid:48)

(cid:107)H .

Proof. (Sketch) To prove the ﬁrst claim, we use the triangle
inequality to show that, for each group, the absolute differ-

4

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

ence in fairness is bounded by a combination of absolute
differences between conditional probabilities. We can then
apply Theorem 3.1. The second claim follows by applying
the ﬁrst one to each group independently. The complete
proof is provided in Appendix C.

Theorem 3.2 implies that classiﬁers that are sufﬁciently
close have similar fairness levels. This has two major con-
sequences when studying a given model. On the one hand,
we have an upper bound on the harm that can be done to
fairness: small variations of the model cannot make it much
more unfair. On the other hand, we have a lower bound on
the distance needed to make a model fair: making the model
signiﬁcantly more fair requires to substantially alter it. In
the next corollary, we instantiate Theorem 3.2 for various
popular group fairness notions, and for accuracy.
Corollary 3.3. Let h, h(cid:48)
, and LX,Y deﬁned as in
∈ H
Assumption 2.1. The difference in fairness or accuracy
between h and h(cid:48) can be bounded as follows.

Equalized Odds (Hardt et al., 2016): the data is divided
into K =
,

groups such that for all (y, r)

∈ Y × S

|Y × S|

χ(y,r)(h, D) = E

(cid:16) LX,Y

|ρ(h,X,Y )|
(cid:16) LX,Y

+ E

|ρ(h,X,Y )|

(cid:17)

(cid:12)
(cid:12)
(cid:12) Y = y
(cid:12)
(cid:12)
(cid:12) Y = y, S = r

(cid:17)

.

Equality of Opportunity (Hardt et al., 2016): we let
⊆ Y
the set of desirable outcomes. The data is divided into
K =

such that for all (y, r)

Y

,

(cid:48)

|Y × S|
χ(y,r)(h, D) = E

(cid:16) LX,Y

|ρ(h,X,Y )|

+ E

(cid:16) LX,Y

|ρ(h,X,Y )|

∈ Y × S
(cid:12)
(cid:12)
(cid:12) Y = y, S = r
(cid:17)

(cid:12)
(cid:12)
(cid:12) Y = y

(cid:17)

,

if y is a desired outcome, and χ(y,r)(h, D) = 0 otherwise.

∈

Corollary 3.3 shows that our results are applicable to several
group fairness notions, but also to accuracy. Importantly,
the pointwise Lipschitz constant χk(h, D) depends both on
the group k
[K], and on the considered fairness notion.
This sheds light on the fact that comparing the fairness
levels of two models requires special attention, as there
may be important disparities depending on the considered
sensitive group or fairness notion. We will use this result in
Section 4, where we bound the disparate impact of privacy
by bounding the difference in fairness levels between private
and non-private models.

it

D

In practice,

Finite sample analysis.
is often as-
sumed that one does not have access to the true
but rather to a ﬁnite sample D =
distribution
of size n. An empirical esti-
(x1, s1, y1), . . . , (xn, sn, yn)
{
}
mate of the expectation from a ﬁnite sample is then deﬁned
as (cid:98)E (f (X)) = 1
i=1 f (xi). The results presented in
n
Theorem 3.1, Theorem 3.2, and Corollary 3.3 also hold in
this ﬁnite sample setting. For instance, denoting by (cid:98)χk the
empirical version of χk, we have that

[K],

(cid:80)n

k

(cid:12)
(cid:12)
(cid:12) (cid:98)Fk(h, D)

−

∀
(cid:12)
(cid:98)Fk(h(cid:48), D)
(cid:12)
(cid:12) ≤ (cid:98)χk(h, D)

∈

h
(cid:107)

−

h(cid:48)

(cid:107)H .

One may then wonder whether it is possible to connect the
true fairness of a model h to the empirical fairness of a
(cid:12)
(cid:12)
second model h(cid:48), that is bound
(cid:98)Fk(h(cid:48), D)
(cid:12). In
the next lemma, we show that such bound can indeed be
obtained when h and h(cid:48) were learned on D.

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

δ

)

examples drawn i.i.d.

Lemma 3.4. Let D be a ﬁnite sample of n
8 log( 2K+1
mink(cid:48) pk(cid:48)

≥
, where pk(cid:48)
is the true proportion of examples from group k(cid:48). As-
(cid:12)
(cid:12)C k(cid:48)
sume that PD∼Dn
≤
B3 exp (cid:0)
be an hypothesis space and
dH be the Natarajan dimension of
. With probability at
least 1

(cid:0) (cid:80)K
Cn(cid:1). Let

δ over the choice of D,

(cid:12)
(cid:12) > αC

B4α2

k −

(cid:98)C k(cid:48)

from

k(cid:48)=0

H

−

D

(cid:1)

k

H
h, h(cid:48)
∀
(cid:12)
(cid:98)Fk(h(cid:48), D)
(cid:12)
(cid:12) ≤ (cid:98)χk(h, D)
(cid:32) K
(cid:12)
(cid:12)
(cid:88)
(cid:12) (cid:98)C k(cid:48)
(cid:12)
(cid:12)
(cid:12)

+ (cid:101)O

(cid:115)

k

k(cid:48)=1

∈ H

h(cid:48)

h
(cid:107)

(cid:107)H

−
dH + log (K/δ)
npk(cid:48)

(cid:33)

.

Accuracy Parity (Zafar et al., 2017): the data is divided
into K =

groups such that for all r

,

|S|

χ(r)(h, D) = E

(cid:16) LX,Y

(cid:17)

|ρ(h,X,Y )|

+ E

∈ S
(cid:16) LX,Y

|ρ(h,X,Y )|

(cid:17)

(cid:12)
(cid:12)
(cid:12) S = r

.

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

−

Demographic Parity (Binary Labels) (Calders et al., 2009):
groups such that for
the data is divided into K =
all (y, r)

|Y × S|

,

∈ Y × S
χ(y,r)(h, D) = E

(cid:16) LX,Y

|ρ(h,X,Y )|

(cid:17)

+E

(cid:16) LX,Y

|ρ(h,X,Y )|

(cid:17)

(cid:12)
(cid:12)
(cid:12) S = r

.

Proof. (Sketch) This lemma follows from bounding the two
terms in the following inequality:

Accuracy: the data is in a single group, such that

χ(h, D) = E

(cid:16) LX,Y

(cid:17)

|ρ(h,X,Y )|

.

(cid:12)
(cid:12)Fk(h(cid:48), D)
(cid:12)

(cid:12)
(cid:12)
(cid:98)Fk(h, D)
(cid:12) ≤

−

(cid:12)
(cid:12) (cid:98)Fk(h(cid:48), D)
(cid:12)
+

−
(cid:12)
(cid:12)Fk(h(cid:48), D)
(cid:12)

(cid:12)
(cid:12)
(cid:98)Fk(h, D)
(cid:12)

(cid:12)
(cid:98)Fk(h(cid:48), D)
(cid:12)
(cid:12)

.

−

Proof. This corollary follows from Theorem 3.2 by replac-
ing the C k(cid:48)
k ’s by their appropriate values (depending on the
considered notion). See Appendix A for more details.

The ﬁrst term can be bounded using the empirical coun-
terpart of Theorem 3.2. The second term is then bounded
with high probability using standard uniform convergence

5

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

bounds (Shalev-Shwartz & Ben-David, 2014). The com-
plete proof can be found in Appendix D where the result is
also extended to the simpler case where h and h(cid:48) are ﬁxed
rather than learned on D.

4. Bounding the Relative Fairness of Private

Models

In this section, we quantify the difference of fairness be-
tween a private model and its non-private counterpart. Let
R be a loss function. Assume (cid:96) is
(cid:96) :
Λ-Lipschitz, and µ-strongly-convex with respect to its ﬁrst
variable. Assume the norm
is
convex. We deﬁne the optimal model h∗

(cid:107)·(cid:107)H is Euclidean, and that

H × X × S × Y →

H

as

h∗ = arg min

f (h) =

h∈H

1
n

n
(cid:88)

i=1

∈ H

(cid:96)(h; xi, si, yi) .

(3)

Note that, since f is strongly-convex, h∗ is unique, and is
thus a natural choice for the non-private reference model.
This strong convexity assumption could be relaxed: this
would require to deﬁne another reference model (e.g., the
output of non-private SGD) and to bound the distance be-
tween this reference model and the private model.

Two mechanisms are commonly used to ﬁnd a differen-
tially private approximation hpriv of h∗: output perturbation
(Chaudhuri et al., 2011; Lowy & Razaviyayn, 2021), and
DP-SGD (Bassily et al., 2014; Abadi et al., 2016). For
both mechanisms, the distance (cid:13)
(cid:13)H can be upper
bounded with high probability. In this section, we recall
these two mechanisms and the corresponding high proba-
bility upper bounds. We then plug these bounds in Theo-
rem 3.2 to bound the fairness level of the private solution
hpriv relatively to the one of the true solution h∗.

(cid:13)hpriv

h∗(cid:13)

−

4.1. Bounding the Distance between Private and

Optimal Classiﬁers

Output perturbation. Output perturbation computes the
non-private solution h∗ of (3), and releases a private esti-
mate by the Gaussian mechanism:

H

(σ2Ip)) ,

hpriv = πH(h∗ +
N
where πH is the projection on
. Let ∆ be the sensitivity
of the function D
arg minw∈H f (w; D). In our setting,
(cid:55)→
we have ∆ = 2Λ/µn. Then, given 0 < (cid:15), δ < 1, hpriv is
((cid:15), δ)-differentially private as long as σ2
2∆2 log(1.25/δ)/(cid:15)2.
We bound the distance between hpriv and h∗ with high
probability in Lemma 4.1.
Lemma 4.1. Let hpriv be the vector released by output
perturbation with noise σ2 = 8Λ2 log(1.25/δ)/µ2n2(cid:15)2, and
0 < ζ < 1, then with probability at least 1

≥

ζ,

(cid:13)
(cid:13)hpriv

h∗(cid:13)
2
(cid:13)
2 ≤

−

32pΛ2 log(1.25/δ) log(2/ζ)
µ2n2(cid:15)2

−

DP-SGD. DP-SGD starts from some h0
it using stochastic gradients. That is, with γ > 0, i

∈ H

and updates

(0, σ2Ip), we iteratively update

∼

([n]), and ηt

U

∼ N

ht+1 = πH(ht

γ(

∇

−

(cid:96)(ht; xi, yi) + ηt)) .

After T > 0 iterations, we release hpriv = hT . Given
0 < (cid:15), δ < 1, hpriv is ((cid:15), δ)-differentially private when
σ2
64Λ2T 2 log(3T /δ) log(2/δ)/n2(cid:15)2. We bound the distance
between hpriv and h∗ with high probability in Lemma 4.2.

≥

Lemma 4.2. Let hpriv be the vector released by DP-SGD
with σ2 = 64Λ2T 2 log(3T /δ) log(2/δ)/n2(cid:15)2. Assume that the
loss function is smooth in its ﬁrst parameter, and that σ2
∗ =
2
Ei∼[n] (cid:107)∇
σ2. Let 0 < ζ < 1, then with
(cid:96)(h∗; xi, yi)
(cid:107)
probability at least 1
−

≤

ζ,

(cid:13)
(cid:13)hpriv

−

h∗(cid:13)
2
2 = (cid:101)O
(cid:13)

(cid:18) pΛ2 log(1/δ)2
ζµ2n2(cid:15)2

(cid:19)

,

where (cid:101)O ignores logarithmic terms in n (the number of
examples) and p (the number of model parameters).

We prove Lemmas 4.1 and 4.2 in Appendices E and F. These
lemmas give upper bounds on the distance between the opti-
mal model and the private models learned by output pertur-
bation and DP-SGD. This distance decreases as the number
of records n or the privacy budget (cid:15) increase. Conversely, it
increases with the complexity of the model (i.e., with larger
dimension p), and with the Lipschitz constant of the loss Λ.

Remark 4.3. For clarity of exposition in Lemma 4.2, we did
not use minimal assumptions and used the simplest variant
of DP-SGD. Notably, the assumption on σ∗ can be removed
by using variance reduction schemes, and tighter bounds
on σ can also be obtained using R´enyi Differential Privacy
(Mironov, 2017). Similarly, the assumption (cid:15) < 1 is only
used to give simple closed-form bounds. Strong convexity
and smoothness assumptions can be relaxed as well.

4.2. Bounding the Fairness of Private Models

We now state our central result (Theorem 4.4), where we
bound the fairness of hpriv relatively to the one of h∗.

Theorem 4.4. Let h∗ be the solution of (3), and hpriv its
private estimate obtained by output perturbation. Let href
∈
hpriv, h∗
, and 0 < ζ < 1. Then, the difference of fairness
}
{
ζ,
of group k
∈

[K] satisﬁes, with probability at least 1

−

(cid:12)
(cid:12)Fk(hpriv, D)

Fk(h∗, D)(cid:12)
(cid:12)
(cid:112)

−

χk(href , D)LΛ

32p log(1.25/δ) log(2/ζ)

.

.

≤

µn(cid:15)

6

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Table 1. Upper bound, with 99% probability, on the difference of fairness between private and non-private models for different fairness
measures and accuracy. Privacy parameters are (cid:15) = 1 and δ = 1/n2 where n is the number of samples in the training data.

Dataset

Equality of Opportunity

Equalized Odds Demographic Parity Accuracy Parity Accuracy

celebA (n = 182, 339)
folktables (n = 1, 498, 050)

0.1044
0.0017

0.0975
0.0026

0.0975
0.0026

0.0975
0.0026

0.0487
0.0013

Similarly, if hpriv is estimated through DP-SGD, we have
that, with probability at least 1

ζ,
−
Fk(h∗, D)(cid:12)
(cid:12)

(cid:12)
(cid:12)Fk(hpriv, D)

(cid:32)

(cid:101)O

≤

−

χk(href , D)LΛ

(cid:112)p log(1/δ)

(cid:33)

,

√ζµn(cid:15)

where (cid:101)O ignores logarithmic terms in n (the number of
examples) and p (the number of model parameters).

Proof. By Lemma 4.1 or Lemma 4.2, we control the dis-
tance (cid:13)
(cid:13)hpriv
(cid:13). Plugging this bound in Theorem 3.2
−
gives the result.

h∗(cid:13)

√

This result shows that, when learning a private model, the
unfairness due to privacy vanishes at a (cid:101)O(
p/n) rate. To the
best of our knowledge, our result is the ﬁrst to quantify this
rate. Importantly, it highlights the role of the conﬁdence mar-
gin of the classiﬁer on the disparate impact of differential
privacy. This is in line with previous empirical and theo-
retical work that identiﬁed the groupwise distances to the
decision boundary as an important factor (Tran et al., 2021;
Fioretto et al., 2022). However, our bounds are the ﬁrst to
quantify this impact through a classic notion of conﬁdence
margin studied in learning theory (Cortes et al., 2013).

Our result may be interpreted and used in various ways. A
ﬁrst example is the case where the private model is known
but its optimal non-private counterpart is not. There, our
result guarantees that, given enough examples, the fairness
level of the private model is close to the one of the opti-
mal non-private model. This allows the practitioner to give
guarantees on the model, that the end user can trust. A sec-
ond example is the case where the true model h∗ is owned
by someone who cannot share it, due to privacy concerns.
Imagine that the model needs to be audited for fairness.
Then, the model owner can compute a private estimate of
their model, and send it to the (honest but curious) auditing
company. The bound allows to obtain fairness bounds for
the true model from the inspection of the private one, and
thus acts as a certiﬁcate of correctness of the audit done on
the private version of the model.

Remark 4.5. The fairness guarantee for the private model
given by Theorem 4.4 is relative to the fairness of the opti-
mal model h∗, which may itself be quite unfair. A standard

7

∈ H

approach to promote fair models is to use convex relaxations
of fairness as regularizers to the ERM problem (Bechavod &
Ligett, 2017; Huang & Vishnoi, 2019; Lohaus et al., 2020).
Interestingly, to be able to use output perturbation, we only
require the objective function of (3) to be strongly convex
, which is the case for these relax-
and Lipschitz over h
ations when they are combined with a squared (cid:96)2-norm. For
binary classiﬁcation with two sensitive groups, Lohaus et al.
(2020) proved that, with a proper choice of regularization
parameters, this approach can yield a fair h∗ (see their The-
orem 1 for more details). Combined with our results, this
paves the way for the design of algorithms that learn prov-
ably private and fair classiﬁers. However, several crucial
challenges remain to make this approach work in practice,
such as (i) ﬁnding the appropriate regularization parame-
ters privately, and (ii) providing guarantees on the resulting
classiﬁers’ accuracy. We leave this for future work.

5. Numerical Experiments

In this section, we numerically illustrate the upper bounds
from Section 4.2. We use the celebA (Liu et al., 2015)
and folktables (Ding et al., 2021) datasets, which re-
spectively contain 202, 599 and 1, 664, 500 samples, with
39 and 10 features (including one sensitive attribute, sex,
that is not not used for prediction), and binary labels. For
each dataset, we use 90% of the records for training, and the
remaining 10% for empirical evaluation of the bounds. We
train (cid:96)2-regularized logistic regression models, ensuring that
the underlying optimization problem is 1-strongly-convex.
This allows learning private models by output perturbation,
for which the bound from Theorem 4.4 holds. 1

In Section 5.1, we show that we obtain non-trivial guaran-
tees on the private model’s fairness and accuracy. Then, we
study the inﬂuence of the number of training samples and of
the privacy budget (cid:15) in Section 5.2, and discuss the tightness
of our result in Section 5.3.

5.1. Value of the Upper Bounds

In Table 1, we compute the value of Theorem 4.4’s bounds.
We learn a non-private (cid:96)2-regularized logistic regression
model, and use it to compute the bounds (averaged over the

1The code is available at https://github.com/

pmangold/fairness-privacy.

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

(a) Equal. Opp. (celebA)

(b) Accuracy (celebA)

(c) Equal. Opp. (folktables)

(d) Accuracy (folktables)

(e) Equal. Opp. (celebA)

(f) Accuracy (celebA)

(g) Equal. Opp. (folktables)

(h) Accuracy (folktables)

Figure 1. Equality of opportunity (Equal. Opp.) and Accuracy levels for optimal non-private model and random private ones as a function
of the number of training records n (ﬁrst line, with (cid:15) = 1 and δ = 1/n2) and of the privacy budget (cid:15) (second line, using all available
training records). For each value of n and (cid:15), we sample 100 private models and take their minimum and maximum fairness/accuracy
values to mark the area of attainable values. The solid blue line gives the theoretical guarantees from Theorem 4.4, while the dashed and
dotted line give ﬁner bounds when more information is available (see Section 5.3 for details).

two groups) for multiple fairness and accuracy measures
on two datasets. In all cases, our results give non-trivial
guarantees on the difference of fairness: it is bounded by
at most 0.105 for celebA and 0.0026 for folktables.
This means that any (1, 1/n2)-DP model learned by output
perturbation will, with high probability, achieve a fairness
level within this margin of that of the non-private model.

5.2. Inﬂuence of the Number of Training Samples and

Privacy Budget

We now verify numerically the rate at which fairness and
accuracy levels decrease when increasing the number of
training records or privacy budget. In Figure 1, we plot the
optimal model’s equality of opportunity and accuracy, as a
function of (i) in the ﬁrst line, the number of samples n used
for training, or (ii) in the second line, the privacy budget (cid:15)
(see Appendix G for results with other fairness measures).
For each value of n and (cid:15), we plot Theorem 4.4’s theoretical
guarantees (solid blue line). With (cid:15) = 1, our bounds give
meaningful guarantees for n
100, 000 records on both
celebA and folktables datasets (Figures 1a to 1d).
When using all records, we obtain meaningful bounds for
0.1 for folktables (Fig-
(cid:15)
≥
ures 1e to 1h). Additionally, note that we obtain both upper
and lower bounds on fairness and accuracy, conﬁrming re-
marks from Section 3.2.

1 for celebA and (cid:15)

≥

≥

We also report the fairness and accuracy levels of 100 pri-
vate models computed by output perturbation (in green in
Figure 1). As predicted by our theory, their fairness and ac-
curacy converges towards the ones of their non-private coun-
terparts as n and (cid:15) increase. Interestingly, our bounds seem
to follow the same tendency as what we observe empirically
(albeit with a larger multiplicative constant), suggesting that
they capture the correct dependence in n and (cid:15). We further
discuss the tightness of our results in next section.

5.3. Tightness of the Bound

−

h∗(cid:13)

(cid:13)hpriv

We now argue that the two major factors of looseness in
our results are (i) the upper bound on (cid:13)
(cid:13) and (ii)
the looseness of Assumption 2.1. While these cannot be
improved in general, speciﬁc knowledge of hpriv and h∗
(that is typically not available due to privacy) can lead to
tighter bounds. First, when the distance (cid:13)
(cid:13) is
known, we can use its actual value rather than the upper
bounds of Section 4.1 (see dashed blue line in Figure 1).
Second, when both hpriv and h∗ are known, Assumption 2.1
can be substantially reﬁned (see details in Appendix G.3).
We evaluate this bound for the private model that is the
farthest away from the non-private one (see dotted blue line
in Figure 1). The resulting bound appears to be tight up
to a small multiplicative constant. These two observations
suggest that our bounds cannot be signiﬁcantly tightened,

(cid:13)hpriv

h∗(cid:13)

−

8

TheoreticalUpperBoundBoundwithEmpiricalDistanceImprovedBoundKnowingbothModelsTheoreticalUpperBoundBoundwithEmpiricalDistanceImprovedBoundKnowingbothModelsNon-privateModelFairnessPrivateModelsFairness102104Numberoftrainingsamples0.00.10.20.30.4Equal.Opp.102104Numberoftrainingsamples0.40.60.81.0Accuracy102104106Numberoftrainingsamples0.000.020.040.060.080.10Equal.Opp.102104106Numberoftrainingsamples0.660.680.700.720.74Accuracy10−2100Valueof(cid:15)0.000.050.100.15Equal.Opp.10−2100Valueof(cid:15)0.750.800.850.90Accuracy10−2100Valueof(cid:15)0.000.010.020.030.04Equal.Opp.10−2100Valueof(cid:15)0.700.720.74AccuracyDifferential Privacy has Bounded Impact on Fairness in Classiﬁcation

unless one can obtain such knowledge through either private
computation or additional assumptions on the data.

6. Conclusion

√

In this work, we proved that the fairness (and accuracy) costs
induced by privacy in differentially private classiﬁcation
vanishes at a (cid:101)O(
p/n) rate, where n is the number of training
records, and p the number of parameters. This rate follows
from a general statement on the regularity of a family of
group fairness measures, that we prove to be pointwise
Lipschitz with respect to the model. The pointwise Lipschitz
constant explicitly depends on the conﬁdence margin of the
model, and may be different for each sensitive group. We
also show it can be computed from a ﬁnite data sample.
Importantly, our bounds do not require the knowledge of
the optimal (non-private) model:
they can thus be used
in practical privacy-preserving scenarios. We numerically
evaluate our bounds on real datasets, and highlight practical
settings where non-trivial guarantees can be obtained.

While we illustrated our results for output perturbation and
DP-SGD on strongly-convex problems, we stress that they
are more general. Indeed, our Theorem 3.2 holds for any
pair of models. Consequently, it would apply to DP-SGD
on non-convex problems, provided that we have a bound
on the distance between the models obtained with and with-
out privacy. Deriving a tight bound on this distance is a
challenging problem, that constitutes an interesting future
direction.

Finally, we stress that our results do not provide fairness
guarantees per se, but only bound the difference of fairness
between models. It is nonetheless a ﬁrst step towards a
more complete understanding of the interplay between pri-
vacy, fairness, and accuracy. We believe that our results can
guide the design of fairer privacy-preserving machine learn-
ing algorithms. A ﬁrst promising direction in this regard
is to combine our bounds with fairness-promoting convex
regularizers, as discussed in Remark 4.5. Another direc-
tion is the design of methods to privately learn models with
large-margin guarantees, as recently considered by Bassily
et al. (2022). Our results, which explicitly depend on the
conﬁdence margin of the model, suggest that better fairness
guarantees could be obtained for these methods.

Acknowledgements

This work was supported by the Inria Exploratory Ac-
tion FLAMED, by the R´egion Hauts de France (Projet
STaRS Equit´e en apprentissage d´ecentralis´e respectueux de
la vie priv´ee), and by the French National Research Agency
(ANR) through the grants ANR-20-CE23-0015 (Project
PRIDE), and ANR 22-PECY-0002 IPOP (Interdisciplinary
Project on Privacy) project of the Cybersecurity PEPR.

References

Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B.,
Mironov, I., Talwar, K., and Zhang, L. Deep Learn-
ing with Differential Privacy. In Proceedings of the 2016
ACM SIGSAC Conference on Computer and Communi-
cations Security, CCS ’16, pp. 308–318, New York, NY,
USA, October 2016. Association for Computing Machin-
ery. 1130 citations (Crossref) [2022-08-19].

Agarwal, S. Trade-offs between fairness and privacy in

machine learning. 2020.

Bagdasaryan, E., Poursaeed, O., and Shmatikov, V. Differ-
ential privacy has disparate impact on model accuracy. In
Advances in Neural Information Processing Systems 32,
2019, Vancouver, BC, Canada, pp. 15453–15462, 2019.

Bassily, R., Smith, A., and Thakurta, A. Private Empirical
Risk Minimization: Efﬁcient Algorithms and Tight Error
Bounds. In 2014 IEEE 55th Annual Symposium on Foun-
dations of Computer Science, pp. 464–473, Philadelphia,
PA, USA, October 2014. IEEE.

Bassily, R., Mohri, M., and Suresh, A. T. Differentially
private learning with margin guarantees. arXiv preprint
arXiv:2204.10376, 2022.

Bechavod, Y. and Ligett, K. Penalizing unfairness in binary
classiﬁcation. arXiv preprint arXiv:1707.00044, 2017.

Beimel, A., Brenner, H., Kasiviswanathan, S. P., and Nissim,
K. Bounds on the sample complexity for private learning
and private data release. Machine Learning, pp. 401–437,
March 2014.

Calders, T., Kamiran, F., and Pechenizkiy, M. Building
classiﬁers with independency constraints. In 2009 IEEE
International Conference on Data Mining Workshops, pp.
13–18. IEEE, 2009.

Caton, S. and Haas, C. Fairness in machine learning: A

survey. arXiv preprint arXiv:2010.04053, 2020.

Chang, H. and Shokri, R. On the privacy risks of algorithmic

fairness. arXiv preprint arXiv:2011.03731, 2020.

Chaudhuri, K., Monteleoni, C., and Sarwate, A. D. Differ-
entially Private Empirical Risk Minimization. Journal
of Machine Learning Research, 12(29):1069–1109, 2011.
ISSN 1533-7928.

Cortes, C., Mohri, M., and Rostamizadeh, A. Multi-class
classiﬁcation with maximum margin multiple kernel. In
International Conference on Machine Learning, pp. 46–
54. PMLR, 2013.

9

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Cummings, R., Gupta, V., Kimpara, D., and Morgenstern, J.
On the compatibility of privacy and fairness. In Adjunct
Publication of the 27th Conference on User Modeling,
Adaptation and Personalization, pp. 309–315, 2019.

Ding, F., Hardt, M., Miller, J., and Schmidt, L. Retiring
adult: New datasets for fair machine learning. Advances
in Neural Information Processing Systems, 34, 2021.

Dwork, C. Differential privacy. In Encyclopedia of Cryp-

tography and Security, 2006.

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,
R. Fairness through awareness. In Proceedings of the 3rd
innovations in theoretical computer science conference,
pp. 214–226, 2012.

Dwork, C., Roth, A., et al. The algorithmic foundations of
differential privacy. Foundations and Trends in Theoreti-
cal Computer Science, 9(3-4):211–407, 2014.

Farrand, T., Mireshghallah, F., Singh, S., and Trask, A.
Neither private nor fair: Impact of data imbalance on
utility and fairness in differential privacy. arXiv preprint
arXiv:2009.06389, 2020.

Fioretto, F., Tran, C., Hentenryck, P. V., and Zhu, K. Differ-
ential privacy and fairness in decisions and learning tasks:
A survey. In International Joint Conference on Artiﬁcial
Intelligence (IJCAI), pp. 5470–5477, 2022.

Ganev, G., Oprisanu, B., and Cristofaro, E. D. Robin hood
and matthew effects: Differential privacy has disparate
impact on synthetic data. In ICML, 2022.

Hardt, M., Price, E., and Srebro, N. Equality of opportunity
in supervised learning. Advances in neural information
processing systems, 29:3315–3323, 2016.

Hastie, T., Tibshirani, R., and Friedman, J. The Elements
of Statistical Learning. Springer Series in Statistics.
Springer, New York, NY, 2009. ISBN 978-0-387-84857-
0 978-0-387-84858-7. doi: 10.1007/978-0-387-84858-7.
URL http://link.springer.com/10.1007/
978-0-387-84858-7.

Huang, L. and Vishnoi, N. Stable and fair classiﬁcation.
In International Conference on Machine Learning, pp.
2879–2890. PMLR, 2019.

Jagielski, M., Kearns, M., Mao, J., Oprea, A., Roth, A.,
Shariﬁ-Malvajerdi, S., and Ullman, J. Differentially pri-
vate fair learning. In International Conference on Ma-
chine Learning, pp. 3000–3008. PMLR, 2019.

Kasiviswanathan, S. P., Lee, H. K., Nissim, K., Raskhod-
nikova, S., and Smith, A. What Can We Learn Privately?
SIAM Journal on Computing, pp. 793–826, January 2011.

Kiefer, J. Sequential minimax search for a maximum. Pro-
ceedings of the American mathematical society, 4(3):
502–506, 1953.

Kilbertus, N., Gasc´on, A., Kusner, M., Veale, M., Gummadi,
K., and Weller, A. Blind justice: Fairness with encrypted
sensitive attributes. In International Conference on Ma-
chine Learning, pp. 2630–2639. PMLR, 2018.

Kusner, M. J., Loftus, J., Russell, C., and Silva, R. Counter-
factual fairness. Advances in neural information process-
ing systems, 30, 2017.

Liu, B., Ding, M., Shaham, S., Rahayu, W., Farokhi, F., and
Lin, Z. When machine learning meets privacy: A survey
and outlook. ACM Computing Surveys (CSUR), 54(2):
1–36, 2021.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face
attributes in the wild. In Proceedings of International
Conference on Computer Vision (ICCV), December 2015.

Lohaus, M., Perrot, M., and Von Luxburg, U. Too relaxed to
be fair. In International Conference on Machine Learning,
pp. 6360–6369. PMLR, 2020.

Lowy, A. and Razaviyayn, M. Output perturbation for differ-
entially private convex optimization with improved popu-
lation loss bounds, runtimes and applications to private
adversarial training. arXiv preprint arXiv:2102.04704,
2021.

Maheshwari, G. and Perrot, M. Fairgrad: Fairness aware
gradient descent. arXiv preprint arXiv:2206.10923, 2022.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and
Galstyan, A. A survey on bias and fairness in machine
learning. ACM Computing Surveys (CSUR), 54(6):1–35,
2021.

Mironov, I. Renyi Differential Privacy. 2017 IEEE 30th
Computer Security Foundations Symposium (CSF), Au-
gust 2017.

Mozannar, H., Ohannessian, M., and Srebro, N. Fair learn-
ing with private demographic data. In International Con-
ference on Machine Learning, pp. 7066–7075. PMLR,
2020.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Pujol, D., McKenna, R., Kuppam, S., Hay, M., Machanava-
jjhala, A., and Miklau, G. Fair decision making using

10

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

privacy-protected data. In Proceedings of the 2020 Con-
ference on Fairness, Accountability, and Transparency,
FAT* ’20, pp. 189–199, New York, NY, USA, 2020. Asso-
ciation for Computing Machinery. ISBN 9781450369367.

Sanyal, A., Hu, Y., and Yang, F. How unfair is private

learning? arXiv preprint arXiv:2206.03985, 2022.

Shalev-Shwartz, S. and Ben-David, S. Understanding ma-
chine learning: From theory to algorithms. Cambridge
university press, 2014.

Song, S., Chaudhuri, K., and Sarwate, A. D. Stochastic
gradient descent with differentially private updates. In
2013 IEEE Global Conference on Signal and Information
Processing, pp. 245–248, Austin, TX, USA, December
2013. IEEE. 145 citations (Crossref) [2022-08-19].

Tran, C., Fioretto, F., and Van Hentenryck, P. Differen-
tially private and fair deep learning: A lagrangian dual
approach. arXiv preprint arXiv:2009.12562, 2020.

Tran, C., Dinh, M., and Fioretto, F. Differentially private
empirical risk minimization under the fairness lens. Ad-
vances in Neural Information Processing Systems, 34:
27555–27565, 2021.

Uniyal, A., Naidu, R., Kotti, S., Singh, S., Kenfack, P. J.,
Mireshghallah, F., and Trask, A. Dp-sgd vs pate: Which
arXiv
has less disparate impact on model accuracy?
preprint arXiv:2106.12576, 2021.

Woodworth, B., Gunasekar, S., Ohannessian, M. I., and
Srebro, N. Learning non-discriminatory predictors. In
Conference on Learning Theory, pp. 1920–1953. PMLR,
2017.

Xu, D., Yuan, S., and Wu, X. Achieving differential privacy
and fairness in logistic regression. In Companion Pro-
ceedings of The 2019 World Wide Web Conference, pp.
594–599, 2019.

Xu, D., Du, W., and Wu, X. Removing disparate impact of
differentially private stochastic gradient descent on model
accuracy. arXiv preprint arXiv:2003.03699, 2020.

Zafar, M. B., Valera, I., Gomez Rodriguez, M., and Gum-
madi, K. P. Fairness beyond disparate treatment & dis-
parate impact: Learning classiﬁcation without disparate
mistreatment. In Proceedings of the 26th international
conference on world wide web, pp. 1171–1180, 2017.

11

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

This appendix provides several examples of group fairness functions compatible with our framework (Appendix A), the
proofs of the main theoretical results that were omitted in the main paper for the sake of readability (Appendices B to F),
and additional experiments (Appendix G).

A. Fairness functions

In this section we recall several well known fairness functions and show that they can be written in the form of Equation (1).
Example 1 (Equalized Odds (Hardt et al., 2016)). A model h is fair for Equalized Odds when the probability of predicting
the correct label is independent of the sensitive attribute, that is,

(y, r)

F(y,r)(h, D) = P (H(X) = Y

Y = y, S = r)

|

We can then write F(y,r)(h, D) in the form of Equation (1) as

−

∀

∈ Y × S
P (H(X) = Y

Y = y) .

|

F(y,r)(h, D) = C 0

(y,r) +

(cid:88)

C (y(cid:48),r(cid:48))

(y,r)

P (H(x) = Y

(y(cid:48),r(cid:48))∈Y×S

Y = y(cid:48), S = r(cid:48))

|

(4)

with

(y,r) = 0

(y,r) = 1

C 0
C (y,r)
= r, C (y,r(cid:48))
, C (y(cid:48),r(cid:48))

(y,r) =

−
(y,r) = 0

r(cid:48)
∀
r(cid:48)
∀

∈ S

P (S = r

−
P (S = r(cid:48)

Y = y)

Y = y)

|

|

y(cid:48)
∀

= y,

Proof. We have that

F(y,r)(h, D) = P (H(X) = Y
= P (H(X) = Y

Y = y, S = r)

Y = y, S = r)

|

|

−

−

which gives the result.

P (H(X) = Y
(cid:88)

|

P (H(X) = Y

Y = y)

r(cid:48)∈S

Y = y, S = r(cid:48)) P (S = r(cid:48)

|

Y = y)

|

Example 2 (Equality of Opportunity (Hardt et al., 2016)). A model h is fair for Equality of Opportunity when the
probability of predicting the correct label is independent of the sensitive attribute for the set of desirable outcomes
,
that is

⊂ Y

Y

(cid:48)

(y, r)

∀

∈ Y × S

F(y,r)(h, D) =

(cid:26) P (H(X) = Y

0

Y = y, S = r)

|

−

P (H(X) = Y

Y = y)

|

(cid:48),
if y
∈ Y
otherwise.

We can then write F(y,r)(h, D) in the form of Equation (1) as

F(y,r)(h, D) = C 0

(y,r) +

(cid:88)

C (y(cid:48),r(cid:48))

(y,r)

P (H(X) = Y

(y(cid:48),r(cid:48))∈Y×S

Y = y(cid:48), S = r(cid:48))

|

(5)

with, if y

(cid:48),

∈ Y

and, if y

(cid:48),

∈ Y \ Y

y(cid:48)
∀

= y,

(y,r) = 0

(y,r) = 1

C 0
C (y,r)
= r, C (y,r(cid:48))
, C (y(cid:48),r(cid:48))

(y,r) =

−
(y,r) = 0

r(cid:48)
∀
r(cid:48)
∀

∈ S

P (S = r

−
P (S = r(cid:48)

Y = y)

Y = y)

|

|

, C (y(cid:48),r(cid:48))

(y,r) = 0.

y(cid:48)
∀

,

r(cid:48)
∀

∈ Y

∈ S

12

(cid:54)
(cid:54)
(cid:54)
(cid:54)
Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Proof. We consider the two cases. On the one hand, when y

(cid:48), then we have that

∈ Y \ Y
F(y,r)(h, D) = 0

which gives the ﬁrst part of the result. On the other hand, when y

(cid:48), then we have that

∈ Y

F(y,r)(h, D) = P (H(X) = Y
= P (H(X) = Y

Y = y, S = r)

Y = y, S = r)

|

|

−

−

which gives the second part of the result.

P (H(X) = Y
(cid:88)

|

P (H(X) = Y

Y = y)

r(cid:48)∈S

Y = y, S = r(cid:48)) P (S = r(cid:48)

|

Y = y)

|

Example 3 (Accuracy Parity (Zafar et al., 2017)). A model h is fair for Accuracy Parity when the probability of being
correct is independent of the sensitive attribute, that is,

(r)

∀

∈ S

We can then write F(r)(h, D) in the form of Equation (1) as

F(r)(h, D) = P (H(X) = Y

S = r)

|

−

P (H(X) = Y ) .

F(r)(h, D) = C 0

(r) +

(cid:88)

C (r(cid:48))

(r)

(r(cid:48))∈S

P (H(X) = Y

S = r(cid:48))

|

(6)

with

Proof. We have that

r(cid:48)
∀

(r) = 0

(r) = 1

C 0
C (r)
= r, C (r(cid:48))

(r) =

P (S = r)

−
P (S = r(cid:48))

−

F(r)(h, D) = P (H(X) = Y
= P (H(X) = Y

S = r)

S = r)

|

|

−

−

P (H(X) = Y )
(cid:88)

P (H(X) = Y

r(cid:48)∈S

S = r(cid:48)) P (S = r(cid:48))

|

which gives the result.

Example 4 (Demographic Parity (Binary Labels) (Calders et al., 2009)). A model h is fair for Demographic Parity with
binary labels when the probability of predicting a label is independent of the sensitive attribute, that is,

(y, r)

∀

∈ Y × S

F(y,r)(h, D) = P (H(X) = y

S = r)

|

−

P (H(X) = y) .

Assuming that given a label y, the second binary label is denoted ¯y, we can then write F(y,r)(h, D) in the form of Equation (1)
as

F(y,r)(h, D) = C 0

(y,r) +

(cid:88)

C (y(cid:48),r(cid:48))

(y,r)

P (H(X) = Y

(y(cid:48),r(cid:48))∈Y×S

Y = y(cid:48), S = r(cid:48))

|

(7)

with

(y,r) = P (Y = y)
C 0
−
C (y,r)
(y,r) = P (Y = y
S = r)
C (¯y,r)
(y,r) = P (Y = ¯y, S = r)
= r, C (y,r(cid:48))
= r, C (¯y,r(cid:48))

(y,r) =
(y,r) = P (Y = ¯y, S = r(cid:48))

−
P (Y = y, S = r(cid:48))

−

|

P (Y = y

S = r)

|

P (Y = y, S = r)

−
P (Y = ¯y

S = r)

|

r(cid:48)
∀
r(cid:48)
∀

13

(cid:54)
(cid:54)
(cid:54)
Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Proof. We have that

S = r) + P (H(X) = y

Y

= y, S = r) P (Y

= y

|

S = r)

|

F(y,r)(h, D) = P (H(X) = y
= P (H(X) = y
(cid:88)

|

P (H(X) = y)
S = r)
Y = y, S = r) P (Y = y

−

|
(cid:0) P (H(X) = y

|
Y = y, S = r(cid:48)) P (Y = y, S = r(cid:48))

−

r(cid:48)∈S

+ P (H(X) = y

Y
|
Y = y, S = r) P (Y = y

= y, S = r(cid:48)) P (Y
S = r)
= y

|

= y, S = r) P (Y

Y
S = r)
Y = y, S = r(cid:48)) P (Y = y, S = r(cid:48))

|

= P (H(X) = y
+ 1
(cid:88)

|
P (H(X)
= y
−
(cid:0) P (H(X) = y

−

r(cid:48)∈S

= y, S = r(cid:48)) (cid:1)

P (H(X)

+ 1

−

Y

= y, S = r(cid:48)) P (Y

= y, S = r(cid:48)) (cid:1).

= y

|

|

|

|

Here, we only consider binary labels, y and ¯y. Hence, H(X)

= y

H(X) = ¯y and Y

= y

⇔

⇔

Y = ¯y. Thus, we obtain

F(y,r)(h, D)

P (H(X) = ¯y

Y = ¯y, S = r)) P (Y = ¯y

|

S = r)

|

= P (H(X) = y
(cid:88)

Y = y, S = r) P (Y = y

S = r) + (1

|
(cid:0) P (H(X) = y

|
Y = y, S = r(cid:48)) P (Y = y, S = r(cid:48))

−

−

r(cid:48)∈S

+ (1
= P (H(X) = y

|
P (H(X) = ¯y

−
Y = y, S = r) [P (Y = y

|

Y = ¯y, S = r(cid:48))) P (Y = ¯y, S = r(cid:48)) (cid:1)
P (Y = y, S = r)]

S = r)

|
+ P (H(X) = ¯y
(cid:88)

−
Y = ¯y, S = r) [P (Y = ¯y, S = r)
Y = y, S = r(cid:48)) (

|
P (H(X) = y

+

|

−

|
P (Y = y, S = r(cid:48)))

P (Y = ¯y

S = r)]

r(cid:48)∈S,r(cid:48)(cid:54)=r
(cid:88)

+

P (H(X) = ¯y

Y = ¯y, S = r(cid:48)) P (Y = ¯y, S = r(cid:48))

|

r(cid:48)∈S,r(cid:48)(cid:54)=r
+ P (Y = ¯y
|
= P (H(X) = Y

S = r)

P (Y = ¯y)
Y = y, S = r) [P (Y = y

−

|
+ P (H(X) = Y
(cid:88)

−
Y = ¯y, S = r) [P (Y = ¯y, S = r)
Y = y, S = r(cid:48)) (

|
P (H(X) = Y

+

|

−

|
P (Y = y, S = r(cid:48)))

P (Y = ¯y

S = r)]

S = r)

P (Y = y, S = r)]

−

−

|

|

r(cid:48)∈S,r(cid:48)(cid:54)=r
(cid:88)

+

P (H(X) = Y

r(cid:48)∈S,r(cid:48)(cid:54)=r
+ P (Y = y)

which gives the result.

P (Y = y

−

S = r)

|

Y = ¯y, S = r(cid:48)) P (Y = ¯y, S = r(cid:48))

|

B. Proof of Theorem 3.1

Theorem (Pointwise Lipschitzness of Conditional Negative Predictions). Let
with LX,Y the Lipschitz constants deﬁned in Assumption 2.1. Let h, h(cid:48)
variables having distribution

, and E be an arbitrary event. Assume that E

be a set of real vector-valued functions
be two models, (X, Y, S) be a triple of random
< +

H
(cid:16) LX,Y

, then

∈ H

(cid:17)

|ρ(h,X,Y )|

(cid:12)
(cid:12)
(cid:12) E

∞

D

P(H(X) = Y
|

|

E)

−

P(H (cid:48)(X) = Y

E

E)

|

| ≤

(cid:18) LX,Y

ρ(h, X, Y )
|
|

(cid:19)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

h

(cid:107)

−

h(cid:48)

(cid:107)H .

Proof. The proof of this theorem is in two steps.
with
P

H
(cid:16) LX,Y

, the triangle inequality, and the union bound to show that

E)
−
. Then, applying Markov’s inequality gives the desired result.

P (H(X) = Y

h(cid:48)

(cid:17)

|

|

h
|ρ(h,X,Y )| ≤ (cid:107)

−

(cid:107)H

(cid:12)
(cid:12)
(cid:12) E

First, we use the Lipschitz continuity property associated

P (H (cid:48)(X) = Y

E)

| ≤

|

14

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Bounding

P (H(X) = Y
|
P (H(X) = Y

E)

|

E)

P (H (cid:48)(X) = Y

E)

. We have that
|

|

|

E)
0

−

−
|
P (H (cid:48)(X) = Y
P (ρ(h, X, Y )
≤
= P (ρ(h(cid:48), X, Y )
= P (ρ(h(cid:48), X, Y )
= P (ρ(h, X, Y )
P (ρ(h, X, Y )

≤

≤ |
Assumption 2.1.

↓
P (ρ(h, X, Y )

≤
= P

(cid:16)

ρ(h, X, Y ) < 0

≥

≤

−

≤

|

−

−

E)
E)

P (ρ(h(cid:48), X, Y ) > 0
P (ρ(h, X, Y ) < 0

|
0
ρ(h, X, Y ) + ρ(h, X, Y )
ρ(h(cid:48), X, Y )
ρ(h, X, Y )
|
ρ(h(cid:48), X, Y )
ρ(h, X, Y )

≤

−

|

E)
E)

|
0
|
E)

E)

−

P (ρ(h, X, Y ) < 0
|
E)

P (ρ(h, X, Y ) < 0
|
P (ρ(h, X, Y ) < 0

E)

E)

|

−
E)

| |

−

−

h(cid:48)

≤

LX,Y
(cid:91)

E)

(cid:107)H |
−
ρ(h, X, Y )

(cid:107)
0

h

≤

−

P (ρ(h, X, Y ) < 0
|
(cid:12)
(cid:12)
(cid:12) E

LX,Y

h(cid:48)

h
(cid:107)

(cid:107)H

−

≤

E)
(cid:17)

−

P (ρ(h, X, Y ) < 0

E)

|

↓
= P (0
P (
|
(cid:18)

≤
= P

ρ(h, X, Y )

Union bound on disjoint events.
h
−
h(cid:48)
(cid:107)H |
(cid:12)
(cid:19)
(cid:12)
E
(cid:12)
(cid:12)

≤
ρ(h, X, Y )
ρ(h, X, Y )
LX,Y

LX,Y
h

≤
LX,Y

| ≤
|

(cid:107)
h(cid:48)

≤ (cid:107)

(cid:107)H

h(cid:48)

−

−

h

(cid:107)

|

(cid:107)H |
E)

E)

Similarly, we have that

P (H (cid:48)(X) = Y

E)

|

−

P (H(X) = Y
P (ρ(h(cid:48), X, Y )

|

E)
0

E)

|

≥

−

≤
= P (ρ(h(cid:48), X, Y ) + ρ(h, X, Y )
= P (ρ(h, X, Y )
P (ρ(h, X, Y )

−
(ρ(h(cid:48), X, Y )
ρ(h(cid:48), X, Y )

≥ −

P (ρ(h, X, Y ) > 0
ρ(h, X, Y )

|
0
ρ(h, X, Y ))
ρ(h, X, Y )

≥

E)

−

−

|

|

P (ρ(h, X, Y ) > 0
P (ρ(h, X, Y ) > 0
P (ρ(h, X, Y ) > 0

E)
E)
E)

−

−

−

E)
E)
E)

|

|

|

| |

≤

≥ − |

Assumption 2.1

↓
P (ρ(h, X, Y )

≤
= P

(cid:16)

≥ −
ρ(h, X, Y ) > 0

LX,Y
(cid:91)

0

h
(cid:107)

h(cid:48)

−

(cid:107)H |
ρ(h, X, Y )

≥

E)

−

≥ −

P (ρ(h, X, Y ) > 0
|
(cid:12)
(cid:12)
(cid:12) E

LX,Y

h(cid:48)

h
(cid:107)

(cid:107)H

−

E)
(cid:17)

−

P (ρ(h, X, Y ) > 0

E)

|

Union bound on disjoint events.

↓
= P (0
P (
− |
(cid:18)
|

≤
= P

≥ −

LX,Y
LX,Y

h
(cid:107)
h

(cid:107)

ρ(h, X, Y )

≥
ρ(h, X, Y )
ρ(h, X, Y )
LX,Y

|

| ≥ −
h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

−

−
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)

h(cid:48)
h(cid:48)
(cid:19)

(cid:107)H |
(cid:107)H |

E)
E)

It implies that

P (H(X) = Y
|

|

E)

−

P (H (cid:48)(X) = Y

E)

|

| ≤

(cid:18)

P

ρ(h, X, Y )
|
|
LX,Y

h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:19)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16) |ρ(h,X,Y )|

h
LX,Y ≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:17)

(cid:12)
(cid:12)
(cid:12) E

. We use the Markov’s Inequality and we assume that E

(cid:16) LX,Y

|ρ(h,X,Y )|

(cid:17)

(cid:12)
(cid:12)
(cid:12) E

<

Bounding P
+

∞

. Hence, we have that

(cid:18)

P

ρ(h, X, Y )
|
|
LX,Y

h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:19)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) LX,Y

≥

h
ρ(h, X, Y )
(cid:107)
|
|
Markov’s inequality.
(cid:18) LX,Y

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

ρ(h, X, Y )
|

|

1
h(cid:48)

−

(cid:107)H

(cid:19)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

h(cid:48)

h
(cid:107)

−

(cid:107)H

= P

↓
E

≤

15

It concludes the proof.

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

Remark B.1. In the last step of the proof of Theorem 3.1, we can also use the Chernoff bound:

(cid:18)

P

ρ(h, X, Y )
|
|
LX,Y

h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

E

(cid:18)

(cid:18)

exp

= P

t |

−

ρ(h, X, Y )
|
LX,Y
ρ(h, X, Y )
|
LX,Y

(cid:19)

≥
(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)

exp (

−

t

h

(cid:107)

−

h(cid:48)

(cid:107)H)

(cid:19)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

(cid:19)

exp (t

h
(cid:107)

−

h(cid:48)

(cid:107)H) .

(cid:18)

(cid:18)

exp

E

−

≤

t |

A correct choice of t would lead to potentially tighter bounds than the Markov’s inequality at the expense of readability.

Remark B.2. Before using Markov’s inequality or Chernoff bound in Theorem 3.1, we can modify the probability as

(cid:18)

P

ρ(h, X, Y )
|
|
LX,Y

h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:32)(cid:20)

(cid:19)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= P

ρ(h, X, Y )
|
|
LX,Y

h−h(cid:48)

(cid:21)
(cid:107)

(cid:107)H

h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:33)

E

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where

(cid:20)

ρ(h, X, Y )
|
|
LX,Y

h−h(cid:48)

(cid:21)
(cid:107)

(cid:107)H

=

(cid:40) |ρ(h,X,Y )|
LX,Y

if

ρ(h, X, Y )
|

| ≤

LX,Y

h

(cid:107)

−

h(cid:48)

(cid:107)H ,

+

∞

otherwise .

This essentially means that, whenever the model’s margin on a data record is large enough, its precise value is no more
meaningful, as its prediction will not change whatsoever. The remaining of Theorem 3.1’s proof is unchanged, except

with

(cid:104) |ρ(h,X,Y )|
LX,Y

(cid:105)

h−h(cid:48)
(cid:107)

(cid:107)H instead of |ρ(h,X,Y )|

LX,Y

.

Note that this can lead to much tighter bounds. Notably, when distance
difference of fairness may even become zero.

h
(cid:107)

−

h(cid:48)

(cid:107)H between h and h(cid:48) is small enough, the

C. Proof of Theorem 3.2

Theorem (Pointwise Lipschitzness of Fairness). Let h, h(cid:48)
For any fairness notion of the form of Equation (1), we have:

∈ H

, LX,Y be deﬁned as in Assumption 2.1, and (X, S, Y )

.

∼ D

with χk(h, D) = (cid:80)K
we have:

k(cid:48)=1

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k

k

∀
(cid:12)
(cid:12)
(cid:12)

E

[K],

∈
(cid:16) 1

|h(X)|

Fk(h, D)
|

−

Fk(h(cid:48), D)

| ≤

χk(h, D)

h
(cid:107)

−

h(cid:48)

(cid:107)H ,

(cid:17)

(cid:12)
(cid:12)
(cid:12) Dk(cid:48)

. Similarly, for the aggregate measure of fairness deﬁned in Equation (2),

Fair(h, D)

|

−

Fair(h(cid:48), D)

1
K

K
(cid:88)

k=1

| ≤

χk(h, D)

h
(cid:107)

−

h(cid:48)

(cid:107)H .

Proof. The ﬁrst part follows from the following derivation. For all k,

Fk(h, D)
|

−

Fk(h(cid:48), D)
|

=

=

K
(cid:88)

k(cid:48)=1

C k(cid:48)
k

P (H(X) = Y

Dk(cid:48))
|

−

C 0

k −

K
(cid:88)

k(cid:48)=1

C k(cid:48)
k

P (H (cid:48)(X) = Y

(cid:12)
(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)
|
(cid:12)

(cid:16)

C k(cid:48)
k

P (H(X) = Y

Dk(cid:48))
|

−

P (H (cid:48)(X) = Y

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Dk(cid:48))

|

k +

(cid:12)
(cid:12)
C 0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k(cid:48)=1

K
(cid:88)

↓
K
(cid:88)

≤

k(cid:48)=1

↓
K
(cid:88)

≤

k(cid:48)=1

Triangle inequality.

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) |

P (H(X) = Y

Dk(cid:48))

|

−

P (H (cid:48)(X) = Y

Dk(cid:48))
|
|

Theorem 3.1.

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

E

(cid:18) LX,Y

ρ(h, X, Y )
|
|

(cid:19)

Dk(cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

h
(cid:107)

−

h(cid:48)

(cid:107)H .

16

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

The second part is obtained thanks to the triangle inequality:

Fair(h, D)

|

Fair(h(cid:48), D)
|

−

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K

K
(cid:88)

k=1

Fk(h, D)
|

| −

1
K

K
(cid:88)

|

k=1

Fk(h(cid:48), D)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
|
(cid:12)

↓
1
K

↓
1
K

≤

≤

Triangle inequality.

K
(cid:88)

Fk(h, D)

||

Fk(h(cid:48), D)
||

| − |

k=1
Reverse triangle inequality.

K
(cid:88)

|

k=1

Fk(h, D)

Fk(h(cid:48), D)
|

−

,

which gives the claim when combined with the ﬁrst part of the theorem.

D. Proof of Lemma 3.4

Lemma (Finite Sample analysis). Let D be a ﬁnite sample of n
the true proportion of examples from group k(cid:48). Assume that PD∼Dn
be an hypothesis space and dH be the Natarajan dimension of
Let

≥

H

)

δ

8 log( 2K+1
mink(cid:48) pk(cid:48)
(cid:0) (cid:80)K

k(cid:48)=0

.

H

examples drawn i.i.d. from
(cid:12)
(cid:12)C k(cid:48)

(cid:12)
(cid:12) > αC

(cid:98)C k(cid:48)

D
B3 exp (cid:0)

, where pk(cid:48) is
Cn(cid:1).
B4α2

(cid:1)

k

≤

−

k −

• Assuming that h and h(cid:48) are independent of D. With probability 1

−

δ over the choice of D,

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

(cid:12)
(cid:98)Fk(h(cid:48), D)
(cid:12)
(cid:12) ≤ (cid:98)χk(h, D)

−

h
(cid:107)

−

h(cid:48)

(cid:107)H + (cid:101)O

(cid:115)

(cid:32) K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:33)

log (K/δ)
npk(cid:48)

• Assuming that h and h(cid:48) are dependent of D. With probability 1

−

δ over the choice of D,

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

(cid:12)
(cid:98)Fk(h(cid:48), D)
(cid:12)
(cid:12) ≤ (cid:98)χk(h, D)

−

h
(cid:107)

−

h(cid:48)

(cid:107)H + (cid:101)O

h, h(cid:48)
∀

,

∈ H
(cid:33)

(cid:115)

(cid:32) K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

dH + log (K/δ)
npk(cid:48)

Proof. First of all, notice that we have

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:12)
(cid:98)Fk(h(cid:48), D)
(cid:12)
(cid:12) =

≤

≤

(cid:12)
(cid:12)
(cid:12)Fk(h, D)
(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−
Theorem 3.2

↓
(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:98)Fk(h, D) + (cid:98)Fk(h, D)
(cid:12)
(cid:12)
(cid:98)Fk(h, D)
(cid:12) +

−
(cid:12)
(cid:12)
(cid:12) (cid:98)Fk(h, D)

(cid:98)Fk(h(cid:48), D)

(cid:12)
(cid:12)
(cid:12)
(cid:98)Fk(h(cid:48), D)

(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12) + (cid:98)χk(h(cid:48), D)
(cid:12)
(cid:98)Fk(h, D)

−

h(cid:48)

h
(cid:107)

−

(cid:107)H

Hence it remains to bound the ﬁrst term. By deﬁnition of our fairness notions, notice that we have the following.

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:12)
(cid:12)
(cid:98)Fk(h, D)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C 0

k +

K
(cid:88)

k(cid:48)=1

C k(cid:48)
k

P (H(X) = Y

Dk(cid:48))

|

(cid:98)C 0

k −

−

K
(cid:88)

k(cid:48)=1

(cid:98)C k(cid:48)
k (cid:98)P (H(X) = Y

(cid:12)
(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)
|
(cid:12)

(cid:12)
(cid:12)C 0
(cid:12)

k −

(cid:98)C 0
k

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

k(cid:48)=1

C k(cid:48)
k

P (H(X) = Y

(cid:12)
(cid:12)C 0
(cid:12)

k −

(cid:98)C 0
k

(cid:12)
(cid:12)
(cid:12) +

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k

P (H(X) = Y

≤

≤

Dk(cid:48))

Dk(cid:48))

|

|

−

−

(cid:98)C k(cid:48)
k (cid:98)P (H(X) = Y

(cid:98)C k(cid:48)

k

P (H(X) = Y

(cid:12)
(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)
(cid:12)

|

Dk(cid:48))
|

+ (cid:98)C k(cid:48)

k

P (H(X) = Y

Dk(cid:48))

|

−

17

(cid:98)C k(cid:48)
k (cid:98)P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)

|

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

(cid:12)
(cid:12)C 0
(cid:12)

k −

(cid:98)C 0
k

(cid:12)
(cid:12)
(cid:12) +

≤

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)C k(cid:48)

k

P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)
|

+

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)C k(cid:48)
k (cid:98)P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)
|

(cid:12)
(cid:12)C 0
(cid:12)

k −

(cid:98)C 0
k

(cid:12)
(cid:12)
(cid:12) +

≤

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:12)
P (H(X) = Y
(cid:12)
(cid:12)

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)

|

+

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))
|

(cid:12)
(cid:12)C 0
(cid:12)

k −

(cid:98)C 0
k

(cid:12)
(cid:12)
(cid:12) +

≤

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:12)
P (H(X) = Y
(cid:12)
(cid:12)

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)

|

+

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

≤

k(cid:48)=0

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) +

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12)
|

We now need to consider two cases, depending on whether h depends on D or not.

Assuming that h is independent of D.

In this case, our goal is to upper bound

(cid:32)
(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

P
D∼Dn

(cid:12)
(cid:12)
(cid:12) > αC +
(cid:98)Fk(h, D)

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1

Notice that, using the same trick that Woodworth et al. (2017) used to prove their Equation (38), we have that

(cid:32)
(cid:12)
(cid:12)
(cid:12)Fk(h, D)

P
D∼Dn

(cid:98)Fk(h, D)

−

(cid:12)
(cid:12)
(cid:12) > αC +

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) +

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
(cid:12) > αC +
Dk(cid:48))
|

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1
(cid:35)(cid:33)

(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
(cid:12) > αk(cid:48)

Dk(cid:48))
|

P
D∼Dn

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:17)

(cid:12)
(cid:12)
(cid:12) > αk(cid:48)

Dk(cid:48))
|

P
D∼Dn

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12) > αk(cid:48)
|

(cid:12)
(cid:12)
(cid:12) |

Dk(cid:48)

|

(cid:17)

= i

P
D∼Dn

(
|

Dk(cid:48)

|

= i)

P
D∼Dn

P
D∼Dn

P
D∼Dn

P
D∼Dn

≤

≤

≤

≤

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:32) K
(cid:88)

k(cid:48)=0

K
(cid:88)

n
(cid:88)

(cid:34) K
(cid:91)

(cid:91)

k(cid:48)=1

K
(cid:88)

k(cid:48)=1

+

(cid:33)

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

+

↓

i=0

k(cid:48)=1
Let pk(cid:48) = P (Dk(cid:48) )

P
D∼Dn

≤

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

K
(cid:88)

np

k(cid:48)/2−1
(cid:88)

+

k(cid:48)=1

i=0

P
D∼Dn

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
(cid:12) > αk(cid:48)
Dk(cid:48))
|

(cid:12)
(cid:12)
(cid:12) |

Dk(cid:48)

|

(cid:17)

= i

P
D∼Dn

(
|

Dk(cid:48)

|

= i)

18

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

P
D∼Dn

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)P (H(X) = Y

(cid:12)
(cid:12)
(cid:12) > αk(cid:48)
Dk(cid:48))
|

(cid:12)
(cid:12)
(cid:12) |

Dk(cid:48)

|

(cid:17)

= i

P
D∼Dn

(
|

Dk(cid:48)

|

= i)

P
D∼Dn

(

Dk(cid:48)
|

|

= i)

P
Dk(cid:48) ∼Di
k(cid:48)

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:17)

(cid:12)
(cid:12)
(cid:12) > αk(cid:48)

Dk(cid:48))
|

P
D∼Dn

(

Dk(cid:48)
|

|

= i)

P
D∼Dn

≤

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

P
D∼Dn

≤

k(cid:48)=0

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

(cid:16)

P
D∼Dn

Dk(cid:48)
|

|

<

(cid:17)

npk(cid:48)
2

K
(cid:88)

n
(cid:88)

+

k(cid:48)=1

k(cid:48)/2

i=np
(cid:32) K
(cid:88)

k(cid:48)=0

K
(cid:88)

np

k(cid:48)/2−1
(cid:88)

k(cid:48)=1

K
(cid:88)

i=0

n
(cid:88)

+

+

k(cid:48)=1

k(cid:48)/2

i=np
(cid:32) K
(cid:88)

K
(cid:88)

k(cid:48)=1

K
(cid:88)

+

+

n
(cid:88)

k(cid:48)=1

i=np

k(cid:48)/2

P
Dk(cid:48) ∼Di
k(cid:48)

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:17)

(cid:12)
(cid:12)
(cid:12) > αk(cid:48)

Dk(cid:48))
|

P
D∼Dn

(

Dk(cid:48)
|

|

= i)

Using Hoeffding’s inequality, we can show that

P
Dk(cid:48) ∼D

k(cid:48)

n
k(cid:48)

(cid:16)(cid:12)
(cid:12)
(cid:12)

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:17)

(cid:12)
(cid:12)
(cid:12) > β

Dk(cid:48))
|

≤

2 exp (cid:0)

−

2β2nk(cid:48)

(cid:1)

which implies

(cid:32)

P
D∼Dn

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:98)Fk(h, D)

(cid:12)
(cid:12)
(cid:12) > αC +

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1

2 exp (cid:0)−2β2i(cid:1) ≥ 2 exp (cid:0)−2β2(i + 1)(cid:1).

↓

P
D∼Dn

≤

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

+

+

K
(cid:88)

k(cid:48)=1

K
(cid:88)

k(cid:48)=1

(cid:16)

P
D∼Dn

Dk(cid:48)
|

|

<

(cid:17)

npk(cid:48)
2

2 exp (cid:0)

(cid:1)

α2

k(cid:48)npk(cid:48)

−

By assumption, P

D∼Dn

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k − (cid:98)C k(cid:48)

k

(cid:33)

(cid:12)
(cid:12)
(cid:12) > αC

≤ B3 exp (cid:0)−B4α2

C n(cid:1).

↓
B3 exp (cid:0)

K
(cid:88)

k(cid:48)=1

K
(cid:88)

+

+

≤

≤

B4α2

−

Cn(cid:1)
(cid:16)

Dk(cid:48)
|

P
D∼Dn

(cid:17)

npk(cid:48)
2

<

|

2 exp (cid:0)

α2

k(cid:48)npk(cid:48)

(cid:1)

−

k(cid:48)=1
Chernoff multiplicative bound.
Cn(cid:1)

B4α2

↓
B3 exp (cid:0)

−

19

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

+

+

K
(cid:88)

k(cid:48)=1

K
(cid:88)

k(cid:48)=1

exp

(cid:16)

(cid:17)

npk(cid:48)
8

−

2 exp (cid:0)

(cid:1)

α2

k(cid:48)npk(cid:48)

−

Now, by assumption that n

8 log( 2K+1
mink(cid:48) pk(cid:48)

δ

)

≥

and setting

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

log

log

αC =

αk(cid:48) =

(cid:17)

(cid:16) B3(2K+1)
δ
B4n
(cid:16) 2(2K+1)
δ
npk(cid:48)

(cid:17)

yields that, with probability at least 1

δ,

−

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

(cid:12)
(cid:12)
(cid:98)Fk(h, D)
(cid:12) ≤

−

(cid:118)
(cid:117)
(cid:117)
(cid:116)

log

(cid:17)

(cid:16) B3(2K+1)
δ
B4n

+

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

log

(cid:17)

(cid:16) 2(2K+1)
δ
npk(cid:48)

Assuming that h is dependent of D.

In this case, our goal is to bound

(cid:32)

P
D∼Dn

sup
h∈H

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:12)
(cid:12)
(cid:12) > αC +
(cid:98)Fk(h, D)

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1

Using similar arguments that in the independent case, we have that

(cid:32)

P
D∼Dn

sup
h∈H

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:98)Fk(h, D)

(cid:12)
(cid:12)
(cid:12) > αC +

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1

P
D∼Dn

≤

(cid:32) K
(cid:88)

k(cid:48)=0

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

(cid:16)

P
D∼Dn

Dk(cid:48)
|

|

<

(cid:17)

npk(cid:48)
2

K
(cid:88)

k(cid:48)=1

K
(cid:88)

+

+

n
(cid:88)

P
Dk(cid:48) ∼Di
k(cid:48)

(cid:18)

(cid:12)
(cid:12)
(cid:12)

sup
h∈H

P (H(X) = Y

Dk(cid:48))

|

−

(cid:98)P (H(X) = Y

(cid:19)

(cid:12)
(cid:12)
Dk(cid:48))
(cid:12) > αk(cid:48)
|

P
D∼Dn

(
|

Dk(cid:48)

|

= i)

k(cid:48)=1

i=np

k(cid:48)/2

Using the Multiclass Fundamental Theorem (Shalev-Shwartz & Ben-David, 2014, Theorem 29.3, Lemma 29.4) with dH the
Natarajan dimension of

, we have that

H

(cid:18)

P
Dk(cid:48) ∼D

k(cid:48)

n
k(cid:48)

(cid:12)
(cid:12)
(cid:12)

sup
h∈H

P (H(X) = Y

Dk(cid:48))
|

−

(cid:98)P (H(X) = Y

(cid:19)

(cid:12)
(cid:12)
(cid:12) > β

Dk(cid:48))
|

8ndH
k(cid:48)

≤

|Y|

2dH exp

(cid:18)

−

(cid:19)

nk(cid:48)β2
32

which implies

(cid:32)

P
D∼Dn

sup
h∈H

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

−

(cid:12)
(cid:12)
(cid:12) > αC +
(cid:98)Fk(h, D)

(cid:33)

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12) αk(cid:48)

K
(cid:88)

k(cid:48)=1

(cid:33)

(cid:12)
(cid:12)C k(cid:48)
(cid:12)

k −

(cid:98)C k(cid:48)

k

(cid:12)
(cid:12)
(cid:12) > αC

P
D∼Dn

≤

(cid:32) K
(cid:88)

k(cid:48)=0

20

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

+

+

K
(cid:88)

k(cid:48)=1

K
(cid:88)

k(cid:48)=1

(cid:16)

P
D∼Dn

Dk(cid:48)
|

|

<

(cid:17)

npk(cid:48)
2

(cid:17)dH

8

(cid:16) npk(cid:48)
2

|Y|

2dH exp

(cid:32)

−

(cid:33)

(cid:0) npk(cid:48)
2

(cid:1) α2
k(cid:48)
32

)

and setting

Now, by assumption that n

8 log( 2K+1
mink(cid:48) pk(cid:48)

δ

log

(cid:17)

≥
(cid:16) B3(2K+1)
δ
B4n
(cid:18) 8( np

64 log

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

αC =

αk(cid:48) =

2 )dH |Y|2dH (2K+1)
k(cid:48)

δ

npk(cid:48)

(cid:19)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

=

(cid:16)

64

dH

(cid:0)log (cid:0) npk(cid:48)

2

(cid:1) + 2 log (
npk(cid:48)

|Y|

)(cid:1) + log

(cid:16) 8(2K+1)
δ

(cid:17)(cid:17)

yields that, with probability at least 1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

log

(cid:12)
(cid:12)
(cid:12)Fk(h, D)

(cid:12)
(cid:12)
(cid:98)Fk(h, D)
(cid:12) ≤

−

This concludes the proof.

δ,

h

∀

−

(cid:16) B3(2K+1)
δ
B4n

∈ H
(cid:17)

+

K
(cid:88)

k(cid:48)=1

(cid:12)
(cid:12) (cid:98)C k(cid:48)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:16)

64

dH

(cid:0)log (cid:0) npk(cid:48)

2

(cid:1) + 2 log (
npk(cid:48)

|Y|

)(cid:1) + log

(cid:16) 8(2K+1)
δ

(cid:17)(cid:17)

.

E. Bound for Output Perturbation (Proof of Lemma 4.1)

Lemma. Let hpriv be the vector released by output perturbation with noise σ2 = 8Λ2 log(1.25/δ)/µ2n2(cid:15)2, and 0 < ζ < 1,
then with probability at least 1

ζ,

−

(cid:13)
(cid:13)hpriv

h∗(cid:13)
2
(cid:13)
2 ≤

−

32pΛ2 log(1.25/δ) log(2/ζ)
µ2n2(cid:15)2

.

Proof. We prove this lemma in two steps. First, we show that for a given sensitivity, the distance (cid:13)
Second, we estimate the sensitivity.

(cid:13)hpriv

−

h∗(cid:13)

(cid:13) is bounded.

Bounding the Error. Let ∆ be the sensitivity of the function D
((cid:15), δ) differential privacy (Chaudhuri et al., 2011; Lowy & Razaviyayn, 2021) as follows:

→

arg minw∈C f (w; D). Its value can be released under

where σ2 = 2∆2 log(1.25/δ)

(cid:15)2

hpriv = h∗ +

(0, σ2Ip) ,

N
and h∗ = arg minh∈C f (h). Then, Chernoff’s bound gives, for t, α > 0,
P((cid:13)

tα) E(exp(t (cid:13)

exp(

α)

))

(cid:13)hpriv

(cid:13)hpriv

h∗(cid:13)
2
(cid:13)

h∗(cid:13)
2
(cid:13)

−

≥

≤

−

= exp(

−

tα)

−
E(exp(t(hpriv

p
(cid:89)

j=1

j )2)) ,
h∗

j −

by independence of the noise’s p coordinates. Since hpriv
we can compute E(exp(t(hpriv

j is a Gaussian random variable of mean 0 and variance σ2,
h∗
2tσ2)−1/2. We then obtain

j −

h∗

j ))) = (1

Let t = 1/4pσ2, then it holds that 1

−

j −

P((cid:13)

(cid:13)hpriv

−
2tσ2 = 1

α)

≥
1/2p

−
h∗(cid:13)
2
(cid:13)

−

2tσ2)−p/2 = exp

(1

−

(cid:18)

−

p
2

log(1

−

exp(

−

≤

tα)(1

−

2tσ2)−p/2 .

≤
1
2p

1 and

(cid:32)

exp

≤

2(1

1

−

(cid:33)

1
p )

≤

exp(1/2)

2 ,

≤

(cid:19)

)

21

(8)

(9)

(10)

(11)

(12)

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

since p

2 log(1

1
2p )

p
2

−1/2p
1−1/2p ≥ −

≥

−

1

2 . Let 0 < ζ < 1, t = 1/4pσ2 and α = 4pσ2 log(2/ζ), we have proven

P((cid:13)

(cid:13)hpriv

h∗(cid:13)
2
(cid:13)

−

α)

≤

≥

2 exp

(cid:19)

(cid:18)

α
4pσ

−

ζ .

≤

The error obtained by output perturbation is thus upper bounded by (cid:13)
8p∆2 log(1.25/δ) log(2/ζ)
(cid:15)2

with probability at least 1

ζ.

(cid:13)hpriv

Estimating the Sensitivity. Deﬁne g(h) = 1
n
convexity, the two following inequalities hold for h, h(cid:48),

−
(cid:80)n
i=1 (cid:96)(w; d(cid:48)

i) with d(cid:48)

i ∈ X × Y

(13)

h∗(cid:13)
2
(cid:13)

−

≤

4pσ2 log(2/ζ) =

such that d(cid:48)

i = di for all i

= 1. By strong

(14)

(15)

(16)

(17)

f (h)

f (h(cid:48))

f (h(cid:48)) +

f (h(cid:48)), h

h(cid:48)

−

(cid:104)∇

f (h) +

f (h), h(cid:48)

h
(cid:105)

−

(cid:104)∇

≥

≥

µ
2 (cid:107)
h

+
µ
2 (cid:107)

(cid:105)
+

h

h(cid:48)

2 ,

−
h(cid:48)

(cid:107)
2 .
−
(cid:107)
2. Let h∗
(cid:107)

Summing these two inequalities give
of f and g over

, taking h = h∗

f (h)
(cid:104)∇
1 and h(cid:48) = h∗

− ∇
2 gives

f (h(cid:48)), h

h(cid:48)

−

(cid:105) ≥

µ
h
2 (cid:107)

−

h(cid:48)

C

1 and h∗

2 be the respective minimizers

h∗
µ
1 −
(cid:107)

2
h∗
2(cid:107)

f (h∗
1)

≤ (cid:104)∇

− ∇

Now, optimality conditions give

f (h∗

2), h∗

1 −

h∗
2(cid:105) ≤ (cid:107)∇

f (h∗
1)

− ∇

f (h∗
2)

(cid:107) · (cid:107)

h∗
1 −

h∗
2(cid:107)

.

f (h∗

1) = 0 =

g(h∗

2) =

f (h∗
2)

(cid:96)(h∗

2; d1) +

(cid:96)(h∗

2; d(cid:48)

1) ,

∇

− ∇
1
f (h∗
2; d(cid:48)
resulting in
1)
1)
n ∇
(cid:107)
−
sensitivity of arg minh∈C f (h) is ∆ = 2Λ
nµ , which concludes the proof.

∇
1
n ∇

∇
f (h∗
2)

∇
(cid:96)(h∗

2; d1)

(cid:96)(h∗

− ∇

(cid:107)∇

=

(cid:107)

(cid:107) ≤

2Λ
n . Combined with (16), this shows that the

F. Convergence of DP-SGD (Proof of Lemma 4.2)

Lemma. Let hpriv be the vector released by DP-SGD with σ2 = 64Λ2T 2 log(3T /δ) log(2/δ)/n2(cid:15)2. Assume that σ2
Ei∼[n] (cid:107)∇

σ2. Let 0 < ζ < 1, then with probability at least 1

2
(cid:96)(h∗; xi, yi)
(cid:107)

≤

ζ,

∗ =

(cid:13)
(cid:13)hpriv

−

h∗(cid:13)
2
2 = (cid:101)O
(cid:13)

(cid:18) pΛ2 log(1/δ)2
ζµ2n2(cid:15)2

−
(cid:19)

,

where (cid:101)O ignores logarithmic terms in n (the number of examples) and p (the number of model parameters).

Proof. We start by recalling that in DP-SGD,

ht+1 = πH(ht

γ(gt + ηt)) .

−

Since h∗

, and

∈ H

is convex, we have

H
(cid:13)
(cid:13)ht+1

h∗(cid:13)
2
(cid:13)

−

(cid:13)
(cid:13)πH(ht
=
= (cid:13)
(cid:13)ht
(cid:13)
(cid:13)ht

−

γ(gt + ηt))
−
h∗(cid:13)
2
(cid:13)
h∗(cid:13)
2
(cid:13)

h∗(cid:13)
2
(cid:13)
−
gt + ηt, ht
(cid:104)
gt + ηt, ht
(cid:104)

2γ

2γ

−

−

≤

−

−
2

h∗

h∗

(cid:105)

+ γ2 (cid:13)
+ 2γ2 (cid:13)

(cid:13)gt + ηt(cid:13)
2
(cid:13)
(cid:13)gt(cid:13)
+ 2γ2 (cid:13)
2
(cid:13)

(cid:13)ηt(cid:13)
2
(cid:13)

−
(cid:105)
2 for a, b
(cid:107)

2 + 2
a
where we developed the square and used
(cid:107)
(cid:107)
the stochastic gradient computation and noise, we obtain

a + b
(cid:107)
(cid:107)

≤

2

b
(cid:107)

Rp. Taking the expectation with respect to

∈

E (cid:13)

(cid:13)ht+1

−

h∗(cid:13)
2
(cid:13)

(cid:13)
(cid:13)ht

h∗(cid:13)
2
(cid:13)

2γ

f (ht), ht

h∗

+ 2γ2 E (cid:13)

(cid:13)gt(cid:13)
2
(cid:13)

+ 2γ2 E (cid:13)

(cid:13)ηt(cid:13)
2
(cid:13)

−

≤
f (ht). Now recall that, by strong-convexity of f , we have

(cid:104)∇

−

−

(cid:105)

since E(ηt) = 0 and E(gt) =

∇

f (h∗)

≥

f (ht) +

(cid:104)∇

f (ht), h∗

ht

(cid:105)

−

+

(cid:13)
(cid:13)ht

µ
2

−

h∗(cid:13)
2
(cid:13)

.

22

,

(22)

(23)

(18)

(19)

(20)

(21)

,

(cid:54)
Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

By reorganizing, we obtain

2γ

−

f (ht), ht

(cid:104)∇

−

h∗

2γ(f (ht)

f (h∗))

(1

γµ) (cid:13)

(cid:13)ht

−

−

(cid:105) ≤ −
h∗(cid:13)
2
(cid:13)

−

2γ(f (ht)

−

−

h∗

γµ

ht
(cid:107)

−
−
(cid:13)gt(cid:13)
f (h∗)) + 2γ2 E (cid:13)
2
(cid:13)

2, which gives
(cid:107)
+ 2γ2 E (cid:13)

(cid:13)ηt(cid:13)
2
(cid:13)

.

(24)

E (cid:13)

(cid:13)ht+1

h∗(cid:13)
2
(cid:13)

−

Finally, remark that if f = 1
n

≤
(cid:80)n

i=1 fi with each fi being β-smooth and E fi = f , we have, for i

[n],

∼

E (cid:13)
(cid:13)

∇

fi(ht)(cid:13)
2
(cid:13)

− ∇

fi(ht)

= E (cid:13)
(cid:13)
∇
E(2 (cid:13)
fi(ht)
(cid:13)
∇
E(4β(fi(ht)

≤

≤
= 4β(f (ht)

fi(h∗) +
∇
fi(h∗)(cid:13)
2
(cid:13)

− ∇
fi(h∗)
−
f (h∗)) + 2 E

− (cid:104)∇

fi(h∗)(cid:13)
2
(cid:13)
2)
fi(h∗)
(cid:107)
h∗

+ 2
(cid:107)∇
fi(h∗), ht
fi(h∗)

−
2 ,
(cid:107)

(cid:107)∇

−

) + 2
(cid:105)

fi(h∗)
(cid:107)

(cid:107)∇

2)

since fi is β-smooth, which implies, for all w, v

∈
2
fi(v)
(cid:107)

Rp,

2β(fi(w)

fi(v)

−

− (cid:104)∇

fi(v), w

v

(cid:105)

−

,

≤

fi(w)

(cid:107)∇

− ∇

and E

∇

fi(h∗) = 0. Combined with the fact that E

fi(h∗)

2
(cid:107)

≤

∗ and E
σ2

2

ηt
(cid:107)

(cid:107)

= pσ2, we obtained

E (cid:13)

(cid:13)ht+1

h∗(cid:13)
2
(cid:13)

−

(1

(1

≤

≤

since γ

≤

1/2β, which implies 4βγ2

−

−

−

γµ)
γµ) (cid:13)

(cid:13)
(cid:13)ht
(cid:13)ht

−

−
0 and σ∗

2γ

≤

(cid:107)∇
h∗(cid:13)
2
(cid:13)
h∗(cid:13)
2
(cid:13)

+ (4βγ2
−
+ 4γ2σ2 ,

2γ)(f (ht)

−

f (h∗)) + 2γ2(σ2

∗ + σ2)

σ. By induction, we obtain that, after T iterations,

≤

E (cid:13)

(cid:13)hT

h∗(cid:13)
2
(cid:13)

−

(1

−

≤

γµ)T (cid:13)

(cid:13)h0

γµ)T (cid:13)

(cid:13)h0

(1

−

≤

h∗(cid:13)
2
(cid:13)

+ 4γ2

T −1
(cid:88)

(1

t=0

−

h∗(cid:13)
2
(cid:13)

+

4γσ2
µ

.

−

−

γµ)T −tσ2

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

Now, recall that DP-SGD is ((cid:15), δ)-differentially private for σ2 = 64Λ2T log(3T /δ) log(2/δ)
(following from the Gaussian
mechanism, advanced composition theorem and ampliﬁcation by subsampling). Thus, taking γ = 1/2β, and setting
T = 2β

/2M 2), where M 2 = 64Λ2 log(2/δ)

, yields

n2(cid:15)2

µ log(µβ (cid:13)

(cid:13)h0

h∗(cid:13)
2
(cid:13)

n2(cid:15)2

−

E (cid:13)

(cid:13)hT

h∗(cid:13)
2
(cid:13)

−

2(T log(3T /δ) + 1)M 2
βµ

8M 2
µ2

≤

≤

log

Using Markov inequality, we obtain

(cid:16) µβ (cid:13)

(cid:13)h0

−
2M 2

h∗(cid:13)
2
(cid:13)

(cid:17)

log

(cid:16) 6β log(

2
(cid:107)

)

(cid:17)

µβ

h0−h∗
(cid:107)
2M 2
µδ

.

(34)



P

(cid:13)
(cid:13)hT



h∗(cid:13)
2
(cid:13)

−

8M 2
ζµ2 log

≥

(cid:16) µβ (cid:13)

(cid:13)h0

−
2M 2

h∗(cid:13)
2
(cid:13)

(cid:17)

log

(cid:16) 6β log(

µβ

h0−h∗
(cid:107)
2M 2
µδ

2
(cid:107)

)

(cid:17)





ζ .

≤

This results in the following upper bound, with probability at least 1

ζ,

−

(cid:13)
(cid:13)hT

h∗(cid:13)
2
(cid:13)

−

≤

512Λ2 log(2/δ)
ζµ2n2(cid:15)2
(cid:18) G2 log(1/δ)
ζµ2n2(cid:15)2

= (cid:101)O

which is the result of our lemma.

(cid:16) µβ (cid:13)

(cid:13)h0

−
2M 2

h∗(cid:13)
2
(cid:13)

(cid:17)

log

(cid:16) 6β log(

2
(cid:107)

)

(cid:17)

µβ

h0−h∗
(cid:107)
2M 2
µδ

log

(cid:19)

,

23

(35)

(36)

(37)

Differential Privacy has Bounded Impact on Fairness in Classiﬁcation

G. Additional Experimental Details

G.1. Experimental Setup

The celebA dataset (Liu et al., 2015) is a face attributes dataset, that can be downloaded at http://mmlab.ie.cuhk.
edu.hk/projects/CelebA.html, and the folktables dataset (Ding et al., 2021) is derived from US Census, and
can be downloaded using a Python package available here https://github.com/zykls/folktables.

On each dataset, for each value of n, we train a (cid:96)2-regularized logistic regression model using scikit-learn (Pedregosa
et al., 2011). Private models are then learned using the output perturbation mechanism as described in Section 4.1. We then
compute our bounds using the non-private model as reference, over a test set containing 10% of the data, that has not been
used for training (containing 20, 260 records for celebA and 166, 450 records for folktables). The value of the bound
is computed by minimizing the experession given by the Chernoff bound using the golden section search algorithm (Kiefer,
1953). The code is in the supplementary, and will be made public.

For the plots with different number of training records, we train 20 non-private models with a number of records loga-
rithmically spaced between 10 and the number of records in the complete training set (that is, 182, 339 for celebA and
1, 498, 050 for folktables). For the plots with different privacy budgets, we use 20 values logarithmically spaced
between 10−3 and 10 for both datasets.

G.2. Results for Other Fairness Measures

Our bounds also hold for accuracy parity, demographic parity and equalized odds. The same plots as those presented in
Figure 1 for these fairness notions are in Figure 2 and Figure 3. The comments from Section 5 on equality of opportunity
and accuracy also hold for these three notions of fairness.

(a) Accuracy Parity (celebA)

(b) Demographic Parity (celebA)

(c) Equalized Odds (celebA)

(d) Accuracy Parity (folktables)

(e) Demographic Parity (folktables)

(f) Equalized Odds (folktables)

Figure 2. Fairness and accuracy levels for optimal non-private model and random private ones as a function of the number n of training
samples. For each value of n, we sample 100 private models and take their minimum and maximum fairness/accuracy values to mark the
area of attainable values. The solid blue line and the dashed one give our guarantees, respectively from Theorem 4.4 with Lemma 4.1’s
bounds and with an empirical evaluation of (cid:13)

(cid:13)hpriv − h∗(cid:13)
(cid:13).

G.3. Reﬁned Bounds with Additional Knowledge of hpriv and h∗

In Assumption 2.1, we use a uniform Lipschitz bound for all h, h(cid:48)
for h

of linear models, where,
H
, we denote by hy the parameters of h associated with the label y, that is h(x, y) = hT
y x. For linear models, we

. Let’s consider the class

∈ H

∈ H

24

TheoreticalUpperBoundBoundwithEmpiricalDistanceImprovedBoundKnowingbothModelsTheoreticalUpperBoundBoundwithEmpiricalDistanceImprovedBoundKnowingbothModelsNon-privateModelFairnessPrivateModelsFairness102104Numberoftrainingsamples0.00.10.20.30.4AccuracyParity102104Numberoftrainingsamples0.00.10.20.30.4DemographicParity102104Numberoftrainingsamples0.00.10.20.30.4Equal.Odds102104106Numberoftrainingsamples0.000.020.040.060.080.10AccuracyParity102104106Numberoftrainingsamples0.000.020.040.060.080.10DemographicParity102104106Numberoftrainingsamples0.000.020.040.060.080.10Equal.OddsDifferential Privacy has Bounded Impact on Fairness in Classiﬁcation

(a) Accuracy Parity (celebA)

(b) Demographic Parity (celebA)

(c) Equality of Opportunity (celebA)

(d) Accuracy Parity (folktables)

(e) Demographic Parity (folktables)

(f) Equality of Opportunity (folktables)

Figure 3. Fairness and accuracy levels for optimal non-private model and random private ones as a function of privacy budget (cid:15). For each
value of (cid:15), we sample 100 private models and take their minimum and maximum fairness/accuracy values to mark the area of attainable
values. The solid blue line and the dashed one respectively give our guarantees, respectively from Theorem 4.4 with Lemma 4.1’s bounds
and with an empirical evaluation of (cid:13)

(cid:13)hpriv − h∗(cid:13)
(cid:13).

derived the bound
−
be very loose whenever x and hy
gives 0 = (hy

ρ(h, x, y)
(cid:107)
y)T x
h(cid:48)

(cid:13)
(cid:13)hy

−

≤

h(cid:48)

h

(cid:107)2 (cid:107)

x
(cid:107)H, as derived in Section 2. Note that this inequality can
2
(cid:107)
) are (close to) orthogonal. When they are orthogonal, this bounds only

(cid:107)H ≤
∈ Y
(cid:107)2. We can thus improve the inequality by remarking that we have

−

ρ(h(cid:48), x, y)
h(cid:48)
y (for y
(cid:13)
x
(cid:13)2 (cid:107)
−
ρ(h(cid:48), x, y)

−
h(cid:48)
y

ρ(h, x, y)
|

−

h(cid:48)(x, y(cid:48))
|

−

h(x, y)

y x

| ≤ |
= (cid:12)
(cid:12)hT
(cid:12)
(cid:12)(hy
(cid:12)
(cid:12)
(cid:12)(hy

=

=

2 max
y(cid:48)∈Y

≤

−

+ max
y(cid:48)(cid:54)=y |
(cid:12)
(cid:12)hT
y(cid:48)x
(cid:12)
(cid:12)(hy(cid:48)
(cid:12)
(cid:12)
(cid:12) + max
y(cid:48)(cid:54)=y

h(x, y(cid:48))
y(cid:48)x(cid:12)
h(cid:48)
(cid:12)
y(cid:48))x(cid:12)
h(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12)(hy(cid:48)

−

h(cid:48)(x, y)
−
|
y x(cid:12)
h(cid:48)T
y)T x(cid:12)
h(cid:48)

(cid:12) + max
y(cid:48)(cid:54)=y

(cid:12) + max
y(cid:48)(cid:54)=y

−

−

y)T phy−h(cid:48)
h(cid:48)
y (x)
(cid:13)
(cid:13)
h
(cid:13) (cid:107)

−
(cid:13)
(cid:13)
(cid:13)phy−h(cid:48)

y (x)

h(cid:48)

(cid:107)H ,

−

h(cid:48)
y(cid:48))phy−h(cid:48)

y (x)

−

(cid:12)
(cid:12)
(cid:12)

y (x) is the projection of x on the axis deﬁned by hy

h(cid:48)
y. We can thus deﬁne a variant of LX,Y which depends

−

where phy−h(cid:48)
on h

h(cid:48)

−

Lh−h(cid:48)

X,Y = 2 max
y∈Y

(cid:13)
(cid:13)
(cid:13)phy−h(cid:48)

y (x)

(cid:13)
(cid:13)
(cid:13) .

(38)

Replacing Assumption 2.1 by this inequality in the proof of Theorem 3.1, we end up with the inequality

P (H(X) = Y
|

|

E)

−

P (H (cid:48)(X) = Y

E)

|

| ≤

(cid:32)

P

|

ρ(h, X, Y )
Lh−h(cid:48)
X,Y

|

h

≤ (cid:107)

h(cid:48)

(cid:107)H

−

(cid:33)

E

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the probability is over (X, S, Y )
X,Y instead of
LX,Y . Note that even if this gives a much tighter bound, this can generally not be computed, as one of h or h(cid:48) is typically
not known.

. We obtained the same bound as Theorem 3.1, except with Lh−h(cid:48)

∼ D

25

TheoreticalUpperBoundBoundwithEmpiricalDistanceImprovedBoundKnowingbothModelsTheoreticalUpperBoundBoundwithEmpiricalDistanceImprovedBoundKnowingbothModelsNon-privateModelFairnessPrivateModelsFairness10−2100Valueof(cid:15)0.000.050.100.15AccuracyParity10−2100Valueof(cid:15)0.000.050.100.15DemographicParity10−2100Valueof(cid:15)0.000.050.100.15Equal.Opp.10−2100Valueof(cid:15)0.000.010.020.030.04AccuracyParity10−2100Valueof(cid:15)0.000.010.020.030.04DemographicParity10−2100Valueof(cid:15)0.000.010.020.030.04Equal.Opp.