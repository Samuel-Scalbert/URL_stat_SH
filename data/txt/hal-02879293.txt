Transformer-based Argument Mining for Healthcare
Applications
Tobias Mayer, Elena Cabrio, Serena Villata

To cite this version:

Tobias Mayer, Elena Cabrio, Serena Villata. Transformer-based Argument Mining for Healthcare
Applications. ECAI 2020 - 24th European Conference on Artificial Intelligence, Aug 2020, Santiago
de Compostela / Online, Spain. ￿hal-02879293￿

HAL Id: hal-02879293

https://hal.science/hal-02879293

Submitted on 23 Jun 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Transformer-based Argument Mining
for Healthcare Applications

Tobias Mayer and Elena Cabrio and Serena Villata1

Abstract. Argument(ation) Mining (AM) typically aims at iden-
tifying argumentative components in text and predicting the rela-
tions among them. Evidence-based decision making in the health-
care domain targets at supporting clinicians in their deliberation pro-
cess to establish the best course of action for the case under evalu-
ation. Although the reasoning stage of this kind of frameworks re-
ceived considerable attention, little effort has been devoted to the
mining stage. We extended an existing dataset by annotating 500 ab-
stracts of Randomized Controlled Trials (RCT) from the MEDLINE
database, leading to a dataset of 4198 argument components and
2601 argument relations on different diseases (i.e., neoplasm, glau-
coma, hepatitis, diabetes, hypertension). We propose a complete ar-
gument mining pipeline for RCTs, classifying argument components
as evidence and claims, and predicting the relation, i.e., attack or sup-
port, holding between those argument components. We experiment
with deep bidirectional transformers in combination with different
neural architectures (i.e., LSTM, GRU and CRF) and obtain a macro
F1-score of .87 for component detection and .68 for relation predic-
tion, outperforming current state-of-the-art end-to-end AM systems.

1

Introduction

In the healthcare domain, there is an increasing interest in the de-
velopment of intelligent systems able to support and ease clini-
cians’ everyday activities. These systems apply to clinical trials,
clinical guidelines, and electronic health records, and their solu-
tions range from the automated detection of PICO2 elements [19]
in health records to evidence-based reasoning for decision mak-
ing [18, 8, 24, 35]. These applications highlight the need of clini-
cians to be supplied with frameworks able to extract, from the huge
quantity of data available for the different diseases and treatments,
the exact information they necessitate and to present this informa-
tion in a structured way, easy to be (possibly semi-automatically)
analyzed. Argument(ation) Mining (AM) [30, 22, 7] deals with ﬁnd-
ing argumentative structures in text. Standard tasks in AM consist
in the detection of argument components (i.e., evidence and claims),
and the prediction of the relations (i.e., attack and support) holding
among them. Given its aptness to automatically detect in text those
argumentative structures that are at the basis of evidence-based rea-
soning applications, AM represents a potential valuable contribution
in the healthcare domain.

However, despite its natural employment in healthcare applica-
tions, only few approaches have applied AM methods to this kind

of text [14, 25, 26], and their contribution is limited to the detection
of argument components, disregarding the more complex phase of
predicting the relations among them. In addition, no huge annotated
dataset for AM is available for the healthcare domain. In this paper,
we cover this gap, and we answer the following research question:
how to deﬁne a complete AM pipeline for clinical trials? To answer
this question, we propose a deep bidirectional transformer approach
combined with different neural networks to address the AM tasks
of component detection and relation prediction in Randomized Con-
trolled Trials, and we evaluate this approach on a new huge corpus
of 659 abstracts from the MEDLINE database.

More precisely, the contributions of this paper are as follows:

1. We build a new dataset from the MEDLINE database, consisting
of 4198 argument components and 2601 argument relations on
ﬁve different diseases (neoplasm, glaucoma, hepatitis, diabetes,
hypertension)3;

2. We present a complete AM pipeline for clinical trials relying
on deep bidirectional transformers combined with different neu-
ral networks, i.e., Long Short-Term Memory (LSTM) networks,
Gated Recurrent Unit (GRU) networks, and Conditional Random
Fields (CRFs)4;

3. Our extensive evaluation of various AM architectures (e.g., for
persuasive essays) reveals that current approaches are unable to
adequately address the challenges raised by medical text and we
show that transformer-based approaches outperform these AM
pipelines as well as standard baselines.

In the following, Section 2 compares the proposed approach with
related work. We then describe the corpus we built, the methods we
employed and the experimental setting. Finally, we report the ob-
tained results and we address an error analysis before concluding.

2 Related work

One of the latest advances in artiﬁcial argumentation [2] is the so-
called Argument(ation) Mining [30, 22, 7]. Argument mining con-
sists of two standard tasks: (i) the identiﬁcation of arguments within
the text, that may be further split in the detection of argument com-
ponents (e.g., claims, evidence) and the identiﬁcation of their textual
boundaries. Different methods have been used for this task (e.g., Sup-
port Vector Machines (SVMs), Na¨ıve Bayes classiﬁers, and Neural
Networks (NNs)); (ii) the prediction of the relations holding between
the arguments identiﬁed in the ﬁrst stage. They are used to build the

1 Universit´e Cˆote d’Azur, CNRS, Inria, I3S, France, email: {tmayer, vil-

lata}@i3s.unice.fr, elena.cabrio@unice.fr

2 Patient Problem or Population, Intervention, Comparison or Control, and

Outcome.

3 The newly created dataset, called AbstRCT, and the annotation guidelines
are available here: https://gitlab.com/tomaye/abstrct/
4 The source code is available here: https://gitlab.com/tomaye/

ecai2020-transformer_based_am

argument graphs, in which the relations connecting the retrieved ar-
gumentative components correspond to the edges. Different methods
have been employed to address these tasks, from standard SVMs to
NNs. AM methods have been applied to heterogeneous types of tex-
tual documents, e.g., persuasive essays [38], scientiﬁc articles [39],
Wikipedia articles [4], political speeches and debates [27], and peer
reviews [17]. However, only few approaches [42, 14, 25, 26] fo-
cused on automatically detecting argumentative structures from tex-
tual documents in the medical domain, such as clinical trials, clinical
guidelines, and Electronic Health Records.

Few approaches consider the whole AM pipeline in different ap-
plication scenarios. In particular, Stab and Gurevych [38] propose a
feature-based Integer Linear Programming approach to jointly model
argument component types and argumentative relations in persua-
sive essays. Differently from our data, essays have exactly one major
claim each. The authors impose the constraint such that each claim
has no more than one parent, while no constraint holds in our case.
In contrast with this approach, Eger et al. [11] present neural end-to-
end learning methods in AM, which do not require the hand-crafting
of features or constraints, using the persuasive essays dataset. They
employ TreeLSTM on dependency trees [28] to identify both compo-
nents and relations between them. They decouple component classiﬁ-
cation and relation classiﬁcation, but they are jointly learned, using a
dependency parser to calculate the features. In our approach, we also
decouple the two classiﬁcation tasks, in line with the claim of [11]
that decoupling component and relation classiﬁcation improves the
performance. Furthermore, the same work addresses component de-
tection as a multi-class sequence tagging problem [37]. Differently
from their approach, which does not scale with long texts as it relies
on dependency tree distance, our approach is distance independent.
In addition, whilst persuasive essay components are usually linked to
components close by in the text, in our dataset links may span across
the whole RCT abstract.

Recent approaches for link prediction rely on pointer net-
works [34] where a sequence-to-sequence model with attention takes
as input argument components and returns the links between them.
In these approaches, neither the boundary detection task nor the rela-
tion classiﬁcation one are tackled. Another approach to link predic-
tion relies on structured learning [12]. The authors propose a general
approach employing structured multi-objective learning with resid-
ual networks, similar to approaches on structured learning on fac-
tor graphs [29]. Recently, the argument classiﬁcation task was ad-
dressed with contextualized word embeddings [36]. However, dif-
ferently from our approach, they assume components are given, and
boundary detection is not considered. In line with their work, we
experimented with the BERT [10] base model to address parts of the
AM pipeline [26]. Contrary to this preliminary work, we now employ
and evaluated various contextualized language models and architec-
tures on each task to span the full AM pipeline.

3 Corpus creation

To address AM on clinical data, we rely on and extend our previous
dataset [25], the only available corpus of Randomized Controlled
Trial abstracts annotated with the different argument components
(evidence, claims and major claims). Such corpus contains the same
abstracts used in the corpus of RCT abstracts of [40], that were re-
trieved directly from PubMed5 by searching for the disease name and

5 PubMed (https://www.ncbi.nlm.nih.gov/pubmed/) is a free
search engine accessing primarily the MEDLINE database on life sciences
and biomedical topics.

specifying that it has to be a RCT. The ﬁrst version of the corpus with
coarse labels contained 919 argument components (615 evidence and
304 claims) from 159 abstracts comprising 4 different diseases (i.e.,
glaucoma, hypertension, hepatitis b, diabetes).

To obtain more training data, we have extracted from PubMed 500
additional abstracts following Strategy 1 in [40]. We selected neo-
plasm6 as a topic, assuming that the abstracts would cover experi-
ments over dysfunctions related to different parts of the human body
(providing therefore a good generalization as for training instances).
Annotation was started after a training phase, where amongst oth-
ers the component boundaries were topic of discussion. Gold labels
were set after a reconciliation phase, during which the annotators
tried to reach an agreement. While the number of annotators vary for
the two annotation phases (component and relation annotation), the
inter-annotator agreement (IAA) was always calculated with three
annotators based on a shared subset of the data. The third annotator
was participating in each training and reconciliation phase as well.

In the following, we describe the data annotation process for the
argument components in the neoplasm dataset, and for the argumen-
tative relations in the whole dataset. Table 1 reports on the statistics
of the ﬁnal dataset.

Dataset
Neoplasm
Glaucoma
Hepatitis
Diabetes
Hypertension
Total

#Evi
2193
404
80
72
59
2808

#Claim #MajCl

993
183
27
36
26
1265

93
7
5
11
9
125

#Sup
1763
334
65
44
53
2259

#Att
298
33
1
8
2
342

Table 1. Statistics of the extended dataset. Showing the numbers of
evidence, claims, major claims, supporting and attacking relations for each
disease-based subset, respectively.

3.1 Annotation of argument components

Following the guidelines for the annotation of argument components
in RCT abstracts provided in [25], two annotators with background
in computational linguistics7 carried out the annotation of the 500 ab-
stracts on neoplasm. IAA among the annotators has been calculated
on 30 abstracts, resulting in a Fleiss’ kappa of 0.72 for argumentative
components and 0.68 for the more ﬁne-grained distinction between
claims and evidence (meaning substantial agreement for both tasks).
Example 1 shows a sample annotated abstract, where claims are writ-
ten in bold, major claims are highlighted with a dashed underline, and
evidence are written in italics.

In the context of RCT abstracts, a claim is a concluding
Claims
statement made by the author about the outcome of the study. It gen-
erally describes the relation of a new treatment (intervention arm)
with respect to existing treatments (control arm) and is derived from
the described results. Major claims are more a general/concluding
claim, which is supported by more speciﬁc claims. The concluding
statements do not have to occur at the end of the abstract, and may

6 While neoplasms can either be benign or malignant, the vast majority of
articles is about malignant neoplasm (cancer). We stick with neoplasm as a
term, since this was the MeSH term used for the PubMed query.

7 In [15], researchers with different backgrounds (biology, computer science,
argumentation pedagogy, and BioNLP) have annotated medical data for an
AM task, showing to perform equally well despite their backgrounds.

also occur at the beginning of the text as an introductory claim, as in
Example 1. Given the negligible occurrences of major claims in our
dataset, we merge them with the claims for the classiﬁcation task.

is to point out a related problem, unconnected to the outcome of the
study itself.

Evidence An evidence in RCT abstracts is an observation or mea-
surement in the study, which supports or attacks another argument
component, usually a claim. Those observations comprise side ef-
fects and the measured outcome of the intervention and control arm.
They are observed facts, and therefore credible without further justi-
ﬁcations, as this is the ground truth the argumentation is based on.

Example 1 Extracellular adenosine 5’-triphosphate (ATP) is in-
volved in the regulation of a variety of biologic processes, in-
cluding neurotransmission, muscle contraction, and liver glu-
cose metabolism, via purinergic receptors. [In nonrandomized
studies involving patients with different tumor types including
non-small-cell lung cancer (NSCLC), ATP infusion appeared to
inhibit loss of weight and deterioration of quality of life (QOL)
and performance status]. We conducted a randomized clinical trial
to evaluate the effects of ATP in patients with advanced NSCLC
(stage IIIB or IV). [...] Fifty-eight patients were randomly as-
signed to receive either 10 intravenous 30-hour ATP infusions,
with the infusions given at 2- to 4-week intervals, or no ATP. Out-
come parameters were assessed every 4 weeks until 28 weeks.
Between-group differences were tested for statistical signiﬁcance
by use of repeated-measures analysis, and reported P values are
two-sided. Twenty-eight patients were allocated to receive ATP
treatment and 30 received no ATP. [Mean weight changes per
4-week period were -1.0 kg (95% conﬁdence interval [CI]= 1.5
to -0.5) in the control group and 0.2 kg (95% CI =-0.2 to +0.6)
in the ATP group (P=.002)]1. [Serum albumin concentration de-
clined by -1.2 g/L (95% CI=-2.0 to -0.4) per 4 weeks in the con-
trol group but remained stable (0.0g/L; 95% CI=-0.3 to +0.3) in
the ATP group (P =.006)]2. [Elbow ﬂexor muscle strength de-
clined by -5.5% (95% CI=-9.6% to -1.4%) per 4 weeks in the con-
trol group but remained stable (0.0%; 95% CI=-1.4% to +1.4%)
in the ATP group (P=.01)]3. [A similar pattern was observed
for knee extensor muscles (P =.02)]4. [The effects of ATP on
body weight, muscle strength, and albumin concentration were
especially marked in cachectic patients (P=.0002, P=.0001, and
P=. 0001, respectively, for ATP versus no ATP)]5. [...] This ran-
domized trial demonstrates that [ATP has beneﬁcial effects on
weight, muscle strength, and QOL in patients with advanced
NSCLC]1.

3.2 Annotation of argumentative relations

As a next step towards modeling the argumentative structures in the
data, it is crucial to annotate the relations, i.e., directed links con-
necting the components. Those relations are connecting argument
components to form the graph like structure of an argument. The
relation is a directed link from an outgoing node (i.e., the source)
to a target node. The nature of the relation can be supporting or at-
tacking, meaning that the source component is justifying or under-
mining the target component. Links can occur only between certain
components: evidence can be connected to either a claim or another
evidence, whereas claims can only point to other claims (including
major claims). The polarity of the relation (supporting or attacking)
does not limit the possibility to what type of component a component
can be connected. Theoretically, all types of relations are possible be-
tween the allowed combination pairs. Practically, some relations oc-
cur rather seldom compared to the frequency of others. The number
of outgoing links from a component may exceed one. Furthermore,
in rare cases, components cannot be connected at all. This can hap-
pen for major claims in the beginning of an abstract, whose function

Attack A component is attacking another one, if it is i) contra-
dicting the proposition of the target component, or ii) undercutting
its implicit assumption of signiﬁcance, i.e., stating that the observed
effects are not statistically signiﬁcant. The latter case is shown in Ex-
ample 2. Here, evidence 1 is attacked by evidence 2, challenging the
generality of the prior observation.

Example 2 [True acupuncture was associated with 0.8 fewer hot
ﬂashes per day than sham at 6 weeks,]1 [but the difference did
not reach statistical signiﬁcance (95% CI, -0.7 to 2.4; P = .3).]2

The partial-attack is used when the source component is not in full
contradiction, but weakening the target component by constraining
its proposition. Those can be implicit statements about the signiﬁ-
cance of the study outcome, which usually occur between two claims
(see Example 3). Attacks and partial-attacks are identiﬁed with a
unique class for the relation classiﬁcation task.

Example 3 [SLN biopsy is an effective and well-tolerated proce-
dure.]1 [However, its safety should be conﬁrmed by the results
of larger randomized trials and meta-analyses.]2

Support All statements or observations justifying the proposition
of the target component are considered as supporting the target (even
if they justify only parts of the target component). In Example 1, all
the evidence support claim 1.

We carried out the annotation of argumentative relations over the
whole dataset of RCT abstracts, including both the ﬁrst version of
the dataset [25] and the newly collected abstracts on neoplasm. An
expert in the medical domain (a pharmacist) validated the annotation
guidelines before starting the annotation process. IAA has been cal-
culated on 30 abstracts annotated in parallel by three annotators (the
same two annotators that carried out the argument component anno-
tation, plus one additional annotator), resulting in a Fleiss’ kappa of
0.62. The annotation of the remaining abstracts was carried out by
one of the above mentioned annotators.

4 The AM pipeline for clinical trials

In this section, we ﬁrst describe the argument component detection
and relation classiﬁcation tasks, and then we report about the exper-
imental setting to solve these tasks.

4.1 Argument Component Detection

The ﬁrst step of the AM pipeline (visualized in Figure 1) is the de-
tection of argumentative components and their boundaries. As de-
scribed above, most of the AM approaches classify the type of com-
ponent assuming the boundaries of argument components as given.
To merge the component classiﬁcation and boundary detection into
one problem, we cast the component detection as sequence tagging
task. Following the BIO-tagging scheme, each token should be la-
beled as either being at the Beginning, Inside or Outside of a compo-
nent. As we have two component types in AM, this translates into a
sequence tagging problem with ﬁve labels, i.e., B-Claim, I-Claim, B-
Evidence, I-Evidence and Outside. To model the temporal dynamics
of sequence tagging problems, usually Recurrent Neural Networks

Figure 1.

Illustration of the full argument mining pipeline on clinical trials.

(RNN) are used. In our experiments, we evaluate different combi-
nations of RNNs with various types of pre-trained word represen-
tations. Each embedding method is combined with uni- or bidirec-
tional LSTMs or GRUs with and without a CRF as a last layer. Fur-
thermore, we are the ﬁrst to do token level classiﬁcation on AM by
ﬁne-tuning different transformer models.

Embeddings There are two ways to create an input word repre-
sentation for sequence modelling. One way is to look up the repre-
sentation from pre-trained embeddings. This static method has the
advantage that one does not need to train its own embeddings. How-
ever, the vocabulary is limited, and the context of the word is not
considered. State-of-the-art embeddings are generated dynamically
from the context of the target word based on pre-trained language
models (LM) [1, 10, 33]. In our experiments, we consider both kinds
of embeddings. Furthermore, since our data is from the medical do-
main containing very speciﬁc terminology which might not be cov-
ered in the vocabulary of general word embeddings, we experiment
with different approaches to overcome this problem.

As for the static embeddings, we employ GloVe [31] and
extvec [20] embeddings, which are commonly used and are based
on aggregated global word-word co-occurence statistics trained on
Wikipedia and the Gigaword 5 corpus. Words are considered to be
the smallest unit. Contrary to that, fastText [13] and byte-pair em-
beddings BPEmb [16] use subword segments to increase the ca-
pability of their vocabulary and might because of that be a better
choice for a setting with unusual and speciﬁc terminology. Moving to
the dynamically generated embeddings, Embeddings from Language
Models (ELMo) [33] are generating the representation of a word by
contextualizing it with the whole input sentence. They use a bidirec-
tional LSTM to independently train a left-to-right and right-to-left
character based LM. We use the ELMo model trained on PubMed
to have a model which is trained on the type of data we are using.
For the same reason, we use the on PubMed trained Contextualized
String Embeddings (FlairPM) [1], another character-based language
model. We compare them directly to embeddings trained on web con-
tent, Wikipedia, subtitles and news (FlairMulti). The third type of
dynamic embedding are Bidirectional Encoder Representations from
Transformers (BERT) [10]. The language model considers subwords
and the position of the word in the sentence to give the ﬁnal repre-

sentation of a word.

Transformers
can be used as features to an RNN, but also have
the possibility to ﬁne-tune the pre-trained model on a target dataset,
which we make use of. Beside the original BERT, which is pre-
trained on the BooksCorpus and English Wikipedia, there exists
multiple other BERT models by now. BioBERT [21] is pre-trained
on large-scale biomedical corpora outperforming the general BERT
model in representative biomedical text mining tasks. The authors
initialize the weights with the original BERT model and train on
PubMed abstracts and full articles. Therefore, the vocabulary is the
same as for the original BERT. Contrary to that, SciBERT [5] is
trained from scratch with an own vocabulary. While SciBERT is
trained on full papers from Semantic Scholar it also contains biomed-
ical data, but to a smaller degree than BioBERT. We chose to use the
uncased SciBERT model, meaning that we ignore the capitalization
of words. As it was the case for the original BERT, the uncased model
of SciBERT performs slightly better for sentence classiﬁcation tasks
than the cased model. Another new model, which outperforms BERT
on the General Language Understanding Evaluation (GLUE) bench-
mark, is RoBERTa [23]. There, the BERT pre-training procedure is
modiﬁed by exchanging static with dynamic masking, using larger
byte-pair encoding and batches size, and increasing the size of the
dataset.

4.2 Relation Classiﬁcation

After the argument component detection, the next step is to deter-
mine which relations hold between the different components (Fig-
ure 1). We extract valid BI tag sequences from the previous step,
which are then considered to be the argumentative components of
one RCT. Those sequences are phrases and do not necessarily corre-
spond to full sentences. The list of components then serves as input
for the relation classiﬁcation. As explained in Section 2, the relation
classiﬁcation task can be tackled with different approaches. We treat
it as a sequence classiﬁcation problem, where the sequence consists
of a pair of two components, and the task is to learn the relation
between them. For this purpose, we use self-attending transformers,
since these models are dominating the benchmarks for tasks which
involve classifying the status between two sentences [10]. Treating it

as a sequence classiﬁcation problem gives us two options to model
it: (i) jointly modelling the relations by classifying all possible ar-
gumentative component combinations or (ii) predicting possible link
candidates for each entity and then classifying the relation only for
plausible entity pairs. In the literature, both methods are represented.
Therefore, we decided to evaluate both ways of solving the problem.
We experiment with various transformer architectures and compare
them with state-of-the-art AM models, i.e., the Tree-LSTM based
end-to-end system from Miwa and Bansal [28] as employed by Eger
et al. [11], and the multi-objective residual network of Galassi et
al. [12]. For option (i), we use bi-directional transformers [10], which
consists of an encoder and decoder which themselves consists of
multi-head self-attention layer each followed by a fully-connected
dense layer. Contrary to the sequence tagging transformer, where
each token of the sequence has a representation which is fed into
the RNN, for sequence classiﬁcation a pooled representation of the
whole sequence is needed. This representation is passed into a lin-
ear layer with a softmax which decodes it into a distribution over the
target classes. We treat it as a three class classiﬁcation problem (Sup-
port, Attack and NoRelation). We refer to this type of transformer as
SentClf. Using this architecture one component can have relations
with multiple other components, since each component combination
is classiﬁed independently. This is not the case in a multiple choice
setting (MultiChoice), where possible links are predicted taking the
other combinations into account and which we employ for (ii). Here,
each component (source) is given the list of all the other components
as possible target relation candidates and the goal is to determine the
most probable candidate as a target component from this list. This
problem deﬁnition corresponds to the grounded common sense in-
ference problem [43]. To model components which have no outgo-
ing link to other components, we add the noLink option to the choice
selection. As an encoder for phrase pairs, we evaluate various BERT
models which are explained in the transformers section, just as we do
for the SentClf task. With respect to the neural transformer architec-
ture, a multiple choice setting means that each choice is represented
by a vector Ci ∈ RH , where H is the hidden size of the output of an
encoder. The trainable weight is a vector V ∈ RH whose dot product
with the choice vector Ci is the score of the choice. The probability
distribution over all possible choices is given by the softmax, where
n is the number of choices:

models are: TuckER [3], TransE [6] and ComplEX [41]. Unfortu-
nately, those models did not learn a meaningful relation representa-
tion. We assume this might be due to our relatively small graph data.
In the literature, the smallest dataset these models have been exper-
imented on has around 93k triples [9], whereas our dataset has less
than 20k.

4.3 Experimental Setup

For sequence tagging, each of the above mentioned embeddings were
combined with either (i) a GRU, (ii) a GRU with a CRF, (iii) a LSTM,
or (iv) a LSTM with a CRF. Additionally, the best performing static
and dynamic embeddings were concatenated and evaluated as if they
were one embedding. The Flair [1] PyTorch NLP framework ver-
sion 0.4.1 was used for implementing the sequence tagging task. For
BERT, we use the PyTorch implementation of huggingface9 version
2.3. Hyper parameter tuning was done with hyperopt10 version 0.1.2.
The learning rate was selected from {0.05, 0.1, 0.15, 0.2}, RNN lay-
ers {1, 2}, hidden size {32, 64, 128, 256}, dropout {0.1, 0.2, 0.5},
and batch size from {8, 16, 32}. The RNNs were trained over 100
epochs with early stopping and SGD optimizer. For ﬁne-tuning the
BERT model, we used the uncased base model with 12 transformer
blocks, a hidden size of 768, 12 attention heads, a learning rate of
2e-5 with Adam optimizer for 3 epochs. The same conﬁguration was
used for ﬁne-tuning Sci- and BioBERT. For SciBERT, we used the
uncased model with the SciBERT vocabulary. For BioBERT, we used
version 1.1. For RoBERTa, we increased the number of epochs for
ﬁne-tuning to 10, as it was done in the original paper. The best learn-
ing rate was 3e-5 on our task. The number of choices for the multiple
choice model was 6. Batch size was 8 with a maximum sequence
length of 256 subword tokens per input example. We split our neo-
plasm corpus such that 350 abstracts are assigned to the train, 50 to
the development, and 100 to the test set. Additionally, we use the ﬁrst
version of the dataset [25] to create two extra test sets, both compris-
ing 100 abstracts. The ﬁrst one includes only glaucoma, whereas the
second is a mixed set with 20 abstracts of each disease in the dataset
(neoplasm, glaucoma, hypertension, hepatitis and diabetes), respec-
tively.

5 Evaluation

Pi =

eV ·Ci
j=1 eV ·Cj

(cid:80)n

(1)

This section presents and discusses the empirical results of our AM
pipeline for RCTs.

The component combination with the highest score of having a link
between them is then passed into a linear layer to determine which
kind of relation is holding between the two components, i.e., Attack
or Support. The MultiChoice model is trained jointly with two losses,
i.e., one for the multiple choice task and one for for the relation clas-
siﬁcation task.

Furthermore, we experimented with linear options for link pre-
diction, such as matrix or tensor factorization. Those methods are
widely used on graph data, e.g., knowledge graphs, to discover new
links between existing nodes [41]. The matrix or tensor representa-
tion of the graph data is decomposed and a model speciﬁc scoring
function, which assigns a score to each triple8, is minimized, like a
loss function in neural architectures. We experiment by combining
those graph-based embeddings and enriching the nodes with linguis-
tic features/embeddings to learn hybrid graph embeddings for rela-
tions and discover new links between arguments. The tested linear

Sequence Tagging We show the results for the best performing
RNN models and the best performing embedding combinations in
Table 2. Results are given on all three test sets in micro and macro
multi-class F1-score and for claim and evidence, respectively. Com-
paring the static word embeddings, fastText with a GRU and a CRF is
the best performing combination, where extvec is only slightly worse
and is usually better for evidence classiﬁcation. For the dynamic em-
beddings coming from LMs, the ones trained on the medical domain
corpus, i.e., FlairPM and ELMo, show similar performances with a
macro F1-score of .68 on the neoplasm test set. They have the edge
over the non-specialized LMs like BERT with .66 or FlairMulti with
.63 macro F1-score. Concatenating static and dynamic embeddings
does not bring a notable difference, when taking all test sets into ac-
count. Generally, evidence scores are higher than claim scores, lead-
ing to the conclusion that claims are more diverse than evidence. The

8 A triple consists of a subject (source node), a predicate (labeled edge be-

tween nodes) and an object (target node).

9 https://github.com/huggingface/transformers
10 https://github.com/hyperopt/hyperopt

Model
Embedding
GRU+CRF
GloVe
GRU+CRF
extvec
GRU+CRF
fastText(fT)
LSTM+CRF
BPEmb
LSTM+CRF
ELMo
LSTM+CRF
BERT
LSTM+CRF
FlairMulti
LSTM+CRF
FlairPM
GRU+CRF
FlairPM + extvec
GRU+CRF
FlairPM + fT
LSTM+CRF
FlairPM + BERT
LSTM+CRF
BERT + fT
LSTM+CRF
ELMo + fT
dense layer
ﬁne-tuning BERT
ﬁne-tuning BERT
GRU+CRF
ﬁne-tuning BioBERT GRU+CRF
GRU+CRF
ﬁne-tuning SciBERT

Neoplasm
C-F1
.50
.58
.61
.59
.59
.58
.53
.60
.54
.53
.61
.55
.59
.69
.78
.87
.88

F1
.58
.65
.66
.60
.68
.66
.63
.68
.65
.64
.69
.65
.68
.60
.85
.84
.87

f1
.61
.67
.68
.64
.70
.69
.66
.70
.68
.68
.70
.68
.71
.82
.89
.90
.90

E-F1
.66
.72
.71
.76
.76
.75
.72
.75
.74
.75
.76
.74
.77
.83
.90
.90
.92

f1
.60
.68
.68
.64
.74
.70
.58
.74
.74
.71
.71
.68
.74
.77
.89
.92
.91

Glaucoma
C-F1
.36
.57
.60
.52
.67
.63
.50
.69
.67
.62
.67
.60
.69
.63
.76
.93
.93

F1
.52
.64
.65
.60
.72
.68
.55
.72
.72
.68
.70
.66
.72
.55
.86
.91
.89

E-F1
.68
.72
.71
.69
.77
.73
.60
.75
.77
.74
.73
.71
.77
.80
.89
.91
.91

f1
.55
.67
.65
.61
.72
.68
.52
.70
.68
.67
.68
.67
.72
.80
.90
.92
.91

Mixed

C-F1
.36
.57
.52
.48
.67
.61
.44
.64
.60
.56
.62
.58
.65
.65
.81
.91
.90

F1
.50
.64
.60
.57
.70
.66
.50
.68
.66
.63
.67
.65
.70
.57
.88
.91
.88

E-F1
.64
.71
.69
.66
.74
.71
.56
.72
.72
.71
.72
.71
.75
.83
.91
.92
.93

Table 2. Results of the multi-class sequence tagging task are given in micro F1 (f1) and macro F1 (F1). The binary F1 for claims are reported as C-F1 and for
evidence as E-F1. Best scores in each column are marked in bold; signiﬁcance was tested with a two-sided Wilcoxon signed rank test.

explanation is that, since natural language reports of measurements
in clinical trials vary mostly only in the measured parameter and its
values, claims can be made about almost everything. Another ob-
servation is that the performance of the models trained on neoplasm
data do not signiﬁcantly decrease for test sets on other disease treat-
ments. This fact supports our choice of a more general high level
disease type like neoplasm for training the models. The performance
for many model combinations even increases on the glaucoma test
set. The glaucoma test set comprises only a handful of different glau-
coma treatments and is therefore less diversiﬁed than the neoplasm
or mixed test sets. Looking at the main difference in the results, ﬁne-
tuning BERT outperforms all other model combinations, where the
version with a GRU and CRF is the best performing model. Fine-
tuning without any kind of sequence modelling on top of it results in
worse performance. Especially with respect to the validity of BIO se-
quences, where disproportionately many invalid sequences are gen-
erated. This is not useful when extracting the components based on
BIO-scheme. Comparing the specialized with the general models,
Bio- and SciBERT show a better performance than the general BERT
model, where the cased BioBERT tends to be more reliable for the
out of domain test data. This is in line with the ﬁndings that the cased
transformer model works better for tasks like Named Entity Recogni-
tion (NER), which is also a sequence tagging task. The difference on
our data is marginal: while for NER the casing of a word is relevant,
in our task it does not seem to be a sensitive information.

Relation Classiﬁcation The results for relation classiﬁcation are
shown in Table 3. The numbers are not calculated on gold stan-
dard, but show the actual relation classiﬁcation performance when
the components come from the sequence tagging module of the
pipeline. We used the best performing sequence tagger, i.e. the ﬁne-
tuned SciBERT with a GRU and CRF. We follow previous work on
AM [32] and consider the overlap percentage of the components to
determine the base if a predicted component matches the annotated
component in the gold standard. Since in our data a lot of the com-
ponents span over 50% or more of a sentence and the exact boundary

detection is not always clear, even for human annotators, we consider
a predicted and a gold standard component as matched, when at least
75% of the words overlap.

Method
Tree-LSTM
Residual network
BERT MultiChoice
BioBERT MultiChoice
SciBERT MultiChoice
BERT SentClf
BioBERT SentClf
SciBERT SentClf
RoBERTa

Neoplasm Glaucoma Mixed

.37
.42
.58
.61
.63
.62
.64
.68
.67

.44
.38
.56
.58
.59
.53
.58
.62
.66

.39
.43
.55
.57
.60
.66
.61
.69
.67

Table 3. Results of the relation classiﬁcation task, given in macro F1-score.

The Tree-LSTM based end-to-end system performed the worst
with a F1-score of .37. This can be explained by the positional encod-
ing in the persuasive essay dataset being more relevant than in ours.
There, components are likely to link to a neighboring component,
whereas in our dataset the position of a component only partially
plays a role, and therefore the distance in the dependency tree is not
a meaningful feature. Furthermore, the authors specify that their sys-
tem does not scale with increasing text length [11]. Especially de-
tailed reports of measurements can make RCT abstracts quite long,
such that this system becomes not applicable for this type of data.

The residual network performed better with a F1-score of .42. The
main problem here is that it learns a multi-objective for link predic-
tion, relation classiﬁcation and type classiﬁcation for source and tar-
get component, where the latter classiﬁcation step is already covered
by the sequence tagger and therefore unnecessary at this step.

Similar to sequence tagging, one can see a notable increase in per-
formance when applying a BERT model. Comparing the specialized
and general BERT model, the Bio- and SciBERT increase the perfor-
mance by up to .06 F1-score. Interestingly, RoBERTa delivers com-

parable results even though it is a model trained on general data. We
speculate that parts of the web crawl data which was used to train
RoBERTa contain PubMed articles, since they are freely available
on the web. Independently of that, RoBERTa shows more reliable re-
sults when looking at the performance on the out of domain test sets.
While SciBERT as the best performing system on the in-domain test
set drops .06 points on the glaucoma test set, RoBERTa stays almost
the same and only drops from .67 to .66 F1-score. Looking at the
difference between the MultiChoice and SentClf architectures, the
SentClf delivers slightly better results, but the drawback is that this
technique tends to link components to multiple components. Since
most of our components have only one outgoing edge, it creates a lot
of false positives, i.e., links which do not exist.

While our dataset consists of only study abstracts for practical rea-
sons, the pipeline can be applied on full text articles as well. Alas, we
cannot provide a quantitative analysis on full articles due to missing
annotated data. In preliminary experiments on full articles, we have
observed a notable increase of false positives in the relation classiﬁ-
cation, which is the expected consequence of an increased number of
components. Furthermore, with the number of components rising in
the double-digit range, the multiple-choice architecture loses its pre-
dictive power. We leave further investigations to determine the exact
limit of this architecture applied on full text articles to future work.

Error Analysis Common mistakes for the sequence tagger are the
invalid BIO sequences. Especially when there are multiple compo-
nents in one sentence, the tagger tends to mislabel B- tokens as I-
tokens. This is due to the natural imbalance between B- and I- to-
kens. Training the sequence tagging without the BIO scheme using
only claim and evidence as labels, poses problems when multiple
components are following each other in the text. They would be ex-
tracted as one single component instead. This is a common case in
concluding sentences at the end of a study, which strikingly often
comprise multiple claims. Further experiments could go in the di-
rection of weighted loss functions like focal loss to overcome this
problem. Notable mistakes arise for determining the exact compo-
nent boundaries. Especially in the case of connectives, e.g., however,
which have sometimes nothing but a conjunctive function, and in
other cases signal a constraint of a previous statement. Another mis-
take is the misclassiﬁcation of the description of the initial state of
the participant groups as an observation of the study and therefore
an evidence, e.g., there were no signiﬁcant differences in pregnancy-
induced hypertension across supplement groups. In the study abstract
these descriptions occur usually relatively close to the actual result
description, which means that adding information of the position in
the text will not avoid this error. While only some abstracts are struc-
tured, the full study report does usually have separated sections. This
structure can be exploited when analysing full reports, and in the
simplest case one would analyse only the sections of interest.

Concerning link prediction, general components like the differ-
ence was not statistically signiﬁcant are problematic, since it could
be linked to most of the components/outcomes of the trial. Here, a
positional distance encoding could be beneﬁcial, since those com-
ponents are usually connected to the previous component. In gen-
eral, most of the errors in the MultiChoice architecture were made
in the multiple choice part by predicting a wrong link and not at
the stage of classifying the relation type. Interestingly, comparing
the two domain adapted models, Bio- and SciBERT, there were no
regular errors, which allows any conclusion about the advantages or
disadvantages of one model. Looking at the confusion matrices, all
tested SentClf models show a higher error rate for the NoRelation

class. Both transformer approaches have in common the problem of
dealing with negations and limitations or associating the polarity of
a measurement and therefore confusing support and attack.

Example 4 [more research about the exact components of a VR
intervention and choice of outcomes to measure effectiveness
is required]source [Conducting a pragmatic trial of effectiveness
of a VR intervention among cancer survivors is both feasible
and acceptable]target

Example 5 [this did not translate into improved progression-
free survival (PFS) or overall survival]source [The addition of
gemcitabine to carboplatin plus paclitaxel increased treatment
burden, reduced PFS time, and did not improve OS in patients
with advanced epithelial ovarian cancer]target

Example 4 shows two claims with a limiting/attacking relation,
which was wrongly classiﬁed as supporting. For Example 5, not im-
proving progression-free survival (PFS) corresponds to a reduced
PFS time, while for other factors reducing the value means it is ben-
eﬁcial and therefore improving some study parameter. Here, the in-
clusion of external expert knowledge is crucial to learn these ﬁne
nuances. The polarity of a measurement cannot be learnt from tex-
tual features alone. Especially in the medical domain, there are com-
plex interrelationships which are not often explicitly mentioned and
therefore are impossible to capture with a model trained solely on
character-based input. Phrases like increased the blood pressure by
X or showed no symptom of Y can connote different messages de-
pending on the context. Future work needs to consider this challenge
of incorporating external expert knowledge. While we do not think
this is a problem limited to a special domain, we consider it greatly
important for understanding and representing medical text.

6 Conclusion

To support clinicians in decision making or in (semi)-automatically
ﬁlling evidence tables for systematic reviews in evidence-based
medicine, we propose a complete argument mining pipeline for the
healthcare domain. To this aim, we built a novel corpus of health-
care texts (i.e., RCT abstracts) from the MEDLINE database, which
are annotated with argumentative components and relations. Indeed,
we show that state-of-the-art argument mining systems are unable to
satisfactorily tackle the two tasks of argument component detection
and relation prediction on this kind of text, given its peculiar features
(e.g., component relations spanning across the whole RCT abstract).
We expect that our work will have a large impact for clinicians as it
is a crucial step towards AI supported clinical deliberation at a large
scale.

We employ a sequence tagging approach combining a domain spe-
ciﬁc BERT model with a GRU and CRF to identify and classify
argument components. We cast the relation classiﬁcation task as a
multiple choice problem and compare it with recent transformers for
sequence classiﬁcation. In our extensive evaluation, addressed on a
newly AM annotated dataset of RCTs, we investigate the use of dif-
ferent neural transformer architectures and pre-trained models in this
pipeline, showing an improvement of the results in comparison with
standard baselines and state-of-the-art AM systems.

For future work, we will annotate relations across different RCTs
to allow reasoning on the resulting argument graphs and clustering
of arguments about the same disease. Furthermore, we will inves-
tigate different ways to efﬁciently deal with medical abbreviations
and incorporate a distance parameter to overcome the problem that

general components talking about limitations are linked to unrelated
components far away in the text of the RCT abstract.

[21]

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
Kim, Chan Ho So, and Jaewoo Kang, ‘BioBERT: a pre-trained biomed-
ical language representation model for biomedical text mining’, Bioin-
formatics, (2019).

ACKNOWLEDGEMENTS

This work is partly funded by the French government labelled PIA
program under its IDEX UCA JEDI project (ANR-15-IDEX-0001).
This work has been supported by the French government, through the
3IA Cˆote d’Azur Investments in the Future project managed by the
National Research Agency (ANR) with the reference number ANR-
19-P3IA-0002

REFERENCES

[1] Alan Akbik, Duncan Blythe, and Roland Vollgraf, ‘Contextual string
embeddings for sequence labeling’, in Proc. of COLING 2018, pp.
1638–1649, (2018).

[2] Katie Atkinson, Pietro Baroni, Massimiliano Giacomin, Anthony
Hunter, Henry Prakken, Chris Reed, Guillermo Ricardo Simari,
Matthias Thimm, and Serena Villata, ‘Towards artiﬁcial argumenta-
tion’, AI Magazine, 38(3), 25–36, (2017).
Ivana Balazevic, Carl Allen, and Timothy Hospedales, ‘TuckER: Ten-
sor factorization for knowledge graph completion’, in Proc. of EMNLP-
IJCNLP 2019, pp. 5185–5194, (2019).

[3]

[4] Roy Bar-Haim, Indrajit Bhattacharya, Francesco Dinuzzo, Amrita
Saha, and Noam Slonim, ‘Stance classiﬁcation of context-dependent
claims’, in Proc. of EACL 2017, pp. 251–261, (2017).
Iz Beltagy, Kyle Lo, and Arman Cohan, ‘SciBERT: A pretrained lan-
guage model for scientiﬁc text’, in Proc. of EMNLP-IJCNLP 2019, pp.
3615–3620, (2019).

[5]

[6] Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-Dur´an, Jason Weston,
and Oksana Yakhnenko, ‘Translating embeddings for modeling multi-
relational data’, in Proc. of NIPS 2013, pp. 2787–2795, (2013).

[7] Elena Cabrio and Serena Villata, ‘Five years of argument mining: a
data-driven analysis’, in Proc. of IJCAI 2018, pp. 5427–5433, (2018).
[8] Robert Craven, Francesca Toni, Cristian Cadar, Adrian Hadad, and
Matthew Williams, ‘Efﬁcient argumentation for medical decision-
making’, in Proc. of KR 2012, pp. 598–602, (2012).

[10]

[9] Tim Dettmers, Minervini Pasquale, Stenetorp Pontus, and Sebastian
Riedel, ‘Convolutional 2d knowledge graph embeddings’, in Proc. of
AAAI 2018, pp. 1811–1818, (February 2018).
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,
‘BERT: Pre-training of deep bidirectional transformers for language
understanding’, in Proc. of NAACL-HLT 2019, pp. 4171–4186, (2019).
[11] Steffen Eger, Johannes Daxenberger, and Iryna Gurevych, ‘Neural end-
to-end learning for computational argumentation mining’, in Proc. of
ACL 2017, pp. 11–22, (2017).

[12] Andrea Galassi, Marco Lippi, and Paolo Torroni, ‘Argumentative link
prediction using residual networks and multi-objective learning’, in
Proc. of ArgMining 2018 workshop, pp. 1–10, (2018).

[13] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and
Tomas Mikolov, ‘Learning word vectors for 157 languages’, in Proc. of
LREC 2018, pp. 3483–3487, (2018).

[14] Nancy Green, ‘Argumentation for scientiﬁc claims in a biomedical re-

search article’, in Proc. of ArgNLP 2014 workshop, (2014).

[15] Nancy Green, ‘Annotating evidence-based argumentation in biomedical

text’, IEEE BIBM 2015, 922–929, (2015).

[16] Benjamin Heinzerling and Michael Strube, ‘Bpemb: Tokenization-free
pre-trained subword embeddings in 275 languages’, in Proc. of LREC
2018, pp. 2989–2993, (2018).

[17] Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang, ‘Argument
mining for understanding peer reviews’, in Proc. of NAACL-HLT 2019,
p. 2131–2137, (2019).

[18] Anthony Hunter and Matthew Williams, ‘Aggregating evidence about
the positive and negative effects of treatments’, Artiﬁcial Intelligence in
Medicine, 56(3), 173–190, (2012).

[19] Di Jin and Peter Szolovits, ‘PICO element detection in medical text via
long short-term memory neural networks’, in Proc. of BioNLP 2018
workshop, pp. 67–75, (2018).

[20] Alexandros Komninos and Suresh Manandhar, ‘Dependency based em-
beddings for sentence classiﬁcation tasks’, in Proc. of NAACL-HLT
2016, pp. 1490–1500, (2016).

[22] Marco Lippi and Paolo Torroni, ‘Argumentation mining: State of the art
and emerging trends’, ACM Trans. Internet Techn., 16(2), 10, (2016).
[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy-
anov, ‘Roberta: A robustly optimized BERT pretraining approach’,
CoRR, abs/1907.11692, (2019).

[24] Luca Longo and Lucy Hederman, ‘Argumentation theory for decision
support in health-care: A comparison with machine learning’, in Proc.
of BHI 2013, pp. 168–180, (2013).

[25] Tobias Mayer, Elena Cabrio, Marco Lippi, Paolo Torroni, and Serena
Villata, ‘Argument mining on clinical trials’, in Proc. of COMMA 2018,
pp. 137–148, (2018).

[26] Tobias Mayer, Elena Cabrio, and Serena Villata, ‘ACTA a tool for ar-
gumentative clinical trial analysis’, in Proc. of IJCAI 2019, pp. 6551–
6553, (2019).

[27] Stefano Menini, Elena Cabrio, Sara Tonelli, and Serena Villata, ‘Never
retreat, never retract: Argumentation analysis for political speeches’, in
Proc. of AAAI 2018, pp. 4889–4896, (2018).

[28] Makoto Miwa and Mohit Bansal, ‘End-to-end relation extraction us-
ing lstms on sequences and tree structures’, in Proc. of ACL 2016, pp.
1105–1116, (2016).

[29] Vlad Niculae, Joonsuk Park, and Claire Cardie, ‘Argument mining with
structured SVMs and RNNs’, in Proc. of ACL 2017, pp. 985–995,
(2017).

[31]

[30] Andreas Peldszus and Manfred Stede, ‘From argument diagrams to ar-
gumentation mining in texts: A survey’, Int. J. Cogn. Inform. Nat. In-
tell., 7(1), 1–31, (2013).
Jeffrey Pennington, Richard Socher, and Christopher D. Manning,
‘Glove: Global vectors for word representation’, in Proc. of EMNLP
2014, pp. 1532–1543, (2014).
Isaac Persing and Vincent Ng, ‘End-to-end argumentation mining in
student essays’, in Proc. of NAACL-HLT 2016, pp. 1384–1394, (2016).
[33] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christo-
pher Clark, Kenton Lee, and Luke Zettlemoyer, ‘Deep contextualized
word representations’, in Proc. of NAACL-HLT 2018, pp. 2227–2237,
(2018).

[32]

[34] Peter Potash, Alexey Romanov, and Anna Rumshisky, ‘Here’s my
point: Joint pointer architecture for argument mining’, in Proc. of
EMNLP 2017, pp. 1364–1373, (2017).

[35] Malik Al Qassas, Daniela Fogli, Massimiliano Giacomin, and Gio-
vanni Guida, ‘Analysis of clinical discussions based on argumentation
schemes’, Procedia Computer Science, 64, 282–289, (2015).

[36] Nils Reimers, Benjamin Schiller, Tilman Beck, Johannes Daxenberger,
Christian Stab, and Iryna Gurevych, ‘Classiﬁcation and clustering of ar-
guments with contextualized word embeddings’, in Proc. of ACL 2019,
pp. 567–578, (2019).

[37] Anders Søgaard and Yoav Goldberg, ‘Deep multi-task learning with
low level tasks supervised at lower layers’, in Proc. of ACL 2016, pp.
231–235, (2016).

[38] Christian Stab and Iryna Gurevych, ‘Parsing argumentation structures
in persuasive essays’, Comput. Linguist., 43(3), 619–659, (2017).
[39] Simone Teufel, Advaith Siddharthan, and Colin Batchelor, ‘Towards
domain-independent argumentative zoning: Evidence from chemistry
and computational linguistics’, in Proc. of EMNLP 2009, pp. 1493–
1502, (2009).

[40] Antonio Trenta, Anthony Hunter, and Sebastian Riedel, ‘Extraction
of evidence tables from abstracts of randomized clinical trials us-
ing a maximum entropy classiﬁer and global constraints’, CoRR,
abs/1509.05209, (2015).

[41] Th´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric Gaussier, and
Guillaume Bouchard, ‘Complex embeddings for simple link predic-
tion’, in Proc. of ICML 2016, pp. 2071–2080, (2016).
Jure Zabkar, Martin Mozina, Jerneja Videcnik, and Ivan Bratko, ‘Ar-
gument based machine learning in a medical domain’, in Proc. of
COMMA 2006, pp. 59–70, (2006).

[42]

[43] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi, ‘SWAG:
A large-scale adversarial dataset for grounded commonsense infer-
ence’, in Proc. of EMNLP 2018, pp. 93–104, (2018).

