Extracting Definienda in Mathematical Scholarly
Articles with Transformers
Shufan Jiang, Pierre Senellart

To cite this version:

Shufan Jiang, Pierre Senellart. Extracting Definienda in Mathematical Scholarly Articles with Trans-
formers. The 2nd Workshop on Information Extraction from Scientific Publications at IJCNLP-AACL
2023, Nov 2023, Online, Indonesia. ￿hal-04282533￿

HAL Id: hal-04282533

https://hal.science/hal-04282533

Submitted on 20 Nov 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Extracting Definienda in Mathematical Scholarly Articles
with Transformers

Shufan Jiang
DI ENS, ENS, PSL University, CNRS
& Inria
Paris, France
shufan.jiang@ens.psl.eu

Pierre Senellart
DI ENS, ENS, PSL University, CNRS
& Inria & Institut Universitaire de France
Paris, France
pierre@senellart.com

Abstract

We consider automatically identifying the de-
fined term within a mathematical definition
from the text of an academic article. Inspired
by the development of transformer-based natu-
ral language processing applications, we pose
the problem as (a) a token-level classification
task using fine-tuned pre-trained transformers;
and (b) a question-answering task using a gen-
eralist large language model (GPT). We also
propose a rule-based approach to build a la-
beled dataset from the LATEX source of papers.
Experimental results show that it is possible to
reach high levels of precision and recall using
either recent (and expensive) GPT 4 or simpler
pre-trained models fine-tuned on our task.

1

Introduction

Mathematical scholarly articles contain mathemat-
ical statements such as axioms, theorems, proofs,
etc. These structures are not captured by traditional
ways of navigating the scientific literature, e.g.,
keyword search. We consider initiatives aiming at
better knowledge discovery from scientific papers
such as sTEX (Kohlhase, 2008), a bottom-up solu-
tion for mathematical knowledge management that
relies on authors adding explicit metadata when
writing in LATEX; MathRepo (Fevola and Görgen,
2022), a crowd-sourced repository for mathemati-
cians to share any additional research data along-
side their papers; or TheoremKB (Mishra et al.,
2021), a project that extracts the location of theo-
rems and proofs in mathematical research articles.
Following these ideas, we aim at automatically
building a knowledge graph to automatically index
articles with the terms defined therein.

As a first step, we consider the simpler prob-
lem of, given the text of a formal mathematical
definition (which is typically obtained from the
PDF article), extracting the definienda (terms de-
fined within). As an example, we show in Figure 1
a mathematical definition (as rendered within a

Figure 1: Rendering of a definition from a mathematical
scholarly article (Nagy, 2013) accompanied with its
LATEX source code. The definienda are “spread” and
“components”.

PDF article, accompanied with its LATEX source
code) that defines two terms (which we call the
definienda): “spread” and “components”. In this
particular example, the two terms are emphasized
in the PDF (by being set in a non-italic font within
an italic paragraph) – this is not always the case but
we will exploit the fact that some authors do this to
build a labeled dataset of definitions and definienda.

After discussing some related work in Section 2,
we describe our approach in Section 3 and show
experimental results in Section 4.

2 Related Work

The difficulties of our task lie in (1) the lack of la-
beled datasets; (2) the diversity in mathematicians’
writing style; and (3) the interplay of discourse and
formulae, which differentiate mathematical text
and text in the general domain. We review poten-
tial corpora and existing approaches in this section.
The most relevant work to our objective is by
Berlioz (2023). The author trains supervised clas-
sifiers to extract definitions from mathematical pa-
pers from arXiv. The best classifier takes static
word embeddings built from arXiv papers, part-
of-speech features of the words, and hand-coded
binary features, such as if a word is an acronym,
and then applies a BiLSTM-CRF architecture for

sequence tagging (Huang et al., 2015). The re-
sulting precision, recall, and F1 are of 0.69, 0.65,
and 0.67 respectively. The author uses the classi-
fier to automatically extract term-definition pairs
from arXiv articles and Wikidata, resulting in the
dataset ArGot (Berlioz, 2021). Note however that
a limitation of ArGot, which makes it unsuitable in
our setting, where the text of definitions is directly
taken from PDFs, is that mathematical expressions
and formulas are masked out in the training set.

Another related task is term-definition extraction
in the general domain of scientific articles. For
example, Scholarphi (Head et al., 2021) is an aug-
mented reading interface for papers with publicly
available LATEX sources. Given a paper (with its
LATEX source), it lets the reader click on specific
words to view their definitions within the paper.
The authors test several models for definition–term
detection, including an original Heuristically En-
hanced Deep Definition Extraction (Kang et al.,
2020), syntactic features, heuristic rules, and dif-
ferent word representation technologies such as
contextualized word representations based on trans-
formers (Vaswani et al., 2017). The results show
that models involving SciBERT (Beltagy et al.,
2019) achieved higher accuracy on most mea-
surements due to the domain similarity between
the scholarly documents for pre-training SciBERT
and those used in the evaluation. Following this
idea, cc_math_roberta (Mishra et al., 2023) is a
RoBERTa-based model pertained from scratch on
mathematical articles from arXiv (Mishra et al.,
2023). This model outperforms Roberta in a
sentence-level classification task while the cor-
pora size for pre-training cc_math_roberta is much
smaller than Roberta’s. We aim to determine in
this work if contextualized word representations
can improve the results of mathematical definienda
extraction.

NaturalProofs (Welleck et al., 2021) is a cor-
pus of mathematical statements and their proofs.
These statements are extracted from different
sources with hand-crafted rules, such as the con-
tent being enclosed by \begin{theorem} and
\end{theorem} in the LATEX source of a text-
book project on algebraic stacks1. Each statement
is either a theorem or a definition. However, this
dataset does not annotate the definienda of each
definition.

3 Proposed Approach

We describe our approach in two steps. First, we
build a ground-truth dataset using the LATEX source
of papers. As the existing large datasets either
concern term–definition extraction from general
corpora like web pages or textbooks (Welleck et al.,
2021) or mask out mathematical expressions in the
text (Berlioz, 2021), we decide to process plain text
as it appears in scholarly papers so that our solu-
tion can be directly applied to texts extracted from
PDF articles when the LATEX source is unavailable.
Second, we study different usages of transformer-
based models to extract definienda. We are inter-
ested in fine-tuning and one-shot learning (prompt
engineering). The source code of our approach,
as well as the constructed dataset, is available on
Github2.

3.1 Dataset Construction

To start with a reasonable corpus, we collected
the LATEX source of all 28 477 arXiv papers in the
area of Combinatorics (arXiv’s math.CO category)
published before 1st Jan 2020 through arXiv’s
bulk access from AWS3. Our goal in building the
dataset was not to be complete, but to produce
as cheaply and reliably as possible a ground-truth
dataset of definitions and definienda. For this
purpose, we rely on two features of definitions
that some authors (but definitely not all!) use:
definienda are often written in italics within the
definition (or, as in Figure 1, in non-italics within
an italics paragraph); and definienda are some-
times shown in parentheses after the definition
header. As we do not need to completely cap-
ture all cases in the building of the dataset, we
assume that definitions are within a definition
LATEX environment and thus extracted text
blocks between \begin{definition} and
\end{definition}; we ignored contents en-
closed in other author-defined environments, such
as \begin{Def}, which might bring us more
definitions but also more noise. For defined terms,
relying on the two features described above, we
extracted the contents within \textit{} and
\emph{} from the text blocks as well as the
content potentially provided as optional argument
to the \begin{definition}[] environment.
We then converted the extracted partial LATEX

2https://github.com/sufianj/def_

extraction

1https://github.com/stacks/

3https://info.arxiv.org/help/bulk_

stacks-project

data_s3.html

code into plain text with Unicode characters us-
ing pylatexenc4. After a brief glance at the
most frequent extracted definienda values, we hand-
crafted regular expressions to filter out the follow-
ing recurrent noises among them:

• irrelevant or meaningless phrases such as re-

peating “i.e.” and “\d”;

• Latin locutions such as “et al.”;
• list entries such as “(i)” and “(iii)”.

After filtering, we got a list of 13 692 text blocks,
of which the average length is 70 tokens, and the
maximum length is 5 266 tokens. We removed
39 text blocks having more than 500 tokens. Fi-
nally, we labeled automatically the texts with IOB2
tagging, where the “B-MATH_TERM” tag de-
notes the first token of every defined term, “I-
MATH_TERM” tag indicates any non-initial to-
ken in a defined term, and the “O” tag means
that the token is outside any definiendum. Con-
sidering partially italics compound terms like
“\emph{non}-k-equivalent”, we annotate
“non-k-equivalent” as a definiendum. We sorted the
labeled texts by the last update time of the papers.
To evaluate the quality of this dataset, we ex-
amined by hand 1 024 labeled entries. We found
that only 30 annotated texts out of 1 024 to be in-
correctly labeled, confirming the quality of our
annotation. We manually removed or corrected
wrong annotations and got 999 labeled texts, which
became our ground truth test data. We built train-
ing/validation sets for 10-fold cross-validation with
the rest of the labeled texts, to separate them from
our test data.

3.2 Fine-tuning Pre-trained Language Models

for Token Classification

For the fine-tuning setup, we consider the extrac-
tion of definienda as a token-level classification
problem: given a text block, the classifier labels
each token as B-MATH_TERM, I-MATH_TERM
or O. We used the implementation for token classifi-
cation RobertaForTokenClassification in the trans-
formers package (Wolf et al., 2020). It loads a
pre-trained language model and adds a linear layer
on top of the token representation output. We ex-
perimented with an out-of-the-box and general lan-
guage model Roberta-base (Liu et al., 2019) and a
domain-specific model cc_math_roberta (Mishra
et al., 2023). Since Mishra et al. (2023) do not
report performance on token-level tasks, we used

4https://github.com/phfaist/pylatexenc

two checkpoints of it, one pretained for 1 epoch
(denoted as cc_ep01)5, and another pre-trained for
10 epochs (denoted as cc_ep10)6. Then we fed the
10 train/validation sets to train the linear layer to
predict the probability of a token’s representation
matching one of the three labels. We set the max-
imum sequence length of the model to 256. We
ran all our experiments with a fixed learning rate of
5 · 10−5 and a fixed batch size of 16. We searched
the best number of epochs among [3, 5, 10]. We
also experimented with 1 024, 2 048, and 10 240
samples from each training set to see the perfor-
mance of the classifiers with low resources. As
Roberta-base and cc_math_roberta have their own
tokenizers, the models’ output loss and accuracy
are based on different numbers of word pieces and
are not comparable. To evaluate the predictions,
we used the predicted tag of the first word piece of
each word and regrouped the IOB2-tagged word
into definienda. We present our unified evaluation
over ground truth data in Section 4.

3.3 Querying GPT

Driven by the growing popularity of few-shot learn-
ing with pre-trained language models (Brown et al.,
2020), we also query the GPT language model,
using different available versions: we first exper-
imented with ChatGPT7 (based on GPT 3.5) and
then used the API versions of GPT-3.5-Turbo and
GPT-4. We initially gave ChatGPT only one ex-
ample in our question and attempted to obtain a
IOB2-compliant output. We quickly realized that
the returned tagging was random, unstable, and
incoherent with the expected terms. However, if
we ask ChatGPT to return the definienda directly,
we get more pertinent results. We thus asked GPT-
3.5-Turbo and GPT-4 to identify the definienda in
our ground truth data via OpenAI’s API. For each
request, we send the same task description (system
input) and a text from our test data (user input). We
fixed the max output length to 128 and temperature
to 0. By the time of writing, the cost of these API
are count by tokens – GPT-4 8K context model’s
input and output token prices are 20 and 30 times
that of GPT-3.5 4K context model. Since GPT-4
tend to give more precise and shorter responses,

5https://huggingface.co/InriaValda/cc_

math_roberta_ep01

6https://huggingface.co/InriaValda/cc_

math_roberta_ep10

7An

example

of

our

conversation:

https://chat.openai.com/share/
c96b156f-cba1-4804-8f19-1622a9bc564e

the cost of GPT-4 on our task is roughly 20 times
that of GPT-3.5. For our test, we spent $0.42 on
GPT-3.5 and $7.80 on GPT-4.

4 Evaluation

Now that we got the predictions from our fine-tuned
token classifiers and the answers from GPT models,
we compared them with ground truth data. We first
removed the repeated expected definienda for each
annotated text and got 1 552 unique definienda in
total. Then we converted both expected terms and
extracted terms to lowercase. For each unique ex-
pected term, if it is the same as an extracted term,
we counted one “True Positive”. We counted one
“Cut Off” if it contains an extracted term. If it is
contained in an extracted term, we counted one
“Too Long”. Finally, we removed all spaces in
the expected term to make an expected no-space
string, and we joined all extracted terms to make
an extracted no-space string; if the extracted no-
space string contains the expected no-space string,
we considered that the expected term is extracted
as one “True Positive or Split Term”. We calcu-
lated the precision, recall, and F1-score using the
“True Positive or Split Term” count to have a higher
tolerance for boundary errors on all models. Ta-
ble 1 shows the results of GPT’s answers. Tables 2
and 3 present the averaged performance of cc_ep01,
cc_ep10 and Roberta over 10-fold cross-validation.
We set the best precision, recall, and F1-scores in
bold across these three tables.

Our first remark is the high recall of GPTs’ an-
swers. Indeed, GPT models, especially GPT-3.5,
tend to return everything in the given text, resulting
in poor precision. After checking the outputs over
the 1024 test data, we found an over-prediction
of formulas and mathematical expressions, which
corresponds to the analysis by (Kang et al., 2020).
Our second remark is that fine-tuned classi-
fiers have more balanced precision and recall, as
the numbers of extracted terms are closer to the
expected number (1 552). To our surprise, al-
though the tokenizer of cc_math_roberta models
produced fewer word pieces than Roberta’s tok-
enizer, Roberta-base yielded the best performance
among the three models in our task, regardless the
size of the training set. Moreover, cc_math_roberta
models’ performance varies more than Roberta’s
(see in Table 4), showing that cc_math_roberta
models are less robust to different input data.

In all the setups, cc_ep01 was always the worst

Model

GPT-3.5 GPT-4

Extracted
True Positive
TP+Split Term
Too Long
Cut Off
Precision
Recall
F1

6867
1072
1315
379
656
0.1929
0.8312
0.3131

2245
942
1383
595
138
0.6248
0.8821
0.7315

Table 1: Performance comparison of extraction by GPT
models. The huge number of extracted terms results in
the poor precision of GPT-3.5 model.

Model

cc_ep01

cc_ep10

Rob.

Extracted
True positive
TP+Split Term
Too Long
Cut Off
Precision
Recall
F1

2093.0
514.9
693.8
170.2
522.6
0.354
0.447
0.383

1710.8
881.2
1056.5
209.1
405.2
0.623
0.681
0.647

1764.2
934.2
1127.5
268.8
326.1
0.646
0.726
0.679

Table 2: Averaged performance of fine-tuned models,
with 2048 training data.

for our task, implying the benefit of pre-training.
The performances of all fine-tuned models im-
prove significantly as the training set size increases.
When given 10 240 training data, fine-tuning a pre-
trained model gives better overall predictions than
GPT-4, and when given 2048 training data, fine-
tuned Roberta-base already gives better precision
than GPT-4.

Finally, note that these finetuned language mod-
els are obviously much less computationally expen-
sive than OpenAI’s GPT models.

5 Conclusion

In this work, we have contributed to the efficient
creation of a labeled dataset for definiendum ex-
traction from mathematical papers. We have then
compared two usages of transformers: asking GPT
vs fine-tuning pre-trained language models. Our
experimental results show GPT-4’s capacity to un-
derstand mathematical texts with only one example
in the prompt. We highlight the good precision–
recall balance and the relatively low cost of fine-
tuning Roberta for this domain-specific information

Model

cc_ep01

cc_ep10

Rob.

Extracted
True positive
TP+Split Term
Too Long
Cut Off
Precision
Recall
F1

1775.2
540.3
733.9
143.5
509.6
0.420
0.473
0.442

1779.2
972.6
1152.5
201.3
438.2
0.652
0.743
0.692

1770.5
1082.6
1232
233.7
274.1
0.697
0.794
0.742

Table 3: Averaged performance of fine-tuned models,
with 10 240 training data samples

Model

cc_ep01

cc_ep10 Rob.

2048
10240

0.044
0.043

0.052
0.026

0.031
0.011

Table 4: The standard deviation of the F1 score of dif-
ferent fine-tuned models, with 2048 and with 10 240
training data samples

extraction task. A constraint of our work comes
from the nature of our labeled data because authors
have their own writing styles: there could be more
than one correct annotation for a phrase. For in-
stance, our definition blocks are compiled from
LATEX sources, and we plan to test our fine-tuned
models on definitions extracted from real PDF
format papers without LATEX sources. Pluvinage
(2020) proposes sentence-level classification and
text segmentation to retrieve mathematical results
from PDF and can provide a preliminary test set for
us. For future work, we will explore the ambigui-
ties of extracted entities and link them to classes.
Our experience with cc_math_roberta models also
open up research about improving the robustness
over different NLP tasks of from-scratched domain-
specific language models.

Acknowledgments

This work was funded in part by the French gov-
ernment under management of Agence Nationale
de la Recherche as part of the “Investissements
d’avenir” program, reference ANR-19-P3IA-0001
(PRAIRIE 3IA Institute).

References

Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 3615–3620.

Luis Berlioz. 2021. ArGoT: A Glossary of Terms ex-
tracted from the arXiv. Electronic Proceedings in
Theoretical Computer Science, 342:14–21.

Luis Berlioz. 2023. Hierarchical Representations from
Large Mathematical Corpora. Ph.D. thesis, Univer-
sity of Pittsburgh.

Tom B. Brown et al. 2020. Language Models are Few-

Shot Learners. ArXiv:2005.14165 [cs].

Claudia Fevola and Christiane Görgen. 2022. The math-
ematical research-data repository mathrepo. arXiv
preprint arXiv:2202.04022.

Andrew Head et al. 2021. Augmenting scientific pa-
pers with just-in-time, position-sensitive definitions
of terms and symbols. In Proceedings of the 2021
CHI Conference on Human Factors in Computing
Systems, pages 1–18.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Dongyeop Kang et al. 2020. Document-Level Def-
inition Detection in Scholarly Documents: Exist-
ing Models, Error Analyses, and Future Directions.
ArXiv:2010.05129 [cs].

Michael Kohlhase. 2008. Using LATEXas a semantic
markup format. Mathematics in Computer Science,
2(2):279–304.

Yinhan Liu et al. 2019. Roberta: A robustly opti-
mized bert pretraining approach. arXiv preprint
arXiv:1907.11692.

Shrey Mishra, Antoine Gauquier, and Pierre Senellart.
2023. Multimodal machine learning for extraction of
theorems and proofs in the scientific literature. arXiv
preprint arXiv:2307.09047.

Shrey Mishra, Lucas Pluvinage, and Pierre Senellart.
2021. Towards extraction of theorems and proofs in
scholarly articles. In Proceedings of the 21st ACM
Symposium on Document Engineering, pages 1–4,
Limerick Ireland. ACM.

Gábor P. Nagy. 2013. Linear groups as right multipli-
cation groups of quasifields. Designs, Codes and
Cryptography, 72(1):153–164.

Lucas Pluvinage. 2020. Extracting scientific results
from research articles. Master’s thesis, Ecole Nor-
male Supérieure (ENS).

Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:
A pretrained language model for scientific text. In
Proceedings of the 2019 Conference on Empirical

Ashish Vaswani et al. 2017. Attention is all you need.
Advances in neural information processing systems,
30.

Sean Welleck et al. 2021. Naturalproofs: Mathematical
theorem proving in natural language. In Thirty-fifth
Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 1).

Thomas Wolf et al. 2020. Transformers: State-of-the-
art natural language processing. In Proceedings of
the 2020 conference on empirical methods in natural
language processing: system demonstrations, pages
38–45.

