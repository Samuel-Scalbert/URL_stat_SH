Enumeration for FO Queries over Nowhere Dense
Graphs
Nicole Schweikardt, Luc Segoufin, Alexandre Vigny

To cite this version:

Nicole Schweikardt, Luc Segoufin, Alexandre Vigny. Enumeration for FO Queries over Nowhere Dense
Graphs. Journal of the ACM (JACM), 2022, 69 (3), pp.1-37. ￿10.1145/3517035￿. ￿hal-03809754￿

HAL Id: hal-03809754

https://inria.hal.science/hal-03809754

Submitted on 11 Oct 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Enumeration for FO Queries over Nowhere Dense Graphs

Nicole Schweikardt, Humboldt-Universität zu Berlin
Luc Segoufin, INRIA and ENS Paris
Alexandre Vigny, Universität Bremen

Abstract

We consider the evaluation of first-order queries over classes of databases that are nowhere dense.
The notion of nowhere dense classes was introduced by Nešetřil and Ossona de Mendez as a formaliza-
tion of classes of “sparse” graphs and generalizes many well-known classes of graphs, such as classes
of bounded degree, bounded tree-width, or bounded expansion.

It has recently been shown by Grohe, Kreutzer, and Siebertz that over nowhere dense classes of
databases, first-order sentences can be evaluated in pseudo-linear time (pseudo-linear time means that
for all (cid:15) there exists an algorithm working in time O(n1+(cid:15)), where n is the size of the database).

For first-order queries of higher arities, we show that over any nowhere dense class of databases, the
set of their solutions can be enumerated with constant delay after a pseudo-linear time preprocessing.
In the same context, we also show that after a pseudo-linear time preprocessing we can, on input of a
tuple, test in constant time whether it is a solution to the query.

1 Introduction

Query evaluation is one of the most central tasks of a database system, and a vast amount of literature
is devoted to the complexity of this problem. Given a database D and a query q, the goal is to compute
the set q(D) of all solutions for q over D. Unfortunately, the set q(D) might be much bigger than the
database itself, as the number of solutions may be exponential in the arity of the query. It can therefore
be insufficient to measure the complexity of answering q on D only in terms of the total time needed to
compute the complete result set q(D). One can imagine many scenarios to overcome this situation. We
could for instance only want to compute the number of solutions or just compute the k most relevant
solutions relative to some ranking function.

We consider here the complexity of the enumeration of the set q(D), i.e., generating one by one all
the solutions for q on D. In this context two parameters play an important role. The first one is the
preprocessing time, i.e. the time it takes to produce the first solution. The second one is the delay, i.e. the
maximum time between the output of any two consecutive solutions. An enumeration algorithm is then
said to be efficient if these two parameters are small. For the delay we aim at constant time: depending
only on the query and independent from the size of the database. For the preprocessing time an ideal
goal would be linear time: linear in the size of the database with a constant factor depending on the
query. When both are achieved we say that the query can be enumerated with constant delay after linear
preprocessing.

Constant delay enumeration after linear preprocessing cannot be achieved for all queries over all
databases (this is known modulo an assumption in parameterized complexity theory, since the evaluation
of boolean FO queries is AW[∗]-complete [9]). But for restricted classes of queries and databases, several
efficient enumeration algorithms have been obtained. This is the case for instance for free-connex acycl-
ic conjunctive queries over arbitrary databases [4], first-order (FO) queries over classes of databases of
bounded degree [10, 20], monadic second-order (MSO) queries over classes of databases of bounded tree-
width [3, 22], and FO queries over classes of databases of bounded expansion [21].

This is the extended version of the conference contribution [29].

1

In some scenarios only pseudo-linear preprocessing time has been achieved. A query can be enu-
merated with constant delay after pseudo-linear preprocessing if for all (cid:15) there exists an enumeration
procedure with constant delay (the constant may depend on (cid:15)) and preprocessing time in O(||D||1+(cid:15)),
where ||D|| denotes the size of the database. This has been achieved for FO queries over classes of data-
bases of low degree [11] or of local bounded expansion [30].

A special case of enumeration is when the query is boolean. In this case the preprocessing computes
the answer to the query. In order to be able to enumerate queries of a given language efficiently, it is
therefore necessary to be able to solve the boolean case efficiently.

It has been shown recently that boolean FO queries can be evaluated in pseudo-linear time over no-
where dense classes of databases [17]. The notion of nowhere dense classes was introduced in [26] as a
formalization of classes of “sparse” graphs and generalizes all the classes mentioned above [27] (except
for the classes of low degree of [11]). Among classes of databases that are closed under subdatabases,
the nowhere dense classes are the largest possible classes enjoying efficient evaluation of FO queries [24]
(modulo an assumption in parameterized complexity theory). It has also been shown that over nowhere
dense classes of databases, counting the number of solutions to a given FO query can be achieved in
pseudo-linear time [18].

In this paper we show that enumeration of FO queries on nowhere dense classes of data-
Main result
bases can be done with constant delay after pseudo-linear preprocessing. This completes the picture of
the complexity of FO query evaluation on nowhere dense classes and, due to the above mentioned result
of [24], on all classes that are closed under subdatabases. We also show that for any nowhere dense class
of databases, given a FO query q and a database D in the class, after a pseudo-linear time preprocessing
we can test in constant time whether an arbitrary input tuple belongs to the result set q(D).

Proof method Our algorithms for enumerating and testing are based on the following ingredients. In-
stead of Gaifman’s normal form (which usually serves as a starting point for algorithmic meta-theorems)
we use a normal form provided by [18]. This normal form works efficiently only in the nowhere dense
case and requires using explicit distance predicates in the formulas. However, it has the advantage of
controlling the quantifier-rank of the local formulas. Towards evaluating local formulas we use the result
that one can compute in pseudo-linear time a representative “cover” of the database by means of neigh-
borhoods [17]. We also make use of the game characterization of nowhere dense classes [17] showing
that any neighborhood can be decomposed in finitely many steps. Then, a local formula is evaluated
within a neighborhood of the cover by induction on the number of remaining steps in the game until the
neighborhood is trivial. The enumeration for a combination of local formulas is then done following a
scheme already present in [30]: Enumeration is supported by precomputing pointers that allow to jump
from one solution to the next one. We use a pointer mechanism similar to the one used in [30] for the
local bounded expansion case.

Technical challenges Constant delay enumeration after pseudo-linear preprocessing was already achieved
for FO queries over classes of databases having bounded expansion [21] or local bounded expansion [30].
The nowhere dense case is significantly harder. The bounded expansion case was solved using a quanti-
fier elimination procedure reducing all FO queries to the quantifier-free ones. It seems unlikely that such
a quantifier elimination procedure exists in the nowhere dense case. Therefore, as for databases with local
bounded expansion, the nowhere dense case relies on locality arguments and the neighborhood covers
needed for solving the boolean case [17]. This was enough for databases with local bounded expansion,
as the neighborhoods had bounded expansion and FO queries could then be evaluated on them. For the
nowhere dense case we need a significantly more complicated argument using further tools: the game
characterization of [17] and the Rank-Preserving Normal Form Theorem of [18].

Additional material The main results of this article have been published first in a conference pa-
per [29]. The two biggest changes are the following:

2

• An explicit storing theorem. In the conference version of that paper and in most related work, the
use of the memory is either not optimal or not explicitly mentioned. In Section 3 we provide a
careful analysis of the amount of memory a RAM uses to store partial functions.

• A stronger statement. The proofs that FO queries can be efficiently tested and enumerated over
nowhere dense classes of graphs are replaced by a proof of a more powerful result: Our main
contribution, Theorem 2.3, states that after a pseudo-linear preprocessing, upon input of any tuple,
we can compute in constant time the smallest next solution. This generalizes the testing problem
(Corollary 2.4) and the enumeration problem (Corollary 2.5).

Organization The rest of the paper is structured as follows. Section 2 provides basic notations. Sec-
tion 3 is devoted to the specific task of efficiently storing and retrieving the values of functions. Section 4
introduces some needed tools and gives an overview of our proof by proving a weaker but insightful
result. Section 5 presents our main algorithm, and Section 6 concludes the paper.

2 Preliminaries and main result

By N we denote the non-negative integers, and we let N≥1 := N \ {0}. By Q>0 we denote the set of
positive rationals. For m, n ∈ N we let [m, n] := {i ∈ N | m ≤ i ≤ n}, and we let [m] := [0, m−1].

Throughout this paper, (cid:15) will always be a positive real number, and (cid:96), r, s, i, j, k will be elements of

N. For a tuple x of arity k, we will write xi to denote its i-th component (for i ∈ [1, k]).

Structures and first-order queries. A relational schema is a finite set of relation symbols, each having
an associated arity. A finite relational structure A over a relational schema consists of a finite set, the
domain of A, together with an interpretation of each relation symbol R of arity k of the schema as a
k-ary relation over the domain, denoted R(A). A database is a finite relational structure.

A structure B is a substructure of A if the domain of B is included in the domain of A and each relation
of B is included in the corresponding relation of A. We say that a class C of structures (or databases) is
closed under substructures (or subdatabases) if for every structure A in C and every substructure B of A
we have that B is in C.

If A is a structure with domain A and B ⊆ A is a subset of its domain, we denote by A[B] the
substructure of A induced by B, i.e., A[B] is the structure B with domain B and R(B) = R(A) ∩ Bk for
each relation symbol R of arity k.

Let σ and σ(cid:48) be relational schemas with σ ⊆ σ(cid:48), and let A be a structure of schema σ (for short: a
σ-structure). A σ(cid:48)-expansion of A is a σ(cid:48)-structure B whose domain is identical to the domain of A and
which satisfies R(B) = R(A) for all R ∈ σ.

We fix a standard encoding of structures as input, see for example [1]. We denote by ||A|| the size of
(the encoding of) A, while |A| denotes the size |A| of its domain. Without loss of generality we assume
If not, we arbitrarily choose one, for instance the one
that the domain A comes with a linear order.
induced by the encoding of A. This order induces a lexicographical order among the tuples over A.

A query is a first-order formula. We assume familiarity with first-order logic, FO, over relational
structures (cf., e.g., [1, 25]). We use standard syntax and semantics for FO. In particular we write q(x)
to denote the fact that the free variables of the query q are exactly the variables in x. The length of x is
called the arity of the query. The size of a query q is the number of symbols needed to write down the
formula and is denoted by |q|.

For a structure A, a query q(x) and a tuple a of elements of A of the appropriate arity, we write
A |= q(a) to indicate that a is a solution for q over A. We write q(A) to denote the set of tuples a such
that A |= q(a).

A sentence is a formula with no free variables, i.e., of arity 0. It is either true or false over a structure
and therefore defines a property of structures, i.e., a boolean query. Given a relational structure A and
a sentence q, the problem of testing whether A |= q is called the model checking problem. Often, the
problem is restricted to a particular class C of relational structures.

3

Model of computation and complexity. As usual when dealing with linear time, we use Random
Access Machines (RAM) with addition, multiplication, and uniform cost measure as a model of computa-
tion.

All problems encountered in this paper have two inputs: a structure A and a query q(x). However,
they play different roles as ||A|| is often very large while |q| is generally small. We adopt the data complex-
ity point of view [33]. When we say linear time we mean in time O(||A||), the constants hidden behind
the “big O” depending on q, on the class C of structures under investigation, and possibly on further
parameters that will be clear from the context. We say that a problem is solvable in pseudo-linear time if,
for all (cid:15) > 0, it can be solved in time O(||A||1+(cid:15)). In this case, the constant factor also depends on (cid:15). If a
subroutine of a procedure depending on (cid:15) produces an output of size O(||A||(cid:15)) we will say that the output
is pseudo-constant.

Fix a structure A of domain A. The Gaifman graph of A is the undi-
Distance and neighborhoods.
rected graph whose set of vertices is A and whose edges are the pairs {a, b} such that a and b occur in a
tuple of some relation of A. Given two elements a and b of A, the distance between a and b is the length
of a shortest path between a and b in the Gaifman graph of A. The notion of distance extends to tuples
in the usual way, i.e., the distance between two tuples a and b is the minimum of the distances between
ai and bj over all i, j.

For a positive integer r, we write N A
The r-neighborhood of a in A, denoted N A
for a tuple a of arity k we let N A
r (a).
induced by N A

r (a) := (cid:83)

r (a) for the set of all elements of A at distance at most r from a.
r (a). Similarly,
r (a) as the substructure of A

r (a), is the substructure of A induced by N A
i∈[1,k] N A

r (ai), and we define N A

For an undirected graph G = (V, E) we let |G| = |V |
Nowhere dense classes of undirected graphs.
and ||G|| = |V | + |E|. Thus, similarly as for databases, |G| denotes the size of the graph’s domain, and
||G|| is the size of a reasonable encoding of G.

Given two undirected graphs G and H and an integer r, the graph H is said to be a shallow minor at
depth r of G (see [27, Section 2.1]) if H can be obtained from G by removing edges, removing vertices,
and contracting into a single vertex sets of vertices of radius at most r. A class C of undirected graphs
is nowhere dense if for every integer r there is an integer N such that no graph of C contains the clique
with N elements as a shallow minor at depth r [27].

This is a robust notion, and there exist many equivalent definitions [27, 26, 17]. In particular, [17]
presented a game characterization of nowhere dense classes that we rely on in our Sections 4 and 5 (see
Definition 4.5 and Theorem 4.6). A further, equivalent characterization is based on the following notion
of weak r-accessibility. Given an undirected graph G with a linear order on its vertices, and two of its
vertices a, b, we say that b is weakly r-accessible from a if there exists a path of length at most r between
a and b such that b is smaller than a and all other vertices on the path. A class C of undirected graphs is
nowhere dense if, and only if, for all r and (cid:15), there is a number N , such that for all graphs G of C with
|G| > N , there is a linear order on the vertices of G, such that for all vertices a of G, the number of
vertices weakly r-accessible from a is bounded by |G|(cid:15) [27].

It is known, for example, that any class of graphs that (locally) excludes a minor or that is of (local)

bounded expansion, is nowhere dense. Nowhere dense classes are “sparse” in the following sense:

Theorem 2.1 (Derived from [27, Theorem 4.1 (iv)]). For every nowhere dense class C of undirected graphs
there is a function fC such that for every (cid:15) > 0 and every G in C, if |G| ≥ fC((cid:15)), then ||G|| ≤ |G|1+(cid:15).

In the special case where for all integers r there is a constant cr such that for all graphs G of C there
is a linear order on the vertices of G, such that for all vertices a of G the number of vertices weakly
r-accessible from a is bounded by cr, the class C is said to have bounded expansion. These constants
cr were the keys to the constant delay enumeration algorithm for FO queries over classes of bounded
expansion [21]. As we no longer have them in the nowhere dense case, we need a different strategy.

In the above characterization of nowhere dense classes via the notion of weak r-accessibility, we do
not require the number N to be computable from the parameters r and (cid:15). When N is computable from
r and (cid:15) we say that the class is effectively nowhere dense. Most of the classical nowhere dense classes of

4

graphs, like bounded treewidth, planar graphs etc. are in fact effectively nowhere dense. Our results will
show that if C is a nowhere dense class of graphs then for all (cid:15) > 0 there is an algorithm depending on
(cid:15) satisfying some properties. If C is furthermore effectively nowhere dense, all our algorithms will be
computable from (cid:15), hence providing a generic variant of our results.

From databases to colored graphs. We define c-colored graphs as structures over the schema σc :=
{E, C1, . . . , Cc} where E is a binary symmetrical relation and (Ci)i≤c are unary relations. A colored
graph is a c-colored graph for some integer c.

A class C of colored graphs is defined to be nowhere dense if the class C(cid:48) consisting of the underlying

undirected graphs of all elements of C is nowhere dense.

Our main enumeration algorithm works for FO-queries on nowhere dense classes of colored graphs.

By using standard techniques, this extends to all relational structures, as we now explain.

Given a database D over a schema σ, we define its adjacency graph A(D) as the relational structure
whose domain is D ∪ T where D is the domain of D and T is the set of tuples occurring in a relation
of D. We have one unary relation PR per relation R of σ containing all tuples t of R(D). We have k
(symmetrical) binary relations E1, . . . , Ek where k is the maximal arity of relations in σ. For a ∈ D and
t ∈ T , we have A(D) |= Ei(a, t) if and only if the element a is the ith element of the tuple t. Currently,
A(D) is an undirected graph with colored vertices and colored edges. With what is following, we can
build a colored graph in the sense of c-colored graph.

The colored graph version A(cid:48)(D) of the adjacency graph A(D) is defined as follows.
The colors of A(cid:48)(D) are the “vertex colors” PR of A(D) and k further colors (Ci)i≤k, where k is
the maximal arity of the relations in σ. The domain of A(cid:48)(D) is the domain of A(D) plus one node
per edge of A(D): For every Ei-edge (a, t) of A(D), we add in A(cid:48)(D) a new node v of color Ci and
such that (a, v) and (v, t) are E-edges in A(cid:48)(D).
In particular, A(cid:48)(D) is a colored graph of schema
{E, C1, . . . , Ck, PR1, . . . , PRm} if D is a database of schema σ = {R1, . . . , Rm} consisting of m re-
lations of maximum arity k. Henceforth, we will identify the schema of A(cid:48)(D) with the schema σc of
c-colored graphs for c = k + m.

The following lemma is classical and reduces relational structures to colored graphs.

Lemma 2.2. Let ϕ(x) be a FO query over some schema σ and let k be the maximal arity of the relations of
σ. In time linear in the size of ϕ, we can compute a FO query ψ(x) over σc, for c = k + |σ|, such that for
every database D over σ, ϕ(D) = ψ(A(cid:48)(D)).

The lemma is an immediate consequence of the fact that

D |= R(a1, . . . , aj) ⇐⇒

A(cid:48)(D) |= ∃t(cid:0)PR(t) ∧

(cid:94)

i≤j

∃z(cid:0)Ci(z) ∧ E(ai, z) ∧ E(z, t)(cid:1)(cid:1).

Let C be a class of relational structures. We say that C is nowhere dense if the class {A(cid:48)(D) | D ∈ C}
is nowhere dense. Note that because A(cid:48)(D) is a 1-subdivision (i.e. an edge is transformed into a path of
length 2) of A(D), the class {A(cid:48)(D) | D ∈ C} is nowhere dense iff the class consisting of the Gaifman
graphs of A(D) for all D ∈ C is nowhere dense; see [27] for further details.

We could have used another definition for nowhere dense classes of structures, using their Gaifman
graphs instead of their adjacency graphs. For a fixed schema this would result in the same notion [34,
Theorem 4.3.6], but when the schema is not fixed our definition is more general [19, Example 3.3.2].

A consequence of Lemma 2.2 is that the model checking, enumeration, counting, and testing problems
for FO-queries reduce to the colored graph case. In the remaining part of the paper we will therefore only
consider classes of colored graphs. The reader should keep in mind, though, that the results stated over
colored graphs extend to relational structures.

From the definition it follows immediately that if a class of graphs is nowhere dense then the class of
all its substructures is also nowhere dense. Moreover, the class of all its possible colorings is also nowhere
dense. Hence without loss of generality, we can assume that all nowhere dense classes C of colored graphs
considered from now on, are closed under substructures and all possible colorings (using arbitrarily many
colors).

5

Enumeration. An enumeration algorithm for a colored graph G and a query q is divided into two
consecutive phases:

• a preprocessing phase, and

• an enumeration phase, outputting one by one and without repetition the elements of the set q(G).

The preprocessing time of the enumeration algorithm is the time taken by the preprocessing phase. Its
delay is the maximum time between any two consecutive outputs. One can view an enumeration al-
gorithm as a compression algorithm that computes a representation of q(G), together with a streaming
decompression algorithm.

We aim for enumeration algorithms with constant delay and pseudo-linear preprocessing time. By
this we mean that for all (cid:15) > 0, there is a preprocessing phase working in time O(||G||1+(cid:15)) and an
enumeration phase with constant delay. Note that the multiplicative constants for the preprocessing
phase and the delay may depend on q and (cid:15).

An enumeration phase with constant delay may use a constant amount of memory while preparing
the next output. Hence it may use a total amount of memory that is linear in the output size. Our
enumeration algorithms will have the property that, apart from the memory used for storing the data
structure (of pseudo-linear size) that is produced during the preprocessing phase, the entire enumeration
phase only uses a constant amount of extra memory. In other words, our enumeration algorithm can be
seen as a finite state automaton running on the index structure produced by the preprocessing phase.

All our enumeration procedures will output their tuples in lexicographical order. We will see that this

is useful for queries in disjunctive normal form.

Our main result. We now state our main theorem. Recall that we assume a linear order on the domain
of our structures. This order induces a lexicographical order on tuples of elements that we denote by ≤.

Theorem 2.3. Let C be a nowhere dense class of colored graphs. There is a function f and an algorithm
which, upon input of a colored graph G ∈ C, an (cid:15) ∈ Q>0, and a first-order query q, performs a preprocessing
in time f (q, (cid:15)) · |G|1+(cid:15) such that afterwards, upon input of a tuple a, we can compute in time f (q, (cid:15)) the
smallest (in lexicographical order) tuple a(cid:48) such that a(cid:48) ≥ a and a(cid:48) ∈ q(G), or an error message in case that
no such tuple a(cid:48) exists.

Furthermore, if C is effectively nowhere dense, then f is computable.

This result both implies a constant delay enumeration and a constant time testing procedures. Testing
whether a tuple is a solution is immediate from the data structure computed in Theorem 2.3, as it is enough
to test on input a whether the output tuple a(cid:48) is equal to a. Thus, we obtain:

Corollary 2.4 (Testing solutions). Let C be a nowhere dense class of colored graphs. There is a function
f and an algorithm which, upon input of a colored graph G ∈ C, an (cid:15) ∈ Q>0, and a first-order query q,
performs a preprocessing in time f (q, (cid:15)) · |G|1+(cid:15), such that afterwards, upon input of a tuple a, we can test
whether G |= ϕ(a) in time f (q, (cid:15)).

Furthermore, if C is effectively nowhere dense, then f is computable.

The data structure of Theorem 2.3 also allows to enumerate the solutions with constant delay. Indeed,
once we have outputted a solution a, we derive in constant time the tuple b which immediately follows
(cid:48) given by Theorem 2.3 (upon input of b) is the next
a in the lexicographical order, and then the tuple b
solution to be outputted during the enumeration phase.

Corollary 2.5 (Enumerating solutions). Let C be a nowhere dense class of colored graphs. There is a function
f and an algorithm which, upon input of a colored graph G ∈ C, an (cid:15) ∈ Q>0, and a first-order query q,
performs a preprocessing in time f (q, (cid:15)) · |G|1+(cid:15), such that afterwards, the set of solutions ϕ(G) can be
enumerated with delay f (q, (cid:15)) in increasing order.

Again, if C is effectively nowhere dense, then f is computable.

6

3 Storing functions and retrieving solutions

This section is devoted to a technical result that uses in an essential way our computational model. We
will often compute partial k-ary functions associating a value to a tuple of nodes of the input graph. Such
functions can be easily implemented in the RAM model using k-dimensional cubes allowing to retrieve
the value of f in constant time. This requires a memory usage of O(nk). However our functions will have
a domain of size pseudo-linear and can be computed in pseudo-linear time. The following theorem states
that we can use the RAM model to build a data structure that stores our functions in a more efficient way.

Theorem 3.1 (Storing Theorem). For every fixed k ∈ N and (cid:15) > 0, there is an integer c ∈ N such that
for every integer n ∈ N there is a data structure that stores the value of a k-ary function f of domain
Dom(f ) ⊆ [n]k with:

• initialization time c · |Dom(f )| · n(cid:15),

• update time c · n(cid:15) whenever a pair1 (a, b) is added to or removed from f ,

• lookup time c,

• and at any point in time, the space used by the data structure is c · |Dom(f )| · n(cid:15).

Here, lookup means that given a tuple a ∈ [n]k, the algorithm either answers b if a ∈ Dom(f ) and f (a) = b;
or a(cid:48) if a (cid:54)∈ Dom(f ) and a(cid:48) := min{x ∈ Dom(f ) : x > a}; or Null if no such tuple exists.

We stress that during the lookup procedure, the data structure may be given a tuple a that is not part
of the domain of the function. This will heavily be used in later sections. However, other perks of the
statement will not be used. For example, we will only construct the data structure when computing f
and then only use the lookup feature. In particular, we will never delete any tuple from the domain of
f . Nonetheless, this result is interesting in its own. A version without updates of that theorem has been
written first in [34, Section 4.1] and is inspired from [32, Figure 1].

The data structure is not that complicated. In fact its core is a trie where each pair (key,value) is a tuple
a in the domain of f and its image b = f (a). See [23, Section 6.3 Digital Searching] for more information.
In order to obtain all our features, our data structure expands the trie structure in the two following ways.
First, we have a backward relation from last child to parent. This relation helps us navigate the structure
for the update feature. Second, we have a relation linking search paths that do not lead to a key, to the
next smallest key. This relation is used for the lookup feature.

In the rest of the section, we describe the data structure together with examples. We then give an
intuition of how to obtain the desired algorithmic features. As this result is not our main contribution,
we wrote the actual proofs in a separated appendix.

3.1 Description of the data structure
Fix (cid:15) and n. Let d := (cid:100)n(cid:15)(cid:101) and h := (cid:6) 1
that x ≤ y.

(cid:15)

(cid:7). As usual, for x ∈ Q, (cid:100)x(cid:101) denotes the smallest integer y such

Every i ∈ [n] can be uniquely decomposed in base d into a string of length h whose letters are from
[0, d−1] since dh ≥ n. We arbitrarily assume that the string starts with the higher powers of d and ends
with the lowest ones. Given all this, every tuple in [n]k can be decomposed into a string of length kh
whose letters are from [0, d−1]. We then associate to the function f a partial tree T (f ) of maximal depth
kh and degree d, where each node has 0 or d children and each leaf at depth kh represents an element of
the domain of f (by looking at the sequence of child numbers in the path from the root to that leaf). The
size of T (f ) is then O(n(cid:15)·|Dom(f )|).

Our data structure is an encoding of T (f ) with extra information in order to navigate efficiently in the
tree and to update it efficiently. As for leaves, to any node of T (f ) at depth i we can associate a string over
[0, d−1] of length i. Given a leaf of T (f ) we associate a tuple b as the smallest tuple (in lexicographical
order) of the domain of f whose encoding has a prefix larger than the one of the current node.

1we identify f with its graph {(a, b) | a ∈ Dom(f ), f (a) = b}

7

Each inner node of the tree associated to f is represented by d+1 consecutive registers in our memory
each containing a pair (δ, r) where δ is either 0, 1 or −1 and r is a value that will help us navigating in
the tree.

Consider an inner node x of T (f ) and assume that x is the ith child of y. Let R be the ith register
representing y and R(cid:48) be the first register representing x. Then the content of R is (1, R(cid:48)) and the content
of the last register representing x contains (−1, R). This encodes the parent/child relation of T (f ). The
rest of the encoding will help updating the structure efficiently.

If the jth child of y is a leaf, for j ≤ d, then the content of the jth register representing y is (0, b)

where b is the tuple associated to that leaf.

In the case when x is at depth kh−1 (i.e. all its children are leaves), for i ≤ d, we set the content of
the ith register representing x as (1, f (a)) if the ith leaf of x represents a tuple a in the domain of f , and
as (0, b) otherwise where b is the tuple associated to that leaf.

Finally, we have a register R0 that contains the next available (unused) register.
Our data structure is illustrated in Figure 1.

Figure 1: An example of the data structure.
In this example n = 27, (cid:15) = 1/3, therefore d = n(cid:15) =
271/3 = 3 and h = 1/(cid:15) = 3. The unary partial function f is the identity function whose domain is
{2, 4, 5, 19, 24, 25} and is not defined otherwise. Note that the decomposition of 2 in base d = 3 is 002,
while 4 is 011, 5 is 012, 19 is 201 and so on. The leaves corresponding to the domain of f are in red. The
arrows are here for readability. For instance R1 is the first register representing the root node of T (f )
and its content is (1, 5) because the first child of the root node of T (f ) is not a leaf and the first register
representing it is R5. Moreover the second register representing the root node of T (f ) is R2 whose
content is (0, 19) because the second child of the root of T (f ) is a leaf and 19 is the smallest element in
the domain of f whose decomposition starts with a 2. The register R8 is the last register representing
the first child of the root of T (f ). Its content is therefore (−1, 1) because R1 is the first register encoding
the root. Finally R19 is the third register encoding the second child of the first child of the roof of T (f ).
It therefore represents the number encoded by 012, i.e. 5, in the domain of f . Its content is therefore
(1, f (5)) = (1, 5).

R0

29

R1

R2

R3

R4

(1,5)

(0,19)

(1,10)

(-1,Null)

R5

R6

R7

R8

R13 R14 R15 R16

(1,9)

(1,17)

(0,19)

(-1,1)

(1,21)

(0,24)

(1,25)

(-1,3)

R9

R10 R11 R12

R17 R18 R19 R20

R21 R22 R23

R24

R25 R26

R27

R28

(0,2)

(0,2)

(1,2)

(-1,5)

(0,4)

(1,4)

(1,5)

(-1,6)

(0,19)

(1,19)

(0,24)

(-1,13)

(1,24)

(1,25)

(0,Null)

(-1,15)

8

3.2 Proof sketch of Theorem 3.1.
Looking up information in this data structure is pretty straightforward: Given a tuple a, one can de-
compose this tuple into a string of length kh which precisely describes the search path for a in the data
structure T (f ). This search can either fail by reaching a cell of the form (0, b) and therefore conclude
that a is not in the domain of the stored function f and that b is the smallest tuple in Dom(f ) greater
than a. The only other possibility is that the search for a reaches a cell of the form (1, b) implying that
a is in the domain of f , and that f (a) = b. All of this takes time linear in the depth kh of the tree T (f ),
which is constant as (cid:15) is fixed.

Adding or removing information in the data structure is somewhat trickier. Adding a tuple may
require the creation of new arrays of length d in the structure, this is where R0 comes in handy as it
points to the end of the used memory i.e. where we can start new arrays. Removing tuples may render
some array useless (only composed of cells of the form (0, b)). While this is not an issue for the lookup
procedure, such useless array must be deleted otherwise the memory usage will not remain linear. This
is performed by copy/pasting the last array of the structure in the place used by the array we want to get
rid of. We then have to take care of the (only) cell pointing toward the moved array, and update R0. So
far, and in both case, the procedure takes time O(d · k · h). For the main contribution of this paper, we
only use the insert and lookup procedures. The remaining procedures, together with the complete proofs
can be found in the Appendix Section 7.

4 Testing distance queries

In this section we prove a weaker version of our main result where we only consider the testing problem
of distance queries. This requires introducing several tools that will also be needed for proving the main
result.

A distance query is a binary query testing the distance within a colored graph between two nodes.

For all r ∈ N, the query dist≤r(a, b) states that there is a path of length at most r between a and b.

Definition 4.1 (Distance queries). Distance queries can formally be defined by induction as follows:

dist≤0(x, y)
dist≤r+1(x, y)

:= (x = y)
:= dist≤r(x, y) ∨ ∃z (cid:0)E(x, z) ∧ dist≤r(z, y)(cid:1)

A slightly different definition can make the quantifier rank of dist≤r equal to log(r) instead of r

above, but this is not important to us. We will also make use of the following queries:

dist=r(x, y)
dist>r(x, y)

:= dist≤r(x, y) ∧ ¬ dist≤r−1(x, y)
:= ¬ dist≤r(x, y)

Example (1-A). Consider the distance two query:

q(x, y)

:= dist≤2(x, y) = ∃z (cid:0) E(x, z) ∧ E(z, y) (cid:1) ∨ E(x, y) ∨ x = y

In this section, we introduce notions and apply them to this example query.

The rest of the section is devoted to the proof of:

Proposition 4.2. Let C be a nowhere dense class of colored graphs. There is a function f and an algorithm
which, upon input of a colored graph G ∈ C, an (cid:15) ∈ Q>0, and r ∈ N, performs a preprocessing in time
f (r, (cid:15)) · |G|1+(cid:15), such that afterwards, upon input of a tuple (a, b) of nodes of G, we can test in time f (r, (cid:15))
whether (a, b) ∈ dist≤r(G).

Furthermore, if C is effectively nowhere dense, then f is computable.

9

4.1 Tools for nowhere dense graphs
Since we are looking at distance queries, it is tempting to precompute the r-neighborhoods of all nodes.
Unfortunately, this cannot be done in pseudo-linear time as the sum of the sizes of those neighborhoods
might be too big. To overcome this situation we use a neighborhood cover that selects a small but suffi-
ciently representative set of neighborhoods.

This notion was crucial already for obtaining a number of previous algorithmic meta-theorems, in-

cluding e.g. [13, 12, 17].

Definition 4.3 (Neighborhood cover). Given a colored graph G and a number r ∈ N, an r-neighborhood
cover of G is a collection X of subsets of the vertices V of G such that

∀ a ∈ V ∃ X ∈ X with N G

r (a) ⊆ X.

For r, s ∈ N, an (r, s)-neighborhood cover of G is an r-neighborhood cover of G satisfying:

∀ X ∈ X ∃ a ∈ V with X ⊆ N G

s (a).

The number r is called the radius of the neighborhood cover, and the sets X ∈ X are called bags.
The degree δ(X ) of the cover is the maximal number of bags that intersect at a given node, i.e.

δ(X )

:= max
a∈V

|{X ∈ X | a ∈ X}|.

Given a neighborhood cover of radius r of G, for every a ∈ V we arbitrarily fix a bag containing the

r-neighborhood of a and denote it by X (a).

Neighborhood covers with small degree can be computed efficiently on nowhere dense classes:

Theorem 4.4 ([17, Theorem 6.2]). Let C be a nowhere dense class of colored graphs. There is a function fC
and an algorithm that, given an (cid:15) > 0, an r ∈ N, and a colored graph G ∈ C whose domain V is larger than
fC(r, (cid:15)), computes in time fC(r, (cid:15)) · |V |1+(cid:15) an (r, 2r)-neighborhood cover of G with degree at most |V |(cid:15).
Furthermore, if C is effectively nowhere dense, then fC is computable.

In the rest of the paper, and without loss of generality modulo taking the maximum, we will use the
same function fC in order to satisfy the requirements of Theorem 4.4 and Theorem 2.1, such that a graph
with a number of vertices greater than fC(0, ·) satisfies the size bound of Theorem 2.1. Notice that for a
nowhere dense class C of colored graphs, for G ∈ C of domain V , for (cid:15) > 0 and a neighborhood cover X
of G with degree |V |(cid:15), the bound on the degree implies that (cid:80)
|X| ≤ |V |1+(cid:15). As any edge of G can
X∈X

appear in at most |V |(cid:15) distinct induced subgraphs of the form G[X] we have

(cid:107)G[X](cid:107) ≤ O(|V |(cid:15) · ||G||)

(1)

(cid:88)

X∈X

Given Theorem 4.4 and in view of our Storing Theorem 3.1, after some pseudo-linear preprocessing
we are able, given a bag X and a node a, to test whether a ∈ X in constant time and, if a (cid:54)∈ X return
the smallest b, bigger than a, such that b ∈ X. This is achieved as follows: Let n := |V | and identify V
with [n]. Let X = {X0, . . . , Xm−1} be the neighborhood cover. As it is enough to have one bag X per
element of V , we can assume that m := |X | ≤ n. Now we identify X with the partial binary function fX
from [n]2 to {1} with (i, a) ∈ Dom(fX ) iff i < m and a ∈ Xi (for all (i, a) ∈ [n]2). Note that δ(X ) ≤ n(cid:15)
implies that |Dom(fX )| ≤ n1+(cid:15). Therefore, the Storing Theorem 3.1 yields the claimed functionality.

10

Let’s get back to our running example.

Example (1-B). The radius of the relevant cover depends on the query. For our query q from Example (1-A),
we have for all nodes a, b that

G |= q(a, b) ⇐⇒ N G

2 (a) |= q(a, b).

Therefore if we compute a (2, 4)-neighborhood cover X of G, since N G
nodes a and b:

2 (a) ⊆ X (a), we have that for all

G |= q(a, b) ⇐⇒ b ∈ X (a) ∧ G[X (a)] |= q(a, b)

Hence, modulo a pseudo-linear preprocessing, given two elements a, b, to test whether they are at distance

≤ 2, it is enough to restrain our attention to the bag X (a) of a. We will see later how this will be useful.

As illustrated by our running example, we will reduce the problem from the whole graph to a bag of
the cover. The following game characterization of nowhere dense classes of colored graphs will give us
an inductive parameter that will decrease when diving within a bag, ensuring termination.

Definition 4.5 (Splitter game [17, Definition 4.1]). Let G = (V, E) be a graph and let λ, r ∈ N≥1. The
(λ, r)-splitter game on G is played by two players called Connector and Splitter, as follows. We let G0 := G
and V0 := V . In round i+1 of the game, Connector chooses a vertex ci+1 ∈ Vi. Then Splitter picks a
r (ci+1) \ {si+1} = ∅, then Splitter wins the game. Otherwise,
vertex si+1 ∈ N Gi
the game continues with Gi+1 := Gi[Vi+1]. If Splitter has not won after λ rounds, then Connector wins.

r (ci+1). If Vi+1 := N Gi

This is not exactly the same definition as the one in [17], where several nodes can be removed in one
step, but it is easy to show that it is equivalent to it [31]. We say that Splitter wins the (λ, r)-splitter game
on G if she has a winning strategy for the game. Interested readers may want to look at [34, Section 4.2.2]
to find examples of Splitter’s winning strategies for several classes of graphs.

Theorem 4.6 ([17, Theorem 4.2]). A class C of graphs is nowhere dense if, and only if, for every r ∈ N≥1
there is a λ(r) ∈ N≥1, such that for every G ∈ C, Splitter wins the (λ(r), r)-splitter game on G.

Furthermore, if C is effectively nowhere dense, then λ is computable.

Remark 4.7. In the proof of our main theorem, we will also have to compute Splitter’s winning strategy
efficiently: si+1 should be computable from the previous moves and ci+1 in time O(||N Gi

r (ci+1)||).

This is also something that is needed in [16, 17]. However, there is a small hiccup. In Remark 4.3 of [16]
and identically in Remark 4.7 from the journal version [17] it is stated that si+1 is computable from the
previous move and ci+1 in time O(||Gi||), which is a weaker statement. A closer look at the uses of these
remarks indicates that the stronger version is needed and that this is actually what is proved!

Our inductive parameter is the number of remaining rounds for Splitter before she wins the game
starting with parameters (λ(r(cid:48)), r(cid:48)) when given an (r(cid:48)/2, r(cid:48))-neighborhood cover of G for a suitable
number r(cid:48).

Example (1-C). For our running example, we have already computed a (2, 4)-neighborhood cover X of G.
Let then λ be such that Splitter wins the (λ, 4)-splitter game on G. Assume that λ = 1. Then G is edgeless
and any naive algorithm will work. Assume now that λ > 1. Let X be a bag of X . By definition, there
is an element c such that X ⊆ N G
4 (c). Let sX be Splitter’s answer if Connector picks c in the game’s first
round. Then, by definition, Splitter wins the (λ−1, 4)-splitter game on G[N G
4 (c) \ {sX }]. In particular,
Splitter wins the (λ−1, 4)-splitter game on G(cid:48) := G[X \ {sX }]. Hence we can use an inductive argument
within G(cid:48). For this we compute a new query q(cid:48)(x, y) such that for all a, b such that X (a) = X, G |= q(a, b)
iff G(cid:48) |= q(cid:48)(a, b). Recall that we already know for such pairs a, b that G |= q(a, b) iff G[X] |= q(a, b). It
therefore remains to encode the removal of the node sX , and this can be done by recoloring the nodes adjacent
to sX . Let R1, R2 be new unary predicates such that:

w ∈ R1(G(cid:48))
w ∈ R2(G(cid:48))

iff G |= dist≤1(w, sX )
iff G |= dist≤2(w, sX ).

11

The new query q(cid:48)(x, y) is then defined as a disjunction of the following queries

q(x, y) ∨ (cid:0) R1(x) ∧ R1(y) (cid:1)
∨ (cid:0) R2(x) ∧ y = sX
∨ (cid:0) x = sX ∧ y = sX

(cid:1) ∨ (cid:0) R2(y) ∧ x = sX
(cid:1).

(cid:1)

The first line takes care of the general case i.e. neither x nor y have been deleted. The first disjunction tests
whether they are still at distance 2 in the new graph, the second one tests whether the unique node connecting
x and y is sX . The second line deals with the cases of having one of the node deleted, y = sX and x = sX .
As sX is not part of G(cid:48), the case R2(x) ∧ y = sX should be understood as: If y = sX , simply test whether
G(cid:48) |= R2(x). Similarly for the dual case. Last, if both have been deleted, the variables were talking about
the same node, being at distance less than two from itself.

4.2 Testing distance queries
We are now ready to prove Proposition 4.2. Let C be a nowhere dense class of colored graphs and let
r ∈ N. We want to test efficiently whether two nodes are within distance at most r in G, for some G ∈ C.
Let (cid:15) > 0.

For every λ ∈ N≥1 let Cλ be the subclass of C consisting of all G ∈ C such that Splitter wins the
(λ, 2r)-splitter game on G. Clearly, Cλ ⊆ Cλ+1 for every λ. Since C is nowhere dense, by Theorem 4.6
there exists a number Λ := λ(2r) ∈ N such that C = CΛ. Thus,

C1 ⊆ C2 ⊆ · · · ⊆ CΛ = C.

We proceed by induction on λ and prove the result for all G ∈ Cλ. The induction base for λ = 1
follows immediately, since by definition of the splitter game every G ∈ C1 has to be edgeless, and thus a
naive algorithm works. For the induction step consider a λ ≥ 2 and assume that Proposition 4.2 already
holds for λ−1 (i.e., for all colored graphs in Cλ−1).

Consider an arbitrary G ∈ Cλ. Let n := |V | be the size of the domain V of G, and let δ > 0 be such

that 3δ + 2δ2 ≤ (cid:15).

4.2.1 The preprocessing phase

The preprocessing phase is composed of the following steps:

1. Let fC(·, ·) be the function provided by the neighborhood cover Theorem 4.4. If n ≤ fC(r, δ), we use a
naive algorithm to compute entirely the query result dist≤r(G) and trivially provide the functionality
claimed by Proposition 4.2.
From now on, consider the case where n > fC(r, δ). Recall from Theorem 2.1 that this implies that
||G|| ≤ O(n1+δ).

2. Using the algorithm provided by Theorem 4.4, and since n > fC(r, δ), we compute a (r, 2r)-neighborhood
cover X of G with degree at most nδ. Furthermore, in the same way as in [17, Lemma 6.10], we also
compute for each X ∈ X a list of all b ∈ V satisfying X (b) = X, and we compute a node cX such that
2r(cX ). All these relations can be efficiently stored and retrieved with the Storing Theorem 3.1.
X ⊆ N G
3. Since G ∈ Cλ, we know that Splitter wins the (λ, 2r)-splitter game on G. For every X ∈ X we now
compute a node sX that is Splitter’s answer if Connector plays cX in the first round of the (λ, 2r)-
splitter game on G. From Remark 4.7 we know that the nodes (sX )X∈X can be computed within total
time O(n1+δ).

4. For every X in X , we define X (cid:48) as G[X \ {sX }] and we compute for every i ≤ r :

Ri(X (cid:48))

:=

{w | G[X] |= dist≤i(w, sX )}

using a simple breadth-first search.

12

5. By Splitter’s choice of the node sX , we know that she wins the (λ−1, 2r)-splitter game on X (cid:48) for
every X in X . Therefore, for each X in X , we spend time O(|X (cid:48)|1+δ) for the preprocessing obtained
by induction on Proposition 4.2 for the query dist≤r. This will allows us, given (a, b) in the domain
of X (cid:48), to test in constant time whether (a, b) ∈ dist≤r(X (cid:48)).

This ends the preprocessing. We now show that it works in time O(n1+(cid:15)) as desired. Step 1 takes
time O(1) and Step 2 and 3 time O(n1+δ). Step 4 and 5 take, for each X ∈ X , time O((cid:107)X (cid:48)(cid:107)1+δ) leading
to a total time:

O

(cid:16) (cid:80)
X∈X

(cid:107)X (cid:48)(cid:107)1+δ(cid:17)

(cid:18)(cid:16) (cid:80)

≤ O

(cid:17)1+δ(cid:19)

(cid:107)X (cid:48)(cid:107)

(cid:18)(cid:16) (cid:80)

≤ O

(cid:17)1+δ(cid:19)

(cid:107)G[X](cid:107)

X∈X

(cid:16)

(nδ(cid:107)G(cid:107))1+δ(cid:17)

≤ O

X∈X

by (1)

≤ O (cid:0)(nδn1+δ)1+δ(cid:1) = O

(cid:16)

n1+3δ+2δ2(cid:17)

≤ O(n1+(cid:15)).

4.2.2 Test procedure

We are now given two nodes a and b. We want to answer in constant time to the question: “Is (a, b) in
dist≤r(G)?” After the preprocessing, in constant time we have access to X (a) and we can test whether
b is in X (a). If it is not the case, then clearly (a, b) (cid:54)∈ dist≤r(G).

Otherwise, we have that (a, b) ∈ dist≤r(G) iff (a, b) ∈ dist≤r(G[X (a)]). Moreover, we have (a, b) ∈

dist≤r(G[X (a)]) if and only if one of the following is true (recall that X (cid:48) = G[X \ {sX }]):

• a (cid:54)= sX and b (cid:54)= sX , and X (cid:48) |= dist≤r(a, b) ∨

(cid:87)
1≤ i,j ≤r−1
i+j≤r

(cid:0)Ri(a) ∧ Rj(b)(cid:1)

• a (cid:54)= sX and b = sX , and X (cid:48) |= Rr(a)

• a = sX and b (cid:54)= sX , and X (cid:48) |= Rr(b)

• a = sX and b = sX .

As we can test all of this in constant time (the test that X (cid:48) |= dist≤r(a, b) is done by using the induction
hypothesis for λ−1 in Proposition 4.2), we are able to decide in constant time whether (a, b) ∈ dist≤r(G).

This ends the proof of Proposition 4.2.

5 Computing the next solution

In this section, we finally prove Theorem 2.3. Let C be a nowhere dense class of colored graphs. Given a
colored graph G of C and a first-order query ϕ we need to construct in pseudo-linear time a data structure
such that, upon input of a tuple a we can return the first tuple b such that b is greater than or equal to a
in the lexicographical order and b is a solution to ϕ for G.

For technical reasons (explained in Section 5.1.2 below) we prove the result for FO+ queries (rather
than FO queries) and proceed by induction on their arity k. In FO+ we are allowed to use atoms of the
form dist(x, y) ≤ d for any constant d, and when evaluated in a structure A, dist(x, y) is interpreted as
the distance between x and y in the Gaifman graph of A. Allowing to use these distance-atoms does not
increase the expressive power of first-order logic, as these atoms can clearly be expressed in FO, but it
will lead to a different notion of quantifier-rank.

We restate Theorem 2.3 as follows.

Theorem 5.1. For every (effectively) nowhere dense class of colored graphs C, there is a (computable) function
f1 and an algorithm which, upon input of k ∈ N≥1, G ∈ C, (cid:15) ∈ Q>0, and an FO+-query ϕ of arity k,
performs a preprocessing in time f1(k, |ϕ|, (cid:15)) · |G|1+(cid:15), such that afterwards, upon input of a k-tuple a, it
computes in time f1(k, |ϕ|, (cid:15)) the smallest tuple a(cid:48) ∈ ϕ(G) such that a(cid:48) ≥ a in the lexicographical order. In
case that no such tuple exists, the algorithm returns Null.

13

The proof of Theorem 5.1 uses the following lemma.

Lemma 5.2. For every (effectively) nowhere dense class of colored graphs C, there is a (computable) function
f2 and an algorithm which, upon input of k ∈ N≥1, G ∈ C, (cid:15) ∈ Q>0, and an FO+-query ϕ of arity k,
performs a preprocessing in time f2(k, |ϕ|, (cid:15)) · |G|1+(cid:15), such that afterwards, upon input of a (k−1)-tuple a
and an element b, it computes in time f2(k, |ϕ|, (cid:15)) the smallest element b(cid:48) such that b(cid:48) ≥ b and G |= ϕ(a, b(cid:48)).
In case that no such b(cid:48) exists, the algorithm returns Null.

The proofs of Theorem 5.1 and Lemma 5.2 are nested one in the other. We actually prove the following:

• For any k ∈ N≥1, if Theorem 5.1 holds for k−1 and Lemma 5.2 holds for k, then Theorem 5.1 also

holds for k.

• For any k ∈ N≥1, using that Theorem 5.1 and Lemma 5.2 hold for k−1, Lemma 5.2 also holds for

k.

The first bullet is easy to prove:

Proof of the first bullet. Let C a nowhere dense class of colored graphs, k ∈ N≥1, (cid:15) ∈ Q>0, G ∈ C, let V
be the domain of G, and let ϕ(x, y) be an FO+-query of arity k. Let ϕ(cid:48)(x) be the query ∃y ϕ(x, y). Since
Theorem 5.1 holds for k−1, we can spend time f1(k−1, |ϕ(cid:48)|, (cid:15)) · |V |1+(cid:15) to compute the preprocessing for
ϕ(cid:48) on G. We also spend time f2(k, |ϕ|, (cid:15)) · |V |1+(cid:15) to compute the preprocessing of Lemma 5.2 for ϕ on
G.

This ends the preprocessing.
We are now given a k-tuple (a, b) and we want to compute the smallest k-tuple in ϕ(G) that is larger
than or equal to (a, b) in the lexicographical order. Let b(cid:48) be the element computed in time f2(k, |ϕ|, (cid:15))
by the algorithm of Lemma 5.2 for ϕ and G on input of a and b. Then, the following is true:

• If b(cid:48) is not Null, it is immediate to see that the desired answer is (a, b(cid:48)).

• If b(cid:48) is Null, let a(cid:48) be the answer computed in time f1(k−1, |ϕ|, (cid:15)) by the algorithm given by Theo-
rem 5.1 for ϕ(cid:48) and G on input of the (k − 1)-tuple a+1 (where a+1 is the tuple following a in the
lexicographical order). Let b0 be the smallest element of V . If a(cid:48) is Null, it is immediate to see that
the desired answer is Null. If a(cid:48) is not Null, let b(cid:48) be the element computed in time f2(k, |ϕ|, (cid:15)) by
Lemma 5.2 for ϕ and G on input of a(cid:48) and b0. As a(cid:48) is not Null, b(cid:48) cannot be Null, and hence the
desired answer is (a(cid:48), b(cid:48)).

Note that the algorithm works in time f1(k, |ϕ|, (cid:15)) after a preprocessing in time
f1(k, |ϕ|, (cid:15)) · |V |1+(cid:15), provided that f1(k, |ϕ|, (cid:15)) > f1(k−1, |ϕ|, (cid:15)) + 2f2(k, |ϕ|, (cid:15)).

The remaining part of this section is devoted to the proof of the second bullet, which is more involved.
In what remains of this paper, for better readability, we replace f (k, |ϕ|, (cid:15)) simply by O(·). Recall that
the “big O” hides these constants, which are computable as soon as C is effectively nowhere dense.

We start by introducing additional notations and tools.

5.1 Additional tools

5.1.1 Model checking result and the unary case

As we mentioned earlier, our proofs are by induction on k. The cases when k is zero, i.e. sentences, or k
is one, are an immediate consequence of the following theorem.

Theorem 5.3 (Grohe, Kreutzer, Siebertz [17, Theorem 8.1]). Let ϕ be a first-order query with at most one
free variable, and let C be a nowhere dense class of colored graphs. There is an algorithm that, on input of
G ∈ C computes ϕ(G) in pseudo-linear time2.

We will often reference this theorem as the Model Checking Theorem or the Unary Theorem (de-

pending on the arity of the used query being 0 or 1).

2Recall that this means that for all (cid:15) there is an algorithm. If moreover C is effectively nowhere dense then the algorithm can be

computed from (cid:15). In view of similar issues in circuit complexity, the second result is said to be uniform.

14

5.1.2 FO+, q-rank, and queries in normal form

The first step of our algorithm is to transform the query into some normal form: we need the queries
to be local. This is usually performed using Gaifman’s Theorem [15]. After that, and as Example 1-C sug-
gests, in order to enumerate a query we will have to enumerate several other queries within some bags of
the neighborhood cover. This will be done by induction, and we make sure that the new queries have a
quantifier-rank not exceeding the one given by Gaifman’s Theorem. This will be presented in Lemma 5.5.
Unfortunately, these new queries are no longer local. Furthermore, applying Gaifman’s Theorem already
blew up the original quantifier-rank of the input query. If at every step of the induction we apply Gaif-
man’s Theorem (which blows up the quantifier-rank) and the mechanism of Lemma 5.5 (which blows
up the locality), then the considered radius for the Splitter game is not fixed, and the induction (on the
number of rounds) is wrecked. To overcome this issue, we reuse a stronger normal form presented in [18]
that maintains some notion of quantifier-rank. This normal form uses the logic FO+ and the notion of
q-rank that we describe now.

Recall that in FO+ we are allowed to use atoms of the form dist(x, y) ≤ d for any constant d. Follow-
ing [17, Section 7.2], we say that a FO+ query ϕ has q-rank at most (cid:96), where q and (cid:96) are in N, if it has
quantifier-rank at most (cid:96) and each distance-atom dist(x, y) ≤ d in the scope of i ≤ (cid:96) quantifiers satisfies
d ≤ (4q)q+(cid:96)−i. We also define (as in [17] and [18])

fq((cid:96))

:= (4q)q+(cid:96).

The use of the FO+ notation enables a particular normal form for queries. The goal is to decom-
pose every query into local queries. Intuitively, it is similar to a Gaifman normal form. However, this
decomposition has the advantage of controlling the quantifier-rank of the generated local queries.

For formulating the normal form result, we need the following notation. An (r, q)-independence sen-

tence is an FO+-sentence of the form

∃z1 · · · ∃zk(cid:48)

(cid:16) (cid:94)

dist>r(cid:48)(zi, zj) ∧

(cid:94)

(cid:17)

ψ(zi)

1≤i<j≤k(cid:48)

1≤i≤k(cid:48)

where k(cid:48) ≤ q and r(cid:48) ≤ r and ψ(z) is quantifier-free.

Given a colored graph G of domain V and a tuple a = (a1, . . . , ak) ∈ V k, for all r ∈ N we define the
r (a) whose nodes are [1, k], and where {i, j} is an edge iff

r-distance type of a as the undirected graph τ G
G |= dist≤r(ai, aj). The set of all possible distance types with k elements is denoted Tk.

For a colored graph G, a neighborhood cover X of G, a number r ∈ N, and a tuple a of nodes of G

we say that a bag X ∈ X r-covers a if N G

r (a) ⊆ X.

symbol E and c unary relation symbols C1, . . . , Cc.

Recall from Section 2 that σc is the schema of c-colored graphs, i.e., σc contains a binary relation

We now have provided all the notions necessary for stating the normal form of [18] that will help us
overcome the issues explained at the beginning of Section 5.1.2. Roughly speaking, this normal form states
the following. Given a c-colored graph G and a kr-neighborhood cover X of G, we can add a number of
suitable colors to the nodes of G, turning G into a c(cid:48)-colored graph G(cid:63). These colors help us to decompose
an FO+-formula ϕ(x) that speaks about G into a set S of very basic sentences (namely, Boolean com-
binations of (r, q)-independence sentences) speaking about G(cid:63) and a set F of formulas speaking about
the subgraphs G(cid:63)[X] induced by the bags X ∈ X . Whenever given a k-tuple a of nodes of G, we can
decide whether G |= ϕ(a) by (1) determining the r-distance type τ of a, (2) determining the particular
sentence ξ of S that fits to τ and that is satisfied by G(cid:63), and (3) considering each connected component
I of τ and (3.1) finding the particular formula ψ in F that fits to I, τ , and ξ, (3.2) finding a bag X ∈ X
that r-covers aI , and (3.3) checking whether G(cid:63)[X] |= ψ(aI ). Thus, checking whether G |= ϕ(x) boils
down to checking very basic sentences in G(cid:63) along with checking the formulas ψ of F “locally” in the
subgraphs of G(cid:63) induced by the bags of X . What is crucial for our application of this decomposition is
that the sets S and F are independent of the particular graph G and, furthermore, if ϕ(x) has q-rank at
most (cid:96), then all the formulas ψ in F also have q-rank at most (cid:96). Let us now turn to the precise formal
statement of this decomposition.

15

Theorem 5.4 (Rank-Preserving Normal Form [18, Theorem 7.1]). Let q, k ∈ N be such that k ≤ q, let
(cid:96) := q−k, r := fq((cid:96)). For every c ∈ N we can compute a c(cid:48) ≥ c such that the following is true for σ := σc
and σ(cid:63) := σc(cid:48). For every FO+[σ]-formula ϕ(x) of q-rank at most (cid:96) where x = (x1, . . . , xk), and for each
distance type τ ∈ Tk we can compute a number mτ ∈ N and, for each i ≤ mτ ,
• a Boolean combination ξi
τ of (r, q)-independence sentences of schema σ(cid:63) and
• for each connected component I of τ an FO+[σ(cid:63)]-formula ψi

τ,I (xI ) of q-rank at most (cid:96)

such that the following holds:
For all c-colored graphs G and all kr-neighborhood covers X of G, there is a σ(cid:63)-expansion G(cid:63) of G (which
only depends on G, X , q, (cid:96)) with the following properties, where V denotes the domain of G and G(cid:63):

(a) For all a ∈ V k and for τ := τ G

r (a) we have:

G |= ϕ(a) iff there is an i ≤ mτ that satisfies the following condition

(∗)i: G(cid:63) |= ξi

τ and for every connected component I of τ there is an X ∈ X that r-covers aI and

G(cid:63)[X] |= ψi

τ,I (aI ).

(b) For all a ∈ V k and for τ := τ G

(c) For all a ∈ V k, all connected components I of τ := τ G

r (a), there is at most one i ≤ mτ such that the condition (∗)i is satisfied.
r (a), all X, X (cid:48) ∈ X that both r-cover aI , and all

i ≤ mτ ,

G(cid:63)[X] |= ψi

τ,I (aI ) ⇐⇒ G(cid:63)[X (cid:48)] |= ψi

τ,I (aI ).

Furthermore, for every nowhere dense3 class C of colored graphs, there is an algorithm which, when given as
input a G ∈ C, a kr-neighborhood cover X of G of degree at most |V |(cid:15), and parameters q, (cid:96) ∈ N, computes
G(cid:63) in time O(|V |1+2(cid:15)).

5.1.3 The Removal Lemma

The following lemma generalizes the idea of Step 4 of the preprocessing for the testing of distance queries
in the proof of Proposition 4.2, where we introduced new relations in order to cope with the removal of
one node (typically, the answer of Splitter in the Splitter game). The goal is to rewrite a query into an
equivalent one when a node is removed from the colored graph.

The lemma is present in a similar form in [18] (see Lemma 7.8); its proof is straightforward. For a
tuple a and a set I of indices, we denote by aI the projection of a onto its components whose indices
belong to I. By a\I we denote the projection of a onto its components whose indices are not in I.
Lemma 5.5 (Removal Lemma). There is an algorithm which takes as input numbers k, q, (cid:96), c ∈ N, a c-
colored graph G of domain V , a k-ary query ϕ(z) ∈ FO+[σc] of q-rank at most (cid:96), a set of free variables
y ⊆ z, and a node s ∈ V , and which produces

• a number c(cid:48) ≥ c,
• a query ϕ(cid:48)(z \ y) ∈ FO+[σc(cid:48)] of q-rank at most (cid:96),
• a graph H that is a coloring of G \ {s} using c(cid:48) colors (i.e. a σc(cid:48)-expansion of G \ {s}),

such that for all tuples b over V where {i ≤ k | bi = s} = {i ≤ k | zi ∈ y} =: I we have

G |= ϕ(b) ⇐⇒ H |= ϕ(cid:48)(b\I ).

Moreover, the running time of this algorithm is linear in the size of G, and

• c(cid:48) only depends on c, q, (cid:96),
• ϕ(cid:48) only depends on c, q, (cid:96), ϕ, y,
• H only depends on c, q, (cid:96), G, s.
3Here the paper does not explicitly mention whether the result is uniform. A quick look at the proof clearly indicates that the
constants are computable if C is effectively nowhere dense. This is the case because the proof is performed by induction on the
number of rounds that Splitter needs to win the game.

16

5.1.4 Kernel

For technical reasons, we not only want to know whether a given vertex belongs to some bags. We also
want to know whether its p-neighborhood (for some well chosen p) is included in a given bag. This is
where the following definition comes into play.

Definition 5.6 (Kernel). Let X be an r-neighborhood cover of a colored graph G with domain V . For all
X ∈ X and p ≤ r, the p-kernel of X is the set Kp(X) := {a ∈ V | N G

p (a) ⊆ X}.

Lemma 5.7 ([13, Lemma 8.1]). Assume X is an r-neighborhood cover of a colored graph G. Given a bag
X and a number p, we can compute Kp(X) in time O(p · ||G[X]||).

Together with the Storing Theorem 3.1 this lemma shows that, after a pseudo-linear time prepro-
cessing we can, given a node a and a bag X, test in constant time whether a belongs to the p-kernel of
X.

5.1.5 Shortcut pointers
Example (2). Our first query example (i.e., Example (1-A)) is limited in the sense that all its solutions satisfy
the same distance type requiring that x is close to y. We therefore consider a slightly more complicated query:

q(x, y)

:= dist>2(x, y) ∧ B(y)

where B is interpreted as the set of “blue” nodes of a colored graph. The goal is, given a node a, to enumerate
all blue nodes that are at distance greater than 2 from a. As previously, we compute a (2, 4)-neighborhood
cover, and given a node a, we consider the bag X := X (a).

We then start two concurrent enumeration processes: the first one only enumerates nodes that are within
X; the second one enumerates those that are not in X. The first one uses ideas previously presented, diving
into G[X \ {v}] using the query constructed by Lemma 5.5 and induction on λ. We now explain how the
second one works.

Note that for every node b outside of X, we have dist(a, b) > 2 as N G

2 (a) ⊆ X. Therefore, it suffices to
enumerate all blue nodes that don’t belong to X. To do so, during the preprocessing phase we compute for all
nodes c of G and all bags X ∈ X with c ∈ X, the smallest blue node bigger than c that is not in X; let us
denote this node by v(c, X). As the degree of our cover is pseudo-constant, the domain of the function v(·, ·)
is pseudo-linear and the computation of the function can be done in pseudo-linear time using the Storing
Theorem 3.1.

However, a naive extension of this idea for queries of larger arities does not work: consider the query

q(x, y, z)

:= dist>2(x, z) ∧ dist>2(y, z) ∧ B(z).

Assume that, given a pair (a, b) of nodes, we want to enumerate all nodes c such that (a, b, c) ∈ q(G). Given
a, b we consider the bags X := X (a) and Y := X (b). Now, we have three concurrent processes, one of them
being in charge of enumerating all blue nodes that are neither in X nor in Y . The previous algorithm can
enumerate all blue nodes that are not in X, but some of those may be in Y . Given c in X ∪ Y , computing
the smallest blue node c(cid:48) bigger than c which falls out of X ∪ Y , may require quadratic time and space.
Therefore, we need another approach.

As previously let v(c, X, Y ) be the smallest blue node bigger than (or equal to) c that is neither in X nor
in Y . With our time and space constraints, it is not possible to compute and store v(c, X, Y ) for every tuple
c, X, Y . It turns out that we only need to know the result of v to a subdomain of small size. We will first only
consider the cases where c ∈ X. For a given c our assumption on the degree of the cover implies that there
would only be few such X, but the number of Y remains too big. We therefore further restrict ourselves to
the cases where c ∈ X and v(c, X) ∈ Y . Now, given c there are few possible X to consider and also few
possible Y i.e. the domain of v(·, ·, ·) satisfying these conditions has a pseudo-linear size and the result of the
function over this domain can be computed and stored in pseudo-linear time.

Finally, let us explain how to use what has been computed to retrieve v(c, X, Y ) for any tuple c, X, Y .
If none are true, then v(c, X, Y ) = c. Second, assume that

First, we test whether c ∈ X and c ∈ Y .

17

c ∈ X (the case c ∈ Y is symmetrical). Then, as for the previous query, we retrieve v(c, X) and test whether
v(c, X) ∈ Y . If not, we are done as v(c, X) = v(c, X, Y ). Otherwise we are in the specific case where c ∈ X
and v(c, X) ∈ Y , and v(c, X, Y ) as been computed and can be retrieved. Lemma 5.8 below generalizes these
ideas.

The idea developed in the previous example is now made concrete is the following lemma.

Lemma 5.8 (Skip pointers [30]). For every nowhere dense4 class C of colored graphs, there is a preprocessing
algorithm with input: G ∈ C of domain V , r ∈ N≥1, (cid:15) ∈ Q>0, k ∈ N, an r-neighborhood cover X of G of
degree at most |V |(cid:15), and L ⊆ V .

The preprocessing works in time O(|V |1+k(cid:15)) and afterwards enables us, when given a node b and a set

S of at most k bags of X , to compute in constant time the node

SKIP(b, S)

:= min

(cid:110)

b(cid:48) ∈ L : b(cid:48) ≥ b ∧ b(cid:48) (cid:54)∈

(cid:111)

Kr(X)

(cid:91)

X∈S

(recall from Definition 5.6 that Kr(X) is the r-kernel of X).

Proof. The lemma was proved already in [30], but in order to make the current paper a bit more self-
contained let us recapitulate the proof details here.

From now on we fix (cid:15), r, G, X and L as in the statement of the lemma.
We assume that all kernels have already been computed. This is without loss of generality modulo a

preprocessing of time O(||G||1+(cid:15)) using Lemma 5.7.

The domain of the SKIP(·, ·)-function is too big (recall that there can be a linear number of bags) so
we cannot compute it during the preprocessing phase. Fortunately, computing only a small part of it will
be good enough for our needs. For each node b we define by induction a set SC(b) of sets of at most k
bags. We start with SC(b) = ∅ and then proceed as follows.

• For all nodes b of G and for all bags X in X with b ∈ Kr(X), we add {X} to SC(b).

• For all nodes b of G, for all sets S of bags from X , and all bags X of X , if |S| < k and S ∈ SC(b)

and SKIP(b, S) ∈ Kr(X), then we add {S ∪ {X}} to SC(b).

In the preprocessing phase we will compute SKIP(b, S) for all nodes b of G and all sets S ∈ SC(b).
Before explaining how this can be accomplished within the desired time constraints, we first show that
this is sufficient for deriving SKIP(b, S) in constant time for all nodes b and all sets S consisting of at most
k bags of X .
Claim 5.9. Given a node b of G, a set S of at most k bags of X , and SKIP(c, S(cid:48)) for all nodes c > b of G
and all sets S(cid:48) ∈ SC(c), we can compute SKIP(b, S) in constant time.

Proof. We consider two cases (testing in which case we fall can be done in constant time as the kernels
have been computed and S has size bounded by k).
Case 1: b ∈ L and b (cid:54)∈ (cid:83)
X∈S

Kr(X). In this case, b is SKIP(b, S) and we are done.

Case 2: b (cid:54)∈ L or b ∈ (cid:83)
X∈S
there is no such c, then SKIP(b, S) = Null and we are done. Otherwise, we proceed as follows.

Kr(X). In this case, let c be the smallest element of L strictly bigger than b. If

If c (cid:54)∈ (cid:83)
X∈S

Kr(X), then c is SKIP(b, S) and we are done. Otherwise, we know that c ∈ Kr(X) for
some X ∈ S. Therefore {X} ∈ SC(c). Let S(cid:48) be a maximal (w.r.t. inclusion) subset of S in SC(c). Since
{X} ∈ SC(c), we know that S(cid:48) is non-empty.

We claim that SKIP(c, S(cid:48)) = SKIP(b, S). To prove this, let us first assume for contradiction that
SKIP(c, S(cid:48)) ∈ Kr(Y ) for some Y ∈ S. By definition, this implies that Y is not in S(cid:48). Hence |S(cid:48)| < |S| ≤
k. Thus, by definition of SC(c) we have S(cid:48) ∪ {Y } ∈ SC(c) and S(cid:48) was not maximal.

4Note that everything in this lemma is computable from the input, even when C is not effectively nowhere dense. While

computing a neighborhood cover is in non-uniform FPT, here the neighborhood cover is part of the input.

18

Moreover, by definition of SKIP(c, S(cid:48)), every point between c and SKIP(c, S(cid:48)) is either not in L or in
some Kr(Z) with Z ∈ S(cid:48) (and therefore Z ∈ S). As all nodes between b and c are not in L, the claim
follows.

We conclude by showing that SC(b) is small for all nodes b of G and that we can compute efficiently

SKIP(b, S) for all nodes b and all sets S ∈ SC(b).
Claim 5.10. For each node b of G, |SC(b)| has size O(|V |k(cid:15)). Moreover, it is possible to compute SKIP(b, S)
for all nodes b of G and all sets S ∈ SC(b) in time O(|V |1+k(cid:15)).

Proof. We start by proving the first statement, and afterwards we use Claim 5.9 to show that we can
compute these pointers inductively.

By SC(cid:96)(b) we denote the subset of SC(b) of sets S with |S| ≤ (cid:96). Let d be the degree of the cover X ,
i.e., d ≤ |V |(cid:15). By definition of d, we know that |SC1(b)| ≤ d for all nodes b of G. For the same reason, we
have that |SC(cid:96)+1(b)| is of size at most O(d · |SC(cid:96)(b)|). Therefore, for all b ∈ V , we have:

|SC(b)| = |SCk(b)| ≤ O(dk).

We compute the pointers for b from bmax to bmin downwards, where bmax and bmin are, respectively,
the biggest and the smallest element of V . Given a node b in V , assume we have computed SKIP(c, S(cid:48))
for all c > b and S(cid:48) ∈ SC(c). We then compute SKIP(b, S) for S ∈ SC(b) using Claim 5.9.

At each step, the pointer is computed in constant time. Since there are O(|V |1+k(cid:15)) of them, the time

required to compute them is as desired.

The combination of these two claims proves Lemma 5.8.

5.2 The main algorithm
We now fix k and assume that Theorem 5.1 and Lemma 5.2 hold for k−1. Our goal is to show that
Lemma 5.2 then holds for k.

Let us fix an arbitrary number q ∈ N with q ≥ k, let (cid:96) := q−k, and let r := fq((cid:96)) = 4qq+(cid:96). Note that
this is the same choice of parameters as for the Rank-Preserving Normal Form Theorem 5.4. Our goal is
to show that the statement of Lemma 5.2 is true for all k-ary queries ϕ of q-rank at most (cid:96).

For every λ ∈ N≥1 let Cλ be the subclass of C consisting of all G ∈ C such that Splitter wins the
(λ, 2kr)-Splitter game on G. Clearly, Cλ ⊆ Cλ+1 for every λ. Since C is nowhere dense, by Theorem 4.6
there exists a number Λ := λ(2kr) ∈ N such that C = CΛ. Thus,

C1 ⊆ C2 ⊆ · · · ⊆ CΛ = C.

We proceed by induction on λ and prove the result for all G ∈ Cλ. The induction base for λ = 1
follows immediately, since by definition of the Splitter game every G ∈ C1 has to be edgeless, and thus
a naive algorithm works. For the induction step consider a λ ≥ 2 and assume that the statement of
Lemma 5.2 already holds for λ−1, i.e., for all colored graphs in Cλ−1 and all k-ary queries of q-rank at
most (cid:96). Additionally, recall that we assume that the full statement of Theorem 5.1 and Lemma 5.2 already
holds for all queries of arity ≤ k−1. This also implies that we can use the statement of Corollary 2.4 for
queries of arities ≤ k−1.

We fix an (cid:15) ∈ Q>0 and an FO+-query ϕ(x, xk) of arity k and q-rank at most (cid:96). Here x = (x1, . . . , xk−1)
is a (k−1)-ary tuple of variables and xk is the kth variable. Our goal throughout the rest of this section
is to provide an algorithm which upon input of a G ∈ Cλ of domain V performs a preprocessing phase
using time O(|V |1+(cid:15)), such that afterwards upon input of a tuple a ∈ V k−1 and an element ak ∈ V , we
k ∈ V such that:
can compute in constant time the element a(cid:48)

• G |= ϕ(a, a(cid:48)

k),

• a(cid:48)

k ≥ ak,

• and a(cid:48)
k

is minimal.

19

If no such element exists, we output Null.

The general idea is to build on the Rank-Preserving Normal Form Theorem (Theorem 5.4) and reduce
the computation of the query ϕ on G to the evaluation of another query within G[X] for the bags X of
a neighborhood cover of G that contains the r-neighborhood of an element of a. In order to do this we
need to

1. compute a kr-neighborhood cover X of G. This can be done thanks to Theorem 4.4.

2. Be able to test distances up to r in order to compute the distance types τ compatible with a. We

have seen how to do this in Section 4.

3. Check for each such τ and each i ≤ mτ whether ξi
τ

Checking Theorem (Theorem 5.3).

holds. This can be done thanks to the Model

4. Let I be a connected component of τ and i ≤ mτ . We need to evaluate the formulas ψi

given by
the Rank-Preserving Normal Form Theorem. If k does not belong to I, then this is just a matter of
testing whether ψτ,I (aI ) holds and this can be done by induction on k.
The difficulty is the case when k ∈ I as a(cid:48)
k
In case that J := I \ {k} (cid:54)= ∅, we are looking for the smallest a(cid:48)
k
In this case we consider a suitable bag X whose kernel contains at least one element from aJ , and
compute the smallest suitable a(cid:48)
within G[X]. This is the difficult part, requiring an induction on
k
λ and whose sketch is given in the next bullet.
In case that I = {k}, we consider the bags X (a1), . . . , X (ak−1), use the SKIP pointers provided
by Section 5.1.5 to compute several answer candidates, and then return the smallest of these.

such that ψi

is not known.

τ,I (aJ , a(cid:48)

k) holds.

τ,I

5. In order to compute the smallest suitable a(cid:48)
k

within G[X] we have to make sure we consider only
that are sufficiently far from all the elements of a not in aI and sufficiently close to all
elements a(cid:48)
k
by adding sufficiently
the elements of aI . To do this we first transform the formula ψi
many free variables in order to include the elements of a that fall within X and adding clauses
enforcing the necessary distance properties (note that we do not need to consider the elements of
a not in X as those are necessarily far from a(cid:48)
. The resulting formula is described at Step 7 of the
k
preprocessing phase).
It remains to find the smallest a(cid:48)
k). Either this is sX , the answer
k
of Splitter in the Splitter game when Connector plays the center cX of X, or we can evaluate the
formula computed from Ψi
by the Removal Lemma and evaluate it by induction on λ on a
component resulting from the Splitter game.

for which G[X] |= Ψi

τ,I,p(a, a(cid:48)

into Ψi

τ,I,p

τ,I,p

τ,I

This is essentially what we do.
We first describe the preprocessing phase, then the answering procedure. While describing these, we

also provide a runtime analysis and a correctness proof.

5.2.1 The preprocessing phase
Consider the query ϕ(x, xk) with x = (x1, . . . , xk−1). We choose δ > 0 such that 2δ + δ2 + kδ ≤ (cid:15).

Let G ∈ Cλ be the input and let n := |V | be the size of the domain V of G. The preprocessing phase

is composed of the following steps:

1. Let fC(·, ·) be the function provided by Theorem 4.4. If n ≤ fC(2kr, δ), we use a naive algorithm to

compute the query result ϕ(G) and trivially provide the functionality claimed by Lemma 5.2.
From now on, consider the case where n > fC(2kr, δ). Recall from Theorem 2.1 that this, without
loss of generality, implies that ||G|| ≤ n1+δ.

2. For every k(cid:48) < k and every distance type τ (cid:48) ∈ Tk(cid:48), consider the k(cid:48)-ary query ρτ (cid:48)(x1, . . . , xk(cid:48)) defined
as the conjunction of the formulas dist≤r(xi, xj) for all edges {i, j} of τ (cid:48) and the conjunction of the
formulas ¬ dist≤r(xi, xj) for all i, j ∈ [1, k(cid:48)] with i (cid:54)= j for which τ (cid:48) does not contain the edge {i, j}.

20

Note that for every a ∈ V k(cid:48) we have G |= ρτ (cid:48)(a) iff τ (cid:48) = τ G
r (a). We spend time O(|V |1+δ) to perform
the preprocessing phase provided by the Proposition 4.2 for the distance query dist≤r(z1, z2) used in
the query ρτ (cid:48). Henceforth, for each k(cid:48) < k and each τ (cid:48) ∈ Tk(cid:48), this will enable us upon input of a tuple
a ∈ V k(cid:48), to test in constant time whether G |= ρτ (cid:48)(a), i.e., whether τ G

r (a) = τ (cid:48).

3. Using the algorithm provided by Theorem 4.4, we compute a (kr, 2kr)-neighborhood cover X of G

with degree at most nδ.
Furthermore, in the same way as in [17, Lemma 6.10], we also compute for each X ∈ X a list of all
b ∈ V satisfying X (b) = X, and we compute a node cX such that X ⊆ N G
In addition, we use Lemma 5.7 to compute for every bag X ∈ X the r- kernel of X, i.e., the set
Kr(X) = {a ∈ X | N G
All of this can be efficiently stored and retrieved with the Storing Theorem 3.1. This can be done in
time O(n1+δ).

r (a) ⊆ X}.

2kr(cX ).

τ,I (xI ) of q-rank at most (cid:96), for each i ≤ mτ and each connected component I of τ .

4. Let σ be the schema of G and use the algorithm provided by the Rank-Preserving Normal Form The-
orem 5.4 upon input of ϕ, G, X to compute in time O(n1+δ) the schema σ(cid:63), the σ(cid:63)-expansion G(cid:63) of
and the FO+[σ(cid:63)]-
G, and for each distance type τ ∈ Tk the number mτ , the FO+[σ(cid:63)]-sentences ξi
τ
formulas ψi
Afterwards, we proceed in the same way as in [17, 18] to compute, within total time O(n1+δ), for
be the expansion of G(cid:63)[X] where the new unary
every X ∈ X the structure G(cid:63)[X], and we let G(cid:63)
X
relation symbol K is interpreted by the r- kernel Kr(X).
has domain X and belongs to the class Cλ.
Note that G(cid:63)
X

5. By the Rank-Preserving Normal Form Theorem 5.4 for all a = (a1, . . . , ak−1) in V k−1 and all ak ∈ V
we have that G |= ϕ(a, ak) if and only if there is a distance type τ ∈ Tk and an i ≤ mτ such that:

r (a, ak)

(a) τ = τ G
(b) G(cid:63) |= ξi
τ
(c) G(cid:63)[X (ak)] |= ψi

τ,J (aJ ), where J is the connected component of τ with k ∈ J.
Note that for ak the bag X (ak) r-covers the tuple aJ , since k ∈ J, J is a connected component
of τ = τ G

r (a, ak), |J| ≤ k, and X is a kr-neighborhood cover of G.

(d) For all connected components I of τ with k (cid:54)∈ I we have G(cid:63)[X (aI )] |= ψi

τ,I (aI ), where X (aI )

is defined to be X (amin(I)); note that this bag r-covers aI .

Using the Model Checking Theorem 5.3, we can test in time O(n1+δ) for every τ ∈ Tk and i ≤ mτ
whether G(cid:63) |= ξi
τ

.

We continue by performing the following preprocessing steps for every distance type τ ∈ Tk and
. If there is no such τ and i then we can safely stop as there
every number i ≤ mτ such that G(cid:63) |= ξi
τ
is no tuple (a, ak) with G |= ϕ(a, ak).

6. For every connected component I of τ with k (cid:54)∈ I, the query ψi

τ,I (xI ) has arity ≤ k−1. By our
induction hypothesis, the statement of Theorem 5.1 (and its corollaries) already holds for this query.
Thus, for each X ∈ X we can use time O(|X|1+δ) to perform the preprocessing phase provided
. Using Corollary 2.4 and having
τ,I (xI ) and the colored graph G(cid:63)
by Theorem 5.1 for the query ψi
performed this preprocessing will henceforth enable us, upon input of a tuple aI of elements in X, to
test in constant time whether G(cid:63)[X] |= ψi
The total time taken by these preprocessing steps is of order at most

τ,I (aI ).

X

(cid:88)

X∈X

|X|1+δ ≤

(cid:16) (cid:88)

(cid:17)1+δ

|X|

≤ (n1+δ)1+δ ≤ n1+(cid:15)

X∈X

(here, we use that X has degree ≤ nδ, which implies that (cid:80)

X∈X |X| ≤ n1+δ).

21

7. Let J be the connected component of τ with k ∈ J and note that xk is the last variable in the tuple

xJ . Let z1, . . . , zk−|J| be new variables and consider for each p ∈ {0, . . . , k−|J|} the query

Ψi

τ,J,p(z1, . . . , zp, xJ )

:= ψi

τ,J (xJ ) ∧ Kr(xk) ∧ ρτ (xJ ) ∧

(cid:94)

dist(xk, zp(cid:48)) > r.

p(cid:48)∈[1,p]

τ,J,p

τ,J,p

has q-rank at most (cid:96).

is k for p = k−|J|, and it is smaller than k for smaller p.

Note that the query Ψi
As explained in the sketch we aim at restricting the evaluation to the substructure induced by a bag
τ,J (xJ ) is satisfied and that the nodes of x that are not in xJ but fall
X. The query then ensures that ψi
in the bag X are sufficiently far away from xk. Since we don’t know in advance how many there will
be, we anticipate all possibilities (by considering every p ≤ k−|J|). Note that the arity of the query
Ψi
For every X ∈ X we would like to provide the following functionality: Upon input of a tuple of
p+|J|−1 elements c1, . . . , cp, aJ\{k} and an element ak, we want to be able to compute in constant
time the smallest a(cid:48)
k
But as the query’s arity p+|J| might be as large as k, we do not have the statement of Lemma 5.2
available for this query. As a remedy, we perform the following steps 8–11 which make use of our
second inductive assumption, stating that Lemma 5.2 already holds for the class Cλ−1 and for queries
of arity up to k.

τ,J,p(c1, . . . , cp, aJ\{k}, a(cid:48)

in X such that G(cid:63)

k) and a(cid:48)

X |= Ψi

k ≥ ak.

8. Recall that in Step 3 we have already computed for every X in X a node cX whose 2kr-neighborhood
contains X. Since G ∈ Cλ, we know that Splitter wins the (λ, 2kr)-Splitter game on G. For every
X ∈ X we now compute a node sX that is Splitter’s answer if Connector plays cX in the first round of
the (λ, 2kr)-Splitter game on G. From Remark 4.7 we know that the nodes (sX )X∈X can be computed
within total time O(n1+δ).

9. Let p ∈ {0, . . . , k−|J|}, let k(cid:48) := p+|J| and let z = (z1, . . . , zk(cid:48)) := (z1, . . . , zp, xJ ). Recall that
k ∈ J and xk is the last variable of the tuple xJ , hence xk = zk(cid:48). For every set y of variables from z,
we proceed as follows. For every X ∈ X we apply the Removal Lemma 5.5 to the colored graph G(cid:63)
,
X
the query Ψi
τ,J,p,y(z \ y) of q-rank at
X \ {sX } by unary predicates, such that for all k(cid:48)-tuples b over X
most (cid:96) and an expansion H (cid:63)
X
where {i ≤ k(cid:48) | bi = sX } = {i ≤ k(cid:48) | zi ∈ y} =: ∆ we have

τ,J,p(z), the variables y, and the node sX . This yields a query Ψ(cid:48)i

of G(cid:63)

G(cid:63)

X |= Ψi

τ,J,p(b) ⇐⇒ H (cid:63)

X |= Ψ(cid:48)i

τ,J,p,y(b\∆).

For every X ∈ X , this takes time O(||G(cid:63)
X∈X ||G(cid:63)
Since a given edge (or a node) can only be found in at most nδ different bags (due to the degree of our
cover), we have (cid:80)
X || ≤ nδ||G||. Moreover, after Step 1 we know that ||G|| ≤ n1+δ. Hence,
nδ||G|| ≤ n1+2δ ≤ n1+(cid:15).

X ||). Hence, the total time taken by Step (9) is in O((cid:80)

X∈X ||G(cid:63)

X ||).

10. By our choice of the node sX we know that Splitter wins the (λ−1, 2kr)-Splitter game on H (cid:63)
X

. Hence,
belongs to Cλ−1, for every X ∈ X . Using our induction hypothesis, we thus spend for every X
, since the queries
τ,J,p,y(z \ y), for

H (cid:63)
X
time at most O(|X|1+δ) to perform the preprocessing phase for Lemma 5.2 on H (cid:63)
X
have arity at most k and q-rank at most (cid:96). Here, we carry this out for the queries Ψ(cid:48)i
all y ⊆ z.
Note that henceforth, this will allow us to do the following for every X ∈ X :
For any y with xk (cid:54)∈ y, when given an assignment a(cid:48) in X \ {sX } to the variables in z \ (y ∪ {xk}),
and for any element b in X \ {sX }, we can compute in constant time the smallest b(cid:48) ∈ X \ {sX } such
that H (cid:63)
The total time taken for this preprocessing step is of order at most (cid:80)

τ,J,p,y(a(cid:48), b(cid:48)) and b(cid:48) ≥ b.

X |= Ψ(cid:48)i

X∈X |X|1+δ ≤ n1+(cid:15).

22

11. In addition of the previous step, we also spend, for every X ∈ X , time at most O(|X|1+δ) to perform
τ,J,p,y(·), when xk ∈ y. Since

the preprocessing phase for Theorem 5.1 on H (cid:63)
X
xk ∈ y, the arity of the query is at most k−1.
This allows us (using Corollary 2.4), given an assignment a(cid:48) in X \ {sX } to the variables in z \ y, to
test in constant time whether H (cid:63)
Again, the total time taken for this preprocessing step is in O(n1+(cid:15)).

for every query Ψ(cid:48)i

τ,J,p,y(a(cid:48)).

X |= Ψ(cid:48)i

The next two steps are only performed when J = {k}; otherwise the preprocessing phase stops
here. Note that if J = {k}, then the tuple xJ only consists of the variable xk. Furthermore, for a tuple
a = (a1, . . . , ak−1) and a node ak, the tuple aJ consists of the single element ak.

12. We compute the set

τ,J

Li

:= { ak ∈ V | G(cid:63)[X (ak)] |= ψi
This can be achieved as follows: For each X ∈ X use the algorithm provided by the Unary Theorem 5.3
, and let LX be the intersection
to compute in time O(|X|1+δ) the result of the unary query ψi
of this query result with the list of all elements b with X (b) = X (recall that we already precomputed
this list in Step 3).

τ,J (ak) }.

on G(cid:63)
X

τ,J

Furthermore, Li
time O( (cid:80)
X∈X

τ,J

|X|1+δ) and hence in time O(n1+(cid:15)).

is the disjoint union of the sets LX for all X ∈ X . It can therefore be computed in

13. We compute the skip pointers with respect to the set L := Li

and Kr(X) for all X ∈ X as in

τ,J

Lemma 5.8. By Lemma 5.8 this is done in time O(n1+kδ) and hence in time O(n1+(cid:15)).

This concludes the preprocessing phase, and we have argued that all preprocessing steps can be done

in total time O(n1+(cid:15)).

5.2.2 The answering phase
We now describe how, upon input of a tuple a = (a1, . . . , ak−1) ∈ V k−1 and an element b := ak ∈ V
we can compute in constant time the minimal b(cid:48) such that

• G |= ϕ(a, b(cid:48)),

• and b(cid:48) ≥ ak,

or the value Null in case that such a b(cid:48) does not exist.

What we actually do is the following: For all possible pairs (τ, i), we compute in constant time the

minimal b(cid:48)

τ,i

such that:

(a) τ = τ G

r (a, b(cid:48)

τ,i),

(b) G(cid:63) |= ξi
τ

,

(c) G(cid:63)[X (b(cid:48)

τ,i)] |= ψi

τ,J (aJ\{k}, b(cid:48)

τ,i), where J is the connected component of τ with k ∈ J,

(d) for all connected components I of τ with k (cid:54)∈ I we have G(cid:63)[X (aI )] |= ψi

τ,I (aI ), where X (aI )

(e) b(cid:48)

denotes the bag X (amin(I)), and
τ,i ≥ ak,
As there are only a constant number of pairs (τ, i), we can compute all b(cid:48)

and output the smallest
of them. By the Rank-Preserving Normal Form Theorem 5.4, this is the correct answer. If all of them are
equal to Null, we output Null.

From now on, we consider a fixed pair (τ, i), and we therefore omit the subscript (τ, i). Let τ (cid:48) be the
subgraph of τ induced on {1, . . . , k−1}. By the functionality provided by Step 2 of the preprocessing

τ,i

23

phase we can test in constant time whether τ G
the condition of item (a) cannot be satisfied by any b(cid:48) ∈ V . Therefore, we can safely output Null.

r (a) = τ (cid:48). If this is not the case, we know that for this τ ,

r (a) = τ (cid:48), we proceed as follows.

Otherwise, i.e., if τ G
By Step 5 of the preprocessing phase, we can check in constant time whether G(cid:63) |= ξi
τ

. We thus know
if item (b) is satisfied. Furthermore, using the functionality provided in Step 6 of the preprocessing phase,
we can test in constant time for all connected components I of τ with k (cid:54)∈ I, whether G(cid:63)[X (aI )] |=
τ,I (aI ). Afterwards, we know if the item (d) is satisfied.
ψi

If one of the items (b) or (d) is not satisfied, we know that there is no matching solution for this a and

(τ, i), and we can therefore safely output Null.

Otherwise, i.e., if the items (b) and (d) are satisfied, we let J be the connected component of τ with

k ∈ J, and we proceed with the two following cases.

Case I: J = {k}.

In this case, every matching solution b(cid:48) for this a, this b := ak, and this (τ, i) has to be of distance
greater than r to every element in a. Consider the bags X (a1), . . . , X (ak−1), let k(cid:48) := |{X (aν) | ν ∈
{1, . . . , k−1}}|, and let X1, . . . , Xk(cid:48) be a list of these bags. Clearly, k(cid:48) ≤ k−1, and for each component aν
of a, there is exactly one κ such that X (aν) = Xκ.

For each κ ≤ k(cid:48), we have the following definitions:

- Let pκ be the number of elements in {a1, . . . , ak−1} that belong to Xκ, and let

cκ := (cκ,1, . . . , cκ,pκ ) be a list of all these elements.

- Let y consist of the variables xν for all ν ∈ {1, . . . , k−1} such that aν = sXκ
- Let c(cid:48)

) be a list of all elements of cκ that are not equal to sXκ

κ,1, . . . , c(cid:48)

κ = (c(cid:48)

.

κ,p(cid:48)
κ

.

- Let bκ be the smallest element of Xκ \ {sXκ} with bκ ≥ b, obtained using the data structure of the

Storing Theorem 3.1 when computing the neighborhood cover.

We compute the following 2k(cid:48) + 1 answer candidates:

• For each κ ∈ {1, . . . , k(cid:48)} we want to compute the smallest element in Xκ \ {sXκ } that is far away

from all the nodes in the list cκ and that is ≥ bκ.
More precisely, we define b(cid:48)
κ
and b(cid:48)
items (a), (c) and (e) .

as the smallest element of Xκ\{sXκ } such that G(cid:63)
(cκ, b(cid:48)
κ)
Xκ
κ ≥ bκ. Note that this is exactly the smallest node in Kr(Xκ) \ {sXκ} that satisfies the

|= Ψi

τ,J,pκ

From the statement made at the end of Step 9 of the preprocessing phase, we know that to compute
, we can use the functionality provided by Step 10 of the preprocessing phase: For all
this node b(cid:48)
κ
κ ≤ k(cid:48), we compute b(cid:48)
κ)
κ
and b(cid:48)

as the smallest element in Xκ \ {sXκ } such that H (cid:63)
Xκ

τ,J,pκ,y(c(cid:48)

|= Ψ(cid:48)i

κ, b(cid:48)

κ ≥ bκ.

• For each κ ∈ {1, . . . , k(cid:48)} we want to check if G(cid:63)
. If this is the case we set b(cid:48)(cid:48)
κ

|= Ψi
τ,J,pκ
, otherwise to Null.

b := sXκ
By the statement made at the end of Step 9 of the preprocessing phase, this check can be performed
by using the functionality provided by Step 11 of the preprocessing phase: We simply check in
constant time if H (cid:63)
Xκ

τ,J,pκ,y∪{xk}(c(cid:48)

(cκ, b) holds for the particular node

to sXκ

|= Ψ(cid:48)i

κ).

Xκ

• Using the functionality provided by Step 13 of the preprocessing phase, we compute in constant

time b(cid:48)
0

, the smallest element of the set

{ b0 ∈ L | b0 (cid:54)∈

(cid:91)

κ≤k(cid:48)

Kr(Xκ) ∧ b0 ≥ b}

where L := Li

τ,J

is the set computed in Step 12 of the preprocessing phase.

24

It should be clear that every b(cid:48)
κ

is a matching solution for a and (τ, i). Let us now argue
that the smallest such matching solution for a and (τ, i) (that is ≥ b) is one of them: Note that any
matching solution b(cid:48) for a and (τ, i) is either in the r-kernel of one of the canonical bags X (aν), and then
it must be one of the b(cid:48)
κ

. Therefore, we can safely output

, or it must be b(cid:48)
0

or one of the b(cid:48)(cid:48)
κ

and b(cid:48)
0

, sXκ

b(cid:48) := min

(cid:16)

{b(cid:48)

κ | κ ≤ k(cid:48)} ∪ {b(cid:48)(cid:48)

κ | κ ≤ k(cid:48)} ∪ {b(cid:48)

0}

(cid:17)

.

Case II: {k} (cid:32) J

W.l.o.g. let us assume that 1 ∈ J and that {1, k} is an edge in τ .
Regarding item (c), note that the Rank-Preserving Normal Form Theorem 5.4 tells us that instead of

the bag X (b(cid:48)) we can use any bag X that r-covers aJ .

We define:

- X := X (a1). Note that every b ∈ V that satisfies item (a) belongs to X and, moreover, X r-covers

aJ for a = (a1, . . . , ak−1).

- Let p be the number of elements of {a1, . . . , ak−1} that belong to X but not to the tuple aJ . Let

c1, . . . , cp be the list of all these elements.

- Let c(cid:48)

1, . . . , c(cid:48)
- Let Γ := J \ {k}.

p(cid:48) be the elements of c1, . . . , cp that are not equal to sX .

- Let a(cid:48)
Γ

be the tuple obtained from aΓ by removing all components whose entry is sX .

- Let y consist of the variables xν for all ν ∈ {1, . . . , k−1} such that aν = sX .
- Let bX be the smallest element of X \ {sX } that is ≥ ak. It is derived from the data structure of

the Storing Theorem 3.1 obtained when computing the neighborhood cover.

Since all matching b(cid:48) must be close to a1 in this case, it suffices to compute the following two elements:

• We want to compute the smallest element in X \ {sX } that is far from all the nodes c1, . . . , cp.
More precisely, we want to compute the smallest b(cid:48) in X that is ≥ ak (and therefore ≥ bX ) such
that

τ,J,p(c1, . . . , cp, aΓ, b(cid:48)).
Note that such a node precisely satisfies the items (a), (c) and (e).

X |= Ψi

G(cid:63)

From the statement made at the end of Step 9 of the preprocessing phase, we know that to compute
such nodes b(cid:48), we can use the functionality provided by Step 10 of the preprocessing phase: We
compute the smallest b(cid:48)
1

in X \ {sX } such that b(cid:48)

H (cid:63)

X |= Ψ(cid:48)i

τ,J,p,y(c(cid:48)

1 ≥ bX and
p(cid:48), a(cid:48)
1, . . . , c(cid:48)

Γ, b(cid:48)

1).

• We also want to check if G(cid:63)

and if so, we want to output it.

X |= Ψi

τ,J,p(c1, . . . , cp, aΓ, b(cid:48)

2) holds for the particular node b(cid:48)

2 := sX ,

From the statement made at the end of Step 9 of the preprocessing phase, we know that this check
can be performed by using the functionality provided by Step 11 of the preprocessing phase: We
simply check in constant time if

H (cid:63)

X |= Ψ(cid:48)i

τ,J,p,y∪{xk}(c(cid:48)

1, . . . , c(cid:48)

p(cid:48), a(cid:48)

Γ).

The final output is b(cid:48) := min{b(cid:48)

2}. This concludes the description of the answering procedure.
While describing this procedure, we have already verified that it outputs exactly the smallest b(cid:48) ∈ V that
is ≥ b and satisfies G |= ϕ(a, b(cid:48)).

1, b(cid:48)

As we compute only a constant number of answer candidates (at most 2k + 1 for each pair (τ, i)) and

then take the smallest of them, the correct solution is computed in constant time.

This completes the proof of Lemma 5.2 and hence also completes the proof of Theorem 5.1.

25

6 Conclusion

We have shown how to efficiently enumerate the results of first-order queries over any nowhere dense
class of databases. We achieved constant delay enumeration after a pseudo-linear time preprocessing.
We also showed that after a pseudo-linear preprocessing we can, on input of an arbitrary tuple, test in
constant time whether it is a solution to the query.

We did not mention the size of the constant factor. Already for boolean queries the constant factor is
at least a tower of exponentials whose height depends on the size of the query. Moreover, an elementary
constant factor is not achievable if the class of structures contains all trees (unless FPT = AW[∗], cf.
[14]).

Furthermore, when C is not effectively nowhere dense, the main algorithm is not even FPT. We
carefully highlighted the steps that require C to be effectively nowhere dense in order to obtain computable
constant factors.

An improvement of our work would be to extend the results to a dynamic setting that avoids recom-
puting from scratch the index built during the preprocessing phase. For instance, the index structure
allowing for constant delay enumeration can be updated in constant time in the setting of FO-queries
over classes of databases of bounded degree [7] and in the setting of q-hierarchical unions of conjunctive
queries over arbitrary databases [6, 8]. In the nowhere dense case, constant update time seems unrealistic,
as already for boolean queries over trees the best we can do so far are logarithmic time updates [5].

It seems plausible that there exists an index structure, computable in pseudo-linear time and allowing
for constant delay enumeration and logarithmic time updates. Preliminary results were obtained in this
direction for very simple structures such as words [28] and trees [2]. Generalizations to more complex
structures remains for future work.

One could finally wonder whether a linear preprocessing time can be achieved. This would in par-
ticular imply that the model checking problem could be solved in time linear in the size of the database.
Up to now, the best time complexity for the model checking problem over nowhere dense databases is
pseudo-linear and it is an open problem whether this can be done in time linear in the size of the input
database.

7 Appendix: Proofs from Section 3

This section is devoted to the proof Theorem 3.1. For ease of read, we recall the Theorem and some parts
of Section 3.

Theorem (Storing Theorem). For every fixed k ∈ N and (cid:15) > 0, there is an integer c ∈ N such that for every
integer n ∈ N there is a data structure that stores the value of a k-ary function f of domain Dom(f ) ⊆ [n]k
with:

• initialization time c · |Dom(f )| · n(cid:15),

• update time c · n(cid:15) whenever a pair5 (a, b) is added to or removed from f ,

• lookup time c,

• and at any point in time, the space used by the data structure is c · |Dom(f )| · n(cid:15).

Here, lookup means that given a tuple a ∈ [n]k, the algorithm either answers b if a ∈ Dom(f ) and f (a) = b;
or a(cid:48) if a (cid:54)∈ Dom(f ) and a(cid:48) := min{x ∈ Dom(f ) : x > a}; or Null if no such tuple exists.

7.1 Description of the data structure
Fix (cid:15) and n. Let d := (cid:100)n(cid:15)(cid:101) and h := (cid:6) 1
that x ≤ y.

(cid:15)

(cid:7). As usual, for x ∈ Q, (cid:100)x(cid:101) denotes the smallest integer y such

5we identify f with its graph {(a, b) | a ∈ Dom(f ), f (a) = b}

26

Every i ∈ [n] can be uniquely decomposed in base d into a string of length h whose letters are from
[0, d−1] since dh ≥ n. We arbitrarily assume that the string starts with the higher powers of d and ends
with the lowest ones. Given all this, every tuple in [n]k can be decomposed into a string of length kh
whose letters are from [0, d−1]. We then associate to the function f a partial tree T (f ) of maximal depth
kh and degree d, where each node has 0 or d children and each leaf at depth kh represents an element of
the domain of f (by looking at the sequence of child numbers in the path from the root to that leaf). The
size of T (f ) is then O(n(cid:15)·|Dom(f )|).

Our data structure is an encoding of T (f ) with extra information in order to navigate efficiently in the
tree and to update it efficiently. As for leaves, to any node of T (f ) at depth i we can associate a string over
[0, d−1] of length i. Given a leaf of T (f ) we associate a tuple b as the smallest tuple (in lexicographical
order) of the domain of f whose encoding has a prefix larger than the one of the current node.

Each inner node of the tree associated to f is represented by d+1 consecutive registers in our memory
each containing a pair (δ, r) where δ is either 0, 1 or −1 and r is a value that will help us navigating in
the tree.

Consider an inner node x of T (f ) and assume that x is the ith child of y. Let R be the ith register
representing y and R(cid:48) be the first register representing x. Then the content of R is (1, R(cid:48)) and the content
of the last register representing x contains (−1, R). This encodes the parent/child relation of T (f ). The
rest of the encoding will help updating the structure efficiently.

If the jth child of y is a leaf, for j ≤ d, then the content of the jth register representing y is (0, b)

where b is the tuple associated to that leaf.

In the case when x is at depth kh−1 (i.e. all its children are leaves), for i ≤ d, we set the content of
the ith register representing x as (1, f (a)) if the ith leaf of x represents a tuple a in the domain of f , and
as (0, b) otherwise where b is the tuple associated to that leaf.

Finally, we have a register R0 that contains the next available (unused) register.
Our data structure is illustrated in Figure 1.

7.2 Accessing the information.

7.2.1 Accessing the values of the function.

Given a k-tuple a ∈ [n]k, our goal is to test whether a is in the domain of f , and if so, to output f (a).

The two following procedures enable us to perform this in time O(kh), hence in constant time. Recall

that d := (cid:100)n(cid:15)(cid:101) and h := (cid:6) 1

(cid:7).

(cid:15)

We will use different registers names, R, S, and S(cid:48) for a better readability. While the R registers store
our functions, the S and S(cid:48) registers can be seen as two working tapes of constant size (kh). The first
procedure decomposes a k-tuple into a sequence of numbers in [0, d−1] of length kh. This is a simple
decomposition in base d using Euclidean division.

Algorithm 1 Decomposition(a1, . . . , ak)
1: for i = 1 to k do
2:
3:
4:

A ← ai
for j = h(i − 1) to hi − 1 do

(cid:5)

B ← (cid:4) A
d
Sj ← A − d · B
A ← B

5:
6:
7:
8: end for

end for

(cid:46) basically an Euclidean division

(cid:46) the quotient of A/d
(cid:46) the remainder of A/d

The above procedure sets registers S0, . . . , Skh−1 so that:

hi−1
(cid:88)

ai =

j=h(i−1)

Sj · dj−h(i−1).

27

The next procedure returns the value of f (a). It does so by navigating the tree structure downward

from the root using the decomposition of a in base d.

Algorithm 2 Access(a)

1: Decomposition(a)

2: l ← 1
3: bool ← 1
4: i ← 0
5: while i ≤ kh − 1 & bool = 1 do
6:

(bool, l) ← R(l+Si)
i ← i + 1

7:
8: end while
9: Return(bool, l)

decompose
S0, . . . , Skh−1

(cid:46)
(cid:46) contains the working register

using

a

registers

(cid:46) the current depth

(cid:46) follow the search path

The procedure returns a pair (bool, l) of the form (1, b) or (0, a(cid:48)). If the first component is 1 then

b = f (a). Otherwise a (cid:54)∈ Dom(f ) and a(cid:48) is the smallest tuple bigger than a in the domain of f .

7.2.2 Computing next and previous tuples.

In order to update our data structure, it will be useful to compute the smallest (resp. biggest) tuple that
is within the domain of f and strictly bigger (resp. smaller) than a given a. Given any k-tuple a, we let
a> := min{b ∈ Dom(f ) | b > a} and a< := max{b ∈ Dom(f ) | b < a}. If there is no element bigger
(resp. smaller) than a in the domain of the function, we set a> := Null (resp. a< := Null).

Recall that Access(a) returns (0, a>) when a does not belong to the domain of f , and (1, f (a))
otherwise. In the second case, Access(a+1) yields the desired result, where a+1 is the tuple immediately
following a in lexicographical order of [n]k. Altogether the computation is performed in time O(kh), i.e. in
constant time.

The computation of a< can be obtained similarly with a dual data structure using the reverse lexico-

graphical order instead of the lexicographical order.

7.3 Initialization and insertions
It remains to compute and update our data structure. The computation will be done by inserting tuples
in the domain of f one by one. If we can show that each insertion can be done in time O(n(cid:15)), then the
total time for computing the data structure is O(|Dom(f )| · n(cid:15)) as desired. In this section we show how
an insertion can be achieved. We first initialize the data structure with an empty f and then show how
to extend its domain with one extra tuple.

The initialization is pretty straightforward. We build the root of the tree where everything points to

Null, this means creating its d children.

Algorithm 3 Init( )
1: R0 ← d + 2
2: for i = 1 to d do
3:
4: end for
5: Rd+1 ← (−1, Null)

Ri ← (0, Null)

(cid:46) update the total memory currently used

Adding information is, however, a bit more challenging. It requires two things: find the correct subtree

to add or remove the information, and update the content of the register with the appropriate values.

We now show that, given a pair (a, b), we can add to the data structure the information that f (a) = b

and accordingly update the data structure in time O(n(cid:15)).

28

The update procedure can be decomposed into two steps. The first one adds (a, b) to the structure
using the Insert procedure described below. The second one updates the content of the relevant registers.
The main goal (an difficulty) of the second step is to update every values of the form (0, b) that are
impacted by the addition (or removal) of the tuple. This is the purpose of the Clean subroutine, which is
used both when we add and when we remove tuples. To do so, we need the two tuples a< and a>, i.e.,
the biggest tuple in Dom(f ) that is smaller than a and the smallest one that is bigger than a. Recall that
both a< and a> can be computed in time O(kh) as explained in Section 7.2.2. The key observation is
that the cells of the form (0, b) that require an update must lie between the search paths for a< and a>.
There are few such cells: O(dkh).

For example, consider the data structure of Figure 1, and the case where 19 must be removed from
the domain. We first compute the surrounding elements of 19: 5 and 24. and look for the path leading to
19. We then conclude that the array stored in cells R21 − −R24 is now irrelevant. We therefore move the
content of the array R25 − −R28 in place of R21 − −R24. Immediately after that we update the content
of R15 that should now contain (1, 21) and R0 that should contain 25. Finally, we look at each cell that
lies between the paths going to 5 and 24, and replace the value (0, 19) by (0, 24) in cells R7, R2, R13, and
R14.

To recapitulate, the next procedure updates the tree structure by adding a in the domain of f and
setting b = f (a). The subroutine Decomposition gives the path in the tree leading to the leaf coding a.
If some nodes along this path are missing they will be created in a top-down fashion when invoking the
subroutine Insert. The Clean subroutines ensure that the leaves of the tree that do not correspond to a
tuple in the domain of f do point to the closest larger tuple within the domain of f .

Algorithm 4 Add(a, b)
1: Compute a< and a>
2: Decomposition(a)
3: Insert(1, 0, b)

4: Clean(a<, a)

5: Clean(a, a>)

(cid:46) See Section 7.2.2
(cid:46) Decompose a using register S0, . . . , Skh−1
(cid:46) Insert the desired leaf and its ancestors at the right

places, i.e. as specified by S0, . . . , Skh−1

(cid:46) the leaf nodes between a< and a whose content is (0, x)

should be updated in order to replace x by a

(cid:46) the leaf nodes between a and a> whose content is (0, x)

should be updated in order to replace x by a>

The main subroutine is Insert(l, i, b) whose goal is to insert if necessary a new node at depth i along
the path specified by S0, . . . , Skh−1 in order to eventually, when i is kh − 1, set the value b for f (a).
Initially it starts with the root, i = 0, and the first register representing the root i.e. R1 and l = 1. This
is done top-down in the obvious way. Recall that each time we create a new node we need to created it
d siblings.

29

Algorithm 5 Insert(l, i, b)
1: if i = kh − 1 then
Rl+Si ← (1, b)
2:

3: else
4:

(bool, l(cid:48)) ← Rl+Si

if bool = 0 then

Rl+Si ← (1, R0)
l(cid:48) ← R0
for j = 0 to d − 1 do
RR0+j ← (0, 0)

5:
6:

7:
8:
9:

10:

end for
RR0+d ← (−1, l + Si)
R0 ← R0 + d + 1

end if
Insert(l(cid:48), i + 1, b)

11:
12:
13:
14:
15: end if

(cid:46) we are at the leaves level,
(cid:46) the content of the register representing a is set to

the value b = f (a)

(cid:46) we look at the content of the Sth
i

register of the
current node. bool says whether there is already
an Sth
i

child in the data structure.

(cid:46) we need to create a new subtree
(cid:46) we use d new registers for that

(cid:46) their content will get their correct value later dur-

ing the Clean procedures

(cid:46) the last register points to the parent
(cid:46) R0 contains the last available memory
(cid:46) in any case, R(l+Si) now contains (1, l(cid:48)).
(cid:46) we continue down within the correct subtree

It remains to describe the cleaning subroutine. Clean(a, b) is expected to replace the content of all
the leaf nodes between a and b of the form (0, x) by (0, b). This is done by a simple depth-first left-first
traversal of the tree, starting from a and ending in b. There are two special cases when a = Null and
when b = Null. Note that it is called with either Clean(a<, a) or Clean(a, a>) hence at least one of its
inputs, namely a is not Null. The other input may be Null if a is the first or last element in the domain of
f . Notice also that by definition of a< all nodes between a< and a are leaves, same with a and a>.

Algorithm 6 Clean(a1, a2)
1: if a1 (cid:54)= Null Decomposition(a1)
2: if a2 (cid:54)= Null Decomposition’(a2)
3: if a1 = Null then
4:

Fill_Left(1, 0, a2)

5: else if a2 = Null then
6:

Fill_Right(1, 0, Null)

7: else
8:

Fill(1, 0, a2)

9: end if

(cid:46) decompose a1 using registers S0, . . . , Skh−1
(cid:46) decompose a2 using registers S(cid:48)
0, . . . , S(cid:48)
kh−1
(cid:46) a2 is the first element in the domain of f
(cid:46) sets to (0, a2) the label of all nodes before the leaf

corresponding to a2

(cid:46) a1 is the last element in the domain of f
(cid:46) sets to (0, Null) the label of all the nodes after the

leaf corresponding to a1

(cid:46) sets to (0, a2) the labels of all leaf nodes between

the leaves corresponding to a1 and a2

The procedures Fill_Left, Fill_Right, and Fill use the information present in the registers S0, . . . , Skh−1

kh−1

0, . . . , S(cid:48)

. The procedure Fill_Right(l, i, a2), which is also invoked within Fill, assumes that
and S(cid:48)
a1 is in the domain of f and the path associated to a1 has been created in the data structure. It then
sets to (0, a2) the label of all nodes that are after the leaf decomposed as S0, . . . , Skh−1 in the depth-first
search order of the tree, starting from the node at depth i, pointed by Rl. It is only invoked in the context
where all those nodes are leaves. Hence it is enough to go along the path specified by the Si and to set
the content of all the siblings to (0, a2).

30

for l + Si < l(cid:48) < l + d do

Algorithm 7 Fill_Right (l, i, a2)
1: if i < kh then
2:
3:
4:
5:
6:
7: end if

end for
(bool, l) ← Rl+Si
Fill_Right(l, i + 1, a2)

Rl(cid:48) ← (0, a2)

(cid:46) we are working with an inner node

(cid:46) we set the appropriate value

(cid:46) as a1 is in the domain, bool = 1 and l is a pointer
(cid:46) we continue down within the tree

31

Similarly, the dual procedure Fill_Left(l, i, a2) assumes that a2 is in the domain of f and the path
associated to a2 has been created in the data structure. It then sets to (0, a2) the label of all nodes that
are before the leaf decomposed as S(cid:48)
in the depth-first search order of the tree, starting from
the node at depth i, pointed by Rl.

0, . . . , S(cid:48)

kh−1

Algorithm 8 Fill_Left (l, i, a2)
1: if i < kh then
2:
3:

for l ≤ l(cid:48) < l + S(cid:48)
Rl(cid:48) ← (0, a2)

i do

end for
(bool, l) ← Rl+S(cid:48)
Fill_Left(l, i + 1, a2)

i

4:
5:
6:
7: end if

(cid:46) we are working with an inner node

(cid:46) we set the appropriate value

(cid:46) as a2 is in the domain, bool = 1 and l is a pointer
(cid:46) we continue down within the tree

Finally, we combine the above two procedures in the appropriate way. Here, Fill(l, i, a) assumes that
a is in the domain of f and that the path associated to a exists already. It is done by first finding the
level i where Si and S(cid:48)
disagree and then call FillLeft and FillRight starting from this level to clean the
i
corresponding subtree.

i then
(bool, l(cid:48)) ← Rl+Si
Fill(l(cid:48), i + 1, a2)

Algorithm 9 Fill (l, i, a2)
1: if Si = S(cid:48)
2:
3:
4: else
5:
6:
7:
8:

end for
if i < kh then

Rl(cid:48) ← (0, a2)

for l + Si < l(cid:48) < l + S(cid:48)

i do

(cid:46) we spot the first level where Si and S(cid:48)
i

disagree

(cid:46) Si < S(cid:48)
i

(cid:46) we take care correctly of the current level

(cid:46) if we are not at a leaf level, we need to set the

subtrees appropriately

(bool, l(cid:48)) ← Rl+Si
Fill_Right(l(cid:48), i + 1, a2)
(bool, l(cid:48)) ← Rl+S(cid:48)
Fill_Left(l(cid:48), i + 1, a2)

i

9:
10:
11:
12:
13:
14: end if

end if

Note that each subroutine works in time linear in khd, hence in O(n(cid:15)). Given f , we can therefore

create the data structure for f in time O(n(cid:15)·|Dom(f )|) as required.

32

7.4 Removing information
When a tuple is removed, we start in the same way as for insertion, but we apply a further operation:
deleting one or several unused subtrees to prevent the data structure to grow indefinitely.

We now show that, given a pair (a, b), we can remove from the data structure the fact that a is in the
domain of f . This can require up to three steps. First, change the label of the leaf corresponding to a in
the data structure. Secondly, possibly remove the subtree containing a off the data structure. And finally,
clean the data structure between a< and a>.

Algorithm 10 Remove(a)
1: Compute a< and a>
2: Decomposition(a)
3: l ← Run(1, 0)
4: Cut(l)
5: Clean(a<, a>)

(cid:46) as explained above
(cid:46) decompose a using registers S0, . . . , Skh−1
(cid:46) find the node representing a in the structure
(cid:46) remove possible subtrees
(cid:46) ensure that all pairs (0, x) of the data structure

have the right value for x

The procedure Run(l, i) processes the tree structure downward returning by induction the register
child of Rl. It is initially invoked with (1, 0) and eventually will return the register of the leaf

for the Sth
i
corresponding to a.

Algorithm 11 Run(l, i)
1: if i < kh − 1 then
2:
3:
4: else
5:
6: end if

(bool, l(cid:48)) ← Rl+Si
Run(l(cid:48), i + 1)

Return(l)

(cid:46) we are not at the leaves level
(cid:46) we look at the content of the Sth
i
(cid:46) we continue in the correct subtree
(cid:46) we are at the leaves level

register of the current node.

The procedure Cut removes the subtree of a node if it no longer contains an element in the domain of
f . This is done bottom-up starting from a leaf corresponding to a tuple that has been removed from the
domain of f . As we always enforce that a node has 0 or d children we need to check whether all siblings
can be safely removed before removing the node and its siblings. We then reuse the newly freed memory
in order to optimize space.

33

Algorithm 12 Cut(l)
1: (bool, i) ← (0, 0)
2: while bool (cid:54)= −1 do
(bool, l(cid:48)) ← Rl+i
3:
i ← i + 1
4:
5: end while
6: l ← l + i − d − 1
7: (bool, i) ← (0, 0)
8: while i < d & bool = 0 do

(bool, l(cid:48)) ← Rl+i
i ← i + 1

9:
10:
11: end while
12: if bool = 0 then

13:
14:
15:

16:
17:

(−1, l(cid:48)) ← Rl+d
Rl(cid:48) ← (0, 0)
for 0 ≤ j ≤ d do

Rl+j ← R(R0−(d+1)+j)

end for

(−1, l(cid:48)(cid:48)) ← RR0−1
Rl(cid:48)(cid:48) ← (1, l)
R0 ← R0 − (d + 1)
Cut(l(cid:48))

18:
19:
20:
21:
22: end if

(cid:46) We go to the last child of the current node

(cid:46) We can now go to the first child

(cid:46) we check whether any sibling of l contains an element in

the domain of f , i.e. whether bool=1

(cid:46) if no, the node and its siblings can be safely removed and

their memory reused

(cid:46) we save the address of the parent of the current node
(cid:46) the value will be corrected later
(cid:46) we now need to save memory, moving the nodes at the end

of the memory in place of those just deleted

(cid:46) it remains to change the child relation of the parent of those

nodes

(cid:46) l(cid:48)(cid:48) now contains the address of their parents
(cid:46) the child relation is updated
(cid:46) update the last available memory
(cid:46) we start again with the parent level

All the procedures take time O(khd) and are therefore in O(n(cid:15)) as desired. This ends the appendix

presenting proof details of the Storing Theorem 3.1.

34

References

[1] Serge Abiteboul, Richard Hull, and Victor Vianu. Foundations of Databases. Addison-Wesley, 1995.

[2] Antoine Amarilli, Pierre Bourhis, Stefan Mengel, and Matthias Niewerth. Enumeration on trees with
tractable combined complexity and efficient updates. In Dan Suciu, Sebastian Skritek, and Christoph
Koch, editors, Proceedings of the 38th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Data-
base Systems, PODS 2019, Amsterdam, The Netherlands, June 30 - July 5, 2019, pages 89–103. ACM,
2019.

[3] Guillaume Bagan. MSO queries on tree decomposable structures are computable with linear delay.
In Zoltán Ésik, editor, Computer Science Logic, 20th International Workshop, CSL 2006, 15th Annual
Conference of the EACSL, Szeged, Hungary, September 25-29, 2006, Proceedings, volume 4207 of Lecture
Notes in Computer Science, pages 167–181. Springer, 2006.

[4] Guillaume Bagan, Arnaud Durand, and Etienne Grandjean. On acyclic conjunctive queries and
constant delay enumeration. In Jacques Duparc and Thomas A. Henzinger, editors, Computer Sci-
ence Logic, 21st International Workshop, CSL 2007, 16th Annual Conference of the EACSL, Lausanne,
Switzerland, September 11-15, 2007, Proceedings, volume 4646 of Lecture Notes in Computer Science,
pages 208–222. Springer, 2007.

[5] Andrey Balmin, Yannis Papakonstantinou, and Victor Vianu. Incremental validation of XML docu-

ments. ACM Trans. Database Syst., 29(4):710–751, 2004.

[6] Christoph Berkholz, Jens Keppeler, and Nicole Schweikardt. Answering conjunctive queries under
updates. In Emanuel Sallinger, Jan Van den Bussche, and Floris Geerts, editors, Proceedings of the 36th
ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2017, Chicago, IL,
USA, May 14-19, 2017, pages 303–318. ACM, 2017.

[7] Christoph Berkholz, Jens Keppeler, and Nicole Schweikardt. Answering FO+MOD queries under

updates on bounded degree databases. ACM Trans. Database Syst., 43(2):7:1–7:32, 2018.

[8] Christoph Berkholz, Jens Keppeler, and Nicole Schweikardt. Answering UCQs under updates and
in the presence of integrity constraints. In Benny Kimelfeld and Yael Amsterdamer, editors, 21st In-
ternational Conference on Database Theory, ICDT 2018, March 26-29, 2018, Vienna, Austria, volume 98
of LIPIcs, pages 8:1–8:19. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2018.

[9] Rodney G. Downey, Michael R. Fellows, and Udayan Taylor. The parameterized complexity of rela-
tional database queries and an improved characterization of W[1]. In Douglas S. Bridges, Cristian S.
Calude, Jeremy Gibbons, Steve Reeves, and Ian H. Witten, editors, First Conference of the Centre
for Discrete Mathematics and Theoretical Computer Science, DMTCS 1996, Auckland, New Zealand,
December, 9-13, 1996, pages 194–213. Springer-Verlag, Singapore, 1996.

[10] Arnaud Durand and Etienne Grandjean. First-order queries on structures of bounded degree are

computable with constant delay. ACM Trans. Comput. Log., 8(4):21, 2007.

[11] Arnaud Durand, Nicole Schweikardt, and Luc Segoufin. Enumerating answers to first-order queries
In Richard Hull and Martin Grohe, editors, Proceedings of the 33rd
over databases of low degree.
ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems, PODS’14, Snowbird,
UT, USA, June 22-27, 2014, pages 121–131. ACM, 2014.

[12] Markus Frick. Generalized model-checking over locally tree-decomposable classes. Theory Comput.

Syst., 37(1):157–191, 2004.

[13] Markus Frick and Martin Grohe. Deciding first-order properties of locally tree-decomposable struc-

tures. J. ACM, 48(6):1184–1206, 2001.

[14] Markus Frick and Martin Grohe. The complexity of first-order and monadic second-order logic

revisited. Ann. Pure Appl. Logic, 130(1-3):3–31, 2004.

35

[15] Haim Gaifman. On local and non-local properties. In Proceedings of the Herbrand Symposium, volume

107 of Studies in Logic and the Foundations of Mathematics, pages 105 – 135. Elsevier, 1982.

[16] Martin Grohe, Stephan Kreutzer, and Sebastian Siebertz. Deciding first-order properties of nowhere
dense graphs. In David B. Shmoys, editor, Symposium on Theory of Computing, STOC 2014, New York,
NY, USA, May 31 - June 03, 2014, pages 89–98. ACM, 2014.

[17] Martin Grohe, Stephan Kreutzer, and Sebastian Siebertz. Deciding first-order properties of nowhere

dense graphs. J. ACM, 64(3):17:1–17:32, 2017.

[18] Martin Grohe and Nicole Schweikardt. First-order query evaluation with cardinality conditions. In
Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,
PODS 2018, Houston, TX, USA, June 10–15, 2018. ACM, 2018. Full version available at CoRR, http:
//arxiv.org/abs/1707.05945, 2017.

[19] Wojciech Kazana. Query evaluation with constant delay. (L’évaluation de requêtes avec un délai con-

stant). PhD thesis, École normale supérieure de Cachan, Paris, France, 2013.

[20] Wojciech Kazana and Luc Segoufin. First-order query evaluation on structures of bounded degree.

Logical Methods in Computer Science, 7(2), 2011.

[21] Wojciech Kazana and Luc Segoufin. Enumeration of first-order queries on classes of structures with
bounded expansion. In Richard Hull and Wenfei Fan, editors, Proceedings of the 32nd ACM SIGMOD-
SIGACT-SIGART Symposium on Principles of Database Systems, PODS 2013, New York, NY, USA - June
22 - 27, 2013, pages 297–308. ACM, 2013.

[22] Wojciech Kazana and Luc Segoufin. Enumeration of monadic second-order queries on trees. ACM

Trans. Comput. Log., 14(4):25:1–25:12, 2013.

[23] Donald Ervin Knuth. The art of computer programming, , Volume III, 2nd Edition. Addison-Wesley,

1998.

[24] Stephan Kreutzer and Anuj Dawar. Parameterized complexity of first-order logic. Electronic Collo-

quium on Computational Complexity (ECCC), 16:131, 2009.

[25] Leonid Libkin. Elements of Finite Model Theory. Texts in Theoretical Computer Science. An EATCS

Series. Springer, 2004.

[26] Jaroslav Nesetril and Patrice Ossona de Mendez. First order properties on nowhere dense structures.

J. Symb. Log., 75(3):868–887, 2010.

[27] Jaroslav Nesetril and Patrice Ossona de Mendez. On nowhere dense graphs. Eur. J. Comb., 32(4):600–

617, 2011.

[28] Matthias Niewerth and Luc Segoufin. Enumeration of MSO queries on strings with constant delay
In Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on

and logarithmic updates.
Principles of Database Systems, PODS 2018, Houston, TX, USA, June 10–15, 2018. ACM, 2018.

[29] Nicole Schweikardt, Luc Segoufin, and Alexandre Vigny. Enumeration for FO queries over nowhere
In Jan Van den Bussche and Marcelo Arenas, editors, Proceedings of the 37th ACM
dense graphs.
SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, Houston, TX, USA, June 10-15,
2018, pages 151–163. ACM, 2018.

[30] Luc Segoufin and Alexandre Vigny. Constant delay enumeration for FO queries over databases
with local bounded expansion.
In Michael Benedikt and Giorgio Orsi, editors, 20th International
Conference on Database Theory, ICDT 2017, March 21-24, 2017, Venice, Italy, volume 68 of LIPIcs,
pages 20:1–20:16. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2017.

[31] Sebastian Siebertz. Nowhere Dense Classes of Graphs: Characterisations and Algorithmic Meta- The-

orems. PhD thesis, Technical University of Berlin, Germany, 2016.

36

[32] Robert Endre Tarjan and Andrew Chi-Chih Yao. Storing a sparse table. Commun. ACM, 22(11):606–

611, 1979.

[33] Moshe Y. Vardi. The complexity of relational query languages (extended abstract).

In Harry R.
Lewis, Barbara B. Simons, Walter A. Burkhard, and Lawrence H. Landweber, editors, Proceedings of
the 14th Annual ACM Symposium on Theory of Computing, May 5-7, 1982, San Francisco, California,
USA, pages 137–146. ACM, 1982.

[34] Alexandre Vigny. Query enumeration and nowhere dense graphs. (Énumération des requêtes et graphes

nulle-part denses). PhD thesis, Paris Diderot University, France, 2018.

37

