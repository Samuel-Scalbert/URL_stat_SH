Querying Inconsistent Prioritized Data with ORBITS:
Algorithms, Implementation, and Experiments
Meghyn Bienvenu, Camille Bourgaux

To cite this version:

Meghyn Bienvenu, Camille Bourgaux. Querying Inconsistent Prioritized Data with ORBITS: Algo-
rithms, Implementation, and Experiments. KR 2022 - 19th International Conference on Principles of
Knowledge Representation and Reasoning, Jul 2022, Haifa, Israel. ￿hal-03770516￿

HAL Id: hal-03770516

https://hal.science/hal-03770516

Submitted on 6 Sep 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Querying Inconsistent Prioritized Data with ORBITS:
Algorithms, Implementation, and Experiments

Meghyn Bienvenu1 , Camille Bourgaux2
1 CNRS & University of Bordeaux, France
2 DI ENS, ENS, CNRS, PSL University & Inria, Paris, France
meghyn.bienvenu@labri.fr, camille.bourgaux@ens.fr

Abstract

We investigate practical algorithms for inconsistency-tolerant
query answering over prioritized knowledge bases, which
consist of a logical theory, a set of facts, and a priority re-
lation between conﬂicting facts. We consider three well-
known semantics (AR, IAR and brave) based upon two no-
tions of optimal repairs (Pareto and completion). Decid-
ing whether a query answer holds under these semantics is
(co)NP-complete in data complexity for a large class of log-
ical theories, and SAT-based procedures have been devised
for repair-based semantics when there is no priority relation,
or the relation has a special structure. The present paper in-
troduces the ﬁrst SAT encodings for Pareto- and completion-
optimal repairs w.r.t. general priority relations and proposes
several ways of employing existing and new encodings to
compute answers under (optimal) repair-based semantics, by
exploiting different reasoning modes of SAT solvers. The
comprehensive experimental evaluation of our implementa-
tion compares both (i) the impact of adopting semantics based
on different kinds of repairs, and (ii) the relative perfor-
mances of alternative procedures for the same semantics.

1

Introduction

The question of how to handle data that is inconsistent w.r.t.
expressed constraints, be they given by database dependen-
cies or ontologies, has great practical relevance. Data clean-
ing addresses this problem by modifying datasets so that
they satisfy the constraints, often using heuristics to decide
how to resolve contradictions, which may result in wrong
facts being kept, or true facts removed (as discussed e.g. in
(Fan 2015)). An alternative, more principled, approach is
to adopt inconsistency-tolerant semantics in order to extract
meaningful information from the contradictory data.

In the database setting, such an approach goes by the
name of consistent query answering (CQA) and has been ex-
tensively studied since the seminal work of Arenas, Bertossi,
and Chomicki (1999), see e.g. the recent survey by Wijsen
(2019). A central notion is that of a (subset) repair, deﬁned
as a maximal subset of the dataset that satisﬁes the con-
straints. Intuitively, repairs represent all different ways of
minimally modifying the data to satisfy the constraints. As
we do not know which repair corresponds to the true part of
the data, the CQA semantics stipulates that a tuple is a query
answer if it is an answer w.r.t. every repair (in line with how
skeptical inference is deﬁned in many KR settings).

Inconsistency-tolerant semantics have also drawn consid-
erable interest in the setting of ontology-mediated query an-
swering (OMQA) (Poggi et al. 2008; Bienvenu and Ortiz
2015; Xiao et al. 2018), where the ontology not only speci-
ﬁes constraints on the data but also captures other forms of
domain knowledge, which can be exploited at query time. In
addition to the AR semantics (the OMQA analog of the CQA
semantics), several other inconsistency-tolerant semantics
have been proposed (see (Bienvenu and Bourgaux 2016;
Bienvenu 2020) for surveys and references), among which:
the brave semantics (Bienvenu and Rosati 2013), which only
requires a tuple to be an answer w.r.t. some repair, provides
a natural notion of possible answer, and the IAR semantics
(Lembo et al. 2010), which answers queries over the inter-
section of the repairs, identiﬁes the most reliable answers.

The basic notion of repair can be reﬁned by exploiting
preference information. A prominent approach, introduced
by Staworko, Chomicki, and Marcinkowski (2012) in the
database setting and recently explored in the OMQA set-
ting (Bienvenu and Bourgaux 2020), assumes that prefer-
ences are given by a binary priority relation between con-
ﬂicting facts. Three notions of ‘best’ repairs w.r.t. a priority
relation were proposed, namely, Pareto-optimal, globally-
optimal, and completion-optimal repairs, and can be used in
place of subset repairs in any repair-based semantics.

The complexity of answering queries under (optimal)
repair-based semantics has been extensively studied in the
database and OMQA settings, refer to (Wijsen 2019; Bi-
envenu and Bourgaux 2016) for an overview and refer-
ences. We can brieﬂy summarize these (many!) complex-
ity results as follows: query answering under the AR (or
CQA) semantics is coNP-hard in data complexity even in
the simplest of settings (e.g. key constraints, class disjoint-
ness), and adopting optimal repairs in place of subset re-
pairs leads to (co)NP-hardness for the brave and IAR se-
mantics as well. Membership in (co)NP holds for AR,
brave, and IAR semantics w.r.t. subset, Pareto-optimal, and
completion-optimal repairs in the most commonly consid-
ered settings i.e. for database constraints given by primary
keys or more generally, functional dependencies (FDs), and
for ontologies formulated in data-tractable description logics
such as those of the DL-Lite family (Calvanese et al. 2007).
The preceding (co)NP complexity results naturally sug-
gest the interest of employing SAT solvers. Two recent sys-

tems, CQAPri and CAvSAT, have begun to explore such
an approach. CQAPri (Bienvenu, Bourgaux, and Goas-
dou´e 2014; 2019) uses tractable approximations together
with calls to SAT solvers to answer queries over inconsis-
tent DL-Lite knowledge bases, under the AR, brave, and
IAR semantics, w.r.t. subset repairs as well as optimal re-
pairs for the restricted class of score-structured priority rela-
tions. CAvSAT (Dixit and Kolaitis 2019) targets relational
databases equipped with denial constraints (which include
FDs as a special case) and computes query answers under
the AR semantics w.r.t. subset repairs. While geared to dif-
ferent forms of constraints, the two systems solve essentially
the same problem, yet they employ SAT solvers in different
ways. CQAPri makes a single SAT call for each candidate
query answer, whereas CAvSAT treats all candidate answers
at the same time via calls to a weighted MaxSAT solver.

This paper presents a comprehensive study of the use of
SAT-based approaches for inconsistency-tolerant query an-
swering, which abstracts from the particular setting and pro-
vides a solid foundation for the future development of such
systems. Our contributions can be summarized as follows.
In Section 3, we provide propositional encodings of the AR,
brave, and IAR semantics, including the ﬁrst encodings for
Pareto- and completion-optimal repairs. Our encodings are
generic and are built in a modular manner from a core set of
basic formulas. Based upon these encodings, we develop
in Section 4 several SAT-based algorithms, which utilize
different functionalities of modern SAT solvers: weighted
MaxSAT, MUS enumeration, iterative SAT calls with as-
sumptions. In Section 5, we present our implemented sys-
tem, ORBITS, which computes query answers under the cho-
sen semantics using the selected encoding and algorithm.
Section 6 presents the result of our extensive experimental
evaluation, using existing OMQA and database benchmarks,
aimed at comparing the different semantics, and understand-
ing the relative performances of different encodings and/or
algorithms for the same semantics. Proofs, pseudo-code for
algorithms, and details on the experimental evaluation are
provided in the appendix of (Bienvenu and Bourgaux 2022).

2 Querying Inconsistent Knowledge Bases
We introduce notation and terminology for talking about
knowledge bases and then recall different inconsistency-
tolerant semantics. We shall assume throughout the paper
that readers are familiar with propositional and ﬁrst-order
logic. Key notions are illustrated in Example 1.

Knowledge bases By knowledge base (KB), we mean a
pair K = (D, T ) consisting of a dataset D and a logical
theory T . The dataset D is a ﬁnite set of ground facts, i.e.
atoms P (c1, . . . , cn) where P is an n-ary predicate and each
ci is a constant. The theory T is a ﬁnite set of ﬁrst-order
logic (FOL) sentences. An L KB is a KB whose theory is
formulated in the L fragment of FOL. Typically, T will be
either an ontology (with L a description logic or decidable
class of existential rules) or a set of database constraints, for
instance, letting L be the class of denial constraints, which
take the form ∀(cid:126)x¬(α1 ∧ . . . ∧ αn), where each αi is a rela-
tional or inequality atom whose variables are among (cid:126)x. De-

nial constraints generalize the more well-known functional
dependencies (FDs) and (primary) key constraints.

A KB K = (D, T ) is consistent, and its dataset D is
called T -consistent, if D ∪ T has at least one model. Oth-
erwise, K is inconsistent, denoted K |= ⊥. To identify
the reasons for a KB K = (D, T ) being inconsistent, we
use the notion of a conﬂict of K, deﬁned as an inclusion-
minimal subset D(cid:48) ⊆ D such that (D(cid:48), T ) |= ⊥. We use
Conf (K), or Conf (D, T ), to refer to the set of all conﬂicts
of K = (D, T ). Note that Conf (D, T ) ⊆ Conf (D(cid:48), T )
whenever D ⊆ D(cid:48).
In particular, this means that adding
more facts cannot render an inconsistent KB consistent.

We will be interested in answering queries over KBs. In
this paper, when we speak of queries, we mean conjunctive
queries, which take the form of conjunctions of relational
atoms P (t1, . . . , tn) (with each ti a constant or variable),
where some variables may be existentially quantiﬁed. Given
a query q((cid:126)x), with free variables (cid:126)x = (x1, . . . , xk), and a
tuple of constants (cid:126)a = (a1, . . . , ak), we denote by q((cid:126)a) the
ﬁrst-order sentence obtained by replacing each variable in (cid:126)x
by the corresponding constant in (cid:126)a. A (certain) answer to
q((cid:126)x) over K = (D, T ) is a tuple of constants (cid:126)a from D such
that q((cid:126)a) holds in every model of K. We use K |= q((cid:126)a) to
indicate that (cid:126)a is a certain answer to q((cid:126)x) over K.

Certain answers are preserved under the addition of facts:
if (D, T ) |= q((cid:126)a) and D ⊆ D(cid:48), then (D(cid:48), T ) |= q((cid:126)a). It
thus makes sense to consider the minimal subsets of the data
responsible for an answer. We call a T -consistent subset
C ⊆ D a cause for q((cid:126)a) w.r.t. K = (D, T ) if (C, T ) |= q((cid:126)a)
and (C(cid:48), T ) (cid:54)|= q((cid:126)a) for every C(cid:48) (cid:40) C; the set of causes for
q((cid:126)a) w.r.t. K is denoted by Causes(q((cid:126)a), K).

Observe that if K is inconsistent, then K |= q((cid:126)a) for every
candidate answer (cid:126)a, which is uninformative and motivates
the need for alternative inconsistency-tolerant semantics.

Repairs
In order to extract meaningful information from
an inconsistent KB, it is useful to consider the parts of the
data that are consistent with the theory. This can be formal-
ized using the notion of repair:
Deﬁnition 1. A (subset) repair of a KB K = (D, T ) is an
inclusion-maximal subset R ⊆ D such that (R, T ) (cid:54)|= ⊥.
We use SRep(K) to denote the set of repairs of KB K.

Subset repairs treats all facts equally. However, prefer-
ences between conﬂicting facts should be taken into account
when they are available. Following (Staworko, Chomicki,
and Marcinkowski 2012; Bienvenu and Bourgaux 2020), we
assume such preferences are given as a priority relation:
Deﬁnition 2. A priority relation (cid:31) for a KB K = (D, T )
is an acyclic binary relation over the facts of D such that if
α (cid:31) β, then there exists C ∈ Conf (K) such that {α, β} ⊆
C. We say that (cid:31) is total if for every pair α (cid:54)= β such that
{α, β} ⊆ C for some C ∈ Conf (K), either α (cid:31) β or β (cid:31) α.
A completion of (cid:31) is a total priority relation (cid:31)(cid:48) ⊇ (cid:31).
Deﬁnition 3. A prioritized KB K(cid:31) consists of a KB K =
(D, T ) and a priority relation (cid:31) for K.

We recall two1 natural ways of reﬁning the notion of re-

1Staworko et al. (2012) deﬁned a third notion, globally-optimal
repairs, which we do not consider due to their higher complexity.

pair to exploit priority relations (Staworko, Chomicki, and
Marcinkowski 2012; Bienvenu and Bourgaux 2020):
Deﬁnition 4. Consider a prioritized KB K(cid:31) with K =
(D, T ), and let R ∈ SRep(K).
• A Pareto improvement of R is a T -consistent B ⊆ D such
that there is β ∈ B \ R with β (cid:31) α for every α ∈ R \ B.

The repair R is a:
• Pareto-optimal repair of K(cid:31) if there is no Pareto improve-

ment of R.

• completion-optimal repair of K(cid:31) if R is a Pareto-optimal

repair of K(cid:31)(cid:48), for some completion (cid:31)(cid:48) of (cid:31).

We denote by PRep(K(cid:31)) and CRep(K(cid:31)) the sets of Pareto-
and completion-optimal repairs.

It is known that CRep(K(cid:31)) ⊆ PRep(K(cid:31)) ⊆ SRep(K),
with each of the inclusions potentially strict. Interestingly,
however, if (cid:31) is induced by assigning scores to facts, then
Pareto- and completion-optimal repairs coincide:
Deﬁnition 5. A priority relation (cid:31) for K = (D, T ) is score-
structured if there exists a scoring function s : D → N such
that for every pair of facts α and β that appear together in a
conﬂict of K, we have α (cid:31) β iff s(α) > s(β).
Theorem 1. (Livshits and Kimelfeld 2017; Bourgaux 2016)
Let K(cid:31) be a prioritized KB such that (cid:31) is score-structured.
Then CRep(K(cid:31)) = PRep(K(cid:31)).

Bourgaux (2016) further shows that for score-structured
priorities, Pareto- and completion-optimal repairs also co-
incide with the ⊆P -repairs from (Bienvenu, Bourgaux, and
Goasdou´e 2014) based upon lexicographic set inclusion.

if

q((cid:126)a),

Repair-based semantics We next recall three prominent
inconsistency-tolerant semantics (brave, AR, and IAR),
which we parameterize by the considered type of repair:
Deﬁnition 6. Fix X ∈ {S, P, C} and consider a prioritized
KB K(cid:31) with K = (D, T ), query q((cid:126)x), and tuple of constants
(cid:126)a from D with |(cid:126)x| = |(cid:126)a|. Then (cid:126)a is an answer to q over K(cid:31)
• under X-brave semantics, denoted K(cid:31) |=X
brave q((cid:126)a), if
(R, T ) |= q((cid:126)a) for some R ∈ XRep(K(cid:31))
• under X-AR semantics, denoted K(cid:31) |=X
AR
(R, T ) |= q((cid:126)a) for every R ∈ XRep(K(cid:31))
• under X-IAR semantics, denoted K(cid:31) |=X
R∈XRep(K(cid:31)) R

(B, T ) |= q((cid:126)a) where B = (cid:84)
The AR semantics is arguably the most natural way of
deﬁning plausible query answers, and it is the semantics
used for consistent query answering in databases (Bertossi
2011). The brave semantics captures the notion of possible
answers, while the IAR semantics identiﬁes the answers that
can be obtained using only the most reliable facts. Observe
that K(cid:31) |=X
Example 1. Let K = (D, T ) where D contains four facts
α = R(a, b), β = R(a, c), γ = S(d, c), and δ = S(d, b),
and T contains FDs ∀x, y, z¬(R(x, y) ∧ R(x, z) ∧ y (cid:54)= z),
∀x, y, z¬(S(x, y) ∧ S(x, z) ∧ y (cid:54)= z) and the denial con-
straint ∀x, y, z¬(R(y, x) ∧ S(z, x)).

IAR q ⇒ K(cid:31) |=X

AR q ⇒ K(cid:31) |=X

IAR q((cid:126)a),

brave q.

if

The conﬂicts of K are {α, β}, {γ, δ}, {α, δ} and {β, γ},
If we deﬁne (cid:31) by

hence SRep(K) = {{α, γ}, {β, δ}}.

α (cid:31) β and γ (cid:31) δ, we obtain PRep(K(cid:31)) = SRep(K) but
CRep(K(cid:31)) = {{α, γ}}. Indeed, any completion (cid:31)(cid:48) of (cid:31) is
such that α (cid:31)(cid:48) δ or γ (cid:31)(cid:48) β by acyclicity.

If q(x) = ∃yR(x, y), Causes(q(a), K) = {{α}, {β}}.

Hence, K(cid:31) |=C

IAR q(a), K(cid:31) |=P

AR q(a) but K(cid:31) (cid:54)|=P

IAR q(a).

We brieﬂy recall what is known about the complexity
of query answering under (optimal) repair-based semantics.
Note that when we speak of complexity, we mean data com-
plexity, measured solely in terms of the size of the dataset.

Theorems 2 and 3 summarize upper and lower bounds
from the database and ontology settings. All of them are
known (Staworko, Chomicki, and Marcinkowski 2012; Bi-
envenu and Bourgaux 2020; Rosati 2011), except the lower
bounds for X-brave and X-IAR semantics in the case of FDs,
proven in (Bienvenu and Bourgaux 2022). We say that a
logic L enjoys PTIME consistency checking (resp. query en-
tailment) if the problem of deciding whether K |= ⊥ (resp.
K |= q((cid:126)a)) for an input L KB is in PTIME.
Theorem 2. Let L be any FOL fragment that enjoys PTIME
consistency checking and query entailment, and let X ∈
{S, P, C}. Then query entailment for L KBs is

• in NP under X-brave semantics, and
• in coNP under X-AR and X-IAR semantics.
Theorem 3. Query entailment for L KBs is
• NP-hard under X-brave semantics (X ∈ {P, C})
• coNP-hard under X-AR semantics (X ∈ {S, P, C})
• coNP-hard under X-IAR semantics (X ∈ {P, C})
for any L that extends DL-Litecore, EL⊥, or FDs.
Remark 1. Query entailment under S-brave and S-IAR is
tractable both for DL-Lite ontologies and denial constraints
(Bienvenu and Rosati 2013).

3 SAT Encodings
The (co)NP complexity results from the previous section
suggest a SAT-based approach to query entailment under
(optimal) repair-based semantics. While our work is not the
ﬁrst to explore SAT encodings for inconsistency-tolerant se-
mantics, our contribution is a uniform approach that covers
a wide range of semantics and settings, and provides the ﬁrst
encodings for Pareto- and completion-optimal repairs.

3.1 Overview
Our propositional encodings are built from:
• the set Conf (K) of conﬂicts of the considered KB K,
• a set PotAns of potential answers, and for each (cid:126)a ∈

PotAns, the (non-empty2) set Causes(q((cid:126)a), K),

• the priority relation (cid:31) of K,

and are of polynomial size w.r.t. to these inputs. Note that for
DL-Lite ontologies and denial constraints, the sets of con-
ﬂicts, candidate answers, and their causes, can be computed
in PTIME via database query evaluation, so our encodings
will yield procedures of the expected co(NP) complexity.

2If Causes(q((cid:126)a), K) = ∅, q((cid:126)a) holds for none of our semantics.

To simplify the treatment, we shall assume that every con-
ﬂict contains at most two facts, a property that is satisﬁed by
the most common DL-Lite dialects and for FDs. (The exten-
sion to non-binary conﬂicts is discussed later in the section.)
By restricting our attention to binary conﬂicts, we can use
a convenient notation α⊥β in place of {α, β} ∈ Conf (K),
and deﬁne a graph representation of conﬂicts and priorities.
The directed conﬂict graph GK(cid:31) has facts from Conf (K) as
nodes and an edge from α to β iff α⊥β and α(cid:54)(cid:31)β.

Our encodings will use variables of the form xα to in-
dicate whether fact α appears in a (partial) repair. Includ-
ing one such variable for each fact in D would yield pro-
hibitively large encodings, which is why we use GK(cid:31) to fo-
cus on relevant facts: given any F ⊆ D, we let R(F ) be the
set of all facts that are reachable in GK(cid:31) from some α ∈ F .
Before proceeding to the details, let us give a high-level
overview of our encodings. All of the encodings try to con-
struct a set of facts that is consistent with the theory (i.e.
does not contain any conﬂicts) and that can be extended to
an optimal repair. For the X-AR semantics, we are trying
to build an optimal repair that does not entail the considered
query answer(s), which can be done by including facts that
contradict each of the answer(s) causes. For the X-brave
semantics, we must ensure that the repair entails the consid-
ered query answer(s), achieved by requiring the presence of
some cause in the chosen subset. Finally, for the X-IAR se-
mantics, we ensure the existence of optimal repairs that omit
a given cause or fact appearing in a cause by including facts
that contradict the considered cause or fact.

In the next section, we will present SAT-based algorithms
that consider each potential answer in turn, as well as algo-
rithms that treat all potential answers conjointly. For that
reason, in what follows, we will present encodings both for
a single answer and for several answers at a time.

3.2 Basic Building Blocks
Our various encodings rely upon some common ingredients,
which are presented next.

Absence or presence of causes To contradict a speciﬁc
cause C, we use ϕ¬C, with two alternative deﬁnitions in-
spired respectively by the CQAPri and CAvSAT encodings:

(cid:95)

(cid:95)

ϕ¬C =

xβ

(neg1)

ϕ¬C =

α∈C
(cid:95)

α∈C

α⊥β,α(cid:54)(cid:31)β

¬xα ∧

(cid:94)

(xα ∨

(cid:95)

xβ)

(neg2)

α∈C

α⊥β,α(cid:54)(cid:31)β

For encodings with multiple answers, we use a variant
ϕ(cid:48)
¬C(y) obtained by adding ¬y as a disjunct of the (ﬁrst)
clause of ϕ¬C, so it is only ‘active’ when the given variable
y is true. To block all causes of the BCQ q((cid:126)a), we can use

ϕ¬q((cid:126)a) =

(cid:94)

ϕ¬C

C∈Causes(q((cid:126)a),K)

or, in the case of encodings for several answers, a variant
¬q((cid:126)a) obtained by replacing ϕ¬C by ϕ(cid:48)
ϕ(cid:48)
If instead we want to force that cause C holds, we use:

¬C(x(cid:126)a).

ϕC =

(cid:94)

α∈C

xα

and to ensure some cause for BCQ q((cid:126)a) is present, we use:

ϕq((cid:126)a) =(

(cid:95)

xC) ∧

(cid:94)

(cid:94)

¬xC ∨ xα

C∈Causes(q((cid:126)a),K)

C∈Causes(q((cid:126)a),K)

α∈C

or, for multi-answer encodings, a variant ϕ(cid:48)
from ϕq((cid:126)a) by adding ¬x(cid:126)a as a disjunct of the ﬁrst clause.

q((cid:126)a) obtained

Consistency We use ϕcons(F ) to ensure that the valuation
of {xα | α ∈ F } corresponds to a T -consistent set of facts:

ϕcons(F ) =

(cid:94)

(cid:16)

¬xα ∨ ¬xβ

(cid:17)

.

α,β∈F,α⊥β

Extension to optimal repair The most intricate part of
the encoding is ensuring that the selected set of facts can be
extended to a repair of the desired type. To this end, we in-
troduce formulas of the form ϕX-max(F ), where F provides
the facts that may appear, and X is the type of repair.
• Subset repairs: we can simply set ϕS-max(F ) = (cid:62), as
every consistent set of facts extends to some S-repair.

• Pareto-optimal repairs: we prove that

the following
encoding of maximality for ⊆P -repairs (w.r.t. score-
structured (cid:31)) from Bienvenu et al.
(2014) in fact also
works for Pareto-optimal repairs and arbitrary (cid:31):

ϕP1-max(F ) =

(cid:94)

(xα ∨

(cid:95)

xβ)

α∈R(F )

α⊥β,α(cid:54)(cid:31)β

Essentially, it states that a relevant fact can only be omit-
ted if we include a non-dominated contradicting fact. We
also propose an alternative encoding with fewer variables:
(cid:94)

(cid:94)

(cid:95)

(¬xα ∨

xγ)

ϕP2-max(F ) =

α∈R−(F )

β(cid:31)α

β⊥γ,β(cid:54)(cid:31)γ

where R−(F ) = (cid:83)∞

i=1 Ri with R0 = F and

Ri+1 = Ri ∪ {γ | ∃α ∈ Ri, β (cid:31) α, β⊥γ, β (cid:54)(cid:31) γ}.
Intuitively, to include α ∈ F , we must contradict every
more preferred fact that conﬂicts with α, then do the same
for selected contradicting facts.

• Completion-optimal repairs (w.r.t. arbitrary (cid:31)): we use

ϕC-max(F ) =ϕpref ∧ ϕcompl ∧ ϕacyc
where the subformulas ϕpref , ϕcompl, and ϕacyc are deﬁned
in Figure 1. The formula ϕpref states that if xα is omitted,
then we must include a contradicting fact β that is pre-
ferred to α according to (cid:31)(cid:48). We use ϕcompl to ensure that
(cid:31)(cid:48) compares all contradicting facts and extends (cid:31), while
the acyclicity of (cid:31)(cid:48) is ensured by ϕacyc, which uses vari-
ables tα,β to compute the transitive closure of (cid:31)(cid:48).

Non-binary conﬂicts We brieﬂy discuss how to modify
the preceding formulas to handle non-binary conﬂicts. The
most essential difference is that instead of choosing a sin-
gle contradicting fact, we may need to choose a conjunction
of facts, and use additional variables to refer to them. We
must also redeﬁne GK(cid:31) as a directed hypergraph, and use
hypergraph reachability to deﬁne R(F ). Details of the re-
quired modiﬁcations are provided in (Bienvenu and Bour-
gaux 2022).

ϕpref =

(cid:94)

(cid:16)

xα ∨

(cid:95)

xβ→α

(cid:17)

∧

(cid:94)

(cid:16)

(¬xβ→α ∨ xβ) ∧ (¬xβ→α ∨ xβ(cid:31)(cid:48)α)

(cid:17)

α∈R(F )
(cid:94)

β∈R(F ),α⊥β
(cid:16)

xα(cid:31)(cid:48)β ∨ xβ(cid:31)(cid:48)α

(cid:17)

∧

α,β∈R(F ),α⊥β
(cid:16)

(cid:94)

¬xα(cid:31)(cid:48)β ∨ ¬xβ(cid:31)(cid:48)α

(cid:17)

∧

(cid:94)

xα(cid:31)(cid:48)β

α,β∈R(F ),α⊥β
(cid:94)

(cid:16)

¬xα(cid:31)(cid:48)β ∨ tα,β

(cid:17)

∧

α,β∈R(F ),α⊥β
(cid:16)

(cid:94)

¬xα(cid:31)(cid:48)β ∨ ¬tβ,α

(cid:17)

∧

α,β∈R(F ),α(cid:31)β
(cid:16)

(cid:94)

¬tα,β ∨ ¬xβ(cid:31)(cid:48)γ ∨ tα,γ

(cid:17)

ϕcompl =

ϕacyc =

α,β∈R(F ),α⊥β

α,β∈R(F ),α⊥β

α,β,γ∈R(F ),β⊥γ

Figure 1: Subformulas of ϕC-max(F ), which is used to ensure it is possible to extend the selected subset of F to a completion-optimal repair.

3.3 Propositional Encodings
We now present our encodings, built from the preceding
components following the intuitions given in Section 3.1.

Note that the following results hold no matter which vari-
ant of ϕ¬C we use and with ϕP-max instantiated as either
ϕP1-max or ϕP2-max. The notation facts(ϕ) will be used for
the set of facts α such that the variable xα occurs in ϕ.

X-AR semantics Formula ΦX-AR(q((cid:126)a)) in Figure 2 is
used to test whether a particular tuple (cid:126)a holds. Roughly
speaking, it selects a way to contradict every clause, check-
ing that the resulting set of facts is contained in a X-optimal
repair. The second formula ΨX-AR(PotAns) simultane-
ously handles all tuples in PotAns. Clauses of the form
x(cid:126)a can be added to activate ϕ(cid:48)

¬q((cid:126)a).
The correctness of our encodings is given in the next re-

sult, which applies to all X ∈ {S, P, C} and (cid:126)a ∈ PotAns:
Theorem 4. The following are equivalent : (i) K(cid:31) |=X
AR
q((cid:126)a), (ii) ΦX-AR(q((cid:126)a)) is unsatisﬁable, and (iii) x(cid:126)a is false
in every satisfying assignment of ΨX-AR(PotAns).

X-brave semantics Figure 2 presents our encodings for
the X-brave semantics. Formula ΦX-brave(q((cid:126)a)) checks
tuple (cid:126)a and is essentially the same as
a particular
ΦX-AR(q((cid:126)a)), but with ϕq((cid:126)a) in place of ϕ¬q((cid:126)a). A variant
ΦX-brave(C) can be used to check whether a particular cause
C holds in some X-optimal repair. For a multi-answer encod-
ings, we use the formula ΨX-brave(PotAns), again adding
clauses of the form x(cid:126)a to activate ϕ(cid:48)
q((cid:126)a). We obtain an anal-
ogous correctness result:
Theorem 5. The following are equivalent: (i) K(cid:31) |=X
brave
q((cid:126)a), (ii) ΦX-brave(q((cid:126)a)) is satisﬁable, (iii) ΦX-brave(C) is
satisﬁable for some C ∈ Causes(q((cid:126)a), K), and (iv) x(cid:126)a is
true in some satisfying assignment of ΨX-brave(PotAns).

X-IAR semantics Formula ΦX-IAR(C) from Figure 2 can
be used to test whether there exists a X-optimal repair
that excludes cause C. To check a potential answer (cid:126)a, we
use ΦX-IAR(q((cid:126)a)), which is essentially the conjunction of
ΦX-IAR(C) for every cause C of q((cid:126)a), but where the con-
juncts for different causes use distinct variables (xC
α in place
of xα). A multi-answer encoding ΨX-IAR(PotAns) can be
obtained by taking the conjunction of ΦX-IAR(q((cid:126)a)) for all
(cid:126)a ∈ PotAns, but with ϕC
¬C(x(cid:126)a) (see the
appendix of (Bienvenu and Bourgaux 2022) for details). We
obtain the following:

¬C replaced by ϕ(cid:48)C

Theorem 6. The following are equivalent: (i) K(cid:31) |=X
IAR
q((cid:126)a), (ii) ΦX-IAR(q((cid:126)a)) is unsatisﬁable, (iii) ΦX-IAR(C) is
unsatisﬁable for some C ∈ Causes(q((cid:126)a), K), and (iv) x(cid:126)a is
false in every satisfying assignment of ΨX-IAR(PotAns).

We shall also require encodings for individual facts, using

ΦX-IAR(α) = ΦX-IAR({α})

to test if α holds in all X-optimal repairs. We also con-
sider a multi-fact version ΨX-IAR(Rel ), parameterized by
a set of facts Rel , and to which we add clause yα to activate
ϕ(cid:48)

¬{α}(yα).

Theorem 7. For every α ∈ D, the following are equivalent:
(i) K(cid:31) |=X
IAR α, (ii) ΦX-IAR(α) is unsatisﬁable, and (iii) if
α ∈ Rel then yα is false in every satisfying assignment of
ΨX-IAR(Rel ).

4 Algorithms
Inspired by the different use of SAT solvers made by
CQAPri and CAvSAT, we propose several algorithms based
on the encodings of Section 3. The pseudo code of all algo-
rithms is available in (Bienvenu and Bourgaux 2022).

Before describing the algorithms, note that we can
show that the encodings of Section 3 are still valid if
Causes(q((cid:126)a), K) is actually a superset of the causes such
that every superﬂuous B in it either (1) includes a real cause
of q((cid:126)a) or (2) contains two distinct facts that form a conﬂict.
Since checking consistency and minimality to obtain the real
causes can be costly in practice, our algorithms accept such
‘sets of causes’. They actually even accept the presence of
self-inconsistent facts in the ‘causes’ (which would make the
encodings for X-AR and X-IAR not applicable) and handle
them in a preprocessing step.

Our high-level algorithm takes as input a semantics Sem ∈
{brave, AR, IAR}, a repair notion X ∈ {S, P, C}, a directed
conﬂict graph GK(cid:31) , and a set of potential answers PotAns
with their causes, and outputs the set of tuples from P otAns
that are answers under the X-Sem semantics.

An initial preprocessing step serves to (1) check whether
GK(cid:31) contains some self-inconsistent facts, remove them
from GK(cid:31), discard all causes that contain such facts, then all
answers that do not have any cause left, and (2) ﬁnd some an-
swers that trivially hold under X-Sem. To do so, it removes
from the causes all facts that do not have any outgoing edge
in the directed conﬂict graph, and thus trivially belong to all

ΦX-AR(q((cid:126)a)) = ϕ¬q((cid:126)a) ∧ ϕX-max(F1) ∧ ϕcons(F2)
¬q((cid:126)a) ∧ ϕX-max(F (cid:48)
ϕ(cid:48)

ΨX-AR(PotAns) =

(cid:94)

1) ∧ ϕcons(F (cid:48)
2)

(cid:126)a∈PotAns

ΦX-brave(q((cid:126)a)) = ϕq((cid:126)a) ∧ ϕX-max(G1) ∧ ϕcons(G2)

ΦX-brave(C) = ϕC ∧ ϕX-max(G∗

1) ∧ ϕcons(G∗
2)

ΨX-brave(PotAns) =

(cid:94)

q((cid:126)a) ∧ ϕX-max(G(cid:48)
ϕ(cid:48)

1) ∧ ϕcons(G(cid:48)
2)

(cid:126)a∈PotAns

F1 = facts(ϕ¬q((cid:126)a)), F2 = F1 ∪ facts(ϕX-max(F1))
1 ∪ facts(ϕX-max(F (cid:48)
F (cid:48)

1 = facts(

2 = F (cid:48)

(cid:94)

1))

¬q((cid:126)a)), F (cid:48)
ϕ(cid:48)
(cid:126)a∈PotAns

G1 = facts(ϕq((cid:126)a)), G2 = G1 ∪ facts(ϕX-max(G1))
G∗

1 = facts(ϕC), G∗
2 = G∗
q((cid:126)a)), G(cid:48)
ϕ(cid:48)
1 = facts(
(cid:126)a∈PotAns

(cid:94)

G(cid:48)

1 ∪ facts(ϕX-max(G∗
2 = G(cid:48)

1 ∪ facts(ϕX-max(G(cid:48)

1))

1))

ΦX-IAR(C) = ϕ¬C ∧ ϕX-max(H1) ∧ ϕcons(H2)
X-max(H C

¬C ∧ ϕC
ϕC

ΦX-IAR(q((cid:126)a)) =

(cid:94)

(cid:16)

1 ) ∧ ϕC

cons(H C
2 )

(cid:17)

H1 = facts(ϕ¬C), H2 = H1 ∪ facts(ϕX-max(H1))
X-max(H C
2 = H C

1 = facts(ϕC

1 ∪ facts(ϕC

¬C), H C

H C

1 ))

C∈Causes(q((cid:126)a),K)

Figure 2: Single- and multi-answer encodings for X-AR (top) and X-brave (middle) semantics, single-answer encodings for X-IAR (bottom).

optimal repairs. The trivial answers that have some cause
that contains only such facts hold under X-Sem semantics
and are ﬁltered during this step.

It then remains to ﬁlter the remaining potential answers.
The four ﬁrst algorithms we propose to do so are generic in
the sense that they can be used for all semantics.
• Simple is similar to the algorithm used by CQAPri. For
each answer to ﬁlter (cid:126)a, it checks whether ΦX-Sem(q((cid:126)a)) is
satisﬁable, which decides whether K(cid:31) |=X

Sem q((cid:126)a).

• All-MaxSAT is similar to the CAvSAT algorithm. It con-
structs a weighted MaxSAT instance ΨX-Sem(PotAns) ∧
(cid:86)
(cid:126)a∈PotAns x(cid:126)a, where the x(cid:126)a are soft clauses, and all other
clauses are hard. It then relies on the solver to maximize
the number of soft clauses satisﬁed, which ﬁlters the cor-
responding answers. After each iteration, the ¬x(cid:126)a corre-
sponding to the satisﬁed soft clauses are added to the set
of assumed literals for the next iteration.

• All-MUSes is based on the observation that if x(cid:126)a is
false in every satisfying assignment of ΨX-Sem(PotAns),
then {x(cid:126)a} is a minimal unsatisﬁable subset (MUS) of
(cid:86)
(cid:126)a∈PotAns x(cid:126)a w.r.t. ΨX-Sem(PotAns). All-MUSes relies
on the solver to compute all MUSes, and decides whether
K(cid:31) |=X
• Assumptions

Sem q((cid:126)a) by looking at those of size one.

iteratively evaluates ΨX-Sem(PotAns),
treating the variables x(cid:126)a, with (cid:126)a ∈ PotAns as assump-
tions. If ΨX-Sem(PotAns)[x(cid:126)a] is satisﬁable, there exists a
satisfying assignment of ΨX-Sem(PotAns) in which x(cid:126)a is
true, which decides whether K(cid:31) |=X

Sem q((cid:126)a).

While we may need to consider all causes to decide whether
an answer holds under X-AR semantics, in the X-brave or
X-IAR case it is sufﬁcient to ﬁnd a single cause that be-
longs to some or all optimal repairs. Moreover, the en-
coding ΦX-IAR(q((cid:126)a)) is a conjunction of independent sub-
problems built on distinct variables for each cause, which
does not seem very ﬁt for a SAT solver. We hence propose
algorithms speciﬁc to these cases.
• Cause-by-cause can be used for X-brave and X-IAR. For
each answer to ﬁlter (cid:126)a, it checks whether there is a cause C
of q((cid:126)a) such that ΦX-Sem(C) is (un)satisﬁable: if Sem =

brave and ΦX-Sem(C) is satisﬁable, or Sem = IAR and
ΦX-Sem(C) is unsatisﬁable, then K(cid:31) |=X
Sem q((cid:126)a); if no
cause witnesses K(cid:31) |=X
Sem q((cid:126)a).

Sem q((cid:126)a), then K(cid:31) (cid:54)|=X

• IAR-causes is speciﬁc to X-IAR. It considers the answers
in turn while maintaining two sets of facts:
the X-IAR
facts that belong to the intersection of the optimal repairs
and the non-X-IAR facts. For each cause C of q((cid:126)a) that
does not contain any known non X-IAR fact, it removes
the known X-IAR facts from C. If C becomes empty, then
K(cid:31) |=X
IAR q((cid:126)a). Otherwise, for each remaining α ∈ C, it
checks whether K(cid:31) |=X
IAR α using ΦX-IAR(α) and adds
α to the corresponding set of facts. If every α ∈ C is such
that K(cid:31) |=X

IAR α, then K(cid:31) |=X

IAR q((cid:126)a).

• IAR-facts is also speciﬁc to X-IAR and considers the an-
swers in turn while maintaining the two sets of X-IAR and
non X-IAR facts. The difference is that for each answer, it
uses ΨX-IAR(Rel ) ∧ (cid:86)
α∈Rel yα and a weighted MaxSAT
solver to decide which facts hold under X-IAR among the
set Rel of facts that belong to some cause and have not al-
ready been assigned to one of the two sets. Then it checks
whether there is a cause that only contains X-IAR facts.

5

Implementation & Experimental Setting

We implemented the algorithms presented in Section 4
in java 11. Our system ORBITS (Optimal Repair-Based
Inconsistency-Tolerant Semantics) takes as input two JSON
ﬁles containing the directed conﬂict graph GK(cid:31) , and the po-
tential answers PotAns of the query associated with their
causes. The user speciﬁes a semantics (AR, IAR, or brave),
a value X (among S, P1, P2, or C) to use in ϕX-max(F ), the
desired encoding for ϕ¬C (neg1 or neg2), and the algorithm
to use to compute the answers w.r.t. the chosen semantics.
The set of answers is output as a JSON ﬁle.

ORBITS relies on the Sat4j java library (version 2.3.4) to
solve the SAT, weighted MaxSAT, and MUS enumeration
problems (Berre and Parrain 2010).
In principle, a stan-
dalone solver could be used, but we found that the time
needed to print out the encoding to pass it to an external
solver tends to be prohibitive compared to using Sat4j.

The source code of ORBITS is available at https://github.
com/bourgaux/orbits, the inputs ﬁles we used in the exper-
iments at https://zenodo.org/record/5946827, and details on
the experimental setting in (Bienvenu and Bourgaux 2022).

Experimental Environment All experiments were run
with 16GB of RAM in a cluster node running CentOS 7.9
with linux kernel 3.10.0, with processor 2x Cascade Lake
Intel Xeon 5218 16 cores, 2.4GHz. Reported times are av-
eraged over 5 runs, with a 30 minutes time-out. Since we
aim at comparing our different algorithms and encodings, in
what follows, we focus on the time needed to ﬁlter the can-
didate answers, excluding the time needed to load the inputs
from the JSON ﬁles or serialize the output. This input load-
ing time is generally below 1 second and never exceeds a few
seconds. However, in real-world applications, we would not
use ORBITS as a standalone tool, but rather make it a library
to be integrated in a full query answering system.

Test KBs We evaluate ORBITS on three (sets of) KBs. The
ﬁrst is the CQAPri benchmark (Bourgaux 2016), a synthetic
benchmark crafted to evaluate inconsistency-tolerant query
answering over DL-Lite KBs, adapted from the LUBM∃
20
benchmark (Lutz et al. 2013). The two others, called Food
Inspection and Physicians, are real-world datasets built from
public open data, which have already been used to evalu-
ate data cleaning and consistent query answering systems
(Rekatsinas et al. 2017; Dixit and Kolaitis 2019). They con-
sist of relational databases built from the original csv ﬁles,
on which typical integrity constraints have been added. We
brieﬂy summarize their main characteristics below.

We use the DL-Lite ontology (which includes 875 dis-
jointness axioms) and 20 queries of the CQAPri bench-
mark, together with the 18 datasets named uXcY with X ∈
{1, 5, 20} and Y ∈ {1, 5, 10, 20, 30, 50}. Parameters X and
Y are related to the size and the proportion of facts involved
in some conﬂicts respectively (the higher the bigger), and
the datasets are such that uXcY ⊆ uXcY(cid:48) for Y ≤ Y (cid:48) and
uXcY ⊆ uX(cid:48)cY for X ≤ X (cid:48). Their sizes range from 75K
to 2M facts and their proportions of facts involved in some
conﬂict from 3% to 46%. The induced conﬂict graphs con-
tain from 2K to 946K facts and from 2K to 3M conﬂicts.

The Food Inspection dataset contains data about inspec-
tion of restaurants in New York and Chicago (Dat c; Dat a).
We use the database schema and six queries proposed by
Dixit and Kolaitis (2019): there are four relations, each hav-
ing a key constraint and one having a further FD. The dataset
contains 523K facts, 37% of them belong to some conﬂict.
The conﬂict graph contains 192K facts and 219K conﬂicts.
We build the Physicians dataset from the National Down-
loadable File provided by the Centers for Medicare & Med-
icaid Services (Dat b). It contains information on medical
professionals and their afﬁliations. We decompose it into
seven relations, add four reasonable key constraints and two
FDs, and design six queries. In total, the dataset contains
more than 8M facts and 2% of them are in some conﬂict.
The conﬂict graph contains 183K facts and 2.7M conﬂicts.

Priority Relations We build score-structured priority re-
lations by randomly assigning each fact a score between 1

Triv.

IAR\Triv. AR\IAR brave\AR not brave

1,655

S
{P,C}-2
{P,C}-5
P-0.5
C-0.5
P-0.8
C-0.8

1,668
1,679
1,671
1,671
1,680
1,680

0
1
1
23
5
24

7
7
7
7
0
7
0

77
48
29
45
30
25
13

0
16
23
15
15
22
22

Table 1: Number of answers of q1 over the Physicians dataset de-
pending on the priority relation (none, score-structured with n = 2
or n = 5, and not score-structured with p = 0.5 or p = 0.8) and
type of repairs (standard, Pareto- or completion-optimal).

and n. To construct a non-score-structured priority relation,
we consider each conﬂict and assign a random direction to
the corresponding edge in the conﬂict graph with a proba-
bility p, except if doing so creates a cycle, and verify that
the resulting priority is indeed not score-structured. On the
Food Inspection and Physicians datasets, we build four pri-
ority relations: two score-structured with n = 2 and n = 5,
and two non-score-structured with p = 0.5 and p = 0.8. For
the CQAPri benchmark, we build two priority relations, one
score-structured with n = 5 and one non-score-structured
with p = 0.8, on our largest dataset (u20c50), then propa-
gate them to the other datasets.

6 Experimental Evaluation
Our experimental evaluation aims at assessing (i) the impact
of adopting different kinds of repairs, and (ii) the relative
performances of alternative procedures for the same seman-
tics. More precisely, we consider the following questions.

• What is the impact in terms of number of answers of
adopting optimal repairs rather than standard repairs, or
completion-optimal repairs instead of Pareto-optimal re-
pairs when the priority relation is not score-structured?

• What is the impact of using one kind of repairs rather than

another on the computation time?

• Given a semantics and type of repair, what is the impact

in terms of computation times of the choice of:

– how to encode optimality (P1 or P2 for Pareto-optimal

repairs, P1, P2 or C when (cid:31) is score-structured)?

– how to encode contradictions (ϕ¬C with neg1 or neg2)?
– the algorithm used to ﬁlter the non-trivial answers?

In what follows, we summarize our main observations.
Detailed results are given in (Bienvenu and Bourgaux 2022).

Comparing Semantics w.r.t. Number of Answers Table
1 shows the impact on the number of answers of the type of
priority relation and chosen notion of optimal repairs for an
example query. For each priority relation and repair type X,
it gives the number of answers that: are trivially X-IAR (i.e.
some cause contains only facts without outgoing edges in the
directed conﬂict graph), hold under X-IAR but not trivially,
hold under X-AR but not under X-IAR, hold under X-brave
but not X-AR, and do not hold under X-brave semantics.

0
0
0
,
2

0
0
0
,
1

0

R
A

2
R
A

2
R
A
I

2
e
v
a
r
b

5
R
A

5
R
A
I

5
e
v
a
r
b

5
.
0
R
A
-
P

5
.
0
R
A
-
C

5
.
0
R
A
I
-
P

5
.
0
e
v
a
r
b
-
P

5
.
0
R
A
I
-
C

5
.
0
e
v
a
r
b
-
C

8
.
0
R
A
-
P

8
.
0
R
A
-
C

8
.
0
R
A
I
-
P

8
.
0
e
v
a
r
b
-
P

8
.
0
R
A
I
-
C

8
.
0
e
v
a
r
b
-
C

Figure 3: Best running times (in milliseconds) for each semantics
and priority relation (none, score-structured with n = 2 or n = 5,
not score-structured with p = 0.5 or p = 0.8) for query q2 over the
Food Inspection dataset. An empty bar means that the query ran out
of time / memory for all possible algorithms and encodings. The
lower part of bars is the time to identify self-inconsistent facts and
trivial answers, the upper part the time to ﬁlter non-trivial answers.

Priority relations leads to fewer edges in the directed con-
ﬂict graph, which in turn makes more answers hold trivially.
The more the priority relation sets preferences between facts
(higher parameter n or p of the priority relation), the more
trivially X-IAR answers we obtain. Adopting optimal re-
pairs also signiﬁcantly increases the number of potential an-
swers that do not hold under X-brave semantics, which are
very rare when using classical subset repairs.

An interesting observation is that while, in the absence of
a priority relation, many queries of the CQAPri benchmark
do not have any AR answers that are not trivial, which makes
trivial answers a good approximation of AR, this is no longer
the case for optimal repairs. It hence seems even more im-
portant to actually compute the answers that hold under the
desired semantics rather simply computing the polynomial
lower bound given by the trivial answers.

Regarding the impact of the choice between Pareto- and
completion-optimal repairs, in many cases ORBITS did not
manage to compute the answers for completion-optimal re-
pairs in our given time and memory limits. When it does
manage to compute them (for 5 queries on the Physicians
dataset; none on the Food Inspection dataset; and from be-
tween 7 and 13 on uXc1 to less than 3 on uXc50), we observe
a difference with Pareto-answers in only two cases.

Comparing Semantics w.r.t. Computation Time Figure
3 shows the best running times (across all algorithms and
encoding variants) for each semantics and an example query.
We did this comparison for all queries on Physicians and
Food Inspection datasets and two CQAPri datasets.

Given a kind of repair X, the relative difﬁculty of the X-
AR, X-IAR and X-brave semantics depends on the query,
dataset and sometimes the nature of the priority relation.

Comparing S-AR, P-AR, and C-AR semantics, we ob-
serve that using optimal repairs may either increase or de-
crease the answer ﬁltering time, intuitively because there are

q1

P1
417
804
P2
C 252,694

q2

141
142
179

P1
P2
C

P1
P2
C

P1
P2
C

268
166
502
163
oom 632

154
272
466
146
oom 624

166
362
566
193
oom 764

383
P1
P2
565
C 192,429

135
157
164

Alg. 1

Alg. 2

Alg. 3

Alg. 4

Alg. 5

q3

350
379
550

326
333
t.o

313
281
t.o

997
972
t.o

335
309
544

q4

q5

12,799
326,594

224
213
oom 284

1,730
2,961
t.o

t.o
t.o
t.o

42,544
36,923
t.o

214
221
551

211
201
550

281
304
846

8,192
225,170

211
207
oom 238

q6

4,009
8,684
t.o

11,263
10,833
oom

245,804
241,030
oom

559,923
546,199
oom

3,419
5,963
t.o

Table 2: Query answer ﬁltering time (in milliseconds, t.o:time
out, oom:out of memory) under X-brave semantics (X ∈ {P,C}),
for each algorithm and encoding ϕX-max, on Physicians dataset
with score-structured priority (n = 2). Alg. 1: Simple, Alg. 2:
All-MaxSAT, Alg. 3: All-MUSes, Alg. 4: Assumptions, Alg. 5:
Cause-by-cause. Best time in bold red and ‘close to best times’
(i.e., not exceeding the best by more than 50ms or 10%) on grey.

more trivial answers, but the encodings are more complex.

Given a dataset, query and semantics, if we compare two
priority relations of the same kind (score-structured or not),
the one that sets preferences between more facts leads to
lower running times. This can be explained by the in-
crease of the number of trivial answers and maybe by the
fact that the encodings involve less facts and encode more
‘forced choices’ between facts so that there are less pos-
sibilities to explore. When we compare the ‘easiest’ non
score-structured priority relations (p = 0.8) and the hard-
est score-structured ones (n = 2), which lead to comparable
sizes of directed conﬂict graphs, score-structured priority re-
lations seem to be easier than non-score-structured ones.

Finally, we conclude that our procedures do not perform
well for completion-optimal repair-based semantics, which
form most of the cases that fail due to lack of time or mem-
ory, and very often have higher running times than Pareto-
optimal-based semantics with the same priority relation.

Choice of Algorithm & Encoding for Given Semantics
Table 2 presents the time needed to ﬁlter the candidate an-
swers that hold under brave semantics based on optimal-
repairs (score-structured case) for some example queries. It
illustrates the huge impact that the choice of an algorithm
and encoding can have. For example, q6 answers are ﬁltered
in 3.5s with algorithm Cause-by-cause and ϕP1-max, but need
at least 546s with the Assumptions algorithm, at least 5.9s
with ϕP2-max, and cannot be ﬁltered in our time and memory
limits with encoding ϕC-max. For X-AR and X-IAR seman-
tics, we also observe sometimes huge variations when using
the neg1 or neg2 version of ϕ¬C. For example, for X-AR
semantics on u20c50 with a score-structured priority rela-
tion, the best times for queries q2 and q18 are both obtained

106

105

104

103

102

q1
q3
q5
q7
q9
q11
q13
q15
q17
q19

q2
q4
q6
q8
q10
q12
q14
q16
q18
q20

106

105

104

103

102

101

0

10

20

30

40

50

0

10

20

30

40

50

q1
q3
q5
q7
q9
q11
q13
q15
q17
q19

q2
q4
q6
q8
q10
q12
q14
q16
q18
q20

106

105

104

103

102

q1
q3
q6
q8
q10
q12
q14
q17
q19

q2
q5
q7
q9
q11
q13
q15
q18
q20

0

10

20

30

40

50

(a) Simple

(b) All-MaxSAT

(c) All-MUSes

Figure 4: Time (in milliseconds, log. scale) to ﬁlter query answers under X-AR semantics (X ∈ {P,C}) w.r.t. percentage of facts involved in
some conﬂict for u20cY with score-structured priority relation (ϕP1-max and neg1 encoding). Missing queries ran out of time or memory.

with algorithm All-MaxSAT and ϕP1-max, but with the neg2
variant for q2 (50 seconds versus 85 with neg1) and neg1 for
q18 (23 seconds versus 47 with neg2). The comparison of
the possible procedures for each semantics on the different
datasets and queries shows that there is not a ‘best’ method
in general. However, we still gain some relevant insights.

The ﬁrst one concerns the choice of ϕX-max. The encoding
ϕP1-max is generally the best one for Pareto-optimal repairs,
in the sense that it achieves ‘close to best times’ (cf. Ta-
ble 2) much more often than the others. However, there
are a few cases where ϕP2-max performs signiﬁcantly bet-
ter, especially on the CQAPri datasets with fewer conﬂicts
(e.g., on u20c1 with a score-structured priority relation, the
best time for ﬁltering q9 answers under P-AR semantics is
480ms with ϕP2-max, while the best time with another encod-
ing is 825ms). When the priority relation is score-structured,
ϕC-max never signiﬁcantly outperforms ϕP1-max and ϕP2-max
and leads to much more time or memory failures.

The second concerns the choice of algorithm for X-IAR
semantics. For all kinds of repairs, algorithm IAR-causes
is generally better than the others in terms of frequency of
‘close to best times’. It is sometimes outperformed by al-
gorithms Cause-by-cause or IAR-facts. The ‘generic’ al-
gorithms (Simple, All-MaxSAT, All-MUSes, Assumptions)
perform quite poorly, except on the simplest cases.

For the AR and brave semantics, it is more difﬁcult to ﬁnd
an algorithm that is superior to the others. For non-score-
structured priority relations and completion-optimal repairs,
algorithm Simple seems to be the best choice for both C-
AR and C-brave semantics, but all algorithms fail in most
cases. A related observation is that algorithms that consider
answers individually and use smaller encodings seems to be
often more robust in terms of time-out and out-of-memory,
with a notable exception for P-AR and P-brave semantics on
u20c50 with non-score-structured priority, where algorithms
All-MaxSAT and All-MUSes are more robust. For S-AR, P-
AR and P-brave, we observe different behaviours depending
on the benchmark: All-MaxSAT and All-MUSes tend to per-
form better for the CQAPri benchmark, while Simple tends
to perform better for the Food Inspection dataset.

Finally, comparing neg1 and neg2 versions of ϕ¬C, we ob-
serve that the relative performance depends on the dataset,
query, algorithm, and choice of ϕX-max. However, we note
that ϕP2-max usually works better with neg1. Even if there

is not a direct relationship between the encoding sizes and
the running times, this is probably due to the fact that neg2
enforces that both the facts that occur in the causes and their
conﬂicts are part of the encoding, which signiﬁcantly in-
creases the size of ϕP2-max, but has little impact on ϕP1-max.
Figure 4 shows the evolution of the running times of three
algorithms using the same encoding variants as the propor-
tion of facts involved in some conﬂicts grows, on the u20cY
datasets with a score-structured priority for X-AR semantics
(X ∈ {P,C}). It illustrates the fact that the relative perfor-
mance of the algorithms depends on the query and dataset
(here, the proportion of facts involved in some conﬂict): For
example, All-MaxSAT is the best for q9 over the three ﬁrst
datasets, but runs out of time on the three last (more that
20% of facts in conﬂict), while Simple can handle them.

7 Conclusion
We have presented a comprehensive exploration of SAT-
based approaches to querying inconsistent data using (op-
timal) repair-based inconsistency tolerant semantics, includ-
ing the proposal of novel encodings and algorithms. Our
generic framework places existing approaches into a broader
context and makes our results and system directly applicable
to both the (pure) database and OMQA settings.

Our experimental comparison of different SAT-based al-
gorithms and encoding variants shows that the choice of al-
gorithm and encoding may have huge impact on the com-
putation time. While in some cases our results can be used
to single out some approaches as more effective, more often
there are no clear winner(s). This suggests that to minimize
runtimes, it may make sense to launch multiple algorithms in
parallel, and/or devise methods that can help predict which
algorithm and encoding will perform best on a given dataset
and query, e.g. using machine learning techniques.

Our work lays important foundations for the future de-
velopment of mature systems for querying inconsistent data.
We plan to investigate different ways of improving the per-
formance for optimal repair-based semantics. For example,
it would be interesting to explore alternative approaches for
completion-optimal repairs based upon SAT modulo graph
techniques (Gebser, Janhunen, and Rintanen 2014). Another
promising direction is to employ more reﬁned polynomial
lower approximations than the trivial answers, such as the
grounded semantics (Bienvenu and Bourgaux 2020).

Acknowledgements

This work was supported by the ANR AI Chair INTENDED
(ANR-19-CHIA-0014).

References

Arenas, M.; Bertossi, L. E.; and Chomicki, J. 1999. Con-
sistent query answers in inconsistent databases. In Proceed-
ings of the 18th ACM SIGMOD-SIGACT-SIGAI Symposium
on Principles of Database Systems (PODS), 68–79.
Berre, D. L., and Parrain, A. 2010. The sat4j library, release
2.2. JSAT 7(2-3):59–64.
Bertossi, L. E. 2011. Database Repairing and Consistent
Query Answering. Synthesis Lectures on Data Management.
Morgan & Claypool Publishers.
Bienvenu, M., and Bourgaux, C. 2016.
Inconsistency-
tolerant querying of description logic knowledge bases. In
Tutorial Lectures of the 12th International Reasoning Web
Summer School, 156–202.
Bienvenu, M., and Bourgaux, C. 2020. Querying and repair-
ing inconsistent prioritized knowledge bases: Complexity
analysis and links with abstract argumentation. In Proceed-
ings of the 17th International Conference on Principles of
Knowledge Representation and Reasoning (KR), 141–151.
Bienvenu, M., and Bourgaux, C. 2022. Querying inconsis-
tent prioritized data with ORBITS: Algorithms, implemen-
tation, and experiments. arxiv.org/abs/2202.07980 [cs.LO].
Bienvenu, M., and Ortiz, M. 2015. Ontology-mediated
query answering with data-tractable description logics.
In
Tutorial Lectures of the 11th Reasoning Web International
Summer School, 218–307.
Bienvenu, M., and Rosati, R. 2013. Tractable approxi-
mations of consistent query answering for robust ontology-
based data access. In Proceedings of the 23rd International
Joint Conference on Artiﬁcial Intelligence (IJCAI).
Bienvenu, M.; Bourgaux, C.; and Goasdou´e, F.
2014.
Querying inconsistent description logic knowledge bases
under preferred repair semantics. In Proceedings of the 28th
AAAI Conference on Artiﬁcial Intelligence (AAAI), 996–
1002.
Bienvenu, M.; Bourgaux, C.; and Goasdou´e, F. 2019. Com-
puting and explaining query answers over inconsistent DL-
Lite knowledge bases. Journal of Artiﬁcial Intelligence Re-
search (JAIR) 64:563–644.
Bienvenu, M. 2020. A short survey on inconsistency han-
dling in ontology-mediated query answering. K¨unstliche In-
telligenz 34(4):443–451.
Bourgaux, C. 2016. Inconsistency Handling in Ontology-
Mediated Query Answering. (Gestion des incoh´erences pour
l’acc`es aux donn´ees en pr´esence d’ontologies). Ph.D. Dis-
sertation, University of Paris-Saclay, France.
Calvanese, D.; De Giacomo, G.; Lembo, D.; Lenzerini, M.;
and Rosati, R. 2007. Tractable reasoning and efﬁcient query
answering in description logics: The DL-Lite family. Jour-
nal of Automated Reasoning (JAR) 39(3):385–429.

Food Inspections, Chicago Data Portal.

Dataset:
https://data.cityofchicago.org/Health-Human-Services/
Food-Inspections/4ijn-s7e5. Accessed December 7, 2020.
Dataset: National Downloadable File, Centers for Medicare
& Medicaid Services. https://data.cms.gov/provider-data/
dataset/mj5m-pzi6. Accessed December 10, 2020.
Dataset: New York City Restaurant Inspection Results,
Department of Health and Mental Hygiene (DOHMH),
NYC Open Data.
https://data.cityofnewyork.us/Health/
DOHMH-New-York-City-Restaurant-Inspection-Results/
43nn-pn8j. Accessed December 7, 2020.
Dixit, A. A., and Kolaitis, P. G. 2019. A SAT-based sys-
tem for consistent query answering. In Proceedings of the
22nd International Conference on Theory and Applications
of Satisﬁability Testing (SAT), 117–135.
Fan, W. 2015. Data quality: From theory to practice. SIG-
MOD Rec. 44(3):7–18.
Gebser, M.; Janhunen, T.; and Rintanen, J. 2014. SAT
In Proceedings of the 14th
modulo graphs: Acyclicity.
European Conference on Logics in Artiﬁcial Intelligence
(JELIA), 137–151.
Lembo, D.; Lenzerini, M.; Rosati, R.; Ruzzi, M.; and Savo,
D. F. 2010. Inconsistency-tolerant semantics for description
logics. In Proceedings of the 4th International Conference
on Web Reasoning and Rule Systems (RR), 103–117.
Livshits, E., and Kimelfeld, B. 2017. Counting and enumer-
ating (preferred) database repairs. In Proceedings of the 36th
ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of
Database Systems (PODS), 289–301.
Lutz, C.; Seylan, I.; Toman, D.; and Wolter, F. 2013. The
combined approach to OBDA: Taming role hierarchies using
In Proceedings of the 12th International Semantic
ﬁlters.
Web Conference (ISWC), 314–330.
Poggi, A.; Lembo, D.; Calvanese, D.; De Giacomo, G.;
Lenzerini, M.; and Rosati, R. 2008. Linking data to on-
tologies. Journal of Data Semantics 10:133–173.
Rekatsinas, T.; Chu, X.; Ilyas, I. F.; and R´e, C. 2017. Holo-
clean: Holistic data repairs with probabilistic inference. Pro-
ceedings of the VLDB Endowment (PVLDB) 10(11):1190–
1201.
Rosati, R. 2011. On the complexity of dealing with incon-
sistency in description logic ontologies. In Proceedings of
the 22nd International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI), 1057–1062.
Staworko, S.; Chomicki, J.; and Marcinkowski, J. 2012.
Prioritized repairing and consistent query answering in rela-
tional databases. Annals of Mathematics and Artifcial Intel-
ligence (AMAI) 64(2-3):209–246.
Wijsen, J. 2019. Foundations of query answering on incon-
sistent databases. SIGMOD Record 48(3):6–16.
Xiao, G.; Calvanese, D.; Kontchakov, R.; Lembo, D.; Poggi,
A.; Rosati, R.; and Zakharyaschev, M. 2018. Ontology-
In Proceedings of the 27th
based data access: A survey.
International Joint Conference on Artiﬁcial Intelligence (IJ-
CAI), 5511–5519.

