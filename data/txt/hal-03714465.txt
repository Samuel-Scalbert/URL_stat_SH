High-Dimensional Private Empirical Risk Minimization
by Greedy Coordinate Descent
Paul Mangold, Aurélien Bellet, Joseph Salmon, Marc Tommasi

To cite this version:

Paul Mangold, Aurélien Bellet, Joseph Salmon, Marc Tommasi. High-Dimensional Private Empirical
Risk Minimization by Greedy Coordinate Descent. AISTATS 2023 - International Conference on
Artificial Intelligence and Statistics, Apr 2023, Valencia, Spain. ￿hal-03714465v3￿

HAL Id: hal-03714465

https://inria.hal.science/hal-03714465v3

Submitted on 9 Apr 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

High-Dimensional Private Empirical Risk Minimization
by Greedy Coordinate Descent

Paul Mangold
Univ. Lille, Inria,
CNRS, Centrale Lille,
UMR 9189 - CRIStAL,
F-59000 Lille, France

Aur´elien Bellet
Univ. Lille, Inria,
CNRS, Centrale Lille,
UMR 9189 - CRIStAL,
F-59000 Lille, France

Joseph Salmon
IMAG, Univ Montpellier,
CNRS, Montpellier, France
Institut Universitaire
de France (IUF)

Marc Tommasi
Univ. Lille, CNRS,
Inria, Centrale Lille,
UMR 9189 - CRIStAL,
F-59000 Lille, France

Abstract

formulation:

In this paper, we study differentially private em-
pirical risk minimization (DP-ERM). It has been
shown that the worst-case utility of DP-ERM re-
duces polynomially as the dimension increases.
This is a major obstacle to privately learning
large machine learning models. In high dimen-
sion, it is common for some model’s parame-
ters to carry more information than others. To
exploit this, we propose a differentially private
greedy coordinate descent (DP-GCD) algorithm.
At each iteration, DP-GCD privately performs
a coordinate-wise gradient step along the gradi-
ents’ (approximately) greatest entry. We show
theoretically that DP-GCD can achieve a loga-
rithmic dependence on the dimension for a wide
range of problems by naturally exploiting their
structural properties (such as quasi-sparse solu-
tions). We illustrate this behavior numerically,
both on synthetic and real datasets.

1

INTRODUCTION

Machine Learning (ML) crucially relies on data, which can
be sensitive or conﬁdential. Unfortunately, trained mod-
els are prone to leaking information about speciﬁc training
points (Shokri et al., 2017). A standard approach for train-
ing models while provably controlling the amount of leak-
age is to solve an empirical risk minimization (ERM) prob-
lem under differential privacy (DP) constraints (Chaudhuri
et al., 2011). In this work, we consider the generic problem

Proceedings of the 26th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2023, Valencia, Spain.
PMLR: Volume 206. Copyright 2023 by the author(s).

w∗

arg min
w∈Rp

∈

(cid:110)

f (w) =

1
n

n
(cid:88)

i=1

(cid:111)

(cid:96)(w; di)

,

(1)

where D = (d1, . . . , dn) is a dataset of n samples drawn
R is a loss function
from a universe
X
D.
which is convex and smooth for all d

, and (cid:96)(
·

, d) : Rp

→
∈

The DP constraint in DP-ERM induces a trade-off between
the precision of the solution (utility) and privacy. Bassily
et al. (2014) proved lower bounds on utility under a ﬁxed
DP budget. These lower bounds scale polynomially with
the dimension p. Since machine learning models are often
high-dimensional (e.g., n
p), this is a
p or even n
massive drawback for the use of DP-ERM.

(cid:28)

≈

To go beyond this negative result, one has to leverage the
fact that high-dimensional problems often exhibit some
structure. In particular, some parameters are typically more
it is notably (but not only) the
signiﬁcant than others:
case when models are sparse, which is often desired in
high dimension (Tibshirani, 1996). Private learning algo-
rithms could thus be designed to exploit this by focusing
on the most signiﬁcant parameters of the problem. Several
works have tried to exploit such high-dimensional prob-
lems’ structure to reduce the dependence on the dimension,
e.g., from polynomial to logarithmic. Talwar et al. (2015),
Bassily et al. (2021), and Asi et al. (2021) proposed a
DP Frank-Wolfe algorithm (DP-FW) that exploits the solu-
tion’s sparsity. However, their algorithm only works on (cid:96)1-
constrained DP-ERM, restricting its range of application.
For sparse linear regression, Kifer et al. (2012) proposed
to ﬁrst identify some support and then solve the DP-ERM
problem on the restricted support. Unfortunately, their ap-
proach requires implicit knowledge of the solution’s spar-
sity. Finally, Kairouz et al. (2021) and Zhou et al. (2021)
used public data to estimate lower-dimensional subspaces,
where the gradient can be computed at a reduced privacy
cost. A key limitation is that such public data set, from the
same domain as the private data, is typically not available
in many learning scenarios involving sensitive data.

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

In this work, we propose a private algorithm that does not
have these pitfalls: the differentially private greedy coordi-
nate descent algorithm (DP-GCD). At each iteration, DP-
GCD privately determines the gradient’s greatest coordi-
nate, and performs a gradient step in its direction. It focuses
on the most useful parameters, avoiding wasting privacy
budget on updating non-signiﬁcant ones. Our algorithm
works on any smooth, unconstrained DP-ERM problem.
We also propose a proximal version to tackle non-smooth
regularizers. Crucially, DP-GCD is adaptive to the sparsity
of the solution, and is able to ignore small (but non-zero)
parameters, improving utility even on non-sparse problems.

Formally, we show that DP-GCD reduces the dependence
on the dimension from √p or p to log(p) for a wide
range of unconstrained problems. This is the ﬁrst algo-
rithm to obtain such gains without relying on (cid:96)1 or (cid:96)0 con-
In fact, DP-GCD’s utility naturally depends on
straints.
(cid:96)1-norm quantities (i.e., distance from initialization to op-
timal or strong-convexity parameter) and spans two differ-
ent regimes. When these (cid:96)1-norm quantities are O(1) as
assumed in DP-FW, DP-GCD attains O(log(p)/n2/3(cid:15)2/3)
and O(log(p)/n2(cid:15)2) utility on convex and strongly-convex
problems respectively, outperforming existing DP-FW al-
gorithms without solving a constrained problem.
In the
second regime, when the (cid:96)2-norm counterpart of the above
quantities are O(1) as assumed for DP-SGD and its vari-
ants, we show that DP-GCD adapts to the problem’s un-
derlying structure. Speciﬁcally, it is able to interpolate be-
tween logarithmic and polynomial dependence on the di-
mension.
In addition to these general utility results, we
prove that for strongly convex problems with quasi-sparse
solutions (including but not limited to sparse problems),
DP-GCD converges to a good approximate solution in few
iterations. This improves utility in the (cid:96)2-norm setting, re-
placing the polynomial dependence on the ambient space’s
dimension by the quasi-sparsity level of the solution. We
evaluate both our algorithms numerically on real and syn-
thetic datasets, validating our theoretical observations.

Our contributions can be summarized as follows:

1. We propose differentially private greedy coordinate
descent (DP-GCD), a method that performs updates
along the (approximately) greatest entry of the gradi-
ent. We formally establish its privacy guarantees, and
derive high probability utility upper bounds.

2. We prove that DP-GCD exploits structural properties
of the problem (e.g., quasi-sparse solutions) to im-
prove utility. Importantly, DP-GCD does not require
prior knowledge of this structure to exploit it.

3. We empirically validate our theoretical results on a va-
riety of synthetic and real datasets, showing that DP-
GCD outperforms existing private algorithms on high-
dimensional problems with quasi-sparse solutions.

The rest of the paper is organized as follows. First, we dis-
cuss related work in more details in Section 2, and present
the relevant mathematical background in Section 3. Sec-
tion 4 then introduces DP-GCD, and formally analyzes its
privacy and utility. We validate our theoretical results nu-
merically in Section 5. Finally, we conclude and discuss
the limitations of our results in Section 6.

2 RELATED WORK

DP-ML in Euclidean geometry. Most of the work
on differentially private empirical
risk minimization
(DP-ERM) and differentially private stochastic con-
vex optimization (DP-SCO)1 consider problem quantities
(e.g., bounds on the domain and regularity assumptions)
expressed in (cid:96)2 norm. In this Euclidean setting, Bassily et
al. (2014) analyzed the theoretical properties of DP-SGD
for DP-ERM, and derived matching utility lower bounds.
Faster algorithms based on SVRG (Johnson and Zhang,
2013; Xiao and Zhang, 2014) were designed by Wang et al.
(2017). Wu et al. (2017) studied a variant of DP-SGD with
output perturbation, that is efﬁcient when only few passes
on the data are possible. For DP-SCO, Bassily et al. (2019)
used algorithmic stability arguments (following work from
Hardt et al., 2016; Bassily et al., 2020) to show that in some
regimes, the population risk is the same as in non-private
SCO. Feldman et al. (2020) and Wang et al. (2022) then de-
veloped efﬁcient (linear-time) algorithm to solve this prob-
lem. In all of the above work, the utility upper bounds scale
polynomially in p, which is not suitable in high dimension.
In contrast, our approach provably achieves logarithmic de-
pendence on the dimension for some problems.

DP-ML in high dimension. Several approaches have
been explored to reduce the dependence on the dimension.
One option is to consider (cid:96)1-constrained problems. For DP-
ERM, Talwar et al. (2015) and Talwar et al. (2016) used
a differentially private Frank-Wolfe algorithm (DP-FW)
(Frank and Wolfe, 1956; Jaggi, 2013) to achieve utility that
scales logarithmically with the dimension. Asi et al. (2021)
and Bassily et al. (2021) proposed stochastic DP-FW algo-
rithms, extending the above results to DP-SCO. For more
general domains (e.g., polytopes), Kasiviswanathan and Jin
(2016) randomly project the data on a smaller-dimensional
space, and lift the result back onto the original space. The
dependence in the dimension is encoded by the Gaussian
width of the domain, again leading to O(log p) error for
the (cid:96)1 ball or the simplex. Wang et al. (2017) derived a
faster mirror descent algorithm for DP-ERM whose utility
also depends on the Gaussian width of the domain. Our
approach matches the O(log p) dependence of the above
methods when key quantities are bounded in (cid:96)1 norm, but
can also achieve such gains for more general problems,

1See (Dwork et al., 2015; Bassily et al., 2016; Jung et al.,

2021) for techniques to convert DP-ERM results to DP-SCO.

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

e.g., when the problem has a quasi-sparse solution. Kifer
et al. (2012) previously leveraged the solution sparsity for
the speciﬁc problem of sparse linear regression: they ﬁrst
identify some support, and then solve DP-ERM on this re-
stricted support. Similarly, Wang and Gu (2019) and Hu
et al. (2022) proposed hard thresholding-based algorithms
for DP-ERM and DP-SCO under sparsity ((cid:96)0 norm) con-
straints. Both approaches achieve an error of O(log p) but
rely either on prior knowledge on the solution’s sparsity,
or on the tuning of an additional hyperparameter. In con-
trast, our approach automatically adapts to the sparsity and
works also when solutions are only quasi-sparse. Finally,
Kairouz et al. (2021) and Zhou et al. (2021) estimate lower-
dimensional gradient subspaces using public data. This re-
duces noise addition, but in practice, public data is only
rarely available.

Coordinate descent. CD algorithms have a long history
in optimization (see Wright, 2015; Shi et al., 2017, for
detailed reviews on CD). Most approaches have focused
on randomized or cyclic choices of coordinates (Tseng,
2001; Nesterov, 2012), with proximal and parallel variants
(Richt´arik and Tak´aˇc, 2014; Fercoq and Richt´arik, 2014;
Hanzely et al., 2018), sometimes applied to the dual prob-
lem (Shalev-Shwartz and Zhang, 2013). In this work, our
focus is on greedy coordinate descent methods, which up-
date the coordinate with greatest gradient entry (Luo and
Tseng, 1992; Tseng and Yun, 2009; Dhillon et al., 2011).
Nutini et al. (2015) showed improved convergence rates for
smooth, strongly-convex functions, by measuring strong
convexity in the (cid:96)1-norm. Our work builds upon these re-
sults to design and analyze the ﬁrst private greedy CD ap-
proach. Although techniques such as fast nearest-neighbor
schemes have been proposed to compute the (approximate)
greedy update more efﬁciently (Dhillon et al., 2011; Nutini
et al., 2015; Karimireddy et al., 2019), greedy CD methods
are often slower (in wall-clock time) than their randomized
or cyclic counterparts (Massias et al., 2017). However, in
the private setting we consider, the main focus is not on
computing time but on achieving the best privacy-utility
trade-off. This gives a distinct advantage to greedy CD,
as it provides a way to perform the (approximately) most
useful coordinate update under a given privacy budget in-
stead of wasting budget on updating random (potentially
useless) coordinates. The analysis of proximal extensions
of greedy CD for composite problems with non-smooth
parts is known to be challenging even in the non-private
setting. Karimireddy et al. (2019) proved convergence rates
only for (cid:96)1- and box-regularized problems, using a modi-
ﬁed greedy CD algorithm. In this work, we propose and
empirically evaluate a proximal extension of our DP-GCD
algorithm with formal privacy guarantees, but leave its util-
ity analysis for future work; see the discussion in Section 6.

Private coordinate descent. Differentially Private Coor-
dinate Descent (DP-CD) was recently studied by Mangold
et al. (2022), who analyzed its utility and derived corre-
sponding lower bounds. They showed that DP-CD can ex-
ploit coordinate-wise regularity assumptions to use larger
step-sizes, outperforming DP-SGD when gradient coordi-
nates are imbalanced. Our DP-GCD also shares this prop-
erty. Damaskinos et al. (2021) proposed a dual coordi-
nate descent algorithm for generalized linear models. Pri-
vate CD has also been used by Bellet et al. (2018) in a
decentralized setting. All these works use random selec-
tion, which fails to exploit key problem’s properties such
as quasi-sparsity, and thus suffer a polynomial dependence
on the dimension p. In contrast, our private greedy selec-
tion rule focuses on the most useful coordinates, thereby
reducing the dependence on p to only logarithmic in such
settings.

3 PRELIMINARIES

In this section, we introduce important technical notions
that will be used throughout the paper.

Norms. We start by deﬁning two conjugate norms that
will allow to keep track of coordinate-wise quantities. Let
M = diag(M1, . . . , Mp) with M1, . . . , Mp > 0, and

w
(cid:107)

M,1 =
(cid:107)

p
(cid:88)

j=1

M

1
2

j |

wj

,

|

w
(cid:107)

(cid:107)M −1,∞ = max

j∈[p]

M − 1

2

j

.

wj

|

|

When M is the identity matrix I,
(cid:96)1-norm and
the Euclidean dot product
responding norms

M,1 is the standard
(cid:107)·(cid:107)
(cid:107)·(cid:107)M −1,∞ is the (cid:96)∞-norm. We also deﬁne
= (cid:80)p
j=1 uivi and cor-
(cid:105)
, M
(cid:107)·(cid:107)M −1,2 =
(cid:104)·
2 . Similarly, we recover the standard (cid:96)2-norm

u, v
(cid:104)
M,2 =

1
2 and

, M −1

(cid:107)·(cid:107)

·(cid:105)

1

(cid:104)·
when M = I.

·(cid:105)

Regularity assumptions. We recall classical regularity
assumptions along with ones speciﬁc to the coordinate-
f the gradient of a differ-
wise setting. We denote by
entiable function f , and by
jf its j-th coordinate. We
denote by ej the j-th vector of Rp’s standard basis.

∇
∇

v

∈

≥

−

→

2 (cid:107)

f (v) +

(Strong)-convexity. For q
tion f : Rp

1, 2
, a differentiable func-
}
R is µM,q-strongly-convex w.r.t. the norm

∈ {
Rp, f (w)
M,q if for all v, w
f (v), w
(cid:107)·(cid:107)
(cid:104)∇
−
+ µM,q
2
= Mp,q = 1
M,q. The case M1,q =
w
v
(cid:105)
(cid:107)
recovers standard µI,q-strong convexity w.r.t. the (cid:96)q-norm.
When µM,q = 0, the function is just said to be convex.
R
Component Lipschitzness. A function f : Rp
is L-component-Lipschitz for L = (L1, . . . , Lp) with
L1, . . . , Lp > 0 if for w
[p],
∈
, f is Λq-
f (w + tej)
|
−
f (w)
Lipschitz w.r.t.
Λq

Rp, t
∈
. For q
∈ {
|
Rp,
f (v)
q if for v, w
|

R and j
1, 2
}
−

f (w)

∈
t
|

| ≤

| ≤

(cid:107)·(cid:107)

· · ·

Lj

→

q.

w

∈

v

(cid:107)

−

(cid:107)

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

Component smoothness. A differentiable function f :
Rp
R is M -component-smooth for M1, . . . , Mp > 0
→
if for v, w
v
smooth.

f (v) +
−
= Mp = β, f is said to be β-

Rp, f (w)
∈
2
M,2. When M1 =
(cid:107)

f (v), w

≤
· · ·

+ 1

(cid:104)∇

2 (cid:107)

−

w

v

(cid:105)

1, 2

∈ {

, Λq-Lipschitzness w.r.t.
}

Component-wise regularity assumptions are not restric-
tive: for q
q implies
(Λq, . . . , Λq)-component-Lipschitzness and β-smoothness
implies (β, . . . , β)-component-smoothness. Yet, the ac-
tual component-wise constants of a function can be much
lower than what can be deduced from their global counter-
parts. In the following of this paper, we will use Mmin =
minj∈[p] Mj, Mmax = maxj∈[p] Mj, and their Lipschitz
counterparts Lmin and Lmax.

(cid:107)·(cid:107)

Differential privacy (DP). Let

a set of possible outcomes. Two datasets D, D(cid:48)

be a set of datasets and
are
D(cid:48)) if they differ on at

∈ D

D

∼

F
said neighboring (denoted by D
most one element.

Deﬁnition 3.1 (Differential Privacy, Dwork 2006). A ran-
is ((cid:15), δ)-differentially pri-
domized algorithm
:
A
vate if, for all neighboring datasets D, D(cid:48)
and all
S

in the range of

D → F

∈ D

⊆ F

:
A

Pr [

(D)

A

S]

e(cid:15)

·

≤

∈

Pr [

(D(cid:48))

A

S] + δ .

∈

In this paper, we consider the classic central model of DP,
where a trusted curator has access to the raw dataset and
releases a model trained on this dataset2.

D →

h(D)
(cid:107)

A common principle for releasing a private estimate of a
Rp is to perturb it with noise. To ensure
function h :
privacy, the noise is scaled with the sensitivity ∆q(h) =
h(D(cid:48))
q of h, with q = 1 for Laplace,
supD∼D(cid:48)
(cid:107)
and q = 2 for Gaussian mechanism. In coordinate descent
methods, we release coordinate-wise gradients. The j-th
R has
coordinate of a loss function’s gradient
∇
jf is a scalar). For
sensitivity ∆1(
∇
L-component-Lipschitz losses, these sensitivities are upper
bounded by 2Lj (Mangold et al., 2022).

jf ) = ∆2(

j(cid:96) : Rp

jf ) (

→

∇

∇

−

In our algorithm, we will also need to compute the index
of the gradient’s maximal entry privately. To this end, we
use the report-noisy-argmax mechanism (Dwork and Roth,
2013). This mechanism perturbs each entry of a vector with
Laplace noise, calibrated to its coordinate-wise sensitivi-
ties, and releases the index of a maximal entry of this noisy
vector. Revealing only this index allows to greatly reduce
the noise, in comparison to releasing the full gradient. This
will be the cornerstone of our greedy algorithm.

2In fact, our privacy guarantees hold even if all intermediate

iterates are released (not just the ﬁnal model).

4 PRIVATE GREEDY CD

In this section, we present our main contribution: the differ-
entially private greedy coordinate descent algorithm (DP-
GCD). As described in Section 4.1, DP-GCD updates only
one coordinate per iteration, which is selected greedily
as the (approximately) largest entry of the gradient so as
to maximize the improvement in utility at each iteration.
We establish privacy (Section 4.2) and utility (Section 4.3)
guarantees for DP-GCD. We further show in Section 4.4
that DP-GCD enjoys improved utility for high-dimensional
problems with a quasi-sparse solution (i.e., with a fraction
of the parameters dominating the others). We then provide
a proximal extension of DP-GCD to non-smooth problems
(Section 4.5) and conclude with a discussion of DP-GCD’s
computational complexity in Section 4.6.

4.1 The Algorithm

At each iteration, DP-GCD (Algorithm 1) updates the pa-
rameter with the greatest gradient value (rescaled by the
inverse square root of the coordinate-wise smoothness con-
stant). This corresponds to the Gauss-Southwell-Lipschitz
rule (Nutini et al., 2015). To guarantee privacy, this selec-
tion is done using the report-noisy-max mechanism (Dwork
and Roth, 2013) with noise scales λ(cid:48)
j along j-th entry
(j
[p]). DP-GCD then performs a gradient step with
step size γj > 0 along this direction. The gradient is pri-
vatized using the Laplace mechanism (Dwork and Roth,
2013) with scale λj.

∈

Remark 4.1 (Sparsity of iterates). When initialized at
w0 = 0, DP-GCD generates sparse iterates. Since it
chooses its updates greedily, this gives a screening ability
to the algorithm (Fang et al., 2020). We discuss the impli-
cations of this property in Section 4.4, where we show that
DP-GCD’s utility is improved when the problem’s solution
is (quasi-)sparse.

4.2 Privacy Guarantees

The privacy guarantees of DP-GCD depends on the noise
scales λj and λ(cid:48)
In Theorem 4.2, we describe how to
j.
set these values so as to ensure that DP-GCD is ((cid:15), δ)-
differentially private.

Theorem 4.2. Let (cid:15), δ
j = 8Lj
λ(cid:48)

(cid:112)T log(1/δ) is ((cid:15), δ)-DP.

∈

n(cid:15)

(0, 1]. Algorithm 1 with λj =

Sketch of Proof. (Detailed proof in Appendix A) Let (cid:15)(cid:48) =
(cid:15)/(cid:112)
16T log(1/δ). At an iteration t, data is accessed twice.
First, to compute the index jt of the coordinate to update.
It is obtained as the index of the largest noisy entry of f ’s
gradient, with noise Lap(λ(cid:48)
j). By the report-noisy-argmax
mechanism, jt is (cid:15)(cid:48)-DP. Second, to compute the gradi-
ent’s jt’s entry, which is released with noise Lap(λj).The

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

Algorithm 1 DP-GCD: Differentially Private Greedy Coordinate Descent
Rp, iteration count T > 0,
1: Input: initial w0
2: for t = 0 to T
3:

−
jt = arg max

Lap(λ(cid:48)

∈
1 do
|∇j(cid:48) f (wt)+χt
√Mj(cid:48)

with χt
j(cid:48)

j
∀

j(cid:48)).

j(cid:48) |

∈

,

[p], noise scales λj, λ(cid:48)

j(cid:48)∈[p]
wt+1 = wt

4:
5: return wT .

γjt(

∇

jtf (wt) + ηt

jt)ejt,

−

∼
with ηt

jt ∼

j, step sizes γj > 0.

(cid:46) Choose jt using report-noisy-max.

Lap(λjt).

(cid:46) Update the chosen coordinate.

Laplace mechanism ensures that this computation is also (cid:15)(cid:48)-
DP. Algorithm 1 is thus the 2T -fold composition of (cid:15)(cid:48)-DP
mechanisms, and the result follows from DP’s advanced
composition theorem (Dwork and Roth, 2013).

with probability at least 1

ζ,

−

f (wpriv)

f ∗ = (cid:101)O

−

(cid:18) L2

max log(1/δ) log(2p/µM ζ)

Mminµ2

M,1n2(cid:15)2

(cid:19)

.

Remark 4.3. The assumption (cid:15)
(0, 1] is only used to
give a closed-form expression for the noise scales λ, λ’s.
In practice, we tune them numerically to obtain the desired
value of (cid:15) > 0 by the advanced composition theorem (see
eq. (2) in Appendix A), removing the assumption (cid:15)

∈

1.

≤

Computing the greedy update requires injecting Laplace
noise that scales with the coordinate-wise Lipschitz con-
stants L1, . . . , Lp of the loss. These constants are typically
smaller than their global counterpart. This allows DP-GCD
to inject less noise on smaller-scaled coordinates.

f (wt)

Sketch of Proof. (Detailed proof in Appendix B). We start
by proving a noisy “descent lemma”. Since f is smooth, we
1
have f (wt+1)
j)2. The
2Mj ∇
≤
greedy selection of j gives that

jf (wt)2 + 1
2Mj
(
≤
∇
2
M −1,∞. We then use the inequality (a +
R, and convexity arguments to

−(cid:107)∇
b)2
≤
prove the lemma. When f is convex, we have

f (wt) + χ
(cid:107)
2a2 + 2b2 for a, b

(ηt
jf (wt) + χj)2

1
Mj

−

−

∈

f (wt+1)

f (w∗)

f (wt)

−
(f (wt)
wt
8
(cid:107)

≤
f (w∗))2
2
w∗
M,1
(cid:107)

−
−

−
ηt
j|
2Mj

+ |

f (w∗)
2

+ |

2

χt
j|
2Mj

+ |

2

χt
j∗
|
4Mj∗

.

4.3 Utility Guarantees

−

We now prove utility upper bounds for DP-GCD. We show
that in favorable settings (see discussion below), DP-GCD
decreases the dependence on the dimension from polyno-
mial to logarithmic. Theorem 4.4 gives asymptotic utility
upper bounds, where (cid:101)O ignores non-signiﬁcant logarithmic
terms. Complete non-asymptotic results can be found in
Appendix B.
; d) is a convex
Theorem 4.4. Let (cid:15), δ
·
,
and L-component-Lipschitz loss function for all d
∈ X
∗ the set of min-
and f is M -component-smooth. Deﬁne
Rp be
imizers of f , and f ∗ the minimum of f . Let wpriv
the output of Algorithm 1 with step sizes γj = 1/Mj, and
1, . . . , λ(cid:48)
noise scales λ1, . . . , λp, λ(cid:48)
p set as in Theorem 4.2
(with T chosen below) to ensure ((cid:15), δ)-DP. Then, the fol-
(0, 1]:
lowing holds for any ζ

(0, 1]. Assume (cid:96)(

W

∈

∈

∈

−

M,1

w∗

f (w)

1. When f is convex, we deﬁne the quantity RM,1 =
f (w0)(cid:9).
maxw∈Rp maxw∗∈W ∗ (cid:8)
w
≤
(cid:107)
f ∗
Assume the initial optimality gap is f (w0)
(cid:112)T log(1/δ) log(2T p/ζ)/Mminn(cid:15),
−
≥
and set
16Lmax
min/L2/3
T = O(n2/3(cid:15)2/3R2/3
M,1M 1/3
max log(1/δ)1/3). Then
with probability at least 1
ζ,
−
M,1L2/3
(cid:18) R4/3

(cid:19)

(cid:107)

|

max log(1/δ) log(p/ζ)
n2/3(cid:15)2/3M 1/3
min

.

f (wpriv)

f ∗ = (cid:101)O

−

2. When f is µM,1-strongly convex w.r.t.

M,1, set
(cid:107)·(cid:107)
(cid:17)
log( MminµM,1n(cid:15)(f (w0)−f (w∗))
. Then
)

Lmax log(1/δ) log(2p/ζ)

(cid:16) 1

µM,1

T = O

There, we observe that, at each iteration, either (i) wt is far
enough from the optimum, and the value of the objective
decreases with high probability, either (ii) wt is close to
the optimum, then all future iterates remain in a ball whose
radius depends on the scale of the noise. We prove this key
property rigorously in Appendix B.3.2.

When f is µM,1-strongly-convex w.r.t.

M,1, we obtain

(cid:107)·(cid:107)

f (wt+1)

f (w∗)

−

(cid:16)

1

−

≤

µM,1
4
2

ηt
j|
2Mj

(cid:17)

(f (wt)

f (w∗))

−

+ |

2

χt
j|
2Mj

+ |

2

χt
j∗
|
4Mj∗

,

+ |

and the result follows by induction. In both settings, we use
Chernoff bounds to obtain a high-probability result.

Remark 4.5. The lower bound on f (w0)
f ∗ in Theo-
rem 4.4 is a standard assumption in the analysis of inexact
coordinate descent methods: it ensures that sufﬁcient de-
crease is possible despite the noise. A similar assumption is
made by Tappenden et al. (2016), see Theorem 5.1 therein.

−

Discussion of the utility bounds. One of the key prop-
erties of DP-GCD is that its utility is dictated by (cid:96)1-norm
quantities (RM,1 and µM,1). Remarkably, this arises with-
out enforcing any (cid:96)1 constraint in the problem, which is
in stark contrast with private Frank-Wolfe algorithms (DP-
FW) that require such constraints (Talwar et al., 2015; Asi

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

et al., 2021; Bassily et al., 2021). To better grasp the impli-
cations of this, we discuss our results in two regimes con-
sidered in previous work (see Section 2): (i) when these
(cid:96)1-norm quantities are bounded (similarly to DP-FW al-
gorithms), and (ii) when their (cid:96)2-norm counterparts are
bounded (similarly to DP-SGD-style algorithms).

Bounded in (cid:96)1-norm. When RM,1 and µM,1 are O(1), as
assumed in prior work on DP-FW (Talwar et al., 2015; Asi
et al., 2021; Bassily et al., 2021), DP-GCD’s dependence
on the dimension is logarithmic. For convex objectives, its
utility is O(log(p)/n2/3(cid:15)2/3), matching that of DP-FW and
known lower bounds (Talwar et al., 2015). For strongly-
convex problems, DP-GCD is the ﬁrst algorithm to achieve
a O(log(p)/n2(cid:15)2) utility. Indeed, the only competing re-
sult in this setting, due to Asi et al. (2021), obtains a worse
utility of O(log(p)4/3/n4/3(cid:15)4/3) by using an impractical
reduction of DP-FW to the convex case. DP-GCD outper-
forms this prior result without suffering the extra complex-
ity due to the reduction.

Bounded in (cid:96)2-norm. Consider RM,2 and µM,2, the (cid:96)2-
norm counterparts of RM,1 and µM,1. Assume that RM,2
and µM,2 are both O(1), as considered in DP-SGD and its
variants (Bassily et al., 2014; Wang et al., 2017). We com-
pare these quantities using the following inequalities (see
Stich et al., 2017; Nutini et al., 2015):

R2

M,2 ≤

R2

M,1 ≤

pR2

M,2 ,

1
p µM,2

µM,1

≤

≤

µM,2 .

In the best case of these inequalities, the O(log p) utility
bounds of the bounded (cid:96)1 norm regime are preserved in
the bounded (cid:96)2 scenario. In the worst case, the utility of
DP-GCD becomes (cid:101)O(p2/3/n2/3(cid:15)2/3) and (cid:101)O(p2/n2(cid:15)2) for
convex and strongly-convex objectives respectively. These
worst-case results match DP-FW’s utility in the convex set-
ting (see e.g., Asi et al. (2021)), but they do not match DP-
SGD’s utility. However, this sheds light on an interesting
phenomenon: DP-GCD interpolates between (cid:96)1- and (cid:96)2-
norm regimes. Indeed, it lies somewhere between the two
extreme cases we just described, depending on how the (cid:96)1-
and (cid:96)2-norm constants compare. Most interestingly, it does
so without a priori knowledge of the problem or explicit
constraint on the domain. Whether there exists an algo-
rithm that yields optimal utility in all regimes is an inter-
esting open question.

to

its

regularity. Due

Coordinate-wise
of
use
coordinate-wise step sizes, DP-GCD can adapt
to
coordinate-wise imbalance of the objective in the same
way as its randomized counterpart, DP-CD, where coor-
dinates are chosen uniformly at random (Mangold et al.,
2022). This adaptivity notably appears in Theorem 4.4
through the measurement of RM,1 and µM,1 relatively
to the scaled norm
M,1 (as deﬁned in Section 3). We
refer to (Mangold et al., 2022) for detailed discussion of
these quantities and the associated gains compared to full

(cid:107)·(cid:107)

gradient methods like DP-SGD.

4.4 Better Utility on Quasi-Sparse Problems

In addition to the general utility results presented above,
we now exhibit a speciﬁc setting where DP-GCD performs
especially well, namely strongly-convex problems whose
solutions are dominated by a few parameters. We call such
vectors quasi-sparse.

Rp is
Deﬁnition 4.6 ((α, τ )-quasi-sparsity). A vector w
(α, τ )-quasi-sparse if it has at most τ entries superior to α
(in modulus). When α = 0, the vector is called τ -sparse.

∈

Note that any vector in Rp is (0, p)-quasi-sparse, and for
any τ there exists α > 0 such that the vector is (α, τ )-
quasi-sparse. In fact, α and τ are linked, and τ (α) can be
seen as a function of α. Of course, quasi-sparsity will only
yield meaningful improvements when α and τ are small
simultaneously.

We now state the main result of this section, which shows
that DP-GCD (initialized with w0 = 0) converges to a good
approximate solution in few iterations for problems with
quasi-sparse solutions.

Theorem 4.7 (Proof in Appendix B.4.3). Consider f satis-
fying the hypotheses of Theorem 4.4, with Algorithm 1 ini-
tialized at w0 = 0. We denote its output wT , and assume
that its iterates remain s-sparse for some s
p. Assume
that f is µM,2-strongly-convex w.r.t.
M,2, and that the
(unique) solution of problem (1) is (α, τ )-quasi-sparse for
[0, 1]. Then
some α, τ
≤
≥
with probability at least 1

0. Let 0

τ and ζ

≤
ζ:

(cid:107)·(cid:107)

−

≤

∈

T

p

−

f (wT )

f ∗

−

≤

µM,2
4(τ + min(t, s))

(cid:17)

1

−

(f (w0)

f ∗)

−

+ (cid:101)O

(T + τ )(p

τ )α2 +

−

L2
maxT (T + τ )
MminµM,2n2(cid:15)2

(cid:17)

.

T
(cid:89)

(cid:16)

t=1
(cid:16)

log((f (w0)

Setting T = s+τ
µM,2
and assuming α2 = O (cid:0)L2
we obtain that with probability at least 1

f ∗)MminµM,2n2(cid:15)2/L2),
M,2pn2(cid:15)2(cid:1),

max(s + τ )/Mminµ2

−

ζ,

−

f (wT )

f ∗ = (cid:101)O

−

(cid:18) L2

max
Mmin

(s + τ )2 log(2p/ζ)
µM,2n2(cid:15)2

(cid:19)

.

Here, strong convexity is measured in (cid:96)2 norm but the de-
pendence on the dimension is reduced from p, the ambi-
ent space dimension, to (s + τ )2, the effective dimension
of the space where the optimization actually takes place.
For high-dimensional sparse problems, the latter is typi-
cally much smaller and yields a large improvement in util-
ity. Note that it is not necessary for the solution to be
perfectly sparse: it sufﬁces that most of its mass is con-
centrated in a fraction of the coordinates. Notably, when
maxT /MminµM,2pn2(cid:15)2), the lack of sparsity is
α2 = O(L2

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

p.

+

→

(cid:28)

smaller than the noise, and does not affect the rate. It gen-
eralizes the results by Fang et al. (2020) for non-private and
sparse settings, that we recover when α = 0 and (cid:15)
.
∞
In practice, the assumption over the iterates’ sparsity is of-
ten met with s
In the non-private setting, greedy
coordinate descent is known to focus on coordinates that
are non-zero in the solution (Massias et al., 2017):
this
keeps iterates’ sparsity close to the one of the solution. Fur-
thermore, due to privacy constraints, DP-GCD will often
run for T
p iterations. This is especially true in high-
dimensional problems, where the amount of noise required
to guarantee privacy does not allow many iterations (cf. ex-
periments in Section 5).

(cid:28)

4.5 Proximal DP-GCD

In Section 4.4, we proved that DP-GCD’s utility is im-
proved when problem’s solution is (quasi-)sparse. This mo-
tivates us to consider problems with sparsity-inducing reg-
ularization, such as the (cid:96)1 norm of w (Tibshirani, 1996). To
tackle such non-smooth terms, we propose a proximal ver-
sion of DP-GCD (for which the same privacy guarantees
hold), building upon the multiple greedy rules that have
been proposed for the nonsmooth setting (see e.g., Tseng
and Yun, 2009; Nutini et al., 2015). We describe this exten-
sion in Appendix C, and study it numerically in Section 5.

4.6 Computational Cost

Each iteration of DP-GCD requires computing a full gra-
dient, but only uses one of its coordinates. In non-private
optimization, one would generally be better off perform-
ing the full update to avoid wasting computation. This is
not the case when gradients are private. Indeed, using the
full gradient requires privatizing p coordinates, even when
only a few of them may be needed. Conversely, the report
noisy max mechanism (Dwork and Roth, 2013) allows to
select these entries without paying the full privacy cost of
dimension. Hence, the greedy updates of DP-GCD reduce
the noise needed at the cost of more computation.

In practice, the higher computational cost of each iteration
may not always translate in a signiﬁcantly larger cost over-
all: as shown by our theoretical results, DP-GCD is able
to exploit the quasi-sparsity of the solution to progress fast
and only a handful of iterations may be needed to reach a
In contrast, most updates of clas-
good private solution.
sic private optimization algorithms (like DP-SGD) may not
be worth doing, and lead to unnecessary injection of noise.
We illustrate this phenomenon numerically in Section 5.

5 EXPERIMENTS

loss with (cid:96)1 and (cid:96)2 regularization. We compare DP-GCD to
two competitors: differentially private stochastic gradient
descent (DP-SGD) with batch size 1 (Bassily et al., 2014;
Abadi et al., 2016), and differentially private randomized
coordinate descent (DP-CD) (Mangold et al., 2022). The
code is available online3 and in the supplementary.

∈

Datasets. The ﬁrst two datasets, coined log1 and log2,
R1,000×100
are synthetic. We generate a design matrix X
with unit-variance, normally-distributed columns.
La-
bels are computed as y = Xw(true) + ε, where ε is
normally-distributed noise and w(true) is drawn from a
log-normal distribution of parameters µ = 0 and σ =
1 or 2 respectively. This makes w(true) quasi-sparse.
The square dataset is generated similarly, with X
∈
R1,000×1,000 and w(true) having only 10 non-zero val-
ues. The california dataset can be downloaded from
scikit-learn (Pedregosa et al., 2011) while mtp,
madelon and dorothea are available in the OpenML
repository (Vanschoren et al., 2014); see summary in Ta-
ble 1. We discuss the levels of (quasi)-sparsity of each
problem’s solution in Appendix D.

(Privacy.) For each algorithm, the
Algorithmic setup.
tightest noise scales are computed numerically to guaran-
tee a suitable privacy level of (1, 1/n2)-DP, where n is the
number of records in the dataset. For DP-CD and DP-SGD,
we privatize the gradients with the Gaussian mechanism
(Dwork and Roth, 2013), and account for privacy tightly
using R´enyi differential privacy (RDP) (Mironov, 2017).
For DP-SGD, we use RDP ampliﬁcation for the subsam-
pled Gaussian mechanism (Mironov et al., 2019).

(Hyperparameters.) For DP-SGD, we use constant step
sizes and standard gradient clipping (Abadi et al., 2016).
For DP-GCD and DP-CD, we set the step sizes to ηj = γ
,
Mj
and adapt the coordinate-wise clipping thresholds from one
hyperparameter, as proposed by Mangold et al. (2022). For
each algorithm, we thus tune two hyperparameters: one
step-size and one clipping threshold; see also Appendix D.

(Plots.)
In all experiments, we plot the relative error to
the non-private optimal objective value for the best set of
hyperparameters (averaged over 5 runs), as a function of
the number of passes on the data. Each pass corresponds
to p iterations of DP-CD, n iterations of DP-SGD and 1
iteration of DP-GCD. This guarantees the same amount of
computation for each algorithm, for each x-axis tick.

DP-GCD exploits problem structure.
dimensional datasets square and dorothea, where p
≥
n, DP-GCD is the only algorithm that manages to do mul-
tiple iterations and to decrease the objective value (see Fig-
ures 1e and 1g). In both problems, solutions are sparse due

In the higher-

In this section, we evaluate the practical performance of
DP-GCD on linear models using the logistic and squared

3https://gitlab.inria.fr/pmangold1/

greedy-coordinate-descent

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

Table 1: Number of records and features in each dataset.

log1, log2 square

mtp

dorothea

california

madelon

Records
Features

1, 000
100

1, 000
1, 000

4, 450
202

800
88, 119

20, 640
8

2, 600
501

(a) log1
Logistic + L2 (λ = 1e-3)

(b) log2
Logistic + L2 (λ = 1e-3)

(c) mtp
LS + L2 (λ = 5e-8)

(d) madelon
Logistic + L2 (λ = 1)

(e) square
LASSO (λ = 30)

(f) california
LASSO (λ = 0.1)

(g) dorothea
Logistic + L1 (λ = 0.01)

(h) madelon
Logistic + L1 (λ = 0.05)

Figure 1: Relative error (min/mean/max over 5 runs) to non-private optimal for DP-GCD (our approach) versus DP-CD
and DP-SGD. On the x-axis, 1 tick represents a full access to the data: p iterations of DP-CD, n iterations of DP-SGD and 1
iteration of DP-GCD. Number of iterations, clipping thresholds and step sizes are tuned simultaneously for each algorithm.

to the (cid:96)1 regularization. This shows that DP-GCD’s greedy
selection of updates can exploit this property to ﬁnd rel-
evant non-zero coefﬁcients (see Table 3 in Appendix D),
even when this selection is noisy. The lower-dimensional
datasets log1, log2 and madelon (where p < n) are
still too high dimensional (relatively to n) for DP-SGD
and DP-CD to make signiﬁcant progress. In contrast, DP-
GCD exploits the fact that solutions are quasi-sparse to
ﬁnd good approximate solutions quickly (see Figures 1a,
1b, 1d, 1e, 1g and 1h). On the low-dimensional dataset
california, DP-GCD is roughly on par with DP-SGD
and DP-CD (see Figure 1f). This is due to the additional
noise term introduced by the greedy selection rule: in such
setting, the lower number of iterations does not compensate
for this as much as in higher-dimensional problems. A sim-
ilar phenomenon arise in mtp (Figure 1c), whose solution
is not imbalanced enough for DP-GCD to be superior to its
competitors.

Computational complexity. As discussed in Section 4.6,
one iteration of DP-GCD requires a full pass on the data.
This is as costly as p iterations of DP-CD or n iterations
of DP-SGD. Nonetheless, on many problems, DP-GCD re-
quires just as many passes on the data as DP-CD and DP-
SGD (Figures 1a and 1c to 1f). When more computation is

required, it also provides signiﬁcantly better solutions than
DP-CD and DP-SGD (Figure 1b). This is in line with our
theoretical results from Section 4.4.

6 CONCLUSION AND DISCUSSION

We proposed DP-GCD, a greedy coordinate descent al-
gorithm for DP-ERM. In favorable settings, DP-GCD
achieves utility guarantees of O(log(p)/n2/3(cid:15)2/3) and
O(log(p)/n2(cid:15)2) for convex and strongly-convex objec-
tives. It is the ﬁrst algorithm to achieve such rates with-
out solving an (cid:96)1-constrained problem. Instead, we show
that DP-GCD depends on (cid:96)1-norm quantities and automat-
ically adapts to the structure of the problem. Speciﬁcally,
DP-GCD interpolates between logarithmic and polynomial
dependence on the dimension, depending on the problem.
Thus, DP-GCD constitutes a step towards the design of an
algorithm that adjusts to the appropriate (cid:96)p structure of a
problem (see Bassily et al., 2021; Asi et al., 2021).

We also showed that DP-GCD adapts to the quasi-sparsity
of the problem, without requiring a priori knowledge about
it. In such problems, it converges to a good approximate so-
lution in few iterations. This improves utility, and reduces
the polynomial dependence on the dimension to a polyno-

RelativeErrortoNon-PrivateOpt.01020Passesondata5×10−16×10−17×10−18×10−101020Passesondata3×10−14×10−16×10−101020Passesondata10−16×10−27×10−28×10−29×10−201020Passesondata4×10−25×10−2RelativeErrortoNon-PrivateOpt.012Passesondata4×10−15×10−16×10−17×10−105101520Passesondata10−410−310−210−10123Passesondata6×10−17×10−18×10−10510Passesondata10−410−310−2DP-CDDP-SGDDP-GCDPaul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

mial dependence on the (much smaller) quasi-sparsity level
of the solution.

We also proposed and evaluated a proximal variant of DP-
GCD, allowing non-smooth, sparsity-inducing regulariza-
tion. While it is not covered by our utility guarantees, we
note that the only existing analysis of such variants in the
non-private setting is the one of Karimireddy et al. (2019)
for (cid:96)1 and box constraints. Their proof relies on an alterna-
tion between good (that provably progress) and bad steps
(that do not increase the objective), which does not transfer
to the private setting. Extending such results to DP-ERM
is an exciting direction for future work.

ACKNOWLEDGMENTS

The authors would like to thank the anonymous reviewers
who provided useful feedback on previous versions of this
work, which helped to improve the paper.

This work was supported by the Inria Exploratory Action
FLAMED and by the French National Research Agency
through grant ANR-20-CE23-0015 (Project
(ANR)
PRIDE), ANR-20-CHIA-0001-01 (Chaire IA CaMeLOt)
and ANR 22-PECY-0002 IPOP (Interdisciplinary Project
on Privacy) project of the Cybersecurity PEPR.

REFERENCES

Abadi, M., A. Chu, I. Goodfellow, H. B. McMahan, I.
Mironov, K. Talwar, and L. Zhang (Oct. 2016). “Deep
Learning with Differential Privacy”. In: Proceedings of
the 2016 ACM SIGSAC Conference on Computer and
Communications Security. CCS ’16. New York, NY,
USA: Association for Computing Machinery, pp. 308–
318.

Asi, H., V. Feldman, T. Koren, and K. Talwar (2021). “Pri-
vate Stochastic Convex Optimization: Optimal Rates in
(cid:96)1 Geometry”. In: International Conference on Machine
Learning. PMLR.

Bassily, R., V. Feldman, C. Guzm´an, and K. Talwar (2020).
“Stability of Stochastic Gradient Descent on Nonsmooth
Convex Losses”. In: Advances in Neural Information
Processing Systems. Vol. 33. Curran Associates, Inc.,
pp. 4381–4391.

Bassily, R., V. Feldman, K. Talwar, and A. Guha Thakurta
(2019). “Private Stochastic Convex Optimization with
Optimal Rates”. In: Advances in Neural Information
Processing Systems. Vol. 32. Curran Associates, Inc.
Bassily, R., C. Guzman, and A. Nandi (2021). “Non-
Euclidean Differentially Private Stochastic Convex Opti-
mization”. In: Proceedings of Thirty Fourth Conference
on Learning Theory. PMLR, pp. 474–499.

Bassily, R., K. Nissim, A. Smith, T. Steinke, U. Stemmer,
and J. Ullman (June 2016). “Algorithmic Stability for
Adaptive Data Analysis”. In: Proceedings of the Forty-
Eighth Annual ACM Symposium on Theory of Comput-
ing. STOC ’16. New York, NY, USA: Association for
Computing Machinery, pp. 1046–1059.

Bassily, R., A. Smith, and A. Thakurta (Oct. 2014).
“Differentially Private Empirical Risk Minimization:
Efﬁcient Algorithms and Tight Error Bounds”. In:
arXiv:1405.7085 [cs, stat].

Bellet, A., R. Guerraoui, M. Taziki, and M. Tommasi (Mar.
2018). “Personalized and Private Peer-to-Peer Machine
Learning”. In: International Conference on Artiﬁcial In-
telligence and Statistics. PMLR, pp. 473–481.

Boyd, S. P. and L. Vandenberghe (2004). Convex Optimiza-
tion. Cambridge, UK ; New York: Cambridge University
Press.

Chaudhuri, K., C. Monteleoni, and A. D. Sarwate (2011).
“Differentially Private Empirical Risk Minimization”.
In: Journal of Machine Learning Research 12.29,
pp. 1069–1109.

Damaskinos, G., C. Mendler-D¨unner, R. Guerraoui, N. Pa-
pandreou, and T. Parnell (May 2021). “Differentially Pri-
vate Stochastic Coordinate Descent”. In: Proceedings of
the AAAI Conference on Artiﬁcial Intelligence. Vol. 35,
pp. 7176–7184.

Dhillon, I., P. Ravikumar, and A. Tewari (2011). “Near-
est Neighbor Based Greedy Coordinate Descent”. In:
Advances in Neural Information Processing Systems.
Vol. 24. Curran Associates, Inc.

Dwork, C. (2006). “Differential Privacy”. In: Automata,
Languages and Programming. Ed. by M. Bugliesi, B.
Preneel, V. Sassone, and I. Wegener. Lecture Notes in
Computer Science. Berlin, Heidelberg: Springer, pp. 1–
12.

Dwork, C., V. Feldman, M. Hardt, T. Pitassi, O. Reingold,
and A. L. Roth (June 2015). “Preserving Statistical Va-
lidity in Adaptive Data Analysis”. In: Proceedings of
the Forty-Seventh Annual ACM Symposium on Theory
of Computing. STOC ’15. New York, NY, USA: Associ-
ation for Computing Machinery, pp. 117–126.

Dwork, C. and A. Roth (2013). “The Algorithmic
Foundations of Differential Privacy”. In: Foundations
and Trends® in Theoretical Computer Science 9.3-4,
pp. 211–407.

Fang, H., Z. Fan, Y. Sun, and M. Friedlander (June 2020).
“Greed Meets Sparsity: Understanding and Improving
Greedy Coordinate Descent for Sparse Optimization”.
In: Proceedings of the Twenty Third International Con-
ference on Artiﬁcial Intelligence and Statistics. PMLR,
pp. 434–444.

Feldman, V., T. Koren, and K. Talwar (June 2020). “Pri-
vate Stochastic Convex Optimization: Optimal Rates

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

in Linear Time”. In: Proceedings of the 52nd Annual
ACM SIGACT Symposium on Theory of Computing.
New York, NY, USA: Association for Computing Ma-
chinery, pp. 439–449.

Fercoq, O. and P. Richt´arik (Mar. 2014). “Acceler-
ated, Parallel and Proximal Coordinate Descent”. In:
arXiv:1312.5799 [cs, math, stat].

Frank, M. and P. Wolfe (Mar. 1956). “An Algorithm for
Quadratic Programming”. In: Naval Research Logistics
Quarterly 3.1-2, pp. 95–110.

Hanzely, F., K. Mishchenko, and P. Richtarik (Dec. 2018).
“SEGA: Variance Reduction via Gradient Sketching”.
In: Proceedings of the 32nd International Conference on
Neural Information Processing Systems. NIPS’18. Red
Hook, NY, USA: Curran Associates Inc., pp. 2086–2097.

Hardt, M., B. Recht, and Y. Singer (June 2016). “Train
Faster, Generalize Better: Stability of Stochastic Gradi-
ent Descent”. In: Proceedings of The 33rd International
Conference on Machine Learning. PMLR, pp. 1225–
1234.

Hu, L., S. Ni, H. Xiao, and D. Wang (June 2022). “High
Dimensional Differentially Private Stochastic Optimiza-
tion with Heavy-tailed Data”. In: Proceedings of the 41st
ACM SIGMOD-SIGACT-SIGAI Symposium on Princi-
ples of Database Systems. PODS ’22. New York, NY,
USA: Association for Computing Machinery, pp. 227–
236.

Jaggi, M.

(Feb.

2013).

“Revisiting Frank-Wolfe:
In:
Projection-Free Sparse Convex Optimization”.
International Conference on Machine Learning. PMLR,
pp. 427–435.

Johnson, R. and T. Zhang (2013). “Accelerating Stochas-
tic Gradient Descent Using Predictive Variance Reduc-
tion”. In: Advances in Neural Information Processing
Systems. Ed. by C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger. Vol. 26. Curran
Associates, Inc.

Jung, C., K. Ligett, S. Neel, A. Roth, S. Shariﬁ-Malvajerdi,
and M. Shenfeld (June 2021). “A New Analysis of
Differential Privacy&#x2019;s Generalization Guaran-
tees (Invited Paper)”. In: Proceedings of the 53rd An-
nual ACM SIGACT Symposium on Theory of Comput-
ing. New York, NY, USA: Association for Computing
Machinery, p. 9.

Kairouz, P., M. R. Diaz, K. Rush, and A. Thakurta (July
2021). “(Nearly) Dimension Independent Private ERM
with AdaGrad Rates via Publicly Estimated Subspaces”.
In: Proceedings of Thirty Fourth Conference on Learn-
ing Theory. PMLR, pp. 2717–2746.

Karimireddy, S. P., A. Koloskova, S. U. Stich, and M. Jaggi
(Apr. 2019). “Efﬁcient Greedy Coordinate Descent for
Composite Problems”. In: The 22nd International Con-

ference on Artiﬁcial Intelligence and Statistics. PMLR,
pp. 2887–2896.

Kasiviswanathan, S. P. and H. Jin (2016). “Efﬁcient Pri-
vate Empirical Risk Minimization for High-dimensional
Learning”. In: p. 10.

Kifer, D., A. Smith, and A. Thakurta (2012). “Private Con-
vex Empirical Risk Minimization and High-dimensional
Regression”. In: p. 40.

Luo, Z.-Q. and P. Tseng (Jan. 1992). “On the Convergence
of the Coordinate Descent Method for Convex Differen-
tiable Minimization”. In: Journal of Optimization The-
ory and Applications 72.1, pp. 7–35.

Mangold, P., A. Bellet, J. Salmon, and M. Tommasi (2022).
“Differentially Private Coordinate Descent for Compos-
ite Empirical Risk Minimization”. In: International Con-
ference on Machine Learning. PMLR.

Massias, M., A. Gramfort, and J. Salmon (2017). “From
safe screening rules to working sets for faster Lasso-type
solvers”. In: NIPS-OPT.

Mironov, I. (Aug. 2017). “Renyi Differential Privacy”. In:
2017 IEEE 30th Computer Security Foundations Sympo-
sium (CSF), pp. 263–275.

Mironov, I., K. Talwar, and L. Zhang (Aug. 2019). “R´enyi
Differential Privacy of the Sampled Gaussian Mecha-
nism”. In: arXiv:1908.10530 [cs, stat].

Nesterov, Y. (2012). “Efﬁciency of Coordinate Descent
Methods on Huge-Scale Optimization Problems”. In:
SIAM Journal on Optimization 22.2, pp. 341–362.

Nutini, J., M. Schmidt, I. Laradji, M. Friedlander, and H.
Koepke (June 2015). “Coordinate Descent Converges
Faster with the Gauss-Southwell Rule Than Random
Selection”. In: International Conference on Machine
Learning. PMLR, pp. 1632–1641.

Parikh, N. and S. Boyd (Jan. 2014). “Proximal Algo-
rithms”. In: Foundations and Trends in Optimization 1.3,
pp. 127–239.

Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos, and D.
Cournapeau (2011). “Scikit-Learn: Machine Learning in
Python”. In: MACHINE LEARNING IN PYTHON, p. 6.
Richt´arik, P. and M. Tak´aˇc (Apr. 2014). “Iteration Com-
plexity of Randomized Block-Coordinate Descent Meth-
ods for Minimizing a Composite Function”. In: Mathe-
matical Programming 144.1-2, pp. 1–38.

Shalev-Shwartz, S. and T. Zhang (Feb. 2013). “Stochastic
Dual Coordinate Ascent Methods for Regularized Loss”.
In: The Journal of Machine Learning Research 14.1,
pp. 567–599.

Shi, H.-J. M., S. Tu, Y. Xu, and W. Yin (Jan. 2017).
“A Primer on Coordinate Descent Algorithms”. In:
arXiv:1610.00040 [math, stat].

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

Wu, X., F. Li, A. Kumar, K. Chaudhuri, S. Jha, and J.
Naughton (May 2017). “Bolt-on Differential Privacy for
Scalable Stochastic Gradient Descent-based Analytics”.
In: Proceedings of the 2017 ACM International Con-
ference on Management of Data. SIGMOD ’17. New
York, NY, USA: Association for Computing Machinery,
pp. 1307–1322.

Xiao, L. and T. Zhang (Jan. 2014). “A Proximal Stochastic
Gradient Method with Progressive Variance Reduction”.
In: SIAM Journal on Optimization 24.4, pp. 2057–2075.
Zhou, Y., Z. S. Wu, and A. Banerjee (2021). “Bypassing the
Ambiant Dimension: Private SGD with Gradient Sub-
space Identiﬁcation”. In: p. 28.

Shokri, R., M. Stronati, C. Song, and V. Shmatikov (May
2017). “Membership Inference Attacks Against Machine
Learning Models”. In: 2017 IEEE Symposium on Secu-
rity and Privacy (SP), pp. 3–18.

Stich, S. U., A. Raj, and M. Jaggi (July 2017). “Approx-
imate Steepest Coordinate Descent”. In: Proceedings of
the 34th International Conference on Machine Learning.
PMLR, pp. 3251–3259.

Talwar, K., A. Guha Thakurta, and L. Zhang (2015).
“Nearly Optimal Private LASSO”. In: Advances in Neu-
ral Information Processing Systems 28.

Talwar, K., A. Thakurta, and L. Zhang (Nov. 2016). “Pri-
vate Empirical Risk Minimization Beyond the Worst
Case: The Effect of the Constraint Set Geometry”. en.
In: arXiv:1411.5417 [cs, stat]. arXiv: 1411.5417.

Tappenden, R., P. Richt´arik, and J. Gondzio (July 2016).
“Inexact Coordinate Descent: Complexity and Precondi-
tioning”. In: Journal of Optimization Theory and Appli-
cations 170.1, pp. 144–176.

Tibshirani, R. (1996). “Regression Shrinkage and Selection
Via the Lasso”. In: Journal of the Royal Statistical Soci-
ety: Series B (Methodological) 58.1, pp. 267–288.

Tseng, P. (June 2001). “Convergence of a Block Coordi-
nate Descent Method for Nondifferentiable Minimiza-
tion”. In: Journal of Optimization Theory and Applica-
tions 109.3, pp. 475–494.

Tseng, P. and S. Yun (Mar. 2009). “A Coordinate Gradient
Descent Method for Nonsmooth Separable Minimiza-
tion”. In: Mathematical Programming 117.1, pp. 387–
423.

Vanschoren, J., J. N. rijnvan Rijn, B. Bischl, and L. Torgo
(June 2014). “OpenML: Networked Science in Machine
Learning”. In: ACM SIGKDD Explorations Newsletter
15.2, pp. 49–60.

Wang, D., M. Ye, and J. Xu (2017). “Differentially Private
Empirical Risk Minimization Revisited: Faster and More
General”. In: Advances in Neural Information Process-
ing Systems. Ed. by I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett.
Vol. 30. Curran Associates, Inc.

Wang, L. and Q. Gu (Aug. 2019). “Differentially private
iterative gradient hard thresholding for sparse learning”.
In: Proceedings of the 28th International Joint Confer-
ence on Artiﬁcial Intelligence. IJCAI’19. Macao, China:
AAAI Press, pp. 3740–3747.

Wang, P., Y. Lei, Y. Ying, and H. Zhang (Jan. 2022).
“Differentially Private SGD with Non-Smooth Losses”.
In: Applied and Computational Harmonic Analysis 56,
pp. 306–336.

Wright, S. J. (June 2015). “Coordinate Descent Algo-
rithms”. In: Mathematical Programming 151.1, pp. 3–
34.

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

A PROOF OF PRIVACY

Theorem 4.2. Let (cid:15), δ

∈

(0, 1]. Algorithm 1 with λj = λ(cid:48)

j = 8Lj

n(cid:15)

(cid:112)T log(1/δ) is ((cid:15), δ)-DP.

Proof. In each iteration of Algorithm 1, the data is accessed twice: once to choose the coordinate and once to compute the
private gradient. In total, data is thus queried 2T times.

j = 2Lj

Let λj = λ(cid:48)
[p], the gradient’s j-th entry has sensitivity 2Lj. Thus, by the report noisy max mechanism
(Dwork and Roth, 2013), the greedy choice of j is (cid:15)(cid:48)-DP. By the Laplace mechanism (Dwork and Roth, 2013), computing
the corresponding gradient coordinate is also (cid:15)(cid:48)-DP.

n(cid:15)(cid:48) . For j

∈

The advanced composition theorem for differential privacy thus ensures that the 2T -fold composition of these mechanisms
is ((cid:15), δ)-DP for δ > 0 and

(cid:15) =

(cid:112)

4T log(1/δ)(cid:15)(cid:48) + 2T (cid:15)(cid:48)(exp((cid:15)(cid:48))

1) ,

−

(2)

where we recall that (cid:15)(cid:48) = 2Lj
nλj
Dwork and Roth, 2013): with (cid:15)(cid:48) = (cid:15)/4

= 2Lj
nλ(cid:48)
j

[p]. When (cid:15)

for all j
(cid:112)T log(1/δ), Algorithm 1 is ((cid:15), δ)-DP for λj = λ(cid:48)

≤

∈

1, we can give a simpler expression (see Corollary 3.21 of

j = 8Lj

(cid:112)T log(1/δ)/n(cid:15).

B PROOF OF UTILITY

In this section, we prove Theorem 4.4 and Theorem 4.7, giving utility upper bounds for DP-GCD. We obtain these high-
probability results through a careful examination of the properties of DP-GCD’s iterates, and obtain high-probability results
by using concentration inequalities (see Appendix B.1).

In Appendix B.2, we prove a general descent lemma, which implies that iterates of DP-GCD converge (with high proba-
bility) to a neighborhood of the optimum. This property is proven rigorously in Appendix B.3.2, and we give the utility
results for general convex functions in Appendix B.3.3. Under the additional assumption that the objective is strongly
convex, we prove better utility bounds in Appendix B.4. These bounds follow from a key lemma (see Appendix B.4.1),
which implies linear convergence to a neighborhood of the optimum. We then use this result in two settings, obtaining
two different rates: ﬁrst in a general setting (in Appendix B.4.2), then under the additional assumption that the problem’s
solution is quasi-sparse (in Appendix B.4.3).

B.1 Concentration Lemma

To prove high-probability utility results, we ﬁrst bound (in Lemma B.1) the probability for a sum of squared Laplacian
variables to exceed a given threshold.
Lemma B.1. Let K > 0 and λ1, . . . , λK > 0. Deﬁne Xk
that

Lap(λk) and λmax = maxk∈[K] λk. For any β > 0, it holds

∼

Pr

(cid:34) K
(cid:88)

k=1

(cid:35)

X 2

k ≥

β

≤

2K exp

(cid:18)

√β
2λmax

−

(cid:19)

.

Proof. We ﬁrst remark that ((cid:80)K

Xk

k=1 |

(cid:80)K

)2 = (cid:80)K
k=1
|
(cid:34)
(cid:35)
(cid:16) K
(cid:88)

X 2

k ≥

a2

Pr

≤

(cid:80)K

Xk

k(cid:48)=1 |

(cid:17)2

Xk

|

|

||

Xk(cid:48)
| ≥
(cid:35)

a2

= Pr

≥

k=1 X 2
(cid:34)
(cid:16) K
(cid:88)

k . Therefore
(cid:35)

(cid:17)

Xk

|

|

≥

a

.

k=1

Pr

(cid:34) K
(cid:88)

k=1

Chernoff’s inequality now gives, for any γ > 0,

k=1

(cid:35)

Pr

(cid:34) K
(cid:88)

k=1

Xk
|

| ≥

a

≤

exp(

(cid:34)
γa)E

−

exp(γ

(cid:35)
)

.

K
(cid:88)

k=1

Xk

|

|

By the properties of the exponential and the Xk’s independence, we can rewrite the inequality as

Pr

(cid:34) K
(cid:88)

k=1

(cid:35)

Xk
|

| ≥

a

exp(

−

≤

(cid:34) K
(cid:89)

γa)E

k=1

(cid:35)

(cid:17)

(cid:16)

exp

Xk

γ

|

|

= exp(

γa)

−

K
(cid:89)

(cid:104)
E

(cid:16)

exp

k=1

(cid:17)(cid:105)

.

Xk

γ

|

|

(3)

(4)

(5)

(6)

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

We can now compute the expectation of exp(γ
(cid:90) +∞

Xk
|

) for k
|

∈

[K],

(cid:104)
E
exp

(cid:16)

γ

Xk
|

(cid:17)(cid:105)
|

=

1
2λk

x
|
|
λk

)dx =

1
λk

(cid:90) +∞

0

(cid:16)

(γ

exp

(cid:17)

)x

dx .

1
λk

−

−

We choose γ = 1/2λmax, such that γ
≤
(cid:104)
E
exp

exp(γ

) exp(

x
|

|

−∞
1/2λk for all k

(cid:16)

Xk

γ

|

|

(cid:17)(cid:105)

=

∈
1
λk

[K] and obtain

1

1
λk −

γ

=

1

1
γλk ≤

−

2 .

Plugging everything together, we have proved that

Pr

(cid:34) K
(cid:88)

k=1

(cid:35)

X 2

k ≥

a2

Pr

≤

(cid:34) K
(cid:88)

k=1

(cid:35)

Xk
|

| ≥

a

≤

2K exp(

−

a
2λmax

) ,

and taking a = √β gives the result.

B.2 Descent Lemma

(7)

(8)

(9)

−

χt
j∗

ηt
j|

2Mj |

χt
j|

2 + 1

2 + 1

We now prove a noisy descent lemma for DP-GCD (Lemma B.2). This lemma bounds the suboptimality f (wt+1)
at time t + 1 as a function of the suboptimality f (wt)
noise. At this point, we remark that when the gradient is large enough, it is very probable that 1

f (w∗)
f (w∗) at time t, of the gradient’s largest entry and of the

2
M −1,∞ ≥
2: this implies that the value of the objective function decreases with high probability,

1
2Mj |
even under the presence of noise. This observation will be crucial for proving utility for general convex functions.
0 and wt, wt+1
Lemma B.2. Let t
chosen as in Theorem 4.2 to ensure (cid:15), δ-DP. We denote by j
arg maxj∈[p] |∇
f (wt+1)

Rp two consecutive iterates of Algorithm 1, with γj = 1/Mj and λj, λ(cid:48)
j
[p] the coordinate chosen at this step t, and by j∗ =
/(cid:112)Mj the coordinate that would have been chosen without noise. The following inequality holds
1
2Mj |

≥
jf (wt)
|
f (w∗)

2
M −1,∞ +
(cid:107)

1
4Mj∗ |

f (wt)
(cid:107)

1
2Mj |

1
8 (cid:107)∇

f (w∗)

f (wt)

f (wt)

2 .
|

4Mj∗ |

χt
j|

ηt
j|

8 (cid:107)∇

χt
j∗

2 +

2 +

(10)

−

−

−

−

≤

∈

∈

|

Proof. The smoothness of f gives a ﬁrst inequality

f (wt+1)

f (wt) +

f (wt), wt+1

≤
= f (wt)

= f (wt)

(cid:104)∇
1
Mj ∇
1
Mj ∇

−

−

+

wt

1
2 (cid:107)
−
(cid:105)
jf (wt) + ηt

j) +

wt+1

wt

2
M
(cid:107)

−
1
2Mj

jf (wt)(

∇

jf (wt)2

1
Mj ∇

−

jf (wt)ηt

j +

(
∇
1
2Mj

jf (wt) + ηt

j)2

jf (wt))2

(
∇

+

1
Mj ∇

jf (wt)ηt

j +

1
2Mj

(ηt

j)2

= f (wt)

1
2Mj ∇

−

jf (wt)2 +

1
2Mj

(ηt

j)2 .

(11)

(12)

(13)

(14)

We will make the noisy gradient appear, so as to use the noisy greedy rule. To do so, we remark that the classical inequality
jf (wt)/(cid:112)Mj and
(a + b)2
b = χt

2 (a + b)2 + b2. Applied with a =

2a2 + 2b2 for any a, b

j/(cid:112)Mj, this results in

R implies that

≤ −

a2

∇

−

≤

∈

1

And, by the noisy greedy rule,

a2

−

≤ −

1

2 (a + b)2 + b2 with a = (

∇
jf (wt)2

1
2Mj ∇

−

1
2Mj ∇

−

1

√Mj∗ |∇

1
4Mj

(

jf (wt)2

jf (wt) + χt

j)2 +

∇

≤ −
j∗ f (wt) + χt
j∗
| ≤
j∗ f (wt) + χj∗ )/(cid:112)Mj∗ and b =

√Mj |∇

1

jf (wt) + χt
j|

1
2Mj

(χt

j)2 .

(15)

. We replace in (15) and use the inequality

χj∗ /(cid:112)Mj∗ to obtain

j∗ f (wt) + χt

−
j∗ )2 +

1
2Mj

(χt

j)2

j∗ f (wt))2 +

1
4Mj∗

(χt

j∗ )2 +

1
2Mj

(χt

j)2 .

(
∇

(
∇

≤ −

1
4Mj∗
1
8Mj∗
≤ −
j∗ f (wt))2 =

(16)

(17)

The result follows from (14) and 1

Mj∗ (

∇

f (wt)

2
M −1,∞.
(cid:107)

(cid:107)∇

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

B.3 Utility for General Convex Functions

In this section, we derive an upper bound on the utility of DP-GCD for convex objective functions. First, we use con-
vexity of f to upper bound the decrease described in Lemma B.2. This gives Lemma B.3 in Appendix B.3.1, where the
suboptimality gap f (wt+1)
f (w∗)
at time t and the noise injected in step t. The novelty of our analysis lies in Lemma B.4, where examine the decrease of
the objective. Speciﬁcally, we show that either (i) f (wt) is far from its minimum, and the suboptimality gap decreases
with high probability, either (ii) f (wt) is close to its minimum, then all future iterates of DP-GCD will remain in a ball
whose radius is determined by the variance of the noise. This observation is essential for proving the utility results stated
in Section 4.3.

f (w∗) at time t + 1 is upper bound by a function of the suboptimality gap f (wt)

−

−

B.3.1 Descent Lemma for Convex Functions

Lemma B.3. Under the hypotheses of Lemma B.2, for a convex objective function f , we have

f (wt+1)

f (w∗)

f (wt)

−

≤

f (w∗)

−

+

1
2Mj |

ηt
j|

2 +

(f (wt)
wt
8
(cid:107)
χt
j|

−
1
2Mj |

−
−
2 +

f (w∗))2
2
w∗
M,1

(cid:107)
1
4Mj∗ |

χt
j∗

2 .
|

Proof. Since f is convex, it holds that

After reorganizing the terms, we can upper bound them using H¨older’s inequality

f (w∗)

≥

f (wt) +

(cid:104)∇

f (wt), w∗

wt

(cid:105)

−

.

where the second inequality holds since

square it and reorganize to get

f (wt)

−

f (w∗)

≤ (cid:104)∇

w∗

f (wt), wt
f (wt)

(cid:105)
−
wt
(cid:107)M −1,∞(cid:107)

w∗

M,1 ,
(cid:107)

(cid:107)·(cid:107)
f (wt)
(cid:107)

M,1 and
2
M −1,∞ ≤ −

−(cid:107)∇

≤ (cid:107)∇
(cid:107)·(cid:107)M −1,∞ are conjugate norms. We now divide (21) by

−
. Replacing in Lemma B.2 gives the result.

(f (wt)−f (w∗))2

wt

−

(cid:107)

w∗

(cid:107)wt−w∗(cid:107)2

M,1

(18)

(19)

(20)

(21)

M,1,

(cid:107)

B.3.2 Key Lemma on the Behavior of DP-GCD’s Iterates

Now that we have an inequality in the form of Lemma B.3, we prove that iterates of DP-GCD converge to a vicinity of the
optimum. In the general lemma below, think of ξt as f (wt)
f (w∗) and of β as the variance of the term. This result will
be combined with Lemma B.1 to obtain high-probability bounds.
ξt

t≥0 be two sequences of positive values that satisfy, for all t
}

Lemma B.4. Let

t≥0 and
}

ct
{

0,

≥

−

{

ξt+1

ξt

−

≤

ξ2
t
ct

+ β,

such that if ξt

ξ0 then ct

≤

≤

c0. Assume that β

c0 and ξ0

≤

≥

2√βc0. Then:

1. For all t > 0, ct

2. For all t

1, ξt

≥

≤

≤

c0, and there exists t∗ > 0 such that ξt+1

ξt if t < t∗ and ξt

2√βc0 if t

t∗.

≥

≤

≤

c0
t + 2√βc0.

Proof. 1. Assume that for t

0, √βc0

ξt

≤

≤

≥

ξ0. Then,

ξt+1

ξt

−

≤

ξ2
t
ct

+ β

ξt

−

≤

2

√βc0
c0

+ β = ξt ,

(22)

(23)

where the second inequality comes from ξt
value t∗, which deﬁnes the point of rupture between two regimes for ξt:

√βc0 and ξt

≤

≥

ξ0 (which implies ct

t∗ = min

(cid:110)
t

(cid:12)
(cid:12)
(cid:12) ξt

0

≥

≤

(cid:111)

(cid:112)βc0

.

c0). We now deﬁne the following

≤

(24)

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

ξ0, then (23) holds, that is ξt+1

ξt

≤

≤

ξ0. By induction, it follows that for all t < t∗,

Let t < t∗, assume that ξt
c0.
ξt+1

ξ0 and ct

ξt

≤

≤

≤
Remark now that ξt∗
ξt

≤

≤

≤

√βc0, we prove by induction that ξt stays under 2√βc0 for t

2√βc0. Then, there are two possibilities. If ξt

√βc0, then

≤

t∗. Assume that for t

≥

t∗,

≥

(25)

ξt+1

ξt

−

≤

ξ2
t
ct

+ β

≤

(cid:112)βc0 + β

(cid:112)βc0 ,

2

≤

and ξt+1
that for t

2√βc0. Otherwise, √βc0
t∗, ξt

ξt

≤

≤

≤
≥

≤

2√βc0, which concludes the proof of the ﬁrst part of the lemma.

≤

2√βc0

ξ0 and (23) holds, which gives ξt+1

ξt

≤

≤

2√βc0. We proved

2. We start by proving this statement for 0 < t < t∗

by the ﬁrst part of the lemma, ξt+1

ξt

−

≤

ξ2
t
ct

+ β

≤

−
ξt

ξ2
t
c0

−

1. Deﬁne ω = 2u
c0

and u = √βc0. The assumption on ξt implies,

+ β, which can be rewritten

ξt+1

u

−

≤

(1

−

ω)(ξt

u)

−

−

(ξt

u)2

,

−
c0

(26)

u2
, and ωu
c0
u) to obtain

−

u2
c0

= u2
c0

= β.

−
ω

−

since (1
Since t < t∗

−

ω)(ξt

u)
−
−
1, ξt+1

−

−

(ξt−u)2
c0
−
u > 0 and ξt

= ξt

ωξt

u + ωu

−

ξ2
t
c0 −

2ξtu
c0 −

u2
c0

−

= ξt

−

u > 0, we can thus divide (26) by (ξt+1

ξ2
t
c0 −
−

u + ωu
u)(ξt

−
−

1

−

ξt

u ≤

1
−
ξt+1

u −

ξt
(ξt+1

−
−

u
u)c0 ≤

1
−
ξt+1

ω

−

u −

1
c0 ≤

1
ξt+1

−

1
c0

,

u −

(27)

where the second inequality comes from ξt+1
recursively and taking the inverse of the result, we obtain the desired resuld ξt
0 < t < t∗.

u from the ﬁrst part of the lemma. By applying this inequality
c0
c0
t + 2√βc0 for all
t + √βc0

ξt

≤

≤

≤

−

−

u

For t

≥

t∗, we have already proved that ξt

2√βc0

≤

≤

c0
t + 2√βc0, which concludes our proof.

B.3.3 Convex Utility Result

, and f is M -component-smooth. Deﬁne

Theorem 4.4. (Convex Case) Let (cid:15), δ
d
be the output of Algorithm 1 with step sizes γj = 1/Mj, and noise scales λ1, . . . , λp, λ(cid:48)
(with T chosen below) to ensure ((cid:15), δ)-DP. Then, the following holds for ζ

; d) is a convex and L-component-Lipschitz loss function for all
(0, 1]. Assume (cid:96)(
·
Rp
p set as in Theorem 4.2

∗ the set of minimizers of f , and f ∗ the minimum of f . Let wpriv

1, . . . , λ(cid:48)

(0, 1]:

∈ X

W

∈

∈

∈

f (wpriv)

f (w∗)

−

8R2
M
T

≤

(cid:113)

+

32R2

M β ,

(28)

log( 8T p

ζ )2, and RM = max
w∈Rp

min
w∗∈W ∗

w

(cid:8)
(cid:107)

−

w∗

M,1

(cid:107)

|

f (w)

≤

f (w0)(cid:9).

If we set T =

where β = 2λ2
max
Mmin
(cid:16) n2(cid:15)2R2
(cid:17)1/3
27L2

M Mmin
max log(1/δ)

, then with probability at least 1

ζ,

−

Proof. Let ξt = f (wt)
the events Et

−

f (wT )

−

f (w0) = (cid:101)O

(cid:16) R4/3

M L2/3
M 1/3

max log(p/ζ)

(cid:17)

.

minn2/3(cid:15)2/3

f (w∗). We upper bound the following probability by the union bound, and the fact that for t

j : “coordinate j is updated at step t” for j

[p] partition the probability space:

∈

(cid:34)

Pr

t, ξt+1

∃

ξt

≥

−

wt

8
(cid:107)

ξ2
t
w∗

−

2
M,1
(cid:107)

T −1
(cid:88)

=

+ β

p
(cid:88)

t=0

j=1

(cid:35)

T −1
(cid:88)

t=0

≤
(cid:34)

Pr

ξt+1

ξt

≥

(cid:34)

Pr

ξt+1

ξt

≥

ξ2
t
w∗

−

2
M,1
(cid:107)

−

wt

8
(cid:107)

(cid:35)

+ β

(cid:35)

ξ2
t
w∗

−

2
M,1
(cid:107)

−

wt
8
(cid:107)

+ β

∧

Et
j

.

(29)

0,

≥

(30)

(31)

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

Lemma B.3 gives ξt+1
bound:

ξt

−

≤

ξ2
t

8(cid:107)wt−w∗(cid:107)2

M,1

+ 1

2Mj |

ηt
j|

2 + 1

χt
j|

2Mj |

2 + 1

4Mj∗ |

χt
j∗

2. We thus have the following upper

|

Pr

(cid:104)
∃

t, ξt+1

ξt

−

≥

1

8(cid:107)wt−w∗(cid:107)2

M,1

(cid:105)

ξ2
t + β

T −1
(cid:88)

p
(cid:88)

≤

t=0

j=1

(cid:104) |ηt

j |2
2Mj

Pr

+

|χt
j |2
2Mj

+

|χt
j∗ |2
4Mj∗ ≥

(cid:105)

β

T −1
(cid:88)

p
(cid:88)

≤

t=0

j=1

Pr (cid:2)
ηt
j|
|

2 +

2 +

χt
j|
|

χt
j∗
|

2
|

≥

2Mminβ(cid:3) .

By Lemma B.1 with X1 = ηt

j ∼

Lap(λj), X2 = χt

j ∼

Lap(λ(cid:48)

j) and X3 = χt
j∗

∼

Lap(λ(cid:48)

j∗ ), it holds that

Pr (cid:2)
|

ηt
j|

2 +

2 +

χt
j|
|

2

χt
j∗
|

|

≥

2Mminβ(cid:3)

8 exp

≤

(cid:18)

√2Mminβ
2λmax

−

(cid:19)

=

ζ
T p

,

where the last equality comes from β = 2λ2

max
Mmin

log( 8T p

ζ )2. We have proved that

(cid:34)

Pr

t, ξt+1

∃

ξt

≥

ξ2
t
w∗

−

2
M,1
(cid:107)

−

wt
8
(cid:107)

(cid:35)

+ β

T −1
(cid:88)

p
(cid:88)

≤

t=0

j=1

ζ
T p

= ζ .

(32)

(33)

(34)

(35)

We now use our Lemma B.4, with ξt = f (wt)
2λ2
max
Mmin
ct
≤
f (w0)

2
M,1 for t > 0; and β =
(cid:107)
ζ )2. These values satisﬁes the assumptions of Lemma B.4 since, by the deﬁnition of RM , it holds that
M β, therefore

log( 8T p
c0 whenever ξt
f (w∗)

wt
M and ct = 8
(cid:107)

f (w∗)). Additionally, f (w0)

f (w∗); c0 = 8R2

ξ0 (i.e., f (wt)

2√βc0, and β

f (w∗)

f (w∗)

f (w0)

32R2

−
c0.

w∗

(cid:112)

−

−

≥

−

−

≤

≤

−

≥

≤

We obtain the result, with probability at least 1

ζ:

f (wt)

−

f (w0)

c0
t

≤

+ 2

8R2
M
t

+

64RM Lmax log(8T p/ζ)
√Mminn(cid:15)

(cid:112)T log(1/δ)

.

(36)

−
(cid:112)βc0 =

For T =

minn2/3(cid:15)2/3

M M 1/3
R2/3
max log(1/δ)1/3 , we obtain that, with probability at least 1
4L2/3

ζ,

−

f (wt)

−

f (w0)

≤

which is the result of the theorem.

64R4/3

M L2/3
M 1/3

max log(1/δ)1/3
minn2/3(cid:15)2/3

log

(cid:16) pR2/3
4ζL2/3

M M 1/3
minn2/3(cid:15)2/3
max log(1/δ)1/3

(cid:17)

,

(37)

B.4 Utility for Strongly-Convex Functions

B.4.1 A Key Inequality for Strongly-Convex Functions

We now prove a link between f ’s largest gradient entry and the suboptimality gap, under the assumption that there exists a
unique minimizer w∗ of f that is (α, τ )-quasi-sparse. Note that this assumption is not restrictive in general as any vector
in Rp is (0, p)-quasi-sparse, and for any τ there exists α > 0 such that the vector is (α, τ )-quasi-sparse. We will denote by

Rp the set of (α, τ )-quasi-sparse vectors of Rp:

τ,α

W

⊆

τ,α =

W

Rp

w

{

∈

j
| |{

∈

[p]

wj

| |

| ≥

α

}| ≤

.

τ

}

(38)

When α = 0, we simply write
operator πα, that puts to 0 the coordinates that are smaller than α, “projecting” vectors from

τ,0, that is the set of τ -sparse vectors. We also deﬁne the associated thresholding
Rp,

τ , i.e., for w

τ,α to

τ =

W

W

W

W

∈

πα(w) =

(cid:40)

0
wj

α ,

wj
|

if
otherwise .

| ≤

(39)

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

Importantly, restricting a function to τ -sparse vectors changes its strong-convexity parameter. Let τ
we say a function is µ(τ )

M,q-strongly-convex when restricted to τ -sparse vectors if for all τ -sparse vectors v, w

≥

0 and q

∈ {
τ ,

∈ W

f (w)

≥

f (v) +

f (v), w

(cid:104)∇

v

(cid:105)

−

+

µ(τ )
M,q
2 (cid:107)

w

v

2
M,q .
(cid:107)

−

1, 2
,
}

(40)

Remark that when τ
be compared using the following inequality (Fang et al., 2020), for all τ

≥

p, we recover the usual strong-convexity parameters. The parameters w.r.t. (cid:96)1- and (cid:96)2-norms can

0,

≥

1
τ

µ(τ )
M,2 ≤

µ(τ )
M,1 ≤

µ(τ )
M,2 .

(41)

We are ready to prove Lemma B.5.
Lemma B.5. Let f : Rp
restricted to τ -sparse vectors, for τ
Let wt

→
Rp be a t-sparse vector for some t

≥

∈

R be a function that is M -component-smooth, and µ(τ )

M,1-strongly-convex w.r.t.

0. Assume that the unique minimizer w∗ of f is (τ, α)-quasi-sparse, for α, τ

(cid:107)·(cid:107)

M,1 when
0.

≥

0. Then we have

≥

1
2 (cid:107)∇

−

f (wt)

(cid:107)M −1,∞ ≤ −

µ(t+τ )
M,1 (f (wt)

f (w∗)) +

−

1
2

Mmaxµ(t+τ )

M,1 (p

τ )α2 .

−

(42)

Proof. Let wt
union of wt and πα(w∗)’s supports (supp(wt) and supp(πα(w∗))) thus satisﬁes
the function f is µ(t+τ )

Rp be a t-sparse vector. Remark that w∗ is (α, τ )-quasi-sparse, meaning that πα(w∗) is τ -sparse. The
t + τ . As

M,1 and t + τ sparse vector,

M,1 -strongly-convex with respect to

supp(πα(w∗))

supp(w)

| ≤

∈

∪

|

(cid:107)·(cid:107)

f (πα(w))

f (wt) +

≥

(cid:104)∇

f (wt), πα(w)

wt

+

(cid:105)

−

µ(t+τ )
M,1
2

(cid:107)

πα(w)

wt

2
M,1 .
(cid:107)

−

Since πα :

τ,α

W

τ,0 is surjective, minimizing this equation for w

→ W

τ,α on both sides gives

∈ W

inf
w∈Wτ

f (w)

≥

f (wt)

(cid:40)

−

sup
w∈Wτ,α
(cid:40)

f (wt), wt

(cid:104)−∇

πα(w)

(cid:105) −

−

µ(t+τ )
M,1
2

(cid:107)

πα(w)

wt

2
M,1
(cid:107)

−

(cid:41)

f (wt)

sup
w∈Rp

−

≥

f (wt), wt

(cid:104)−∇

w

−

(cid:105) −

µ(t+τ )
M,1
2

w
(cid:107)

−

wt

2
M,1
(cid:107)

(cid:41)

.

(43)

(44)

(45)

The second term corresponds to the conjugate of the function 1
This gives

M,1, that is 1
2
2 (cid:107)·(cid:107)

2
M −1,∞ (Boyd and Vandenberghe, 2004).
2 (cid:107)·(cid:107)

inf
w∈Wτ

f (w)

≥

f (wt)

= f (wt)

(cid:33)∗

(cid:32) µ(t+τ )
M,1
2

2
1
(cid:107)·(cid:107)

f (w(cid:48)))

(
−∇

1
2µ(t+τ )
M,1

2
f (w(cid:48))
M −1,∞ .
(cid:107)

(cid:107)∇

−

−

Finally, w∗ is the minimizer of f (which is convex), thus
2
f (w)
M,2. Hence

f (w∗) + 1

w∗

w

≤

2 (cid:107)

−

(cid:107)

f (w∗) = 0. The smoothness of f gives, for any w

∇

inf
w∈Wτ

f (w)

≤

f (w∗) + inf
w∈Wτ

1
2 (cid:107)

w

2
M,2.

w∗

(cid:107)

−

≤

f (w∗) +

1
2 (cid:107)

πα(w∗)

w∗

(cid:107)

−

2
M,2 ,

(46)

(47)

Rp,

∈

(48)

where the second inequality comes from πα(w∗)
Mmax(p

τ )α2 to get the result.

−

τ , since w∗

∈ W

τ,α. It remains to observe that

∈ W

πα(w∗)
(cid:107)

−

w∗

2
M,2 ≤
(cid:107)

Corollary B.6. For τ -sparse vectors, we have α = 0 and thus (p

τ )α = 0. Lemma B.5 can thus be simpliﬁed as

1
2 (cid:107)∇

−

2
f (wt)
M −1,∞ ≤ −
(cid:107)

−
µ(t+τ )
M,1 (f (wt)

f (w∗)) .

−

When vectors are not sparse (τ = p), we recover the inequality

−

1
2 (cid:107)∇

2
f (wt)
M −1,∞ ≤ −
(cid:107)

µM,1(f (wt)

(49)

f (w∗)).

−

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

B.4.2 General Strongly-Convex Utility Result

Theorem 4.4. (Strongly-Convex Case) Let (cid:15), δ
component-Lipschitz loss function for all d
and f ∗ the minimum of f . Let wpriv
1, . . . , λ(cid:48)
λ1, . . . , λp, λ(cid:48)
(0, 1]:
ζ

M,1 and L-
∗ be the set of minimizers of f ,
Rp be the output of Algorithm 1 with step sizes γj = 1/Mj, and noise scales
p set as in Theorem 4.2 (with T chosen below) to ensure ((cid:15), δ)-DP. Then, the following holds for

∈
, and f is M -component-smooth. Let

; d) is a µM,1-strongly-convex w.r.t.

(0, 1]. Assume (cid:96)(
·

∈ X

(cid:107)·(cid:107)

W

∈

∈

f (wT )

f (w∗)

−

(1

−

≤

µM,1
2

)T (f (w0)

f (w∗)) +

−

64T L2

max log(1/δ)

MminµM,1n2(cid:15)2

log(

2T p
ζ

) .

If we set T = 2

µM,1

log( MminµM,1n2(cid:15)2(f (w0)−f (w∗)

32L2

max log(1/δ)

), then with probability at least 1

ζ,

−

f (wT )

−

f (w∗) = (cid:101)O

(cid:16) L2

max log(p/ζ)
M,1n2(cid:15)2

Mminµ2

(cid:17)

.

(50)

(51)

Proof. When f is µM,1-strongly-convex w.r.t. the norm
vector) yields

M,1, Corollary B.6 with τ = p and α = 0 (which holds for any

(cid:107)·(cid:107)

1
2 (cid:107)∇

f (wt)
(cid:107)

−

2
M −1,∞ ≤ −

µM,1(f (wt)

−

f (w∗)) .

We replace this in Lemma B.2 to obtain

f (wt+1)

f (w∗)

−

(1

−

≤

µM,1
4

)(f (wt)

−

f (w∗)) +

1
2Mj |

ηt
j|

2 +

1
2Mj |

χt
j|

2 +

(52)

(53)

1
4Mj∗ |

χt
j∗

2 .
|

Analogously to the proof of Theorem 4.4, we deﬁne ξt = f (wt)
Pr (cid:2)
∃

ζ/T p, with β = 2λ2

)ξt + β(cid:3)

log( 8T p

t, ξt+1

µM,1
4

max
Mmin

(1

≥

−

≤

−

f (w∗) for all 0

t

≤

≤

T , and show that

ζ )2. This yields that, with probability at least 1

f (wT )

f (w∗)

−

(1

−

≤

(1

−

≤

µM,1
4

µM,1
4

)T (f (w0)

)T (f (w0)

−

−

f (w∗)) +

T −1
(cid:88)

(1

f (w∗)) +

t=0
4
µM,1

µM,1
4

−

)T −tβ

32T L2

max log(1/δ)

Mminn2(cid:15)2

log

(cid:17)2

(cid:16) 8T p
ζ

,

ζ,

−

(54)

(55)

With T = 4

µM,1

log

(cid:16) µM,1Mminn2(cid:15)2(f (w0)−f (w∗))

(cid:17)

128L2

max log(1/δ) log(8p/ζ)

we have, with probability at least 1

ζ,

−

f (wT )

f (w∗)

−

≤

128L2

max log(1/δ) log(8p/ζ)2

µM,1Mminn2(cid:15)2
max log(1/δ) log(8T p/ζ)2

512L2

µ2
M,1Mminn2(cid:15)2

+

log

which is the desired result.

B.4.3 Better Utility for Quasi-Sparse Solutions

(cid:16) µM,1Mminn2(cid:15)2(f (w0)

f (w∗))

(cid:17)

128L2

max log(1/δ) log(8p/ζ)2

−

,

(56)

Theorem 4.7. Consider f satisfying the hypotheses of Theorem 4.4, with Algorithm 1 initialized at w0 = 0. We denote
0, f is µ(τ (cid:48))
its output wT , and assume that its iterates remain s-sparse for some s
M,1-
M,2, and that the (unique) solution
strongly-convex w.r.t.
log(T P/ζ)2. Then for all
of problem (1) is (α, τ )-quasi-sparse for some α, τ
ζ:
T we have that, with probability at least 1
t

M,1 for τ (cid:48)-sparse vectors and µM,2-strongly-convex w.r.t.

≤
(cid:107)·(cid:107)
[0, 1], and β = 2λ2

p. Assume that, for all τ (cid:48)

0. Let T

max
Mmin

0, ζ

(cid:107)·(cid:107)

≥

≥

≥

∈

≤

−

f (wT )

−

≤

µ(min(s,T )+τ )
M,1
4

f (w∗)

(cid:16)

1

≤

−
µM,2
4(min(s, T ) + τ )

(cid:16)

1

−

(cid:17)T

(f (w0)

f (w∗)) +

−

4(min(s, T ) + τ )β
µM,2

(cid:17)T

(f (w0)

f (w∗)) +

−

4(min(s, T ) + τ )β
µM,2

+

+

min(s, T ) + τ
8
min(s, T ) + τ
8

(p

−

τ )α2

(57)

(p

−

τ )α2 .

(58)

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

Setting T = s+τ
µM,2
obtain that with probability at least 1

log((f (w0)

−

ζ,

−
f (wT )

f ∗)MminµM,2n2(cid:15)2/L2), and assuming α2 = O (cid:0)L2

max(s + τ )/Mminµ2

M,2pn2(cid:15)2(cid:1), we

f ∗ = (cid:101)O

−

(cid:18) L2

max
Mmin

(s + τ )2 log(2p/ζ)
µM,2n2(cid:15)2

(cid:19)

.

(59)

Proof. First, we remark that at each iteration, we change only one coordinate. Therefore, after t iterations, the iterate
wt is at most t-sparse. Since all iterates are also s-sparse, it is min(s, t)-sparse. Additionally, we assumed that w∗ is
(τ, α)-almost-sparse. Therefore, Lemma B.5 yields

1
2 (cid:107)∇
and Lemma B.2 becomes

−

f (wt)

(cid:107)M −1,∞ ≤ −

µ(min(s,t)+τ )
M,1

(f (wt)

−

f (w∗)) +

µ(min(s,t)+τ )
M,1
2

(p

−

τ )α2 ,

f (wt+1)

f (w∗)

−

(1

≤

+

µ(min(s,t)+τ )
M,1
4
2 +

ηt
j|

−
1
2Mj |

)(f (wt)

−
2 +

1
2Mj |

χt
j|

1
4Mj∗ |

χt
j∗

2 .
|

f (w∗)) +

µ(min(s,t)+τ )
M,1
8

τ )α2

(p

−

(60)

(61)

Then by Chernoff’s equality, we obtain (similarly to the proof of Theorem 4.4 for the convex case) that with probability at
least 1

ζ, for T

0,

−

≥

f (wT )

f (w∗)

−

T
(cid:89)

(cid:16)

≤

t=0

µ(min(s,t)+τ )
M,1
4

(cid:17)

1

−

(f (w0)

−

f (w∗))

T −1
(cid:88)

T
(cid:89)

+

t=0

k=T −t

(cid:16)

1

−

µ(min(s,k)+τ )
M,1
4

(cid:17)(cid:16)

β +

µ(min(s,t)+τ )
M,1
8

(p

−

τ )α2(cid:17)

.

(62)

µmin(s,T )+τ
M,1

, we can further upper bound µ(min(s,t)+τ )

M,1

≥

µ(τ )
M,1, and 1

−

≤

M,1

µ(min(s,t)+τ )
4

≤

Since for k

[T ], µmin(s,k)+τ
M,1

∈

M,1

µ(min(s,T )+τ )
4

1

−

and

T −1
(cid:88)

T
(cid:89)

t=0

k=T −t

(cid:16)

1

−

µ(min(s,k)+τ )
M,1
4

(cid:17)

T −1
(cid:88)

(cid:16)

≤

t=0

µ(min(s,T )+τ )
M,1
4

(cid:17)t

1

−

4
µ(min(s,T )+τ )
M,1

,

≤

which allows to simplify the above expression to

f (wT )

−

≤

≤

f (w∗)

(cid:16)

1

−

≤

µ(min(s,T )+τ )
M,1
4

(cid:17)T

(f (w0)

−

f (w∗)) +

(cid:16)

1

(cid:16)

1

−

−

µM,2
4(min(s, T ) + τ )
µM,2
4(min(s, T ) + τ )

(cid:17)T

(cid:17)T

(f (w0)

(f (w0)

−

−

f (w∗)) +

f (w∗)) +

4
µ(min(s,T )+τ )
M,1
(cid:16)
4(min(s, T ) + τ )
µM,2
4(min(s, T ) + τ )β
µM,2

(cid:16)

β +

µ(τ )
M,1
8

(p

β +

µM,2
8

(p

−

τ )α2(cid:17)

−
τ )α2(cid:17)

+

min(s, T ) + τ
8

(p

−

τ )α2 ,

(66)

where the second inequality follows from µ(min(s,T )+τ )
inequalities (57) and (58) of the theorem.
When α2 = O (cid:0)L2
τ

max(s + τ )/Mminµ2

s + τ , we choose T = s+τ
µM,2

log((f (w0)

M,1

≤

−

M,2

µ(min(s,T )+τ )
min(s,T )+τ ≥

≥

µM,2

min(s,T )+τ and µ(τ )

M,1 ≤

µM,2. We have proven

M,2pn2(cid:15)2(cid:1), the two additive terms of (66) are O((s + τ )β/µM,2). Since min(s, T ) +

f ∗)MminµM,2n2(cid:15)2/L2) to balance all the terms and obtain the result.

C GREEDY COORDINATE DESCENT FOR COMPOSITE PROBLEMS

Consider the problem of privately approximating

w∗

arg min
w∈Rp

∈

1
n

n
(cid:88)

i=1

(cid:96)(w; di) + ψ(w),

(67)

(63)

(64)

(65)

High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

where D = (d1, . . . , dn) is a dataset of n samples drawn from a universe
is convex and smooth in w, and ψ : Rp
typically nonsmooth (e.g., (cid:96)1-norm).

→

X
R is a convex regularizer which is separable (i.e., ψ(w) = (cid:80)p

× X →

R is a loss function which
j=1 ψj(wj)) and

, (cid:96) : Rp

Algorithm 2 DP-GCD (Proximal Version): Private Proximal Greedy CD
Rp, iteration count T > 0,
1: Input: initial w0
2: for t = 0 to T
3:
4:
5: return wT .

Select jt by the noisy GS-s, GS-r or GS-q rule.
jf (wt) + ηt
wt+1 = wt + (prox γjψj(wt

∈
1 do

j)ej,

jt))

j
∀

γj(

wt

∇

−

−

−

∈

[p], noise scales λj, λ(cid:48)

j, step sizes γj > 0.

ηt
jt ∼

Lap(λjt ).

We propose a proximal greedy algorithm to solve (67), see Algorithm 2. The proximal operator is the following (we refer
to Parikh and Boyd, 2014, for a detailed discussion on proximal operator and related algorithms):

proxγψ(v) = arg min
x∈Rp {

1
v
2 (cid:107)

x

2
2 + γψ(x)
}
(cid:107)

−

.

(68)

The same privacy guarantees as for the smooth DP-GCD algorithm hold since, privacy-wise, the proximal step is a post-
processing step. We also adapt the greedy selection rule to incorporate the non-smooth term. We can use one of the
following three rules

jt = arg max

j∈[p]

min
ξj ∈∂ψj (wj )

1
(cid:112)Mj |∇

jf (wt) + ηt

j + ξj

,

|

jt = arg max

(cid:112)Mj

j∈[p]

prox 1
Mj

|

ψj (wt

1
Mj

(

∇

jf (wt) + ηt
j)

wt
j|

−

,

jt = arg max

j∈[p]

min
α∈R ∇

jf (wt)α +

α2 + ψj(wt

j + α)

ψj(wt

j) .

−

j −
Mj
2

(GS-s)

(GS-r)

(GS-q)

These rules are commonly considered in the non-private GCD literature (see e.g., Tseng and Yun, 2009; Shi et al., 2017;
Karimireddy et al., 2019), except for the noise ηt

j and the rescaling in the GS-s and GS-r rules.

D EXPERIMENTAL DETAILS

In this section, we provide more information about the experiments, such as details on implementation, datasets and the
hyperparameter grid we use for each algorithm. We then give the full results on our L1-regularized, non-smooth, problems,
with the three greedy rules (as opposed to Section 5 where we only plotted results for the GS-r rule). Finally, we provide
runtime plots.

Code and setup. The algorithms are implemented in C++ for efﬁciency, together with a Python wrapper for simple use.
It is provided as supplementary. Experiments are run on a computer with a Intel (R) Xeon(R) Silver 4114 CPU @ 2.20GHz
and 64GB of RAM, and took about 10 hours in total to run (this includes all hyperparameter tuning).

Datasets. The datasets we use are described in Table 1. In Figure 2, we plot the histograms of the absolute value of each
problem solution’s parameters. The purple line indicates the value of α that ensures that the parameters of the solution are
(α, 5)-quasi-sparse. Note the logarithmic scale on the y-axis. On the log1, log2, madelon, square, california
and dorothea datasets, the solutions are very imbalanced. In these problems, a very limited number of parameters stand
out, and DP-GCD is able to exploit this property. This illustrates the results from Section 4.4, since DP-GCD can exploit
this structure even in quasi-sparse problems, where α is non zero. Conversely, the mtp solution is more balanced: the
structural properties of this dataset are not strong enough for DP-GCD to outperform its competitors.

Hyperparameters. On all datasets, we use the same hyperparameter grid. For each algorithm, we choose between
roughly the same number of hyperparameters. The number of passes on data represents p iterations of DP-CD, n iterations
of DP-SGD, and 1 iteration of DP-GCD. The complete grid is described in Table 2, and the chosen hyperparameters for
each problem and algorithm are given in Table 4.

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

(a) log1
Logistic + L2
(λ = 1e-3)

(b) log2
Logistic + L2
(λ = 1e-3)

(c) mtp
Least Squares + L2
(λ = 5e-8)

(d) madelon
Logistic + L2
(λ = 1)

(e) square
LASSO
(λ = 30)

(f) california
LASSO
(λ = 0.1)

(g) dorothea
Logistic + L1
(λ = 0.01)

(h) madelon
Logistic + L1
(λ = 0.05)

Figure 2: Histograms of the absolute value of each problem solution’s parameters. Purple line indicates the α for which
the plotted vector is (α, 5)-quasi-sparse. Y-axis is logarithmic.

Recovery of the support.
In Table 3, we report the number of coordinates that are correctly/incorrectly identiﬁed as
non-zero on (cid:96)1 regularized problems. Contrary to DP-SGD and DP-CD, DP-GCD never incorrectly identiﬁes a coordinate
as non-zero. Additionally, the suboptimality gap is lower for DP-GCD: its updates thus lead to better solutions.

Additional experiments on proximal DP-GCD.
after tuning the hyperparameters with the grid described above for each of the GS-s, GS-r and GS-q rules.

In Figure 3, we show the results of the proximal DP-GCD algorithm,

The three rules seem to behave qualitatively the same on square, dorothea and madelon, our three high-dimensional
non-smooth problems. There, most coordinates are chosen about one time. Thus, as described by Karimireddy et al.
(2019), all the steps are “good” steps (along their terminology): and on such good steps, the three rules coincide. On the
lower-dimensional dataset california, coordinates can be chosen more than one time, and “bad” steps are likely to
happen. On these steps, the three rules differ.

Table 2: Hyperparameter grid used in our experiments.

Algorithm

Parameter

Values

DP-CD

DP-SGD

DP-GCD

Passes on data
Step sizes
Clipping threshold

[0.001, 0.01, 0.1, 1, 2, 3, 5, 10, 20]
np.logspace(-2, 1, 10)
np.logspace(-4, 6, 50)

Passes on data
Step sizes
Clipping threshold

[0.001, 0.01, 0.1, 1, 2, 3, 5, 10, 20]
np.logspace(-6, 0, 10)
np.logspace(-4, 6, 50)

Passes on data
Step sizes
Clipping threshold

[1, 2, 4, 7, 10, 15, 20]
np.logspace(-2, 1, 10)
np.logspace(-4, 6, 50)

0.00.51.0Value10−1102Count01Value10−1102Count0.00.1Value10−1102Count0.000.020.04Value10−1102Count0100Value10−1102Count0.00.5Value10−2100Count0.00.5Value100104Count0.00.1Value10−1102Countvalueofαfor(α,5)-quasi-sparsityHigh-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent

Table 3: Coordinates correctly/incorrectly identiﬁed as non-zeros by each algorithm, and relative suboptimality gap
(f (wpriv)

f ∗)/f ∗ (averaged over 5 runs).

−

w∗
0
(cid:107)
(cid:107)
DP-CD
DP-SGD
DP-GCD

square

7
0 / 0 (0.75)
0 / 3 (0.75)
2 / 0 (0.35)

california

dorothea

madelon

3
3 / 2 (0.0024)
3 / 5 (0.020)
2 / 0 (0.00056)

72
1 / 1 (0.77)
0 / 0 (0.78)
1 / 0 (0.64)

3
0 / 0 (0.0085)
0 / 0 (0.012)
1 / 0 (0.0015)

Table 4: Selected hyperparameters for every dataset and algorithm.

Dataset

Loss

Algorithm Passes on data Clipping threshold

Step size

california LeastSquares + L1 DP-CD
LeastSquares + L1 DP-CD
square
LeastSquares + L2 DP-CD
mtp
DP-CD
Logistic + L1
madelon
DP-CD
Logistic + L2
log1
DP-CD
Logistic + L2
log2
DP-CD
Logistic + L2
madelon
dorothea
DP-CD
Logistic + L1
california LeastSquares + L1 DP-SGD
LeastSquares + L1 DP-SGD
square
LeastSquares + L2 DP-SGD
mtp
DP-SGD
Logistic + L1
madelon
DP-SGD
Logistic + L2
log1
DP-SGD
Logistic + L2
log2
DP-SGD
Logistic + L2
madelon
dorothea
DP-SGD
Logistic + L1
california LeastSquares + L1 DP-GCD
LeastSquares + L1 DP-GCD
square
LeastSquares + L2 DP-GCD
mtp
DP-GCD
Logistic + L1
madelon
DP-GCD
Logistic + L2
log1
DP-GCD
Logistic + L2
log2
DP-GCD
Logistic + L2
madelon
DP-GCD
Logistic + L1
dorothea

5.0
0.01
3.0
0.1
10.0
1.0
10.0
3.0
20.0
0.01
20.0
10.0
20.0
20.0
20.0
0.001
4
2
7
1
10
20
10
2

2.02e+01
9.10e+03
2.02e+01
7.91e+00
1.84e-01
7.54e-01
1.21e+00
4.50e-02
1.26e+01
4.94e+00
1.26e+01
6.87e-03
1.84e-01
1.84e-01
1.84e-01
1.00e-04
5.18e+01
1.46e+04
2.02e+01
7.91e+00
3.09e+00
1.93e+00
7.91e+00
1.26e+01

1.00e+00
1.00e+01
2.15e-02
2.15e+00
1.00e+00
2.15e+00
1.00e-01
4.64e+00
2.15e-05
1.00e-04
2.15e-05
1.00e+00
4.64e-04
4.64e-04
1.00e-04
1.00e-06
1.00e+00
2.15e+00
4.64e-01
2.15e+00
2.15e+00
4.64e-01
1.00e+00
2.15e+00

Runtime. Finally, we report the runtime of DP-GCD, in comparison with DP-CD and DP-SGD in Figure 4, that is the
counterpart of Figure 1, except with runtime on the x-axis. These results conﬁrm the fact that DP-GCD can be efﬁcient,
although its iterations are expensive to compute. Indeed, in imbalanced problems, the small number of iterations of DP-
GCD enables it to run faster than DP-SGD, and in roughly the same time as DP-CD, while improving utility.

Paul Mangold, Aur´elien Bellet, Joseph Salmon, Marc Tommasi

(a) sparse
LASSO
(λ = 30)

(b) california
LASSO
(λ = 0.1)

(c) dorothea
Logistic + L1
(λ = 0.01)

(d) madelon
Logistic + L1
(λ = 0.05)

Figure 3: Relative error to non-private optimal for DP-CD, proximal DP-GCD (with GS-r, GS-s and GS-q rules) and
DP-SGD on different problems. On the x-axis, 1 tick represents a full access to the data: p iterations of DP-CD, n iterations
of DP-SGD and 1 iteration of DP-GCD. Number of iterations, clipping thresholds and step sizes are tuned simultaneously
for each algorithm. We report min/mean/max values over 5 runs.

(a) log1
Logistic + L2
(λ = 1e-3)

(b) log2
Logistic + L2
(λ = 1e-3)

(c) mtp
Least Squares + L2
(λ = 5e-8)

(d) madelon
Logistic + L2
(λ = 1)

(e) sparse
LASSO
(λ = 30)

(f) california
LASSO
(λ = 0.1)

(g) dorothea
Logistic + L1
(λ = 0.01)

(h) madelon
Logistic + L1
(λ = 0.05)

Figure 4: Relative error to non-private optimal for DP-CD, DP-GCD and DP-SGD on different problems, as a function of
running time. Number of iterations, clipping thresholds and step sizes are tuned simultaneously for each algorithm. We
report min/mean/max values over 5 runs.

RelativeErrortoNon-PrivateOpt.01020Passesondata3×10−14×10−16×10−101020Passesondata10−310−1024Passesondata5×10−16×10−17×10−18×10−1010Passesondata10−410−2DP-CDDP-SGDDP-GCDDP-GCD-GSsDP-GCD-GSrDP-GCD-GSqRelativeErrortoNon-PrivateOpt.0.000.050.10Time(s)5×10−16×10−17×10−18×10−10.000.050.10Time(s)3×10−14×10−16×10−10.000.250.500.75Time(s)10−16×10−27×10−28×10−29×10−20.00.51.01.5Time(s)4×10−25×10−2RelativeErrortoNon-PrivateOpt.0.0000.0050.0100.015Time(s)4×10−15×10−16×10−17×10−10.00.10.2Time(s)10−310−10.00.20.4Time(s)6×10−17×10−18×10−10.00.20.40.60.8Time(s)10−410−310−2DP-CDDP-SGDDP-GCD