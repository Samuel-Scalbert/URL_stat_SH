Revisiting Artistic Style Transfer for Data
Augmentation in A Real-Case Scenario
Stefano d’Angelo, Frédéric Precioso, Fabien Gandon

To cite this version:

Stefano d’Angelo, Frédéric Precioso, Fabien Gandon. Revisiting Artistic Style Transfer for Data
Augmentation in A Real-Case Scenario. IEEE ICIP 2022 - 29th IEEE International Conference on
Image Processing, Oct 2022, Bordeaux, France. pp.4178-4182, ￿10.1109/ICIP46576.2022.9897728￿.
￿hal-03921565￿

HAL Id: hal-03921565

https://hal.science/hal-03921565

Submitted on 17 Nov 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Authors' version 

4178

REVISITINGARTISTICSTYLETRANSFERFORDATAAUGMENTATIONINAREAL-CASESCENARIOStefanoD’Angelo⋆,Fr´ed´ericPrecioso⋆,FabienGandon†⋆Universit´eCˆoted’Azur,CNRS,Inria,I3S†Universit´eCˆoted’Azur,Inria,CNRS,I3SABSTRACTAtremendousnumberoftechniqueshavebeenproposedtotransferartisticstylefromoneimagetoanother.Inpar-ticular,techniquesexploitingneuralrepresentationofdata;fromConvolutionalNeuralNetworkstoGenerativeAdver-sarialNetworks.However,mostofthesetechniquesdonotaccuratelyaccountforthesemanticinformationrelatedtotheobjectspresentinbothimagesorrequireaconsiderabletrainingset.Inthispaper,weprovideadataaugmentationtechniquethatisasfaithfulaspossibletothestyleofthereferenceartist,whilerequiringasfewtrainingsamplesaspossible,asartworkscontainingthesamesemanticsofanartistareusuallyrare.Hence,thispaperaimstoimprovethestate-of-the-artbyfirstapplyingsemanticsegmentationonbothimagestothentransferthestylefromthepaintingtoaphotowhilepreservingcommonsemanticregions.ThemethodisexemplifiedonVanGogh’spaintings,showntobechallengingtosegment.IndexTerms—NeuralStyleTransfer,ImageSegmen-tation,Image-to-ImageTranslation,ArtisticStyleTransfer,Photorealism1.INTRODUCTIONWitnessingtheimpressiveresultsDeepLearningtechniqueshaveprovidedtoothersciences,moreandmorenon-AI-expertsareconsideringthesemethodsintheirownfield.MuseumcuratorsareprogressivelykeenonbenefitingfromthepotentialofDeepLearninginthetedioustaskofana-lyzingartworks,maintainingthesaurusanddatabases,andcombiningunstructuredcontentwithmetadatatobringanal-ysistoanotherlevel[1].Inordertosupportthelevelofrequirementfromverypreciseanalysisofartworksbycu-rators,theneedforafaithfuldataaugmentationmethodisimmediate.Indeed,ifonewantstoapplyadeepnetworktoanalyzeartworksemanticcontent,itisobviouslybettertogetanetworkbetterfittedtothedata[1],forinstancebyfine-tuninganexistingarchitectureonthetargetartworks.Apreviouswork[1]showedthatdatascarcityforsometypesofartpieceshinderedtransferlearning.StandardbaseCNNmodels(VGG,ResNet,...)werenotprovidinganaccurateclassification/detectionofobjectsandthefine-tuningwasnotsatisfactoryeither,duetothelackoftrainingdata.Thus,withoutdataaugmentation,transferlearningandfine-tuningcouldnotprovidetheexpectedsupporttotheworkofcu-rators-inparticulartheautomationofculturaldataqualitymaintenance.WhenVincentVanGoghpaintedStarryNight,hesawnoneotherthanabeautifullandscape,andhecapturedhisim-pressionofthesceneonthepaintingbyapplyinghisuniquestyle.Luckily,hisstyleisnotcompletelylost,sinceDeepLearninghaslatelyenabledonetobeabletocaptureartisticstyleandtotransferittoanewimage[2].However,mostofthecurrentmethodsrelyon(very)largetrainingsetstobuildarelevantneuralmodelforstyletransfer.Furthermore,thecon-sistencywiththeoriginalartstyleismostoftenglobalwhichleadstomanyvisualartefactsinthegeneratedartworkfromarealphoto.Thisarticlecontributesanewmethodtoenablese-manticstyletransferbyfirstapplyingsemanticsegmentationonbothimages,andthentransferringthestylefromthepaint-ingtoaphotowhilepreservingcommonsemanticregions.InSec.2,wepositionourworkwithrespecttothestate-of-the-art,whileinSec.3,wedetailourapproach.Theexper-imentalresultsarepresentedinSec.3,andwethenconcludethepaperinSec.5.2.NEURALSTYLETRANSFERNeuralStyleTransferisanareaofapplicationofImage-to-Imagetranslation.Thegoalistotransferthestyleofanimage,calledstyleimage,toanotherimage,calledcontentimage.Thisfieldhasbeenwidelyexplored,andmanymeth-odshavebeenproposed[2].Theseminalneuralmodelinthestate-of-the-arthasbeendevelopedbyGatysetal.[3],andconsistsof:(i)first,providingthecontentimageasinputofafixedpre-trainedVGG19network(onImageNet),thentotrainanotherVGG19networkfromscratchbyprovidingawhitenoiseimageasinputandaligningitsfeaturemapswiththoseoftheformernetwork;(ii)second,applyingalmostthesameproceduretothestyleimage,butaligningtheGramma-tricesoftheirfeaturemapsratherthanthefeaturemapsthem-selves.Afurtherimprovementwasproposedin[4]bycom-biningtheCNNarchitecturewithaMarkovRandomField,4179 

aregularizerthatmaintainslocalpatternsofthe“style”ex-emplars.Thesemodelsstillstruggletocorrectlytransferthestyle,oftenresultinginanoverlapbetweencontentandstyleimagesandthus,inmanylocalartefacts.Onebranchofstyletransferarchitecturesthathasmadesomeimprovementsistheoneinvolvingunpaireddatasetscontain-ingsamplesfromtwodifferentdomains(thesourcedomainandthetargetdomain).Thisisapointofadvantageoverpre-viousarchitectures,asitisalwaysatedioustasktopairthesamples.Inthisbranch,thepredominantmodelsareGener-ativeAdversarialNetworks(GANs),whosepropertieshavebeenexploitedtogenerate“fake”imagesmappingtogetherthetwodifferentdomains.ThisisthecaseofGANilla[5],anarchitectureconsistingofaCNNbasedonResnet18usedinanautoencoderstructuretolearnalatentspaceandskipcon-nectionsallowingtotranslatetheinputimagefromthesourcedomaintothetargetdomain.InCycleGAN[6]thenetworknotonlytranslatestheinputimagefromthesourcedomaintothetargetdomainbutalsotranslatestheresultingimagebacktothesourcedomain,enforcingthevisualconsistencybetweenthetwoimagesinthesourcedomain:theinitialim-ageandtheimageresultingfromtwoconsecutivetranslationsteps.However,theproblemwiththesemethodsisthattheyarenotsemantic-aware.Inpractice,theymakelittledistinctionbe-tweenobjectsinanimage.Moreover,theydonotexploitthespecificmappingthatexistsbetweenimagesofpaireddatasets.Severalothermodelshavetriedtoincludesemanticinfor-mationwhentransferringthestyle.Liaoetal.,forexample,enforcedmappingbetweensourceandtargetimagesatdiffer-entfeaturemaplevels[7].Theirresultsareconvincingwhenthesourceanddomaincontentsarealreadyfairlysemanti-callyaligned.Inamorerecentwork[8],softsemanticmasksofregionsthatshouldmatchbetweensourceandtarget,areextracted.Hereagain,theexistingalignmentbetweenseman-ticregionsthatshouldmatchimpactsthequalityoftheresultsgreatly.Oneofthemostimpressiverecentachievementsinthisfieldisbasedonadvancesindeepsemanticsegmentationwhichaimtoidentifytheclassesinbothsourceandtargetimages.Thetwotwosegmentationmasksarethensemanti-callyaligned,afterwhichtheyareintegratedintothetransfer[9].However,aswiththepreviousmodels,Parketal.’smodelisnotyetabletohandlepaintingsinwhichtheshapeoftheobjectsisveryfarfromreality.InVanGogh’spaintings,forexample,thelineisnotusedtodescriberealitybuthasanexpressivefunction-transfiguringrealityitself.3.PROPOSEDAPPROACHDifferenttoothermethods,theapproachpresentedhereusesapaireddataset,whichconsistsofpairsofimagessharingasimilarvisualcontent.Itsmainstrengthisbasedonthefactthatitcanexploittheone-to-onemappingbetweenim-agesofeachpairtoguaranteeresultsthataremorerelevanttothestylecontainedinaspecificpainting.ThestartingpointwashencethedatasetusedbyZhuetal.totrainCy-cleGAN[6],whereVanGogh’spaintingshavebeenretrievedfromWikiArt,whilerealphotoshavebeendownloadedfromFlickrbyusinglandscapes-relatedhashtags.Then,imagesweremanuallypairedinordertomatchthemasbestaspos-sible.Sinceallthepre-trainedsegmentationmodelshavebeentrainedonrealpictures,itishardtodirectlysegmentpaint-ings.Hence,apre-processingstepprecedestheactualtrain-ingphaseandconsistsoffirstconvertingpaintingstorealim-agesandthenextractingsegmentationmasksintherealimagedomainaccordingtotheapproachcoinedbyPenhou¨etetal.[10].Thelatterusesimagesegmentationandsemanticgroup-ingtomergeminorityclassesinorderforthemasksofeachpairofimagestomatch.Theresultingmasksarethensimplymappedbackontothepaintingimagetoprovideareliablese-manticsegmentationofboththepaintingandthetargetrealimage.Thispre-processingphaseisalsothemainnoveltybroughtbythispapertothecurrentstyletransferliterature,sinceitsolvesthemainissueofsemanticallysegmentingpaintings.Resultshavebeenthenevaluatedaccordingtohowsim-ilartheylooktoVanGogh’sartworks,or,inotherwords,how”fake”theyare.AllthecodeispubliclyavailableatthisGitHubrepository,includingtheoneforresultsevaluation.3.1.Pre-processingBeforeapplyingstyletransfer,apre-processingphasehasbeenstudiedandapplied.Sinceallthepre-trainedmodelsforimagesegmentationhavebeentrainedonrealpictures,wecannotexpectgoodresultsifthesegmentationisdirectlyappliedtopaintings.Hence,astrategytoovercomethisissueconsistsofconvertingpaintingsintorealphotos.Forthispurpose,aCycleGANhasbeentrainedontwosets:thesetofVanGogh’spaintings,consistingof400samples(setA),andthesetofphotographs(setB),containing6853images.Themodelhasbeentrainedwiththedefaultparametersusedintheoriginalpaper([6])foratotalof120epochs.Resultsandbenefitsofthispre-processingphasearediscussedinSec.4.CycleGANhasbeenchosenforthistaskbecauseiten-surescycleconsistency:whenwetranslatefromonedo-maintotheotherandbackagain,weshouldarrivewherewestarted.Itslossiscomposedoftwoterms:theadversariallossandcycleconsistencyloss.Thefirstisusedtoimprovethequalityoffakeimagesgeneratedfromonedomaintotheotherwhilethesecondloss,instead,incentivizesthecycleconsistency.ForfurtherdetailsonCycleGANsreferto[6].Onceallimageshavebeenconverted,asubsetof21VanGogh’spaintingshavebeenselectedandpairedwithrealpho-tographs.Then,alltheselectedimageshavebeensegmentedusingtheapproachofPenhou¨etetal.[10],whichiscomposed4180 

Fig.1.Visualsummaryofthepre-processingphase.oftwoparts:first,apre-trainedCNNcalledPyramidSceneParsingNetwork(PSPNet)createsasegmentationimage;sec-ondarily,aKnowledgeGraphfromaPythonlibrary,calledSematch,measuresthesimilaritybetweentwoclasswords(e.g.:skyandground).Thepurposeofthissecondpartistosemanticallygroupsimilarclassesintoawiderclass,sothatboththecontentandthestyleimagesharethesameclasses.Semanticgroupingisregulatedbyaparameterθ∈[0,1]calledsemanticthreshold,whichgroupssimilarclassesto-gether,therebyallowingboththecontentandstylemaskstohavethesamenumberofclasses.Whenθ=1nogroupingisapplied,whilewithθ=0alltheclassesaremergedintoasingleclass.Here,θ=0.6asisintheoriginalpaper([10]),whichshowshowqualityofsegmentationmasksvarieswhenθdeviatesfromthisvalue.Fig.1illustrateswhatthepre-processingphaselookslikeforasinglepairofimages.Notethatinthispaper,whensegmentationmasksaremen-tioned,wearereferringtothesemanticallygroupedmasks.3.2.StyleTransferThearchitecturepresentedhereisbasedonbothPenhou¨etetal.’sAutomatedDeepPhotoStyleTransfer(ADPST)model[10],andtheclassicalNeuralStyleTransfer(NST)model[3].TheformerwasintroducedinSub-Sec.3.1,whilethelatterinSec.2.Inbrief,NSTextractsfeaturesfromaVGG19forbothstyleandcontentimages,thenjointlyminimizestheirlossesLcandLs:Ltotal(−→p,−→a,−→x)=αLc(−→p,−→x)+βLs(−→a,−→x),(1)whereαandβaretheweightingfactorsforcontentandstylereconstruction,while−→p,−→a,and−→xarethephotograph,theartwork,andthegeneratedimage,respectively[3].ADPSTmodelisbasedonDeepPhotoStyleTransfer([11]),withthedifferencethatinADPSTsegmentationmasksarecreatedautomatically.Theobjectiveistominimizethefollowingloss:L=L(cid:88)l=1αlLlc+ΓL(cid:88)l=1βlLls+λLm+ηLa,(2)whereLc,Ls,Lm,LaarethecontentlossofNST(Eq.1),theaugmentedstyleloss,theaffineloss,andtheimageassess-mentloss,respectively,whichareallregulatedbydifferentparameters.Fig.2.Schemaofthearchitecture.Thelossisminimizedforacertainnumberofiterations,which,accordingtoPenhou¨etetal.,shouldbeatleast1000.Theauthorsalsoobservedthatgoodresultsareachievedwithabout2000iterationsandimprovementsafter4000iterationsaremostofthetimenegligible;therefore,thenumberofiter-ationswasheresetto3000.Asmentionedabove,theworkflowpresentedinthisSectioncombinesbothADPSTandNSTmodels,andisshowninFig.2.Eachpairofcontentandpre-processedstyleimagesisfedintotheADPSTarchitecture,whichtransfersthestyleinthephotorealisticdomain.Inthisway,weareabletoexploitthesegmentationmasksofboththeimages,mainlytransfer-ringthepaletteandthesemanticcontentofthepainting.However,westillneedtomaptheresultingimagetothepaint-ingsdomain.Toaccomplishthistask,weresortedtotheNSTarchitectureduetoitslossfunctionwhichisthefoundationofADPST’sloss,asevidentfromEquations(1)and(2).Hence,thefinalpainting-likefakeVanGoghistheresultofastyletransferfromtheoriginal(notpre-processed)paintingtothephotorealisticfakeVanGoghobtainedfromADPST.4.EXPERIMENTALRESULTSThreebaselineswerechosentocomparetheoutcomesofthearchitecturepresentedinthispaperwiththestate-of-the-art:(i)theclassicalNSTmodel[3],(ii)CNNMRFarchitecture[4]duetoitbeingsimilartoNST,butratherfocusingondif-ferentpatchesofimages,and(iii)CycleGAN[6],whichin-volvestheusageofanunpaireddataset.InFig.3theresultsofourmodelare,atfirstglance,thosethatbalancecontentandstylebetter.Acloserlookshowsthatstyleismorecorrectlytransferredbetweentwosemanticallysimilarareasoftheimages.Concerningthethreebaselines,4181

Fig.3.Someoftheresultscomparedtothreestate-of-the-artmodels.withavisualassessmentwecaneasilyobservethatresultsofCNNMRFaretooclosetothecontentimage,inthesensethatthestyleisingeneralpoorlytransferred.CycleGANgen-eratesthemostdifferingimagescomparedtoalltheothers,becauseittakeselementsfromallphotosinthedataset.In-deed,duetothenatureofGANs,thestyleisnotcapturedbyaprecisecorrelationbetweencontentandstyleimage,butisratherdispersedthroughouteachimage.ResultsofNSTare,instead,morevisuallyappealingthanintheothermodels,whichisespeciallyimpressivebecauseitistheoldestmodel.Still,theimagesitgeneratesaretoosimplistic.Infact,sincetheobjectiveofthismodelistojointlyminimizeonlythecon-tentandstylelosses,thefinalimagetendstobetheresultofatrade-offbetweencontentandstyle.Anumericalevaluationoftheresultsfurtherconfirmedtheeffectivenessofourwork.Indeed,wefine-tunedtheclassifierofapre-trainedResnet18topredictwhetherapaintingwasrealorgenerated,andourmodelyieldedthehighesterror,thusfoolingtheCNNmorethantheotherthreearchitectures.Aspreviouslymentioned,thenoveltythispaperwantstobringtocurrentliteratureismostlyinherenttothepre-processingphase,wheretheconversionofpaintingstorealphotographsallowsonetohaveabettermappingofsemanticinformationbetweeneachpairofimages.Theintermediateresultstobeinterpretedarederivedfromthepre-processingFig.4.Benefitsofpaintingtophotoconversion.phaseitself.Theadvantagesofthisareeasilyseeninthequalityofthesemanticmasksgeneratedbythestyleimages,whichdependsonboththequalityofthephotorealisticcon-versionandthesemanticgroupingapplied.InFig.4,thestylemaskextractedfromthephotorealisticpaintingisclearlyclosertotheactualcontentofthepaintingitself.Indeed,inthemaskgenerateddirectlyfromtheoriginalartworktheskyissegmentedinmultipleinstances.Furthermore,thegroundisconfusedwiththegreenery,aswecanseefromthemixtureofpurpleandgreeninthestylemask.Thisisnotthecasewhenthemaskisinsteadgeneratedfromtheconvertedstyleimage,whoseresultismorepertinenttothecontentofthepainting.5.DISCUSSIONANDCONCLUSIONInthispaper,wepresentedanewwayofgeneratingfaux-realisticpaintingsofanartist,movingtowardsapreciseaug-mentationofdata.Thiscouldbeusedtopre-trainamorefaithfulCNNandthenuseittosupporttheworkofmuseumcuratorsintheclassificationandanalysisofartworks.Thesimplenovelideaweproposehereistofirstconvertthestyleimageintoarealimagesuchthatamoreprecisese-manticsegmentationcanbeextracted.Mappingtheresultingsemanticregionsbackontothestyleimageisthenstraight-forward.Thankstothisprecisesemanticsegmentationofthestyleimage,wecanbettermapsemanticsbetweenthepaint-ingandrealphoto,leadingtoahighlyimprovedstyletransfer.Toimprovetheresults,afutureworkmightfocusonim-provingthequalityofsemanticmasks.Indeed,thesegmen-tationinADPSTisoptimizedfor“SceneParsing”,obtainingthebestresultsforlandscapepaintings.Therefore,afirstpos-siblesolutionistoincludephotoscontainingcommonobjectsinourdataset.Bydoingso,CycleGANwillbeawareofhowtopre-processthosekindsofimages,andtheresultsmayberefined.WecouldalsoconsidertuningtheparametersoftheADPSTmodel.AfinaloptionwouldbetouseourmodelasageneratorinaGANarchitecture,fine-tuningthegenerationofimagesaccordingtotheperformanceofthediscriminator.4182

References[1]A.Bobasheva,F.Gandon,andF.Precioso,“LearningandReasoningforCulturalMetadataQuality,”JournalonComputingandCulturalHeritage(ACMJOCCH),2022.[Online].Available:https://hal.archives-ouvertes.fr/hal-03363442[2]Y.Jing,Y.Yang,Z.Feng,J.Ye,Y.Yu,andM.Song,“Neuralstyletransfer:Areview,”IEEETransactionsonVisualization&ComputerGraphics,vol.26,no.11,pp.3365–3385,nov2020.[3]L.A.Gatys,A.S.Ecker,andM.Bethge,“Aneuralalgorithmofartisticstyle,”arXivpreprintarXiv:1508.06576,2015.[4]C.LiandM.Wand,“CombiningMarkovRandomFieldsandConvolutionalNeuralNetworksforImageSynthesis,”in2016IEEEConferenceonComputerVi-sionandPatternRecognition(CVPR).LasVegas,NV,USA:IEEE,Jun.2016,pp.2479–2486.[5]S.Hicsonmez,N.Samet,E.Akbas,andP.Duygulu,“Ganilla:Generativeadversarialnetworksforimagetoillustrationtranslation,”ImageandVisionComputing,vol.95,p.103886,2020.[6]J.-Y.Zhu,T.Park,P.Isola,andA.A.Efros,“Unpairedimage-to-imagetranslationusingcycle-consistentad-versarialnetworks,”in2017IEEEInternationalConfer-enceonComputerVision(ICCV),2017,pp.2242–2251.[7]J.Liao,Y.Yao,L.Yuan,G.Hua,andS.B.Kang,“Visualattributetransferthroughdeepimageanalogy,”ACMTransactionsonGraphics(TOG),vol.36,no.4,pp.1–15,2017.[8]H.-H.Zhao,P.L.Rosin,Y.-K.Lai,andY.-N.Wang,“Automaticsemanticstyletransferusingdeepconvolu-tionalneuralnetworksandsoftmasks,”TheVisualCom-puter,vol.36,no.7,pp.1307–1324,2020.[9]J.H.Parketal.,“Semantic-awareneuralstyletransfer,”ImageandVisionComputing,vol.87,pp.13–23,2019.[Online].Available:https://www.sciencedirect.com/science/article/pii/S0262885619300435[10]S.Penhou¨etetal.,“Automateddeepphotostyletransfer,”CoRR,vol.abs/1901.03915,2019.[Online].Available:http://arxiv.org/abs/1901.03915[11]F.Luan,S.Paris,E.Shechtman,andK.Bala,“DeepPhotoStyleTransfer,”in2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR).Honolulu,HI:IEEE,Jul.2017,pp.6997–7005.