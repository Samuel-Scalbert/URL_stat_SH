Persistence-Based Discretization for Learning Discrete
Event Systems from Time Series
Lénaïg Cornanguer, Christine Largouët, Laurence Rozé, Alexandre Termier

To cite this version:

Lénaïg Cornanguer, Christine Largouët, Laurence Rozé, Alexandre Termier. Persistence-Based Dis-
cretization for Learning Discrete Event Systems from Time Series. MLmDS 2023 - AAAI Workshop
When Machine Learning meets Dynamical Systems: Theory and Applications, Feb 2023, Washington
(DC), United States. pp.1-6. ￿hal-03934438v2￿

HAL Id: hal-03934438

https://inria.hal.science/hal-03934438v2

Submitted on 14 Jun 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Persistence-Based Discretization for Learning Discrete Event Systems from
Time Series

L´ena¨ıg Cornanguer1, Christine Largou¨et2, Laurence Roz´e3, Alexandre Termier4
1Inria, Univ Rennes, CNRS, IRISA
2Institut Agro, Univ Rennes, Inria, CNRS, IRISA
3Univ Rennes, INSA Rennes, CNRS, Inria, IRISA
4Univ Rennes, Inria, CNRS, IRISA
lenaig.cornanguer@irisa.fr

Abstract

To get a good understanding of a dynamical system, it is con-
venient to have an interpretable and versatile model of it.
Timed discrete event systems are a kind of model that respond
to these requirements. However, such models can be inferred
from timestamped event sequences but not directly from nu-
merical data. To solve this problem, a discretization step must
be done to identify events or symbols in the time series. Per-
sist is a discretization method that intends to create persisting
symbols by using a score called persistence score. This allows
to mitigate the risk of undesirable symbol changes that would
lead to a too complex model. After the study of the persis-
tence score, we point out that it tends to favor excessive cases
making it miss interesting persisting symbols. To correct this
behavior, we replace the metric used in the persistence score,
the Kullback-Leibler divergence, with the Wasserstein dis-
tance. Experiments show that the improved persistence score
enhances Persist’s ability to capture the information of the
original time series and that it makes it better suited for dis-
crete event systems learning.

Introduction
With the ever-growing type and variety of available sen-
sors, more and more systems can be monitored in real-time.
This monitoring results in the collection of multiple time
series, i.e. sequences of numerical values captured by the
sensors. For example, in a smart city, streets can be moni-
tored through pedestrian counting as well as pluviometers,
to study the effect of weather conditions and the time of the
day on the number of people walking.

The classical use of the sensor time series is to feed them
to machine learning models for tasks such as classiﬁca-
tion or anomaly detection. However, one may also be in-
terested to understand better the dynamical system being
monitored by the sensors. For such goal, an interpretable
model is required. A good solution is to produce a model in
the form of a timed Discrete Event System from the sensor
data. A ﬁrst difﬁculty is that there are globally very few ap-
proaches learning timed Discrete Event Systems from data.
The learning of one of these Discrete Event Systems for-
malisms, Timed Automata (TA), has been well studied with
algorithms like RTI+ and TAG (Verwer, de Weerdt, and Wit-
teveen 2010; Cornanguer et al. 2022). However these algo-
rithms take as input sequences of timestamped events, and
not time series.

In order to use these approaches with sensor data, a solu-
tion is to discretize the time series before using the automata
learning algorithm. There is a large literature on time series
discretization. However, the proposed discretization meth-
ods are not designed for automata learning: they may exhibit
too frequent consecutive changes of symbols, which would
lead to needlessly large and complex automata. With the idea
of minimizing the number of symbol changes, M¨orchen at
Ultsch (M¨orchen and Ultsch 2005) proposed the Persist ap-
proach. Persist is based on the assumption that the time se-
ries reﬂect the dynamics of an underlying system composed
of recurring persisting states and aims at recovering these
states in the form of a sequence of symbols issued from the
time series discretization.

While the idea of Persist is great, using it in practice
for automata learning reveals that Persist may often “go
too far” by focusing on extreme values leading to a dis-
cretized sequence with hardly any symbol changes. In this
paper, through a thorough analysis of the decision criterion
of Persist, the persistence score, we identify the source of
this behavior leading to a more balanced distribution of the
symbols. Our experiments on numerous real and synthetic
datasets demonstrate that the improved persistence score al-
lows to better capture the information of the original time se-
ries, and can help in producing interpretable timed automata.

Motivation and State-of-the-Art
We consider the problem of converting a numerical time se-
ries, a sequence of values measured at regular time inter-
vals (Lin et al. 2003), into a symbolic representation, a se-
quence of symbols from a ﬁnite alphabet. Given a time se-
ries, x = {xi|xi ∈ R, i = 1, ..., n}, the purpose is to pro-
vide its discretized version y = {yi|yi ∈ Σ, i = 1, ..., n}
where each symbol yi is an element of a ﬁnite alphabet
Σ. Symbolic representation is an efﬁcient way to deal with
the inherent dimensionality of time series so that they can
be used in a low-dimensional space with data-mining and
machine-learning algorithms. To address this problem, many
approaches have been proposed. The simplest discretization
methods are equal-width (EW) and equal-frequency (EF) in-
terval binning which subdivide continuous ranges into inter-
vals through user speciﬁcation of width or frequency. SAX
(Symbolic Aggregate approXimation) (Lin et al. 2003) is an-
other simple and widely implemented method based on the

piecewise aggregate approximation (PAA) technique. The
time series is divided into equal-size time intervals for which
only the mean value is kept. Given the assumption that time
series follow a normal distribution, the Gaussian curve is
then divided through breakpoints producing equiprobable
symbols. SAX requires two parameters, the number of time
intervals and the number of symbols. SAX suffers from lim-
itations such as the normal distribution assumption and user
parameters which impact the quality of the results. Several
variants have been proposed to attempt to overcome these
limitations. Some other approaches like ABBA and fABBA
(Chen and G¨uttel 2022) have been developed to capture the
shape and the trend of the time series. These methods are
motivated by different purposes and appear to be best suited
for applications dedicated to the analysis of time series such
as trend prediction or anomaly detection. Yet, no discretiza-
tion method has been speciﬁcally proposed with the end goal
of analyzing or learning behaviors of dynamical systems.
However, M¨orchen and Ultsch (M¨orchen and Ultsch 2005)
have proposed a discretization algorithm, Persist, based on
an interesting property over time series, called persistence.
Persist attempts to produce discretized time series with per-
sisting symbols, which would be an advantage for the learn-
ing of a timed discrete event system. Our motivation in this
paper is to improve Persist to obtain a more accurate sym-
bolic representation suited for the description of dynamical
systems.

Persist
Persist (M¨orchen and Ultsch 2005) is a method for unsu-
pervised discretization of univariate time series proposed
by M¨orchen and Ultsch. It was employed as preprocess-
ing step to ﬁnd patterns in time series in a language called
Time Series Knowledge Representation (TSKR) (M¨orchen
and Ultsch 2007). Persist is based on the assumption that
the time series are the reﬂection of an underlying process
that consists of recurring persisting states and it aims to re-
store these states in the form of symbols in a discretized ver-
sion of the time series. M¨orchen and Ultsch state that “if
there is no temporal structure in the time series, the symbols
[in its discretized version] can be interpreted as independent
observations of a random variable according to the marginal
distribution of symbols”. Thus, the idea is to look for states
showing a persisting behavior by creating symbols whose
probability of repetition will be much higher than their prob-
ability of appearance.

To create these symbols, Persist produces a set of break-
points creating intervals in the value space. Each interval is
associated with a symbol s that will replace the numerical
values falling within it in the discretized time series.

The breakpoints are iteratively chosen in a set of candi-
date breakpoints (candidates in Algorithm 1) according to
a score called persistence score (described in the next sec-
tion). The set of candidates is initialized by an equal fre-
quency binning (with a number of bins ﬁxed to 100 by de-
fault). At each iteration, the function best bp individu-
ally tests every candidate breakpoint added to the already
selected breakpoints (bps). The candidate increasing persis-
tence score the most is returned with its score. Persist stops

Figure 1: Symmetric KL divergence between two probabil-
ity distributions with two possible outcomes P = (p, 1 − p)
and Q = (q, 1 − q).

when no more candidate breakpoint increases the persis-
tence score, ﬁnding thereby automatically an adequate num-
ber of symbols.

Algorithm 1: Persist

Require: univariate time series ts
Return: a set of breakpoints bps

bps = ∅
candidates = equal frequency binning(ts, 100)
score = 0
new score = 0
(new bp, new score) = best bp(ts, bps, candidates)
while new score < score do

score = new score
bps = bps ∪ new bp
candidates = candidates − new bp
(new bp, new score) = best bp(ts, bps, candidates)

end while
return bps

Persistence score
The persistence score measures the persisting behavior of
the symbols created by the discretization. It is based on
the Kullback-Leibler (KL) divergence. The KL divergence
(Kullback and Leibler 1951) measures how a probability dis-
tribution P is different from another probability distribution
Q. For discrete probability distributions deﬁned on X , the
KL divergence is deﬁned as follows:

DKL(P ||Q) =

(cid:88)

P (x) log(

P (x)
Q(x)

)

(cid:54)=
This divergence is not
DKL(Q||P )). This is why M¨orchen and Ultsch use a sym-
metric version obtained as follows:

x∈X
symmetric (DKL(P ||Q)

SKL(P, Q) =

1
2

(DKL(P ||Q) + DKL(Q||P ))

p0.00.20.40.60.81.0q0.00.20.40.60.81.0Divergence0.00.51.01.52.02.53.03.54.0Figure 2: Wasserstein distance between two probability dis-
tributions with two possible outcomes P = (p, 1 − p) and
Q = (q, 1 − q).

s
s1
s2

P(s)
0. 97
0.03

Pr(s)
0.99
0.62

s
s1
s2

P(s)
0.54
0.47

Pr(s)
0.92
0.94

(a) Breakpoint 1

(b) Breakpoint 2

Table 1: Two candidate breakpoints, each creating two sym-
bols (s1 and s2). The KL divergence will give a better score
to breakpoint 1 while the Wasserstein distance will give a
better score to breakpoint 2.

In Persist, the probability distributions P and Q are based
on the probability of appearance of the symbols (P (s)) and
their probability of repetition (Pr(s)): P = (P (s), 1−P (s))
and Q = (Pr(s), 1 − Pr(s)). The persistence score is com-
puted as follows:

Persistence(s) = sgn(Pr(s) − P (s))SKL(P, Q)

The ﬁrst element of the equation (sgn(Pr(s)−P (s))) allows
to favor only the cases when the probability of repetition is
superior to the probability of appearance, otherwise, it con-
tributes negatively to the persistence score.

The symmetric KL divergence between two probability
distributions with two possible outcomes as in our case is
represented in Figure 1. One of the properties of the KL
divergence is that it has no upper bound, a property inher-
ited by the persistence score. The shape of the surface pro-
duced by this divergence is also particular. The symmetric
KL divergence is null when the probability distributions are
equal and increases non-linearly as the difference between
the distribution grows. To achieve a high value of symmet-
ric KL, p or q (i. e. Pr(s) or P (s)) have to be close to 0
or 1. The direct consequence of these observations is that
the persistence score based on the KL divergence will fo-
cus on extreme cases. Table 1 and Figure 3 illustrate this
phenomenon. In this example, at the beginning of the al-
gorithm, the ﬁrst breakpoint will be selected to create two
symbols. Two candidate breakpoints are examined. The ﬁrst

Figure 3: Two candidate breakpoints, each creating two
symbols (s1 and s2).

breakpoint (Table 1a) will create a ﬁrst symbol that covers
almost the entire discretized time series and thus has a prob-
ability of appearance and repetition close to 1, and a sec-
ond symbol that almost never appears and doesn’t show a
particularly recurring behavior. The second breakpoint (Ta-
ble 1b) will create two symbols about equally probable and
with very high probabilities of repetition (greater than 0.90).
Persist based on the KL divergence will choose breakpoint
1. The discretized version of the time series in Figure 3 will
consist of the succession of about 90 “s1”, then a few “s2”
and again “s1” until the end, while it would have consisted
of an alternation of persistent “s1” and “s2” if breakpoint 2
had been chosen.

Improving Persist
The Wasserstein distance (Kantorovich 1939), also called
Kantorovitch distance, Kantorovitch - Rubinstein distance,
or earth mover’s distance, is another measure of difference
between probability distributions. It corresponds to the mini-
mal cost to transform a distribution P in another distribution
Q in the same space. The Wasserstein p-distance between
two probability distributions P and Q is deﬁned by the fol-
lowing equation where Γ(P, Q) are all the possible joint dis-
tributions for (X, Y ) with marginal probability distributions
P and Q:

Wp(P, Q) = inf

(E(x,y)∼γd(x, y)p)1/p

γ∈Γ(P,Q)

In the case of discrete probability distributions with only
two possible outcomes, the Wasserstein distance becomes a
simple subtraction and is deﬁned as follows:

W (P, Q) = |P (x1) − Q(y1)|
This distance is symmetric, bounded, easier to compute
than the KL divergence, and it increases linearly as the dif-
ference between the distributions grows (Figure 2). There-
fore, we use the Wasserstein distance to measure how the
probability of appearance of the symbols and their probabil-
ity of repetition are different in the score of persistence in
place of the KL divergence:

PersistenceW (s) = sgn(Pr(s) − P (s))W(P, Q)
In front of the choice presented in table 1, the persistence
score computed with the KL divergence will be higher for

p0.00.20.40.60.81.0q0.00.20.40.60.81.0Distance0.00.20.40.60.802040608010012002505007501000125015001750Breakpoint 1Breakpoint 2only). For each dataset, Persist has produced a set of break-
points from the train subset, used for the discretization. We
trained a Random Forest classiﬁer with 100 trees with the
discretized train subset. The classiﬁcation was then per-
formed on the discretized test subset. We measured the clas-
siﬁcation performance using the accuracy, i.e. the rate of
good classiﬁcation. We tested Persist using either the KL
divergence or the Wasserstein distance, and either an equal
frequency binning or an equal width binning for the can-
didate breakpoints initialization. We also performed the ex-
periment using SAX to have a performance reference. Un-
like Persist, a number of symbol must be given for SAX. We
used a number of symbols ranging from 2 to 10 and a time
interval width of 2 and we report all these results.

We are interested in Persist to obtain a discrete event
model of the dynamical system at the origin of the time se-
ries. Hence, to evaluate its improvement, we need to mea-
sure how good the models are. As discrete event model,
we choose timed automata (Alur and Dill 1994) which is
a common and well-studied formalism for dynamical sys-
tems. Timed automata (TA) are used to model systems in
which time inﬂuences the behavior. A timed automaton de-
ﬁnes states connected by transitions and the transitions from
one state to another are conditioned by events and timing
constraints. The model can then be used for many purposes
such as to check properties of the system (e.g. safety prop-
erties), or to perform anomaly detection in new data. The
modeling of a system by a timed automaton can be real-
ized thanks to expert knowledge or automatically from ex-
ecution data of the system. If the execution data takes the
form of logs, a learning process can be applied to produce
a timed automaton where the transitions labels correspond
to the events present in the logs. However, if the execu-
tion takes the form of time series (e.g. sensor data), a pre-
processing step is needed to identify events in the numerical
data, the discretization. Figure 4 illustrates the experimen-
tal setup. As in the ﬁrst experiment, Persist and SAX were
used to obtain the discretized time series. However, instead
of using the discretized data to train a classiﬁer, it was here
used to learn one discrete event model per class. For each
class, the corresponding discretized train time series were
given to a timed automata learner called TAG (Cornanguer
et al. 2022), which produced a timed automaton accepting
all the input sequences (i.e. there exists a path in the au-
tomaton for the sequence). Then the discretized test time se-
ries were injected in the timed automata. Each automaton re-
ceived the discretized test time series of its class and as many
discretized test time series of other classes. An automaton
should accept the data of its own class and reject the oth-
ers. The accuracy corresponds to the good acceptance rate
for the automaton. The Time Series Classiﬁcation repository
gathers time series of various types (motion, sensor, trafﬁc,
image, spectrographs...). The image type, as spectrographs,
differs from the others as it consists of shapes converted into
pseudo time series. As it doesn’t belong to the problem of
modeling dynamical systems, these datasets were excluded
from the experiment.

Figure 4: Time series classiﬁcation using timed automata
learned after a discretization step.

breakpoint 1 while the persistence score computed with the
Wasserstein distance will be higher for breakpoint 2. The
Wasserstein distance leads here to a discretized time series
with more persisting symbols, better respecting the initial
intuition of the persistence score.

Finally, the initialization of the candidate breakpoints
based on an equal frequency (EF) binning allows to have
more possible breakpoints in high-density regions. However,
some time series such as electrocardiograms have a structure
that could be missed with this kind of binning. In such cases,
an equal-width (EW) binning can be preferable. It is then im-
portant to let the user choose in function of the structure of
its data.

We re-implemented Persist (originally coded for MAT-
LAB) in Python with the possibility to choose between the
KL divergence and the Wasserstein distance, and between
an equal-frequency or equal-width binning. It is available
online1.

Experiments
The experiment is in two parts. First, we want to evaluate
the raw discretization quality provided by Persist. Then, we
look at its qualities to discretize time series for dynamical
systems modeling in the form of discrete event models.

Experimental setups
We ﬁrst want to measure the information retained in the data
after the discretization. To allow a quantitative evaluation,
we choose to evaluate the discretization through a classiﬁca-
tion task. If a good part of information is retained in the dis-
cretized time series, the classiﬁcation performance should
be high. We performed this evaluation on 111 datasets of the
Time Series Classiﬁcation Repository2 (univariate datasets

1Link to the repository of Persist re-implementation in Python:

https://gitlab.inria.fr/lcornang/persist discretization

2Anthony Bagnall, Jason Lines, William Vickers and Eamonn
Keogh, The UEA & UCR Time Series Classiﬁcation Repository,
www.timeseriesclassiﬁcation.com

TATATATrainTestDiscretization(Persist)DiscretizationTAlearnerAcceptedNot acceptedtime series(ℝ)discretizedtime series(Σ)One colorper classFigure 5: Classiﬁcation accuracy with random forest for the
different discretization strategies. The diamond indicates the
mean value. EF: equal-frequency, EW: equal-width.

Figure 6: Classiﬁcation accuracy with timed automata for
the different discretization strategies.

Results

We ﬁrst present the results for the classiﬁcation task using
Random Forest. Figure 5 displays the accuracy achieved ac-
cording to the discretization method. The classiﬁcation per-
formance is increased by the replacement of the Kullback-
Leibler divergence by the Wasserstein distance. The initial-
ization of the candidate breakpoints by an equal frequency
binning leads generally to a better performance, however,
it depends on the dataset which conﬁrms our hypothesis.
Thanks to the Wasserstein distance, using Persist globally
leads to better results than using SAX in this setup. This in-
dicates that Persist using the Wasserstein distance allows a
good information retention in the discretized data.

We now move on to the results of the second experiment.
Using automata to perform a classiﬁcation task is unusual
and not optimal. Indeed, each automaton is meant to rep-

(a) Weekdays.

(b) Weekends.

Figure 7: Instances of time series from the Chinatown
dataset and breakpoints selected by Persist (on the whole
train set) with the Wasserstein distance and an equal-width
binning.

Discretization method
Persist (KL, EF)
Persist (Wasserstein, EF)
Persist (KL, EW)
Persist (Wasserstein, EW)
SAX

Accuracy
0.686
0.687
0.780
0.819
0.652

Table 2: Classiﬁcation accuracy with timed automata for the
Chinatown dataset.

resent a normal global behavior. There is no emphasis for
the modeling on what makes the data of the different classes
singular. For this reason, we cannot expect as good perfor-
mances as while using a real classiﬁer. Nevertheless, it is in-
teresting to compare the classiﬁcation performance accord-
ing to the discretization method. If the discretization method
is pertinent for discrete event modeling, a good part of the
information contained in the time series would be retained in
the models and therefore leading to good classiﬁcation per-
formance. Figure 6 displays the classiﬁcation accuracy using
timed automata. When using SAX, the classiﬁcation perfor-
mance suffers the most from the use of timed automata. Per-
sist, in particular while using the Wasserstein distance and
an equal-width binning, preserves a better classiﬁcation ac-
curacy. This conﬁrms the interest in using an improved ver-
sion of Persist to preprocess time series in order to obtain a
discrete event model of a dynamical system.

To provide an insight into what can be obtained with this

PersistKLEFPersistKLEWPersistWassersteinEFPersistWassersteinEWSAX0.00.20.40.60.81.0Accuracy0.6680.5420.7170.7090.691PersistKLEFPersistKLEWPersistWassersteinEFPersistWassersteinEWSAX0.00.20.40.60.81.0Accuracy0.6140.6020.6070.6450.59805101520Hour0250500750100012501500Pedestrian countBreakpoint05101520Hour0250500750100012501500Pedestrian countBreakpointextreme cases. We also suggested a different initialization
strategy for the algorithm. Our experiments based on a clas-
siﬁcation task have shown that the metric substitution en-
ables a better information retention in the discretized time
series, and that Persist is better suited than the state-of-the-
art symbolic data representation SAX when the purpose of
the discretization is to learn a model formalized as discrete
event systems. Classiﬁcation is not the most common use
of discrete event systems and future work will thus focus
on applications for which discrete event systems are usually
used such as anomaly detection or model-checking. Find-
ing a criterion to determine automatically the best binning
for the data would also be convenient. Another perspective
could be to associate the persistence score with other quality
scores such as the reconstruction error.

References
Alur, R.; and Dill, D. L. 1994. A theory of timed automata.
Theoretical Computer Science, 126(2): 183–235.
Chen, X.; and G¨uttel, S. 2022. An Efﬁcient Aggregation
Method for the Symbolic Representation of Temporal Data.
ACM Trans. Knowl. Discov. Data.
Cornanguer, L.; Largou¨et, C.; Roz´e, L.; and Termier, A.
2022. TAG: Learning Timed Automata from Logs. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence,
36(4): 3949–3958.
Kantorovich, L. V. 1939. Mathematical Methods of Orga-
nizing and Planning Production. Management Science, 6(4):
366–422.
Kullback, S.; and Leibler, R. A. 1951. On Information and
Sufﬁciency. The Annals of Mathematical Statistics, 22(1):
79–86.
Lin, J.; Keogh, E.; Lonardi, S.; and Chiu, B. 2003. A sym-
bolic representation of time series, with implications for
streaming algorithms. In Proceedings of the 8th SIGMOD
DMKD workshop, 2. ACM Press.
M¨orchen, F.; and Ultsch, A. 2005. Optimizing time series
In Grossman, R.;
discretization for knowledge discovery.
Bayardo, R. J.; and Bennett, K. P., eds., Proceedings of the
Eleventh ACM SIGKDD Conference, USA, 660–665. ACM.
M¨orchen, F.; and Ultsch, A. 2007. Efﬁcient mining of un-
derstandable patterns from multivariate interval time series.
Data Mining and Knowledge Discovery, 15(2): 181–215.
Verwer, S.; de Weerdt, M.; and Witteveen, C. 2010. A
likelihood-ratio test for identifying probabilistic determin-
istic real-time automata from positive data. In International
Colloquium on Grammatical Inference, 203–216. Springer.

Figure 8: Discrete event model learned for each class of the
Chinatown dataset.

method, we show the discretization and the discrete event
models obtained for one dataset (Chinatown dataset). It con-
sists of the pedestrian trafﬁc along the day in a street of Mel-
bourne. The goal is to classify the days between weekend
and weekday. Figure 7 shows instances of time series from
this dataset. The best accuracy using timed automata for the
classiﬁcation was obtained using the breakpoints from Per-
sist with the Wasserstein distance and an equal-width bin-
ning (accuracy results in Table 2). These breakpoints are
shown in Figure 7 and the intervals they create can be associ-
ated with a symbol (very low to very high). Figure 8 displays
the timed discrete event models obtained with the timed au-
tomata learner for each class. A circle represents a state and
the transitions from one state to another are labeled with a
symbol, an interval of accepted delay since the last event,
and a probability. One can note that the activity in the street
in generally higher during the night (until 3 or 4 a.m.) on
weekends than on weekdays. The street also shows a more
pronounced afﬂuence during the weekend than in the week-
days afternoons. On weekdays, the end of the day is either
calm, or more animated than during weekend days (with a
lower probability, so probably one speciﬁc day of the week).

Conclusion
This work studies Persist, a state-of-the-art discretization al-
gorithm originally conceived as pre-processing step for pat-
tern mining in time series. Persist is based on the notion of
persistence which is interesting for the modeling of dynam-
ical systems in the form of discrete event models. We re-
placed the metric used to compute the score of persistence,
originally the Kullback-Leibler divergence, with the Wasser-
stein distance, to avoid an undesirable over-emphasis on the

Weekendlow[0, 0]Weekdaylow[0, 0]p=0.4very low[0, 0]p=0.6very low[3, 4]low[4, 6]high[1, 9]very high[1, 7]p=0.58low[8, 11]p=0.42verylow[1, 2]low[6, 8]high[3, 5]low[9, 11]p=0.8very high[6, 7]p=0.2high[2, 3]