Enumerating Regular Languages with Bounded Delay
Antoine Amarilli, Mikaël Monet

To cite this version:

Antoine Amarilli, Mikaël Monet. Enumerating Regular Languages with Bounded Delay. STACS, Mar
2023, Hamburg, Germany. ￿10.4230/LIPIcs.STACS.2023.8￿. ￿hal-03940590￿

HAL Id: hal-03940590

https://hal.science/hal-03940590

Submitted on 16 Jan 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Enumerating Regular Languages with Bounded
Delay

Antoine Amarilli ! ˇ (cid:18)
LTCI, Télécom Paris, Institut Polytechnique de Paris, France

Mikaël Monet ! ˇ (cid:18)
Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France

Abstract

We study the task, for a given language L, of enumerating the (generally inﬁnite) sequence of its
words, without repetitions, while bounding the delay between two consecutive words. To allow for
delay bounds that do not depend on the current word length, we assume a model where we produce
each word by editing the preceding word with a small edit script, rather than writing out the word
from scratch. In particular, this witnesses that the language is orderable, i.e., we can write its words
as an inﬁnite sequence such that the Levenshtein edit distance between any two consecutive words
is bounded by a value that depends only on the language. For instance, pa ` bq˚ is orderable (with
a variant of the Gray code), but a˚ ` b˚ is not.

We characterize which regular languages are enumerable in this sense, and show that this can
be decided in PTIME in an input deterministic ﬁnite automaton (DFA) for the language. In fact,
we show that, given a DFA A, we can compute in PTIME automata A1, . . . , At such that LpAq is
partitioned as LpA1q \ . . . \ LpAtq and every LpAiq is orderable in this sense. Further, we show that
the value of t obtained is optimal, i.e., we cannot partition LpAq into less than t orderable languages.
In the case where LpAq is orderable (i.e., t “ 1), we show that the ordering can be produced by a
bounded-delay algorithm: speciﬁcally, the algorithm runs in a suitable pointer machine model, and
produces a sequence of bounded-length edit scripts to visit the words of LpAq without repetitions,
with bounded delay – exponential in |A| – between each script. In fact, we show that we can achieve
this while only allowing the edit operations push and pop at the beginning and end of the word,
which implies that the word can in fact be maintained in a double-ended queue.

By contrast, when ﬁxing the distance bound d between consecutive words and the number of
classes of the partition, it is NP-hard in the input DFA A to decide if LpAq is orderable in this sense,
already for ﬁnite languages.

Last, we study the model where push-pop edits are only allowed at the end of the word,
corresponding to a case where the word is maintained on a stack. We show that these operations are
strictly weaker and that the slender languages are precisely those that can be partitioned into ﬁnitely
many languages that are orderable in this sense. For the slender languages, we can again characterize
the minimal number of languages in the partition, and achieve bounded-delay enumeration.

2012 ACM Subject Classiﬁcation Theory of computation Ñ Formal languages and automata theory

Keywords and phrases Regular language, constant-delay enumeration, edit distance

Related Version Full version: https://arxiv.org/abs/2209.14878 [5]

Funding Antoine Amarilli: Partially supported by the ANR project EQUUS ANR-19-CE48-0019
and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 431183758.

Acknowledgements We thank Florent Capelli and Charles Paperman for their insights during initial
discussions about this problem. We thank user pcpthm on the Theoretical Computer Science Stack
Exchange forum for giving the argument for Proposition 6.1 in [19]. We thank Jeﬀrey Shallit for
pointing us to related work. We thank Torsten Mütze and Arturo Merino for other helpful pointers.
We thank the anonymous reviewers for their valuable feedback. Finally, we are grateful to Louis
Jachiet and Lê Thành Dũng (Tito) Nguyễn for feedback on the draft.

3
2
0
2

n
a
J

7

]
L
F
.
s
c
[

3
v
8
7
8
4
1
.
9
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Enumerating Regular Languages with Bounded Delay

1

Introduction

Enumeration algorithms [24, 27] are a way to study the complexity of problems beyond
decision or function problems, where we must produce a large number of outputs without
repetitions. In such algorithms, the goal is usually to minimize the worst-case delay between
any two consecutive outputs. The best possible bound is to make the delay constant, i.e.,
independent from the size of the input. This is the case, for example, when enumerating the
results of acyclic free-connex conjunctive queries [7] or of MSO queries over trees [6, 15].

Unfortunately, constant-delay is an unrealistic requirement when the objects to enumerate
can have unbounded size, simply because of the time needed to write them out. Faced by
this problem, one option is to neglect this part of the running time, e.g., following Ruskey’s
“Do not count the output principle” [22, p. 8]. In this work, we address this challenge in
a diﬀerent way: we study enumeration where each new object is not written from scratch
but produced by editing the previous object, by a small sequence of edit operations called an
edit script. This further allows us to study the enumeration of inﬁnite collections of objects,
with an algorithm that runs indeﬁnitely and ensures that each object is produced after some
ﬁnite number of steps, and exactly once. The size of the edit scripts must be bounded, i.e., it
only depends on the collection of objects to enumerate, but not on the size of the current
object. The algorithm thus outputs an inﬁnite series of edit scripts such that applying them
successively yields the inﬁnite collection of all objects. In particular, the algorithm witnesses
that the collection admits a so-called ordering: it can be ordered as an inﬁnite sequence with
a bound on the edit distance between any two consecutive objects, namely, the number of
edit operations.

In this paper, we study enumeration for regular languages in this sense, with the Lev-
enshtein edit distance and variants thereof. One ﬁrst question is to determine if a given
regular language L admits an ordering, i.e., can we order its words such that the Levenshtein
distance of any two consecutive words only depends on L and not on the word lengths? For
instance, the language a˚ is easily orderable in this sense. The language a˚b˚ is orderable,
e.g., following any Hamiltonian path on the inﬁnite N ˆ N grid. More interestingly, the
language pa ` bq˚ is orderable, for instance by considering words by increasing length and
using a Gray code [18], which enumerates all n-bit words by changing only one bit at each
step. More complex languages such as apa ` bcq˚ ` bpcbq˚ddd˚ can also be shown to be
orderable (as our results will imply). However, one can see that some languages are not
orderable, e.g., a˚ ` b˚. We can nevertheless generalize orderability by allowing multiple
“threads”: then we can partition a˚ ` b˚ as a˚ and b˚, both of which are orderable. This
leads to several questions: Can we characterize the orderable regular languages? Can every
regular language be partitioned as a ﬁnite union of orderable languages? And does this lead
to a (possibly multi-threaded) enumeration algorithm with bounded delay (i.e., depending
only on the language but not on the current word length)?

Contributions. The present paper gives an aﬃrmative answer to these questions. Speciﬁcally,
we show that, given a DFA A, we can decide in PTIME if LpAq is orderable. If it is not, we can
compute in PTIME DFAs A1, . . . , At partitioning the language as LpAq “ LpA1q\. . .\LpAtq
such that each LpAiq is orderable; and we show that the t given in this construction is optimal,
i.e., no smaller such partition exists. If the language is orderable (i.e., if t “ 1), we show in
fact that the same holds for a much more restricted notion of distance, the push-pop distance,
which only allows edit operations at the beginning and end of the word. The reason we are
interested in this restricted edit distance is that edit scripts featuring push and pop can be

A. Amarilli and M. Monet

3

easily applied in constant-time to a word represented in a double-ended queue; by contrast,
Levenshtein edit operations are more diﬃcult to implement, because they refer to integer
word positions that change whenever characters are inserted or deleted.1

And indeed, this result on the push-pop distance then allows us to design a bounded-delay
algorithm for LpAq, which produces a sequence of bounded edit scripts of push or pop
operations that enumerates LpAq. The length of the edit scripts is polynomial in |A| and the
delay of our algorithm is exponential in |A|, but crucially it remains bounded throughout
the (generally inﬁnite) execution of the algorithm, and does not depend on the size of the
words that are achieved. Formally, we show:

§ Result 1. Given a DFA A, one can compute in PTIME automata A1, . . . , At for some t ď
|A| such that LpAq is the disjoint union of the LpAiq, and we can enumerate each LpAiq with
bounded delay for the push-pop distance with distance bound 48|A|2 and exponential delay in
|A|. Further, LpAq has no partition of cardinality t ´ 1 into orderable languages, even for the
Levenshtein distance.

Thus, we show that orderability and enumerability, for the push-pop or Levenshtein edit
distance, are in fact all logically equivalent on regular languages, and we characterize them
(and ﬁnd the optimal partition cardinality) in PTIME. By contrast, as was pointed out
in [19], testing orderability for a ﬁxed distance d is NP-hard in the input DFA, even for ﬁnite
languages.

Last, we study the push-pop-right distance, which only allows edits at the end of the word.
The motivation for studying this distance is that it corresponds to enumeration algorithms
in which the word is maintained on a stack. We show that, among the regular languages,
the slender languages [20] are then precisely those that can be partitioned into ﬁnitely many
orderable languages, and that these languages are themselves enumerable. Further, the
optimal cardinality of the partition can again be computed in PTIME:

§ Result 2. Given a DFA A, then LpAq is partitionable into ﬁnitely many orderable languages
for the push-pop-right distance if and only if LpAq is slender (which we can test in PTIME
in A). Further, in this case, we can compute in PTIME the smallest partition cardinality,
and each language in the partition is enumerable with bounded delay with distance bound 2|A|
and linear delay in |A|.

In terms of proof techniques, our PTIME characterization of Result 1 relies on a notion
of interchangeability of automaton states, deﬁned via paths between states and via states
having common loops. We then show orderability by establishing stratum-connectivity,
i.e., for any stratum of words of the language within some length interval, there are ﬁnite
sequences obeying the distance bound that connect any two words in that stratum. We
show stratum-connectivity by pumping and de-pumping loops close to the word endpoints.
We then deduce an ordering from this by adapting a standard technique [26] of visiting
a spanning tree and enumerating even and odd levels in alternation (see also [23, 14]).
The bounded-delay enumeration algorithm then proceeds by iteratively enlarging a custom
data structure called a word DAG, where the construction of the structure for a stratum is
amortized by enumerating the edit scripts to achieve the words of the previous stratum.

1 There is, in fact, an Ωplog |w|{ log log |w|q lower bound on the complexity of applying Levenshtein edit
operations and querying which letter occurs at a given position: crucially, this bound depends on the size
of the word. See https://cstheory.stackexchange.com/q/46746 for details. This is in contrast to the
application of push-pop-right edit operations, which can be performed in constant time (independent
from the word length) when the word is stored in a double-ended queue.

4

Enumerating Regular Languages with Bounded Delay

Related work. As we explained, enumeration has been extensively studied for many struc-
tures [27]. For regular languages speciﬁcally, some authors have studied the problem of
enumerating their words in radix order [16, 1, 2, 11]. For instance, the authors of [1, 2]
provide an algorithm that enumerates all words of a regular language in that order, with
a delay of Op|w|q for w the next word to enumerate. Thus, this delay is not bounded, and
the requirement to enumerate in radix order makes it challenging to guarantee a bounded
distance between consecutive words (either in the Levenshtein or push-pop distance), which
is necessary for bounded-delay enumeration in our model. Indeed, our results show that not
all regular languages are orderable in our sense, whereas their linear-delay techniques apply
to all regular languages.

We have explained that enumeration for pa ` bq˚ relates to Gray codes, of which there
exist several variants [18]. Some variants, e.g., the so-called middle levels problem [17], aim
at enumerating binary words of a restricted form; but these languages are typically ﬁnite
(i.e., words of length n), and their generalization is typically not regular. While Gray codes
typically allow arbitrary substitutions, one work has studied a variant that only allows
restricted operations on the endpoints [10], implying the push-pop orderability of the speciﬁc
language pa ` bq˚.

Independently, some enumeration problems on automata have been studied recently in the
database theory literature, in particular for document spanners [9], which can be deﬁned by
ﬁnite automata with capture variables. It was recently shown [12, 3] that we can enumerate
in constant delay all possible assignments of the capture variables of a ﬁxed spanner on an
input word. In these works, the delay is constant in data complexity, which means that it
only depends on the (ﬁxed) automaton, and does not depend on the word; this matches
what we call bounded delay in our work (where there is no input word and the automaton is
given as input). However, our results do not follow from these works, which focus on the
enumeration of results of constant size. Some works allow second-order variables and results
of non-constant size [4] but the delay would then be linear in each output, hence unbounded.

Paper structure. We give preliminaries in Section 2. In Section 3 we present our PTIME
construction of a partition of a regular language into ﬁnitely many orderable languages, and
prove that the cardinality of the obtained partition is minimal for orderability. We then
show in Section 4 that each term of the union is orderable, and then that it is enumerable in
Section 5. We present the NP-hardness result on testing orderability for a ﬁxed distance and
our results on push-pop-right operations in Section 6. We conclude and mention some open
problems in Section 7. Due to space constraints, we mostly present the general structure
of the proofs and give the main ideas; detailed proofs of all statements can be found in the
appendix.

2

Preliminaries

We ﬁx a ﬁnite non-empty alphabet Σ of letters. A word is a ﬁnite sequence w “ a1 ¨ ¨ ¨ an
of letters. We write |w| “ n, and write (cid:15) for the empty word. We write Σ˚ the inﬁnite
set of words over Σ. A language L is a subset of Σ˚. For k P N, we denote Lăk the
language tw P L | |w| ă ku. In particular we have Lă0 “ H.

In this paper we study regular languages. Recall that such a language can be described by
a deterministic ﬁnite automaton (DFA) A “ pQ, Σ, q0, F, δq, which consists of a ﬁnite set Q
of states, an initial state q0 P Q, a set F Ď Q of ﬁnal states, and a partial transition function
δ : Q ˆ Σ Ñ Q. We write |A| the size of representing A, which is Op|Q| ˆ |Σ|q. A (directed)

A. Amarilli and M. Monet

5

path in A from a state q P Q to a state q1 P Q is a sequence of states q “ q0, . . . , qn “ q1 where
for each 0 ď i ă n we have qi`1 “ δpqi, aiq for some ai. For a suitable choice a0, . . . , an´1,
we call the word a0 ¨ ¨ ¨ an´1 P Σ˚ a label of the path. In particular, there is an empty path
with label (cid:15) from every state to itself. The language LpAq accepted by A consists of the
words w that label a path from q0 to some ﬁnal state. We assume without loss of generality
that all automata are trimmed, i.e., every state of Q has a path from q0 and has a path to
some ﬁnal state; this can be enforced in linear time.

Edit distances. For an alphabet Σ, we denote by δLev : Σ˚ ˆ Σ˚ Ñ N the Levenshtein
edit distance: given u, v P Σ˚, the value δLevpu, vq is the minimum number of edits needed
to transform u into v, where the edit operations are single-letter insertions, deletions or
substitutions (we omit their formal deﬁnitions).

While our lower bounds hold for the Levenshtein distance, our positive results already
hold with a restricted set of 2|Σ| ` 2 edit operations called the push-pop edit operations:
pushLpaq and pushRpaq for a P Σ, which respectively insert a at the beginning and at the end
of the word, and popLpq and popRpq, which respectively remove the ﬁrst and last character
of the word (and cannot be applied if the word is empty). Thus, we deﬁne the push-pop edit
distance, denoted δpp, like δLev but allowing only these edit operations.

Orderability. Fixing a distance function δ : Σ˚ ˆ Σ˚ Ñ N over Σ˚, for a language L Ď Σ˚
and d P N, a d-sequence in L is a (generally inﬁnite) sequence s of words w1, . . . , wn, . . .
of L without repetition, such that for every two consecutive words wi, wi`1 in s we have
δpwi, wi`1q ď d. We say that s starts at w1 and, in case s is ﬁnite and has n elements, that s
ends at wn (or that s is between w1 and wn). A d-ordering of L is a d-sequence s in L such
that every word of L occurs in s; equivalently, it is a permutation of L such that any two
consecutive words are at distance at most d. An ordering is a d-ordering for some d P N.
If these exist, we call the language L, respectively, d-orderable and orderable. We call L
pt, dq-partition-orderable if it can be partitioned into t languages that each are d-orderable:

§ Deﬁnition 2.1. Let L be a language and t, d P N. We call L pt, dq-partition-orderable if L
has a partition L “

1ďiďt Li such that each Li is d-orderable.2

Ů

Note that, if we allowed repetitions in d-orderings, then the language of any DFA A would
be Op|A|q-orderable: indeed, any word w can be transformed into a word w1 of length Op|A|q
by iteratively removing simple loops in the run of w. By contrast, we will see in Section 3
that allowing a constant number of repetitions of each word makes no diﬀerence.

§ Example 2.2. We consider the Levenshtein distance in this example. The language paaq˚
is p1, 2q-partition-orderable (i.e., 2-orderable) and not pk, 1q-partition-orderable for any k P N.
The language a˚ ` b˚ is p2, 1q-partition-orderable and not orderable, i.e., not d-orderable for
any d P N. Any ﬁnite language is d-orderable with d the maximal length of a word in L. The
non-regular language tan2

| n P Nu is not pt, dq-partition-orderable for any t, d P N.

Enumeration algorithms. We study enumeration algorithms, which output a (generally
inﬁnite) sequence of edit scripts σ1, σ2, . . .. We only study enumeration algorithms where
each edit script σi is a ﬁnite sequence of push-pop edit operations. The algorithm enumerates
a language L if the sequence satisﬁes the following condition: letting w1 be the result of

Ů

2 We use

for disjoint unions.

6

Enumerating Regular Languages with Bounded Delay

applying σ1 on the empty word, w2 be the result of applying σ2 to w1, and so on, then all wi
are distinct and L “ tw1, w2, . . .u. If L is inﬁnite then the algorithm does not terminate, but
the inﬁnite sequence ensures that every w P L is produced as the result of applying (to (cid:15))
some ﬁnite preﬁx σ1, . . . , σn of the output.

We aim for bounded-delay algorithms, i.e., each edit script must be output in time that
only depends on the language L that is enumerated, but not on the current length of the
words. Formally, the algorithm can emit any push-pop edit operation and a delimiter Output,
it must successively emit the edit operations of σi followed by Output, and there is a bound
T ą 0 (the delay) depending only on L such that the ﬁrst Output is emitted at most T
operations after the beginning of the algorithm, and for each i ą 1 the i-th Output is emitted
at most T operations after the pi ´ 1q-th Output. Note that our notion of delay also accounts
for what is usually called the preprocessing phase in the literature, i.e., the phase before the
ﬁrst result is produced. Crucially the words wi obtained by applying the edit scripts σi are
not written, and T does not depend on their length.

We say that a bounded-delay algorithm d-enumerates a language L if it produces
a d-ordering of L (for the push-pop distance). Thus, if L is d-enumerable (by an algo-
rithm), then L is in particular d-orderable, and we will show that for regular languages, the
converse also holds.

§ Example 2.3. Consider the regular language L :“ a˚b˚ `b˚a˚. This language is 2-orderable
for the push-pop distance. Indeed, we can order it by increasing word length, ﬁnishing for
word length i by the word ai as follows. We start by length zero with the empty word (cid:15) (so
the ﬁrst edit script is empty), then, assuming we have ordered all words of L of size ď i
while ﬁnishing with ai, we continue with words of L of size i ` 1 in the following manner:
we push-right the letter b to obtain aib, and then we “shift” with edit scripts of the form
ppushRpbq; popLpqq until we obtain bi`1, and then we shift again with edit scripts of the
form ppushRpaq; popLpqq until we obtain ai`1 as promised. This gives us an enumeration
algorithm for L, shown in Algorithm 1. As such, the delay of Algorithm 1 is not bounded,
because of the time needed to increment the integer variable size: this variable becomes
arbitrarily large throughout the enumeration, so it is not realistic to assume that we can
increment it in constant time. This can however be ﬁxed by working in a suitable pointer
machine model, as explained next.

Note that our enumeration algorithms run indeﬁnitely, and thus use unbounded memory:
this is unavoidable because their output would necessarily be ultimately periodic otherwise,
which is not suitable in general (see Appendix A.1). To avoid specifying the size of memory
cells or the complexity of arithmetic computations (e.g., incrementing the integer size in
Algorithm 1), we consider a diﬀerent model called pointer machines [25] which only allows
arithmetic on a bounded domain. We use this model for our enumeration algorithms (but
not, e.g., our other complexity results such as PTIME bounds).

Intuitively, a pointer machine works with records consisting of a constant number of
labeled ﬁelds holding either data values (in our case of constant size, i.e., constantly many
possible values) or pointers (whose representation is not speciﬁed). The machine has memory
consisting of a ﬁnite but unbounded collection of records, a constant number of which are
designated as registers and are always accessible. The machine can allocate records in
constant time, retrieving a pointer to the memory location of the new record. We can access
the ﬁelds of records, read or write pointers, dereference them, and test them for equality,
all in constant time, but we cannot perform any other manipulation on pointers or other
arithmetic operations. (We can, however, count in unary with a linked list, or perform
arbitrary operations on the constant-sized data values.) See Appendix A.2 for details.

A. Amarilli and M. Monet

7

Algorithm 1 Push-pop enumeration algorithm for the language a˚b˚ ` b˚a˚ from

Example 2.3.

// The first edit script is empty, corresponding to the empty word.
Output;
int size “ 0;
while true do
size``;
pushRpbq ; Output;
for int j “ 0; j ă size ´ 1; j`` do
pushRpbq ; popLpq ; Output;

end
for int j “ 0; j ă size; j`` do
pushRpaq ; popLpq ; Output;

end

end

§ Example 2.4. Continuing Example 2.3, Algorithm 1 can easily be adapted to a pointer-
machine algorithm that 2-enumerates L, maintaining the word in a double-ended queue
(deque) and keeping pointers to the ﬁrst and last positions in order to know when to stop
the for loops. Deques can indeed be simulated in this machine model, e.g., with linked lists.

3

Interchangeability partition and orderability lower bound

In this section, we start the proof of our main result, Result 1. Let A be the DFA and let Q
be its set of states. The result is trivial if the language LpAq is ﬁnite, as we can always
enumerate it naively with distance Op|A|q and some arbitrary delay bound, so in the rest of
the proof we assume that LpAq is inﬁnite.

We will ﬁrst deﬁne a notion of interchangeability on DFAs by introducing the notions of
connectivity and compatibility on DFA states (this notion will be used in the next section
to characterize orderability). We then partition LpAq into languages LpA1q \ ¨ ¨ ¨ \ LpAtq
following a so-called interchangeability partition, with each Ai having this interchangeability
property. Last, we show in the section our lower bound establishing that t is optimal.

Interchangeability. To deﬁne our notion of interchangeability, we ﬁrst deﬁne the loopable
states of the DFA as those that are part of a non-empty cycle (possibly a self-loop):

§ Deﬁnition 3.1. For a state q P Q, we let Aq be the DFA obtained from A by setting q as the
only initial and ﬁnal state. We call q loopable if LpAqq ‰ t(cid:15)u, and non-loopable otherwise.

We then deﬁne the interchangeability relation on loopable states as the transitive closure

of the union of two relations, called connectivity and compatibility:

§ Deﬁnition 3.2. We say that two loopable states q and q1 are connected if there is a directed
path from q to q1, or from q1 to q. We say that two loopable states q, q1 are compatible
if LpAqq X LpAq1q ‰ t(cid:15)u. These two relations are symmetric and reﬂexive on loopable states.
We then say that two loopable states q and q1 are interchangeable if they are in the transitive
closure of the union of the connectivity and compatibility relations. In other words, q and q1
are interchangeable if there is a sequence q “ q0, . . . , qn “ q1 of loopable states such that for

8

Enumerating Regular Languages with Bounded Delay

a

1

b

a, b

0

a

0

b

b

1

c

0

a

(a) DFA A1

(b) DFA A2

(c) DFA A3

a

b

0

a

1

3

d

b

c

c

b

2

5

4

d

6

d

b

2

0

a

1

b

b

a

a

0

(d) DFA A4

a

1

b

3

a

b

b

2

b

2

a

4

(e) DFA A5

(f) DFA A6

Figure 1 Example DFAs from Example 3.4

any 0 ď i ă n, the states qi and qi`1 are either connected or compatible. Interchangeability
is then an equivalence relation over loopable states.

Note that if two loopable states q, q1 are in the same strongly connected component
(SCC) of A then they are connected, hence interchangeable. Thus, we can equivalently see
the interchangeability relation at the level of SCCs (excluding those that do not contain a
loopable state, i.e., excluding the trivial SCCs containing only one state having no self-loop).

§ Deﬁnition 3.3. We call classes of interchangeable states, or simply classes, the equivalence
classes of the interchangeability relation. Recall that, as LpAq is inﬁnite, there is at least one
class. We say that the DFA A is interchangeable if the partition has only one class, in other
words, if all loopable states of A are interchangeable.

§ Example 3.4. The DFA A1 shown in Figure 1a for the language pa ` bq˚ has only one
loopable state, so A1 is interchangeable.

The DFA A2 shown in Figure 1b for the language a˚b˚ has two loopable states 0 and 1

which are connected, hence interchangeable. Thus, A2 is interchangeable.

The DFA A3 shown in Figure 1c for the language c˚pa˚ ` b˚q has three loopable states:
0, 1 and 2. The states 0 and 1 are connected, and 0 and 2 are also connected, so all loopable
states are interchangeable and A3 is interchangeable.

The DFA A4 shown in Figure 1d for the language a˚ ` b˚ has two loopable states 1 and 2

which are neither connected nor compatible. So A4 is not interchangeable.

The DFA A5 shown in Figure 1e for the language apa ` bcq˚ ` bpcbq˚ddd˚ mentioned in
the introduction has ﬁve loopable states: 1, 2, 3, 4, and 6. Then 1 and 2 are connected, 3
and 4 are connected, 3 and 6 are connected, and 1 and 4 are compatible (with the word bc).
Hence, all loopable states are interchangeable and A5 is interchangeable.

The DFA A6 shown in Figure 1f for the language a˚b˚ ` b˚a˚ from Example 2.3 has
four loopable states: 1, 2, 3, and 4. Then 1 and 2 are connected, 3 and 4 are connected,
and (for instance) 1 and 4 are compatible (with the word a). Hence all loopable states are
interchangeable and A6 is interchangeable.

A. Amarilli and M. Monet

9

Interchangeability partition. We now partition LpAq using interchangeable DFAs:

§ Deﬁnition 3.5. An interchangeability partition of A is a sequence A1, . . . , At of DFAs such
that LpAq is the disjoint union of the LpAiq and every Ai is interchangeable. Its cardinality
is the number t of DFAs.

Let us show how to compute an interchangeability partition whose cardinality is the
number of classes. We will later show that this cardinality is optimal. Here is the statement:

§ Proposition 3.6. We can compute in polynomial time in A an interchangeability partition
A1, . . . , At of A, with t ď |A| the number of classes of interchangeable states.

Intuitively, the partition is deﬁned following the classes of A. Indeed, considering any
word w P LpAq and its accepting run ρ in A, for any loopable state q and q1 traversed in ρ,
the word w witnesses that q and q1 are connected, hence interchangeable. Thus, we would like
to partition the words of LpAq based on the common class of the loopable states traversed in
their accepting run. The only subtlety is that LpAq may also contain words whose accepting
run does not traverse any loopable state, called non-loopable words. For instance, (cid:15) is a
non-loopable word of LpA5q for A5 given in Figure 1e. Let us formally deﬁne the non-loopable
words, and our partition of the loopable words based on the interchangeability classes:

§ Deﬁnition 3.7. A word w “ a1 ¨ ¨ ¨ an of LpAq is loopable if, considering its accepting run
q0, . . . , qn with q0 the initial state and qi “ δpqi´1, aiq for 1 ď i ď n, one of the qi is loopable.
Otherwise, w is non-loopable. We write NLpAq the set of the non-loopable words of LpAq.

Letting C be a class of interchangeable states, we write LpA, Cq the set of (loopable) words

of LpAq whose accepting run traverses a state of C.

We then have the following, with ﬁniteness of NLpAq shown by the pigeonhole principle:

§ Claim 3.8. The language LpAq can be partitioned as NLpAq and LpA, C1q, . . . , LpA, Ctq over
the classes C1, . . . , Ct of interchangeable states, and further NLpAq is ﬁnite.

We now construct an interchangeability partition of A of the right cardinality by deﬁning
one DFA Ai for each class of interchangeable states, where we simply remove the loopable
states of the other classes. These DFAs are interchangeable by construction. We modify
the DFAs to ensure that the non-loopable words are only captured by A1. This construc-
tion (explained in the appendix) is doable in PTIME, in particular the connectivity and
compatibility relations can be computed in PTIME, testing compatibility by checking the
nonemptiness of product automata. This establishes Proposition 3.6.

Lower bound. We have shown how to compute an interchangeability partition of a DFA A
with cardinality the number t of classes. Let us now show that this value of t is optimal,
in the sense that LpAq cannot be partitioned into less than t orderable (even non-regular)
languages. This lower bound holds even when allowing Levenshtein edits. Formally:

§ Theorem 3.9. For any partition of the language LpAq as LpAq “ L1 \ ¨ ¨ ¨ \ Lt1 if for each
1 ď i ď t1 the language Li is orderable for the Levenshtein distance, then we have t1 ě t for t
the number of classes of A.

This establishes the negative part of Result 1. Incidentally, this lower bound can also be
shown even if the unions are not disjoint, indeed even if we allow repetitions, provided that
there is some constant bound on the number of repetitions of each word.

Theorem 3.9 can be shown from the following claim which establishes that suﬃciently

long words from diﬀerent classes are arbitrarily far away for the Levenshtein distance:

10

Enumerating Regular Languages with Bounded Delay

§ Proposition 3.10. Letting C1, . . . , Ct be the classes of A, for any distance d P N, there
is a threshold l P N such that for any two words u P LpA, Ciq and v P LpA, Cjq with i ‰ j
and |u| ě l and |v| ě l, we have δLevpu, vq ą d.

This proposition implies Theorem 3.9 because, if we could partition LpAq into less than t
orderable languages, then some ordering must include inﬁnitely many words from two
diﬀerent classes LpA, Ciq and LpA, Cjq, hence alternate inﬁnitely often between the two. Fix
the distance d, and consider a point when all words of L of length ď maxpl, maxwPNLpAq |w|q
have been enumerated, for l the threshold of the proposition: then it is no longer possible for
any ordering to move from one class to another, yielding a contradiction. As for the proof of
Proposition 3.10, we give a sketch below (the complete proofs are in appendix):

Proof sketch. Given a suﬃciently long word u P LpA, Ciq, by the pigeonhole principle its
run must contain a large number of loops over some state q P Ci. Assume that we can
edit u into v P LpA, Cjq with d edit operations: this changes at most d of these loops. Now,
considering the accepting run of v and using the pigeonhole principle again on the sequence
of endpoints of contiguous unmodiﬁed loops, we deduce that some state q1 occurs twice; then
q1 P Cj by deﬁnition of LpA, Cjq. The label of the resulting loop on q1 is then also the label of
a loop on q, so q and q1 are compatible, hence Ci “ Cj.
đ

4 Orderability upper bound

We have shown in the previous section that we could ﬁnd an interchangeability partition
of any regular language LpAq into languages LpA1q, . . . , LpAtq of interchangeable DFAs,
for t the number of classes. We know by our lower bound (Theorem 3.9) that we cannot
hope to order LpAq with less than t sequences. Thus, in this section, we focus on each
interchangeable Ai separately, and show how to order LpAiq as one sequence. Hence, we
ﬁx for this section a DFA A that is interchangeable, write k its number of states, and show
that LpAq is orderable. We will in fact show that this is the case for the push-pop distance:

§ Theorem 4.1. For any interchangeable DFA A, the language LpAq is 48k2-orderable for
the push-pop distance.

We show this result in the rest of this section, and strengthen it in the next section to a
bounded-delay algorithm. Before starting, we give an overview of the structure of the proof.
The proof works by ﬁrst introducing d-connectivity of a language (not to be confused with
the connectivity relation on loopable automaton states). This weaker notion is necessary
for d-orderability, but for ﬁnite languages we will show a kind of converse: d-connectivity
implies 3d-orderability. We will then show that LpAq is stratum-connected, i.e., the ﬁnite
strata of words of LpAq in some length interval are each d-connected for some common d.
Last, we will show show that this implies orderability, using the result on ﬁnite languages.

Connectivity implies orderability on ﬁnite languages. We now deﬁne d-connectivity:

§ Deﬁnition 4.2. A language L is d-connected if for every pair of words u, v P L, there exists
a d-sequence in L between u and v.

Clearly d-connectivity is a necessary condition for d-orderability: indeed if w1, w2, . . .
is a d-ordering of L, and u “ wi, v “ wj are two words of L with i ď j (without loss of
generality), then wi, wi`1, . . . , wj is indeed a d-sequence in L between u and v. What is
more, for ﬁnite languages, the converse holds, up to multiplying the distance by a constant
factor:

A. Amarilli and M. Monet

11

§ Lemma 4.3. Let L be a ﬁnite language that is d-connected and s ‰ e be words of L. Then
there exists a 3d-ordering of L starting at s and ending at e.

Proof sketch. We use the fact, independently proved by Sekanina and by Karaganis [23, 14],
that the cube of every connected graph G has a Hamiltonian path between any pair of
vertices (see also [18]). One algorithmic way to see this is by traversing a spanning tree of G
and handling odd-depth and even-depth nodes in preﬁx and postﬁx fashion (see, e.g., [26]).
Applying this to the graph G whose vertices are the words of L and where two words w, w1
are connected by an edge when δpw, w1q ď d yields the result.
đ

The constant 3 in this lemma is optimal, as follows from [21]; see Appendix C for more
details. Note that the result does not hold for inﬁnite languages: a˚ ` b˚ is 1-connected (via
(cid:15)) but not d-orderable for any d.

Stratum-connectivity. To show orderability for inﬁnite languages, we will decompose them
into strata, which simply contain the words in a certain length range. Formally:

§ Deﬁnition 4.4. Let L be a language, let ‘ ą 0 be an integer, and let i ą 0. The i-th
stratum of width ‘ (or ‘-stratum) of L, written strat‘pL, iq, is Lăi‘zLăpi´1q‘.

We will show that, for the language LpAq of our interchangeable DFA A, we can pick ‘
and d such that every ‘-stratum of LpAq is d-connected, i.e., LpAq is p‘, dq-stratum-connected:

§ Deﬁnition 4.5. Let L be a regular language and ﬁx ‘, d ą 0. We say that L is p‘, dq-
stratum-connected if every ‘-stratum strat‘pL, iq is d-connected.

Note that our example language a˚ `b˚, while 1-connected, is not p‘, dq-stratum-connected
for any ‘, d, because any i-th ‘-stratum for i ą d is not d-connected. We easily show that
stratum-connectivity implies orderability:

§ Lemma 4.6. Let L be an inﬁnite language recognized by a DFA with k1 states, and assume
that L is p‘, dq-stratum-connected for some ‘ ě 2k1 and some d ě 3k1. Then L is 3d-orderable.

Proof sketch. We show by pumping that we can move across contiguous strata. Thus, we
combine orderings on each stratum obtained by Lemma 4.3 with well-chosen endpoints. đ

We can then show using several pumping and de-pumping arguments that the language

of our interchangeable DFA A is p‘, dq-stratum-connected for ‘ :“ 8k2 and d :“ 16k2.

§ Proposition 4.7. The language LpAq is p8k2, 16k2q-stratum-connected.

Proof sketch. As there are only a ﬁnite number of non-loopable words, we focus on loopable
words. Consider a stratum S and two loopable words u and v of S. Their accepting runs
involve loopable states, respectively q and q1, that are interchangeable because A is. We ﬁrst
show that u is d-connected (in S) to a normal form: a repeated loop on q plus a preﬁx and
suﬃx whose length is bounded, i.e., only depends on the language. We impose this in two
steps: ﬁrst we move the last occurrence of q in u near the end of the word by pumping at
the left end and de-pumping at the right end, second we pump the loop on q at the right
end while de-pumping the left end. This can be done while remaining in the stratum S. We
obtain similarly a normal form consisting of a repeated loop on q1 with bounded-length preﬁx
and suﬃx that is d-connected to v in S.

Then we do an induction on the number of connectivity and compatibility relations
needed to witness that q and q1 are interchangeable. If q “ q1, we conclude using the normal

12

Enumerating Regular Languages with Bounded Delay

forms of u and v. If q is connected to q1, we impose the normal form on u, then we modify
it to a word whose accepting run also visits q1, and we apply the previous case. If q is
compatible with q1, we conclude using the normal form with some loop label z in Aq X Aq1
(of length ď k2) that witnesses their compatibility. The induction case is then easy.
đ

From this, we deduce with Lemma 4.6 that LpAq is 48k2-orderable, so Theorem 4.1 holds.
Note that the construction ensures that the words are ordered stratum after stratum, so
“almost” by increasing length: in the ordering that we obtain, after producing some word w,
we will never produce words of length less than |w| ´ ‘.

5

Bounded-delay enumeration

In this section, we show how the orderability result of the previous section yields a bounded-
delay algorithm. We use the pointer-machine model from Section 2, which we modify for
convenience to allow data values and the number of ﬁelds of records to be exponential in the
automaton (but ﬁxed throughout the enumeration, and independent on the size of words):
see Appendix D.1 for explanations on why we can do this. We show:

§ Theorem 5.1. There is an algorithm which, given an interchangeable DFA A with k states,
enumerates the language LpAq with push-pop distance bound 48k2 and exponential delay
in |A|.

Let us accordingly ﬁx the interchangeable DFA A with k states. Following Proposition 4.7,

we let d :“ 16k2 and ‘ :“ 8k2.

Overall amortized scheme. The algorithm will consider the strata of the input language L
and will run two processes in parallel: the ﬁrst process simply enumerates a previously
prepared sequence of edit scripts that gives a 3d-ordering of some stratum, while the second
process computes the sequences for subsequent strata (and of course imposing that the
endpoints of the sequences for contiguous strata are suﬃciently close).

The challenging part is to prepare eﬃciently the sequences for all strata, and in particular
to build a data structure that represents the strata. We will require of our algorithm that
it processes each stratum in amortized linear time in its size. Formally, letting Nj :“
|strat‘pL, jq| be the number of words of the j-th stratum for all j ě 1, there is a value C P N
i
j“1 Nj steps, the algorithm is
that is exponential in |A| such that, after having run for C
done processing the i-th stratum. Note that this is weaker than processing each separate
stratum in linear time: the algorithm can go faster to process some strata and spend this
spared time later so that some later strata are processed arbitrarily slowly relative to their
size.

ř

If we can achieve amortized linear time, then the overall algorithm runs with bounded
delay. To see why, notice that the prepared sequence for the i-th stratum has length at least
its size Ni, and we can show that the size Ni`1 of the next stratum is within a factor of Ni
that only depends on L (this actually holds for any inﬁnite regular language and does not
use interchangeability):

§ Lemma 5.2. Letting CA :“ pk ` 1q|Σ|‘`k`1, for all i ě 1 we have Ni{CA ď Ni`1 ď CANi.

Proof. Each word in the pi ` 1q-th stratum of L can be transformed into a word in the i-th
stratum as follows: letting k be the number of DFA states, ﬁrst remove a preﬁx of length at
most ‘ ` k to get a word (not necessarily in L) of length i‘ ´ k ´ 1, and then add back a
preﬁx corresponding to some path of length ď k from the initial state to get a word in the

A. Amarilli and M. Monet

13

i-th stratum of L as desired. Now, for any word w of the i-th stratum, the number of words
of the pi ` 1q-th stratum that lead to w in this way is bounded by CA, by considering the
reverse of this rewriting, i.e., all possible ways to rewrite w by removing a preﬁx of length
at most k and then adding a preﬁx of length at most ‘ ` k. A simple union bound gives
Ni`1 ď CANi. Now, a similar argument in the other direction gives Ni{CA ď Ni`1.
đ

Thanks to this lemma, it suﬃces to argue that we can process the strata in amortized
linear time, preparing 3d-orderings for each stratum: enumerating these orderings in parallel
with the ﬁrst process thus guarantees (non-amortized) bounded delay.

Preparing the enumeration sequence. We now explain in more detail the working of
the amortized linear time algorithm. The algorithm consists of two components. The
ﬁrst component runs in amortized linear time over the successive strata, and prepares a
sequence Γ1, Γ2, . . . of concise graph representations of each stratum, called stratum graphs;
ř
i
for each i ě 1, after C
j“1 Nj computation steps, it has ﬁnished preparing the i-th stratum
graph Γi in the sequence. The second component will run as soon as some stratum graph Γi is
ﬁnished: it reads the graph Γi and computes a 3d-ordering for strat‘pL, iq in (non-amortized)
linear-time, using Lemma 4.3. Let us formalize the notion of a stratum graph:

§ Deﬁnition 5.3. Let ∆ be the set of all push-pop edit scripts of length at most d; note
that |∆| ď p2|Σ| ` 2qd`1, and this bound only depends on the alphabet and on d. For
i ě 1, the i-th stratum graph is the edge-labeled directed graph Γi “ pVi, ηiq where the nodes
Vi “ tvw | w P strat‘pL, iqu correspond to words of the i-th stratum, and the directed (labeled)
edges are given by the function ηi : Vi ˆ ∆ Ñ Vi Y tKu and describe the possible scripts: for
each vw P Vi and each s P ∆, if the script s is applicable to w and the resulting word w1 is
in strat‘pL, iq then ηpvw, sq “ vw1, otherwise ηpvw, sq “ K.

In our machine model, each node vw of Γi is a record with |∆| pointers, i.e., we do not

store the word w. Hence, Γi has linear size in Ni.

A stratum graph sequence is an inﬁnite sequence pΓ1, vs1 , ve1q, pΓ2, vs2 , ve2q, . . . consisting
of the successive stratum graphs together with couples of nodes of these graphs such that, for
all i ě 1, si and ei are distinct words of the i-th stratum, and we have δpppei, si`1q ď d.

We can now present the second component of our algorithm. Note that the algorithm
runs on the in-memory representations of the stratum graphs, in which, e.g., the subscripts
are not stored.

§ Proposition 5.4. For i ě 1, given the stratum graph Γi and starting and ending nodes
vsi ‰ vei of Γi, we can compute in time Op|Γi|q a sequence of edit scripts σ1, . . . , σNi´1
such that, letting si “ u1, . . . , uNi be the successive results of applying σ1, . . . , σNi´1 starting
with si, then u1, . . . , uNi is a 3d-ordering of strat‘pL, iq starting at si and ending at ei.

Proof sketch. We apply the spanning tree enumeration technique from Lemma 4.3 (in Op|Γi|q)
on Γi, starting with vsi and ending with vei , and read the scripts from the edge labels.
đ

In the rest of the section we present the ﬁrst component of our enumeration algorithm:

§ Proposition 5.5. There is an integer C P N exponential in |A| such that we can produce a
stratum graph sequence pΓ1, vs1, ve1 q, pΓ2, vs2, ve2 q, . . . for L in amortized linear time, i.e., for
ř
i
j“1 Nj steps, the algorithm is done preparing pΓi, vsi, veiq.
each i ě 1, after having run C

14

Enumerating Regular Languages with Bounded Delay

Word DAGs. The algorithm to prove Proposition 5.5 will grow a large structure in memory,
common to all strata, from which we can easily compute the pΓi, vsi, veiq. We call this
structure a word DAG. A word DAG is informally a representation of a collection of words,
each of which has outgoing edges corresponding to the possible left and right push operations.

§ Deﬁnition 5.6. Let Λ :“ tpushRpaq | a P Σu Y tpushLpaq | a P Σu be the set of labels
corresponding to the possible left and right push operations. A pre-word DAG is an edge-
labeled directed acyclic graph (DAG) G “ pV, η, rootq where V is a set of anonymous vertices,
root P V is the root, and η : V ˆ Λ Ñ V Y tKu represents the labeled edges in the following
way: for each node v P V and label s P Λ, if ηpv, sq ‰ K then v has one successor ηpv, sq for
label s, and none otherwise. We impose:

The root has no incoming edges. All other nodes have exactly two incoming edges: one
labeled pushRpaq for some a P Σ, the other labeled pushLpbq for some b P Σ. Each node
stores two pointers leading to these two parents, which may be identical.
All nodes can be reached from the root via at least one directed path.
The root has one outgoing edge for each child, i.e., for all s P Λ, we have ηproot, sq ‰ K.

The word represented by a directed path from the root to a node n is deﬁned inductively:

the word represented by the empty path is (cid:15),
the word represented by a path P, pushRpaq is wa where w is the word represented by P ,
the word represented by a path P, pushLpaq is aw where w is the word represented by P .
The pre-word DAG G is called a word DAG if for each node n, all paths from root to n
represent the same word. This word is then called the word represented by n.

Example pre-word DAGs and word DAGs are shown on Figures 3 and 4 in the appendix.
In our machine model, each node is represented by a record; crucially, like for stratum graphs,
the word that the node represents is not explicitly written.

Crucially, word DAGs do not us allow not to create two diﬀerent nodes that represent
the same word – these would be problematic since we have to enumerate without repetition.

§ Fact 5.7. There are no two diﬀerent nodes in a word DAG that represent the same word.

We can then show the following theorem, intuitively saying that we can discover all the

words of the language by only visiting words that are not too far from it:

§ Proposition 5.8. We can build a word DAG G representing the words of L in amortized
linear time: speciﬁcally, for some value C that is exponential in |A|, for all i, after C ˆ
ř
i
j“1 Nj computation steps, for each word w of Σ˚ whose push-pop distance to a word
Ť
i
j“1 strat‘pL, jq is no greater than d, then G contains a node that represents w. Moreover,
of
there is also a value D exponential in |A| such that any node that is eventually created in the
word DAG represents a word that is at push-pop distance at most D from a word of L.

Proof sketch. We progressively add nodes to a word DAG while eﬃciently preserving its
properties, and thus avoid creating duplicate nodes. By labeling each node with the element
of Q Y tKu achieved by the word represented by that node, and also by the distance to the
closest known word of L, we can restrict the exploration to nodes corresponding to words
that are close to the words of L, which ensures the amortized linear time bound.
đ

This is enough to prove Proposition 5.5: we run the algorithm of Proposition 5.8 and,
whenever it has built a stratum, construct the stratum graph Γi and nodes vsi , vei by
exploring the relevant nodes of the word DAG. Full proofs are deferred to the appendix.

A. Amarilli and M. Monet

15

6

Extensions

Complexity of determining the optimal distance. We have shown in Result 1 that, given
a DFA A, we can compute in PTIME a minimal cardinality partition of LpAq into languages
that are each d-orderable, for d “ 48|A|2. However, we may achieve a smaller distance d
if we increase the cardinality, e.g., a˚ ` bbba˚ is p1, 3q-partition-orderable and not p1, dq-
partition-orderable for d ă 3, but is p2, 1q-partition-orderable. This tradeoﬀ between t and d
seems diﬃcult to characterize, and in fact it is NP-hard to determine if an input DFA is
pt, dq-partition-orderable, already for ﬁxed t, d and for ﬁnite languages. Indeed, there is a
simple reduction pointed out in [19] from the Hamiltonian path problem on grid graphs [13]:

§ Proposition 6.1 ([19]). For any ﬁxed t, d ě 1, it is NP-complete, given a DFA A with
LpAq ﬁnite, to decide if LpAq is pt, dq-partition-orderable (with the push-pop or Levenshtein
distance).

Push-pop-right distance. A natural restriction of the push-pop distance would be to only
allow editions at the right endpoint of the word, called the push-pop-right distance. A
d-ordering for this distance witnesses that the words of the language can be produced
successively while being stored in a stack, each word being produced after at most d edits.
Unlike the push-pop distance, one can show that some regular languages are not even
partition-orderable for this distance, e.g., a˚b˚ is not pt, dq-partition-orderable with any t, d P
N. The enumerable regular languages for this distance in fact correspond to the well-known
notion of slender languages. Recall that a regular language L is slender [20] if there is a
bound C P N such that, for each n ě 0, we have |L X Σn| ď C. It is known [20] that we can
test in PTIME if an input DFA represents a slender language. Rephrasing Result 2 from
the introduction, we can show that a regular language is enumerable for the push-pop-right
distance if and only if it is slender; further, if it is, then we can tractably compute the optimal
number t of sequences (by counting the number of diﬀerent paths to loops in the automaton),
and we can do the enumeration with bounded delay:

§ Theorem 6.2. Given a DFA A, the language LpAq is pt, dq-partition-orderable for the
push-pop-right distance for some t, d P N if and only if LpAq is slender. Further, if LpAq is
slender, we can compute in PTIME the smallest t such that LpAq is pt, dq-partition-orderable
for some d P N for the push-pop-right distance.

In addition, there is an algorithm which, given a DFA A for which LpAq is slender and
t “ 1, enumerates the language LpAq with push-pop-right distance bound 2k and linear delay
in |A|. Further, the sequence of edit scripts produced by the algorithm is ultimately periodic.

Of course, our results for the push-pop-right distance extend to the push-pop-left distance
up to reversing the language, except for the complexity results because the reversal of the
input DFA is generally no longer deterministic.

7

Conclusion and future work

We have introduced the problem of ordering languages as sequences while bounding the
maximal distance between successive words, and of enumerating these sequences with small
edit scripts to achieve bounded delay. Our main result is a PTIME characterization of the
regular languages that can be ordered in this sense for the push-pop distance (or equivalently
the Levenshtein distance), for any speciﬁc number of sequences; and a bounded-delay
enumeration algorithm for the orderable regular languages. Our characterization uses the

16

Enumerating Regular Languages with Bounded Delay

number of classes of interchangeable states of a DFA A for the language, which, as our results
imply, is an intrinsic parameter of LpAq, shared by all (trimmed) DFAs recognizing the same
language. We do not know if this parameter can be of independent interest.

Our work opens several questions for future research. The questions of orderability
and enumerability can be studied for more general languages (e.g., context-free languages),
other distances (in particular substitutions plus push-right operations, corresponding to the
Hamming distance on a right-inﬁnite tape), or other enumeration models (e.g., reusing factors
of previous words). We also do not know the computational complexity, e.g., of optimizing
the distance while allowing any ﬁnite number of threads, in particular for slender languages.
Another complexity question is to understand if the bounded delay of our enumeration
algorithm could be made polynomial in the input DFA rather than exponential, or what
delay can be achieved if the input automaton is nondeterministic.

References

1 Margareta Ackerman and Erkki Mäkinen. Three new algorithms for regular language enu-
meration. In ICCC, 2009. URL: https://maya-ackerman.com/wp-content/uploads/2018/
09/ThreeNewAlgorithmsForRegularLanEnum.pdf.

2 Margareta Ackerman and Jeﬀrey Shallit. Eﬃcient enumeration of words in regular lan-
guages. Theoretical Computer Science, 410(37), 2009. URL: https://maya-ackerman.com/
wp-content/uploads/2018/09/Enumeration_AckermanShallit_TCS.pdf.

3 Antoine Amarilli, Pierre Bourhis, Stefan Mengel, and Matthias Niewerth. Constant-delay
enumeration for nondeterministic document spanners. In ICDT, 2019. URL: https://arxiv.
org/abs/1807.09320.

4 Antoine Amarilli, Pierre Bourhis, Stefan Mengel, and Matthias Niewerth. Enumeration
on trees with tractable combined complexity and eﬃcient updates. In PODS, 2019. URL:
https://arxiv.org/abs/1812.09519.

5 Antoine Amarilli and Mikaël Monet. Enumerating regular languages with bounded delay. Full

version with proofs, 2023. URL: https://arxiv.org/abs/2209.14878.

6 Guillaume Bagan. MSO queries on tree decomposable structures are computable with linear

delay. In CSL, 2006.

7 Guillaume Bagan, Arnaud Durand, and Étienne Grandjean. On acyclic conjunctive queries
and constant delay enumeration. In CSL, 2007. URL: https://grandjean.users.greyc.fr/
Recherche/PublisGrandjean/EnumAcyclicCSL07.pdf.

8 Adrian Bondy and U.S.R. Murty. Graph Theory. Graduate Texts in Mathematics. Springer,

2008.

9 Ronald Fagin, Benny Kimelfeld, Frederick Reiss, and Stijn Vansummeren. Document spanners:
A formal approach to information extraction. J. ACM, 62(2), 2015. URL: https://pdfs.
semanticscholar.org/8df0/ad1c6aa0df93e58071b8afe3371a16a3182f.pdf.

10 Rainer Feldmann and Peter Mysliwietz. The shuﬄe exchange network has a Hamiltonian path.

11

12

Mathematical systems theory, 29(5), 1996.
Lukas Fleischer and Jeﬀrey Shallit. Recognizing lexicographically smallest words and computing
successors in regular languages. International Journal of Foundations of Computer Science,
32(06), 2021.
Fernando Florenzano, Cristian Riveros, Martin Ugarte, Stijn Vansummeren, and Domagoj
Vrgoc. Constant delay algorithms for regular document spanners. In PODS, 2018. URL:
https://arxiv.org/abs/1803.05277.

13 Alon Itai, Christos H Papadimitriou, and Jayme Luiz Szwarcﬁter. Hamilton paths in grid
graphs. SIAM Journal on Computing, 11(4):676–686, 1982. URL: http://www.cs.technion.
ac.il/~itai/publications/Algorithms/Hamilton-paths.pdf.
Jerome J. Karaganis. On the cube of a graph. Canadian Mathematical Bulletin, 11(2), 1968.

14

A. Amarilli and M. Monet

17

16

15 Wojciech Kazana and Luc Segouﬁn. Enumeration of monadic second-order queries on trees.
TOCL, 14(4), 2013. URL: https://hal.archives-ouvertes.fr/docs/00/90/70/85/PDF/
cdlin-survey.pdf.
Erkki Mäkinen. On lexicographic enumeration of regular and context-free languages. Acta Cy-
bernetica, 13(1):55–61, 1997. URL: http://cyber.bibl.u-szeged.hu/index.php/actcybern/
article/view/3479/3464.
Torsten Mütze. Proof of the middle levels conjecture. Proceedings of the London Mathematical
Society, 112(4):677–713, 2016. URL: https://arxiv.org/abs/1404.4442.

17

18 Torsten Mütze. Combinatorial gray codes—An updated survey. 2022. URL: https://arxiv.

19

20

21

22

org/abs/2202.01280.
pcpthm (https://cstheory.stackexchange.com/users/65605/pcpthm). Enumerating ﬁnite set
of words with Hamming distance 1. Theoretical Computer Science Stack Exchange. Version:
2022-07-02. URL: https://cstheory.stackexchange.com/q/51653.
Jean-Éric Pin. Mathematical foundations of automata theory. https://www.irif.fr/~jep/
PDF/MPRI/MPRI.pdf, 2019.
Jakub Radoszewski and Wojciech Rytter. Hamiltonian paths in the square of a tree. In ISAAC,
2011. URL: https://www.mimuw.edu.pl/~rytter/MYPAPERS/isaac2011_rytter.pdf.
Frank Ruskey. Combinatorial generation. Preliminary working draft, 2003. URL: https:
//page.math.tu-berlin.de/~felsner/SemWS17-18/Ruskey-Comb-Gen.pdf.

23 Milan Sekanina. On an ordering of the set of vertices of a connected graph. Publ. Fac. Sci.

Univ. Brno, 412, 1960.

24 Yann Strozecki et al. Enumeration complexity. Bulletin of EATCS, 3(129), 2019. URL:

http://eatcs.org/beatcs/index.php/beatcs/article/view/596.

25 Robert Endre Tarjan. A class of algorithms which require nonlinear time to maintain disjoint

sets. Journal of computer and system sciences, 18(2):110–127, 1979.

26 Takeaki Uno. Two general methods to reduce delay and change of enumeration algorithms.
Technical report, 2003. URL: https://www.nii.ac.jp/TechReports/public_html/03-004E.
pdf.

27 Kunihiro Wasa. Enumeration of enumeration algorithms. CoRR, 2016. URL: https://arxiv.

org/abs/1605.05102.

18

Enumerating Regular Languages with Bounded Delay

A

Proofs for Section 2 (Preliminaries)

A.1 Necessity of unbounded memory

We substantiate the claim that in general it is necessary for the memory usage to grow
indeﬁnitely. First note that, if the memory usage is bounded by some constant, then it is
clear that the enumeration is ultimately periodic: once the memory has reached its maximal
number of records, as there are only ﬁnitely many possible graph structures of pointers and
ﬁnitely many possible choices of data values (from a constant alphabet), then there are
only ﬁnitely many possible states of the memory, so considering the algorithm as a function
mapping one memory state to the next and optionally producing some values, this function
must be eventually periodic, and so is the output.

We show that, if the memory usage is bounded so that the sequence of edit scripts is
ultimately periodic, then we can only achieve slender languages (c.f. Section 6 for the formal
deﬁnition of slender languages). Hence, this is in general not suﬃcient (consider, e.g., pa ` bq˚,
which is enumerable but not slender).

§ Proposition A.1. Let L be a language achieved by a sequence of edit scripts which is
ultimately periodic. Then L is slender.

With regular languages, this proposition admits a converse: the regular slender languages
can be achieved by unions of ultimately periodic sequences of edit scripts (see Proposition E.7
which shows it for one term of the union).

We now prove the proposition:

Proof of Proposition A.1. Consider the ultimately periodic suﬃx u of edit scripts, and let
N be its length. Let ∆ be the diﬀerence in length between the size of the current word under
construction between the beginning and end of an occurrence of ∆, and let M be a value
such the length varies by at most M while reading u: speciﬁcally, if the length before reading
u is N , then the length after reading u is N ` ∆ and the intermediate lengths are between
N ´ M and N ` M .

If ∆ ď 0, then the language is ﬁnite as the lengths of words visited during the rest of
the enumeration when starting the periodic part at length N0 is upper bounded by N0 ` M .
Hence, it is in particular slender.

If ∆ ą 0, let us assume that we start the periodic part of the enumeration at a point where
the current size of the word minus M is greater than any word size seen in the non-periodic
part of the enumeration. Now, for any word length, we can only edit scripts producing
words of this length for p2M ` 1q ˆ |u| steps, which is constant, so we can only produce a
constant number of words of every length. This is the deﬁnition of a slender language, so we
conclude.
đ

A.2 Details about the machine model

We ﬁrst elaborate on the standard notion of a pointer machine from [25]. In this model, the
memory consists of a ﬁnite but unbounded collection of records. Each record consists of a
constant number of ﬁelds. The ﬁelds are either data values or pointers, and the number of
possible data values is also constant. The machine can allocate records in constant time
(retrieving a pointer to the new record), dereference a pointer in constant time to access a
ﬁeld of the corresponding record, test pointers for equality, and perform arbitrary operations
on the data values (as there is only a constant set of possible values). However, no arithmetic
operations involving pointers are permitted. In contrast with the RAM model, the pointer

A. Amarilli and M. Monet

19

machine model makes it possible to work with memory that grows in an unbounded fashion,
without worrying about how pointers are represented (they are assumed to take constant
space) and how the memory is addressed.

We next clarify how we represent the automaton given as input to the algorithm. The
automaton is given as a linked list of states, with a pointer to the initial state, and an
indication on each state of whether it is ﬁnal or not and a pointer to an adjacency list for
this state. The adjacency list of a state q is again a linked list containing one element for
each letter a of the alphabet Σ, in order. For each state q and letter a, the adjacency list
item contains a pointer to an object representing the letter a, which the machine can use
to output in constant time pushLpaq or pushRpaq and a pointer to the target state of the
transition (or an indication that the transition is undeﬁned).

B

Proofs for Section 3 (Interchangeability partition and orderability
lower bound)

§ Proposition 3.6. We can compute in polynomial time in A an interchangeability partition
A1, . . . , At of A, with t ď |A| the number of classes of interchangeable states.

To prove this result, we ﬁrst show the auxiliary claim stated in the main text:

§ Claim 3.8. The language LpAq can be partitioned as NLpAq and LpA, C1q, . . . , LpA, Ctq over
the classes C1, . . . , Ct of interchangeable states, and further NLpAq is ﬁnite.

Proof. We ﬁrst show that these sets are pairwise disjoint. This is clear by deﬁnition for NLpAq
and LpA, Cq for any class C of interchangeable states. Further, for C ‰ C1, we also have
that LpA, Cq X LpA, C1q “ H, because for any word w in their intersection, considering the
accepting run of w, it must go via a loopable state q of C and a loopable state q1 of C1, so this
run witnesses that q and q1 are connected, hence interchangeable, which would contradict
the fact that C and C1 are in diﬀerent classes.

We next show both inclusions. By deﬁnition, a word of LpAq is either loopable, hence in
some LpA, Cq, or non-loopable and in NLpAq. For the converse inclusion, by deﬁnition again
all words of NLpAq and LpA, Cq for any class C are in LpAq.

Last, the fact that NLpAq is ﬁnite is because any non-loopable word w has length at
most |A| ´ 1, because otherwise by the pigeonhole principle the same state q would appear
twice in its accepting run, witnessing that q is loopable and contradicting the fact that w is
non-loopable. Thus, there are ﬁnitely many words in NLpAq.
đ

We are now ready to prove Proposition 3.6:

Proof of Proposition 3.6. Given the automaton A with state space Q, we ﬁrst materialize
in PTIME the connectivity and compatibility relations by naively testing every pair of states.
Note that to test the compatibility of two states q and q1 we build the automaton Aq and
Aq1, build their intersection automaton by the product construction, and check for emptiness,
which is doable in PTIME. We then materialize the interchangeability relation, and compute
the classes C1, . . . , Ct. Note that t ď |Q|.

The high-level argument is that we create one copy of A for each class Ci, but ensure
that only the ﬁrst copy accepts the non-loopable words. For this, we need to duplicate
the automaton in two copies, so as to keep track of whether the run has passed through a
loopable state or not, and ensure that in all automata but the ﬁrst, the states of the ﬁrst
copy (where we accept the non-loopable words) are non-ﬁnal.

20

Enumerating Regular Languages with Bounded Delay

Formally, we modify A to a DFA A1 by doing the following: the new state space is
Q ˆ t0, 1u, the ﬁnal states are the pq, bq for q ﬁnal in A and b P t0, 1u, the initial state
is pq0, bq for q0 the initial state of A and b being 1 if q0 is loopable (which happens only
when NLpAq “ H) and 0 otherwise, and the transitions are:

For every q P Q and a P Σ such that δpq, aq is non-loopable, the transitions δppq, bq, aq “
pδpq, aq, bq for all b P t0, 1u
For every q P Q and a P Σ such that δpq, aq is loopable, the transitions δppq, bq, aq “
pδpq, aq, 1q for all b P t0, 1u.

We then trim A1. Note that if q0 is loopable in A then this in fact removes all states of
the form pq, 0q: indeed no such state is reachable from pq0, 1q, simply because there are no
transitions from states with second component 1 to states with second component 0.

Clearly the DFA A1 is such that LpA1q “ LpAq. However, A1 intuitively keeps track of
whether a loopable state has been visited, as is reﬂected in the second component of the
states. Further, the loopable states of A are in bijection with the loopable states of A1 by
the operation mapping a loopable state q of A to the state pq, 1q of A1. Indeed, pq, 1q is
clearly a loopable state of A1, and this clearly describes the loopable states of A1 with second
component 1. Further, the states of A1 with second component 0 are non-loopable because
any loop involving such a state pq, 0q would witness a loop on the corresponding states of A,
so that they would be loopable in A, and each transition of the loop in A1 would then lead
by deﬁnition to a state of the form pq1, 1q, which is impossible because no transition can then
lead back to a state of the form pq2, 0q. Last, the interchangeability relationship on A1 is the
same as that relationship on A up to adding the second component with value 1, because
all loopable states of A1 all have that value in their second component as we explained, and
the states and transitions with value 1 in the second component are isomorphic to A so the
compatibility and connectivity relationships are the same.

We now compute copies A1, . . . , At of the DFA A1, and modify them as follows: for each
1 ď i ď t, we remove from Ai the states pq, 1q for each q in a class Cj with j ‰ i. Further,
for i ą 1, we make non-ﬁnal all states with second component 0 (but leave these states ﬁnal
in A1).

This process is in polynomial time. Further, we claim that each Ai is interchangeable.
Indeed, its loopable states are precisely the loopable states of A that are in Ci (up to adding
the second component with value 1), because removing states from diﬀerent classes does not
change the loopable or non-loopable status of states in Ci. Further, for any two loopable
states q and q1 of Ci (modiﬁed to add the second component with value 1), the fact that they
are interchangeable in A was witnessed by a sequence q “ q0, . . . , qn “ q1 with qj and qj`1
being either connected or compatible in A for all 0 ď j ă n. Note that when two states qi and
qi`1 are connected, then there is a sequence of loopable states qi “ q1
where any
two consecutive loopable states q1
i,j`1 for 0 ď j ă ni are connected via a (directed)
path consisting only of non-loopable states, which we call immediately connected. Hence, up
to modifying the previous path to a longer path q “ q0, . . . , qm “ q1, we can assume that two
successive loopable states qj and qj`1 with all 0 ď j ă n are either compatible or immediately
connected. Now, the path witnesses that all qj for 0 ď j ď m are also in Ci (up to adding
the second component). Further, for any two loopable states in A that are compatible or
immediately connected in A, this compatibility or immediate connectivity relation still holds
when removing loopable states that are not in Ci (up to adding the second component).
Thus, the same path witnesses that the loopable states q and q1 are still interchangeable
in Ai. This implies that indeed Ai is interchangeable, as all its loopable states are in the
same class, which is exactly Ci (up to changing the second component).

i,j and q1

i,0, . . . , q1

i,ni

A. Amarilli and M. Monet

21

It remains to show that LpA1q, . . . , LpAtq is a partition of LpAq. For this, we show that
LpA1q “ LpA, C1q Y NLpAq and LpAiq “ LpA, Ciq for i ą 1; this suﬃces to conclude by
Claim 3.8.

We ﬁrst show that LpA, Ciq Ď LpAiq for 1 ď i ď t. This is because the accepting run
of a word of LpA, Ciq must go through a state of Ci, and we have argued in the proof of
Claim 3.8 that it does not use any state of Cj for j ‰ i. Thus, we can build a corresponding
path in Ai, where the second component of each state is 0 until we reach the ﬁrst state
of Ci in the accepting run, and 1 afterwards. Note that if the initial state is in Ci then we
deﬁned the initial state of A1 to have second component 1, so the path can start with second
component one. In particular, letting q the ﬁnal state reached in A by the accepting run,
the corresponding run in Ai reaches pq, 1q, which is ﬁnal, and the word is accepted by Ai.

We then show that NLpAq Ď LpA1q. This is because the accepting run of a non-loopable
word of A goes only through non-loopable states, hence does not go through states of Cj
with j ‰ 1, hence we can reﬂect the accepting path in A1 and reach a ﬁnal state, witnessing
that the word is accepted.

Now, we show that LpAiq Ď LpA, Ciq for i ą 1. This is because an accepting run for a
word w of LpAiq must ﬁnish by a state of the form pq, 1q where q is ﬁnal, as these are the
only ﬁnal states of Ai. Thus, there is an accepting path for the word in A, and w P LpAq.
Further, the construction of A1, hence of Ai, guarantees that, if we reach a state with second
component 1, then we have gone via a state pq, 1q with q loopable in A. The construction
of Ai further guarantees that this q must be in Ci. Considering the accepting path of w in A,
we see that this path goes through the state q, so that w P LpA, Ciq.

Last, we show that LpA1q Ď LpA, C1q Y NLpAq. Considering a word w P LpA1q and its
accepting run, there are two possibilities. Either the run ends at a state of the form pq, 1q, in
which case the same reasoning as in the previous paragraph shows that w P LpA, C1q. Or
the run ends at a state of the form pq, 0q. In this case, the construction of A1, hence of A1,
guarantees that the entire accepting path only goes via states of the form pq1, 0q, so the
corresponding run in A only goes via non-loopable states and w P NLpAq.

The claims that we established together with Claim 3.8 show that LpA1q, . . . , LpAtq is
đ

indeed a partition of LpAq, concluding the proof.

§ Proposition 3.10. Letting C1, . . . , Ct be the classes of A, for any distance d P N, there
is a threshold l P N such that for any two words u P LpA, Ciq and v P LpA, Cjq with i ‰ j
and |u| ě l and |v| ě l, we have δLevpu, vq ą d.

Proof. Fix the distance d P N, and let k be the number of states of the automaton A. We
prove that the claim holds for the threshold l :“ k ˆ pkpd ` 1q ` 1q.

Assume by contradiction that there exist words u P LpA, Ciq and v P LpA, Cjq with i ‰ j
such that |u| ě l, |v| ě l and δpu, vq ď d. By the pigeonhole principle, there must be a
state q occurring in at least kpd ` 1q ` 1 positions in the accepting run for u, i.e., we can
write u “ hu1 ¨ ¨ ¨ ukpd`1qt for some h and t where each ui starts and ends at q. This implies
in particular that q is loopable so it is in the class Ci as explained in the proof of Claim 3.8.
As δpu, vq ď d, consider an arbitrary edit script that transforms u into v using at
most d edit operations of the Levenshtein distance. Observe then that some contiguous
sequence of k of the uj’s must be untouched by these edit operations. That is, we can write
v “ h1ui ¨ ¨ ¨ ui`k´1t1 for some h1 and t1.

Now, by the pigeonhole principle again, in the accepting run of v, of the k ` 1 states
at the endpoints of the ui, . . . , ui`k´1, two must be the same, say q1. So we can write
v “ h2ul ¨ ¨ ¨ urt2 for some h2 and t2 and i ď l ă r ď i ` k ´ 1, where in the accepting run

22

Enumerating Regular Languages with Bounded Delay

1

11

2

21

0

3

31

4

41

5

51

Figure 2 Tree used in the proof of Claim C.2

of v we have the same state q1 at the beginning and end of ul ¨ ¨ ¨ ur. We know in particular
that q1 is loopable, so it is in Cj, again using the argument in the proof of Claim 3.8.

But now the word ul ¨ ¨ ¨ ur is the label of a path in A from q1 to q1 as seen on the accepting
run of v in the previous paragraph, and it is also the label of a path in A from q to q, as
seen on the accepting run of u earlier. Thus, the states q and q1 are compatible, and they
are in the same class, which contradicts the fact that q and q1 were in diﬀerent classes. This
concludes the proof.
đ

From Proposition 3.10, we can show Theorem 3.9:

Ů

Proof of Theorem 3.9. We proceed by contradiction and assume that LpAq is pt ´ 1, dq-
partition-orderable for some bound d ą 0 on the Levenshtein distance, i.e., we have partitioned
1ďiďt´1 Li where each Li is d-orderable, i.e., has a d-sequence si. Let l be the
LpAq “
threshold given by Proposition 3.10, let l1 be the maximal length of a non-loopable word
in NLpAq, which is ﬁnite by Claim 3.8, and let l2 “ maxpl, l1q. There is some index p ą 0
such that, for all 1 ď i ď t ´ 1, all words of length ď l2 of LpAq are only present in the initial
preﬁx of length p of the si, in particular this is the case of all non-loopable words. Now, as
there are t ´ 1 d-sequences and t classes, and as the languages of each of the t classes are
inﬁnite, there must be a d-sequence which after point p contains inﬁnitely many words from
two diﬀerent subsets LpA, Ciq and LpA, Cjq with i ‰ j. In particular, there must be an index
p1 ą p such that the p1-th word of si is a word of LpA, Ciq of length ą l2, hence ě l, and the
pp1 ` 1q-th word of s is a loopable word of LpAq also of length ě l2 which is not in LpA, Ciq,
hence it is in some LpA, Ckq with k ‰ i. However, we know by Proposition 3.10 that the
words in LpA, Ciq of length ě l are at Levenshtein distance ą d from the words of LpA, Ckq
of length ě l. This contradicts the fact that si is a d-sequence, and concludes the proof.

We last substantiate the remark made in the main text that the proof still applies if we
allow each word to be repeated some constant number C of times. The proof above works
as-is, except that we now deﬁne p ą 0 to be the smallest index after which there are no
longer any occurrences of any word of length ď l2 in the remainder of all sequences si: as
there are only ﬁnitely many words occurring ﬁnitely many times, this is well-deﬁned. The
rest of the proof is unchanged.
đ

C

Proofs for Section 4 (Orderability upper bound)

C.1 Proof of Lemma 4.3

§ Lemma 4.3. Let L be a ﬁnite language that is d-connected and s ‰ e be words of L. Then
there exists a 3d-ordering of L starting at s and ending at e.

The result follows from the following claim on trees:

A. Amarilli and M. Monet

23

§ Lemma C.1 ([23, 14]). Let T be a tree, i.e., an acyclic connected undirected graph. Let s ‰ e
be arbitrary nodes of T . We can compute in linear time in T a sequence s “ n1, . . . , nm “ e
enumerating all nodes of T exactly once, starting and ending at s and e respectively, such
that the distance between any two consecutive nodes is at most 3.

As we explained in the main text, Lemma C.1 was already shown independently by
Sekanina and by Karaganis [23, 14]. However, we give our own algorithmic proof of this
result, to emphasize the fact that the Hamiltonian path in question can be computed in
linear time: we will need this fact in Section 5. Our algorithmic proof uses the method of
exploring trees and enumerating nodes diﬀerently depending on the parity of their depth
(see, e.g., [26]). The same algorithm can be easily derived from the proof of [14], by turning
the inductive argument into a recursive algorithm.

Optimality of the constant 3. Before giving our proof of Lemma C.1, we show that the
value 3 in this lemma is optimal, as was claimed in the main text:

§ Claim C.2. There is an acyclic connected undirected graph T such that any sequence
enumerating all nodes of T exactly once must contain a pair of vertices at distance ě 3.

Claim C.2 is implied by the results of [21] giving a (non-trivial) characterization of the trees
for which it is possible to achieve a distance of 2 instead of 3. A slightly weaker result
(applying to Hamiltonian cycles, corresponding in our setting to sequences that additionally
have to start and end at vertices which are at distance ď 3) is also given as Exercise 4.1.19 b
in [8]. We again give a self-contained proof for convenience:

Proof of Claim C.2. Consider the tree pictured on Figure 2, and some sequence n1, . . . , n11
enumerating its nodes. Assume by contradiction that the sequence achieves a distance ď 2,
i.e., all pairs of consecutive vertices in the sequence is at distance ď 2. Consider the vertices
11, 21, 31, 41, and 51 of the tree. The ﬁrst and last position of the sequence may be covering
at most two of these vertices, so there must be at least three of these values that occur at
positions of the sequence which are not the ﬁrst or last position. Without loss of generality,
we assume up to symmetry that these are 11, 21, and 31, occurring in this order, and we
write l, m, r the positions where they occur in the sequence. In other words, we have nl “ 11,
nm “ 21, and nr “ 31, for some positions 2 ď l ă m ă r ď 10.

Now, given that nl “ 11 and nm “ 21 and the distance between 11 and 21 in the graph
is four, we know that these values cannot be consecutive, i.e., we must have m ´ l ě 2.
For the same reason, we must have r ´ m ě 2. From this, we deduce that the indices
l ´ 1, l, l ` 1, r ´ 1, r, r ` 1 are pairwise distinct and are all indices in t1, . . . , 11u.

Now, as nl “ 11, we know that the only possibility to respect the distance bound is that
one of nl´1, nl`1 is 0 and the other is 1. Likewise, one of nr´1 and nr`1 is 0 and the other
is 3. As 0 occurs only once and the indexes are pairwise distinct, this is a contradiction.
đ

Now, to justify that the constant 3 is optimal, not only in Claim C.2, but also in
Lemma 4.3, it suﬃces to notice that the tree used in the proof (Figure 2) can be realized
in the proof, with the push-pop distance or push-pop-right distance. Indeed, consider for
instance the language L “ t(cid:15), a, aa, b, bb, c, cc, d, dd, e, eeu. This language is 1-connected, and
the graph connecting the nodes at edit distance 1 in the push-pop or (equivalently) the
push-pop-right distance is exactly the tree of Figure 2, so while it has a 3-ordering we know
that it cannot have a 2-ordering. For the Levenshtein distance, we replace (cid:15) by xxxx, replace
a by axxxx and a2 by a2xxxx, replace b by xbxxx and b2 by xb2xxx, and so on. The
language is 1-connected, and the graph connecting the nodes at distance 1 in the Levenshtein

24

Enumerating Regular Languages with Bounded Delay

distance is again exactly the tree of Figure 2. This is because no edit can insert, remove, or
substitute an x (it would not give a word of the language because the number of x-es would
not be correct); clearly the tree root is connected to its children (by an insertion) and each
child to its respective child (by an insertion again); and no other connections are possible (in
particular substitutions applied to words of the language never give a word of the language).

Proving the result. We now prove Lemma C.1:

Proof of Lemma C.1. We root T at the starting node s, yielding a rooted tree where s is
the root and e is an arbitrary node diﬀerent from s. We call special the nodes in the unique
path from s to e, including s and e. We order the children of each special node except e to
ensure that their unique special child is the last child.

We ﬁrst explain how to enumerate all nodes of T except e and its descendants, while
obeying the distance requirement. To do this, we will handle diﬀerently the nodes depending
on (the parity of) their depth in T . We will ensure that the enumeration starts at s, and
ends:

If e is at even depth, then we ﬁnish at the parent of e.
If e is at odd depth and was the only child of its parent, then we ﬁnish at the parent of e.
If e is at odd depth and its parent has other children, then we ﬁnish at a sibling of e.
To do this, we start processing at the root, where processing a node n means the following:
For nodes n at even depth (in particular n “ s), no matter whether they are special
or not, we process them in preﬁx order: enumerate n, then recursively processing their
children in order.
For non-special nodes n at odd depth, we process them in postﬁx order: recursively
processing their children in order, then enumerate n.
For special nodes n at odd depth, we process them in a kind of inﬁx order: recursively
process their children in order except the last (if any), then enumerate n, then recursively
process their last child (which is again special, or is e).

We ﬁnish this process when we recursively process e. Then it is clear that the enumeration
starts at s, and visits all nodes of T except e and its children. We check that it ﬁnishes at
the right node:

If e is at even depth, then its parent n was a node at odd depth which was special. We
ﬁnished the enumeration by recursively processing n, processing all other children of n,
then enumerating n, so indeed we ﬁnish at the parent of n.
If e is at odd depth, then its parent n was a node at even depth. We ﬁnished the
enumeration by recursively processing n. Then there are two cases:

If e was the only child of n, then the processing of n enumerated n and then recursed
on e, so the claim is correct.
If e was one of the children of n, then it was the last child, so the processing of n
enumerated n, then recursively processed all its other children, before recursively
processing e. Now, the other children were non-special nodes at odd depth, so when
we processed the last children, the enumeration ﬁnished by enumerating that other
child, which is a sibling of e as claimed.

We then check that this enumeration respects the distance condition, by checking that

any two consecutive nodes in this enumeration are at distance less than 3.

When we enumerate a node n at even depth, then we have just started to process it; let
us study what comes after n. Either n has children or it does not:

A. Amarilli and M. Monet

25

If n has no children, then n is not the root s. Then the parent n1 of n is a node at odd
depth. Either it is special or non-special. If n1 is non-special, then either n was the
last child or not. If n was the last child, we next enumerate n1, so the distance is 1.
If n was not the last child, we next enumerate the next child of n1, which is at even
depth and is not e, and the distance is 2. If n1 is special, then n was not the last child
because the last child of a special node diﬀerent from e has children (as the special
nodes are the nodes in the path from the root to e). Either n was the penultimate
child or not. If it is not the penultimate child, then we next enumerate the next child
of n1 and the distance is 2. If it is the penultimate child, then we next enumerate n1
and the distance is 1.
If n has children, then if its only child is e we have ﬁnished. Otherwise, we will produce
another node. Speciﬁcally, we next recurse on the ﬁrst child n1, which is not e, and
either we ﬁrst enumerate n1 (if it has no children, or is a special node whose only child
is e) and the distance is 1, or we enumerate the ﬁrst child of n1 (which is at even depth)
and the distance is 2.

When we enumerate a node n at odd depth which is non-special, then we have just
ﬁnished processing it. Then its parent n1 was at even depth. Either n was the last child
of n1 or not:

If n was the last child of n1, we go back to the parent n2 of n1, which is at odd depth.
If n1 was the last child of n2, we enumerate n2, at distance 2. If n1 was not the last
child of n2 but was its last non-special child, then n2 itself is special and we next
enumerate n2, at distance 2 (before recursing in the special child). Last, if the next
sibling of n1 is non-special, then from n2 we recurse into it (it is at even depth) and
enumerate it, for a distance of 3.
If n was not the last child of n1, then we next recurse in the next child n2, which
is at odd depth. If n2 has no other children, or if its only child is special, we next
produce n2 for a distance of 2. Otherwise we recurse in the ﬁrst child of n2, which is
at even depth, and produce it for a distance of 3.

When we enumerate a node n at odd depth which is special, then we are about to recurse
in its special child. Either it is e and we have ﬁnished, or it is a node at even depth and
we enumerate it for a distance of 1.

Hence, in all cases the distance within that ﬁrst part of the enumeration is at most 3.

We now explain how to enumerate the remaining nodes, i.e., the subtree of T rooted
at e, including e. For this, we consider the depth of the nodes from the parent of e, i.e., e is
now considered at odd depth, its children (if they exist) are at even depth, and so on. We
re-do the previously described enumeration scheme on that subtree, except that there are no
special nodes. It is clear that this deﬁnes an enumeration sequence which visits the entire
subtree, ends at e (as it is at odd depth), and respects the distance bound from the previous
proof. (More speciﬁcally, considering a subtree of T rooted at a non-special node n at odd
depth in the previous proof, when we started processing that subtree, the enumeration clearly
covered all its nodes, ﬁnished at n, respected the distance bound, and there were no special
nodes in that subtree, so this shows that correctness also holds when applying that scheme
on the subtree rooted at e.)

Hence, the last point to show is that the distance between the last node of the ﬁrst part
of the enumeration and the ﬁrst node of the second part of the enumeration is at most 3.
Now, the ﬁrst part ended either at the parent of e or a sibling of e, i.e., at distance 2 to e.
Now, the second phase of the enumeration starts on e at odd depth, so either e was in fact a

26

Enumerating Regular Languages with Bounded Delay

leaf and we enumerate it for a distance 2, or we recurse on the ﬁrst child of e, which is now
at even depth, and enumerate it, for a total distance of 3. Hence, the entire enumeration
respects the distance bound of 3. This concludes the proof.
đ

Last, we can easily prove Lemma 4.3 from Lemma C.1:

Proof of Lemma 4.3. Consider the (ﬁnite) undirected graph G “ pL, ttu, vu | δpu, vq ď du.
Since L is d-connected, G is connected (in the usual sense) and so it has a spanning tree T .
Now, applying Lemma C.1 on T gives us a sequence enumerating exactly once every vertex
of G starting and ending at the requisite nodes, such that the distance between any two
nodes in the sequence is at most 3 in T , hence at most 3 in G, so the distance between the
words is at most 3d.
đ

C.2 Proof of Lemma 4.6

§ Lemma 4.6. Let L be an inﬁnite language recognized by a DFA with k1 states, and assume
that L is p‘, dq-stratum-connected for some ‘ ě 2k1 and some d ě 3k1. Then L is 3d-orderable.

Proof. A p‘, dq-ladder of L consists of two inﬁnite sequences e1, . . . , en, . . . of exit points, and
s2, . . . , sn, . . . of starting points, such that ei P strat‘pL, iq for all i ě 1 and si P strat‘pL, iq
for all i ě 2, and such that si ‰ ei for all i ě 2 and δpppei, si`1q ď d. We will show that
such a ladder exists, which will suﬃce to show that L is 3d-orderable. Indeed, because
each p‘, dq-stratum is d-connected, we know by Lemma 4.3 that there exists a 3d-ordering s1
of strat‘pL, 1q that ends at e1 (and starts at an arbitrary word of strat‘pL, 1q), and also,
for i ě 2, that there exists a 3d-ordering si of strat‘pL, iq that starts at si and ends at ei.
Now, the order s0s1 . . . is clearly a 3d-order of L.

Hence, let us show that a ladder exists. As L is inﬁnite, we know that the DFA A has a
loopable state q. Let rz be a word of q such that z is the label of a simple loop on a loopable
state on q: by the pigeonhole principle, we can ensure |rz| ď k1. Now, using the fact that
the DFA is trimmed, let t be a word such that rt is accepted: we can ensure |t| ă k1. Now
consider the sequence of words wi :“ rzit for all i ě 0, which are all accepted. By deﬁnition
the length diﬀerence between wi and wi`1 is at most k1 for each i ě 0, and the push-pop
distance between them is at most 3k1, as evidenced by popping right t and pushing right zt;
so it is ď d.

We now observe that, as ‘ ě 2k1, each stratum contains at least two distinct wi:
in
particular the rt and rzt are words of the ﬁrst stratum because their length is strictly less
than 2k1. For each stratum i ě 1, we choose si to be the shortest wj such that wj is
in strat‘pL, iq, and choose ei to be the longest such wj. Thus, ei ‰ si for all i ě 2, and
δpppei, si`1q ď d for all i ě 1. We have thus deﬁned a ladder, which as we argued concludes
the proof.
đ

C.3 Proof of Proposition 4.7

We now prove the main technical result of this section, namely:

§ Proposition 4.7. The language LpAq is p8k2, 16k2q-stratum-connected.

We let p‘, dq :“ p8k2, 16k2q. We show this result in the remainder of the appendix. If LpAq
is ﬁnite, then clearly it consists only of words of length ď k, so the ﬁrst stratum is trivially
d-connected because d ě 2k and the other strata are empty, hence vacuously d-connected.

A. Amarilli and M. Monet

27

Thus, in the sequel, we assume that LpAq is inﬁnite, in particular it contains inﬁnitely many
loopable words.

To show that LpAq is d-stratum connected, take i ě 1 and let us show that the i-th
‘-stratum S “ strat‘pLpAq, iq is d-connected. This will indeed be enough by Lemma 4.6.
In the rest of the proof we only work with the language S, so d-sequences, d-connectivity
between words, always require that the words of the sequences are in S.

Let us ﬁrst take care of the non-loopable words, and show that each non-loopable word is
at distance ď d from a loopable word of S. For this, taking a non-loopable word w, we know
from the proof of Claim 3.8 that |w| ď k ´ 1. From the deﬁnition of ‘ (speciﬁcally, since
‘ ě k ´ 1), we know that we are in the ﬁrst stratum, i.e., i “ 1. As d ě 2k, we can simply
edit w into a loopable word by ﬁrst removing all letters of w, then adding all letters of some
loopable word of length at most k ´ 1, which must exist because LpAq is inﬁnite and we can
simply choose any word whose accepting path goes via some loopable state and does not go
twice through the same state.

We will now show the result statement: any two words u and v of S are d-connected. By

what we showed, we can assume that u and v are loopable.

It will often be necessary to adjust our constructions depending on the length of a
word w P S, because when modifying w we wish to ensure that it remains in S, in particular
that it has the right length. So we will call a word w P S short if it is in the lower half
of the stratum, and long otherwise. Formally, considering a word w1 of S, we know that
its length is pi ´ 1q ˆ 8k2 ď |w1| ă i ˆ 8k2, i.e., p2i ´ 2q ˆ 4k2 ď |w1| ă 2i ˆ 4k2: we call
valid a word satisfying this condition on length, so that a word is in S iﬀ it is valid and is
accepted by A. Now, we call w1 short if p2i ´ 2q ˆ 4k2 ď |w1| ă p2i ´ 1q ˆ 4k2 and long if
p2i ´ 1q ˆ 4k2 ď |w1| ă 2i ˆ 4k2. This ensures that increasing the length of a short word by
at most k2, or by at most 4k, gives a valid word (which may be long or short); and likewise
decreasing the length of a long word by at most k2 or by at most 4k gives a valid word
(which may also be long or short). Further the deﬁnition ensures that ‘ ě 7k, which we will
use at some point as a bound on the size of the ﬁrst stratum.

We will ﬁrst show that all loopable words of S are d-connected to loopable words of S of
a speciﬁc form, where, up to a preﬁx and suﬃx whose length only depends on the language,
the words consists of a power of a word which labels a loop on some loopable state (on which
we must impose a length bound). We will apply this to loops of length at most k2, because
of our choice of ‘.

§ Lemma C.3. Let w be a loopable word of S, let q be a loopable state that occurs in the run
of w, let e be the label of a loop on q, i.e., a non-empty word of LpAqq, such that |e| ď k2.
Then w is d-connected to some word sent with |s| ď k and |t| ď k.

To show the lemma, we will ﬁrst show that we can “move” the latest occurrence of q
near the end of the word by making edits of length at most 2k. To this end, we deﬁne the
q-measure of a word w1 and a state q occurring in w1 as the smallest possible length of t1
when writing w1 “ r1t1 such that q is reached between r1 and t1. We then claim:

§ Claim C.4. Any loopable word w of S where some loopable state q occurs is 2k-connected
to some word w1 of q-measure ď k.

Proof. We show the claim by induction on the measure of w. The base case is when the
measure is ď k, in which case the result is immediate. So let us show the induction claim:
given w with measure ą k, we will show that it is 2k-connected to some word (in S) with
strictly smaller measure. Note that we can assume that |w| ě k, as otherwise its measure
is ă k.

28

Enumerating Regular Languages with Bounded Delay

Intuitively, we want to de-pump a simple loop in the suﬃx of length k to decrease the
measure, but if the word is short we ﬁrst want to make it long by pumping some loop as
close to the beginning as possible. So the ﬁrst step is to make w long if it is short, and then
the second step is to decrease the measure (potentially making the word short again) and
conclude by induction hypothesis.

The ﬁrst step works by repeating the following process, which does not increase the
measure. Formally, as w contains a loopable state and the non-loopable states occur at most
once, we can write w “ ρτ with the length |ρ| of ρ being minimal (in particular ď k) such
that the state between ρ and τ is loopable. Let σ be a simple loop on q, i.e., σ P LpAqqzt(cid:15)u.
Observe that then |ρσ| ď k. By pumping, we know that ρστ is accepted by A. We obtain
ρστ by editing w “ ρτ in the following way: we pop left elements of ρ, i.e., at most k edits,
then push ρσ, i.e., at most k edits. Note that, writing the original w as s1t1 with q reached
between s1 and t1 and |s1| being maximal, the modiﬁcation described here modiﬁes a preﬁx of
the word which is no longer than s1, and makes s1 longer (thus making w longer). Repeating
this process thus gives us a word w1 which is no longer short, and where the measure is
unchanged because σ was added on the ﬁrst occurrence of a loopable state, hence on or
before the last occurrence of q. As we enlarge the word at each step by at most |σ| ď k,
and stop it when the word is no longer short, the deﬁnition of being short ensures that the
eventual result w1 of this process is not too big, i.e., w1 is still valid; and w is 2k-connected
to it.

Now, the second step on a word w1 which is long is to make the measure decrease. For
this, we simply write w1 “ r1t1 with |t1| the measure of w1. If |t1| ď k, we conclude by the
base case. Otherwise, by the pigeonhole principle, we can ﬁnd two occurrences of the same
state in the run within the suﬃx t1; we write w1 “ ρστ with |στ | ď k as small as possible
(i.e., we take the ﬁrst occurrence of a repeated state when reading the run from right to left)
such that some state q1 occurs between ρ and σ and between σ and τ . Note that q1 ‰ q,
otherwise we would have concluded by the base case. By depumping, we know that ρτ is
accepted by the automaton, and as q ‰ q1 it has strictly smaller q-measure. We obtain ρτ
from w1 “ ρστ in a similar way to that of the previous paragraph: we pop right elements
of στ , i.e., at most k edits, then we push right the elements of τ , i.e., at most k edits. The
length of the resulting word is decreased by σ, i.e., by at most k, so the deﬁnition of being
long ensures that the resulting word is in S. Thus, we know that w1, hence w is 2k-connected
to a word of S with smaller q-measure. We conclude by the induction hypothesis that w is
2k-connected to a word with q-measure ď k, which concludes the proof.
đ

With this claim, we can show Lemma C.3:

Proof of Lemma C.3. Fix the word w and the loopable state q. By Claim C.4, we know
that w is 2k-connected to a word w1 of S with w1 “ r1t1, |t1| ď k, and q is achieved between r1
and t1. Fix e the label of the loop to achieve. We will intuitively repeat two operations,
starting with the word w1, depending on whether the current word is long or short, with
each word obtained being a word of S that is at distance ď d from the previous one. We
stop the process as soon as we obtain a word of the desired form.

If the current word is short, we add a copy of e just before t1, by popping t1 and then
pushing et1. This takes 2k ` |e| operations, which is ď d because |e| ď k2, and we know that
the result is accepted by the automaton. Further, as the length increased by |e| and |e| ď k2,
the deﬁnition of ‘ ensures that the word is still in S (but it may be long).

If the current word is long, we make it shorter by editing the left endpoint. Formally, as
a long word has length ě k, we can write w1 “ ρτ with |ρ| ď k, and furthermore ρ does not

A. Amarilli and M. Monet

29

overlap the copies of e that have been inserted so far, otherwise the word would already be
of the desired form. By the pigeonhole principle, there is a state q1 occurring twice in ρ, i.e.,
we can write ρ “ ρ1σρ2, such that ρ1ρ2τ is accepted by the automaton, and by taking q1 to
be the ﬁrst such state we can have |ρ1σ| ď k. We can obtain ρ1ρ2τ from w1 by popping left
ρ1σ and pushing left ρ1, i.e., at most 2k operations. Further, as the length decreased by at
most k, the deﬁnition of ‘ ensures that the word is still in S (but it may be short).

It is clear that repeating this process enlarges a factor of the form ei inside the word
(speciﬁcally, the word ends with eit with i increasing), while the word remains in S, so we
will eventually obtain a word of S of the form sent with |s| ď k and |t| ď k to which the
word w1, hence the original word w, is d-connected. This establishes the claimed result.
đ

Now that we have established Lemma C.3, we have a kind of “normal form” for words,

i.e., we will always be able to enforce that form on the words that we consider.

Let u and v be two loopable words, and let p and q be any loopable states reached in the
accepting run of u and v respectively. We know that the automaton A is interchangeable,
so p and q are interchangeable, and there exists a sequence p “ q0, . . . , qh “ q witnessing
this, with any two successive qi and qi`1 for 1 ď i ă h being either connected or compatible.
We show that u and v are d-connected by induction on the value h.

Base case h “ 0. The base case is h “ 0, i.e., p “ q. In this case, let e be an arbitrary
non-empty word in Aq of length ď k. We know by applying Lemma C.3 to u that u is
d-connected to a word of the form u1 “ sent with |s| ď k and |t| ď k, and by applying the
lemma to v we know that v is d-connected to a word of the form v1 “ xemy with |x| ď k
and |y| ď k. We now claim the following, to be reused later:

§ Claim C.5. Consider words of S of the form u1 “ su2t and v1 “ xv2y with |s| ď k and
|t| ď k and |x| ď k and |y| ď k and u2 and v2 both being powers of some word λ. Then
δpppu1, v1q ď ‘ ` 8k.

Proof. First note that, because u2 and v2 are powers of a common word, we have δpppu2, v2q “
||u2| ´ |v2||, simply by making their length equal by adding or removing powers of λ. Now,
the distance from u1 to v1 is at most |s| ` |t| ` |x| ` |y| ` ||u2| ´ |v2||, by popping s and t,
then making the length of the middle parts u2 and v2 equal via the previous observation,
then pushing x and y. As the two words u1 and v1 are in S, their length diﬀerence is at
most ‘, i.e., ||u1| ´ |v1|| ď ‘. Now |u1| “ |s| ` |t| ` |u2| and |v1| “ |x| ` |y| ` |v2|, so by the
triangle inequality the length diﬀerence ||u2| ´ |v2|| between the middle parts u2 and v2 is
at most ‘ ` |s| ` |t| ` |x| ` |y|, so δpppu1, v1q ď ‘ ` 2p|s| ` |t| ` |x| ` |y|q, i.e., at most ‘ ` 8k,
establishing the result.
đ

Now, applying Claim C.5 to our u1 and v1 with λ “ e, we know that δpppu1, v1q ď ‘ ` 8k,
so the distance is at most d because d ě ‘ ` 8k. This shows that u1 and v1, hence u and v,
are d-connected, establishing the base case h “ 0.

Base case h “ 1. We show a second base case, with h “ 1, which will make the induction
case trivial afterwards. In this case, u and v respectively contain loopable states q and q1
which are either connected or compatible. We deal with each case separately.

if q and q1 are connected, it means that
Base case h “ 1 with two connected states.
there is path from q to q1 in the automaton, or vice-versa. We assume that there is a path
from q to q1, otherwise the argument is symmetric up to exchanging u and v. Up to making

30

Enumerating Regular Languages with Bounded Delay

this path simple, let π be the label of such a path in the automaton, with |π| ď k, and let π1
be the label of a simple path in the automaton from q1 to some ﬁnal state; hence |ππ1| ď 2k.
Let e be a non-empty word of length ď k accepted by Aq. We know by Lemma C.3 that u is
d-connected to a word of the form u1 “ sent with |s| ď k and |t| ď k and q occurring before
and after each occurrence of e. Intuitively, we want to replace t by ππ1, to obtain a word to
which u1 is d-connected and which goes through q1, so that we can apply the ﬁrst base case
h “ 0, but the subtlety is that doing this could make us leave the stratum S. We adjust for
this by adding or removing occurrences of e to achieve the right length. Formally, if u1 ď 5k
then because ‘ ě 5k we know that u1 is in the ﬁrst stratum and that senππ1 has length at
most 7k, so as ‘ ě 7k and d ě 3k we know that u1 is d-connected to u2 “ senππ1 which is
in S. Otherwise, we assume that u1 ě 5k, so that en ě 3k. There are three subcases: either
|t| “ |ππ1|, or |t| ă |ππ1|, or |t| ą |ππ1|.

In the ﬁrst subcase where |t| “ |ππ1|, then we know that u1 is 3k-connected to the word
u2 “ senππ1, which is accepted by A and is in S because |u1| “ |u2|. In the second subcase
where |t| ă |ππ1|, if u1 is short then we conclude like in the previous subcase, using the fact
that |u1| ď |u2| ď |u1| ` 2k so u2 is in S by the deﬁnition of ‘. If u1 is long then we can
choose some number η such that 2k ď |eη| ď 3k, which is possible because |e| ď k. Now we
know that from u1 we can obtain the word sen´ηππ1, which is well-deﬁned because en ě 3k
so n ě η, is accepted by A, and has length at most |u1| and at least |u1| ´ |eη|, i.e., length at
least |u1| ´ 3k, so it is still in S by the deﬁnition of ‘. In the third subcase where |t| ą |ππ1|,
if u1 is long then we conclude like in the ﬁrst subcase because |u1| ´ k ď |u2| ď |u1|, so u2 is
in S. If u1 is short then we choose some number η like in the second subcase, and obtain
from u1 the word sen`ηππ1, which is accepted by A, is longer than u1, and has length at
most |u1| ` 3k so is still in S.

In all three subcases we have shown that u1 is d-connected to a word u2 in S whose
accepting path goes through q1 because u2 ﬁnishes by ππ1 and the state immediately before
was q so π brings us to q1. By the base case h “ 0, we know that u2 and v, whose accepting
runs both include the state q1, are d-connected.

Base case h “ 1 with two compatible states. Second, if q and q1 are compatible, we know
that there is some non-empty witnessing word z which is both in Aq and in Aq1. Up to taking
a simple path in the product of these two automata, we can choose z to have length ď k2.
By Lemma C.3, we know that u is d-connected to a word u1 “ sznt with |s| ď k and |t| ď k
and q at the beginning and end of every occurrence of z. Likewise, applying the lemma to v,
we know that v is d-connected to a word v1 “ xzmy with |x| ď k and |y| ď k and q at the
beginning and end of every occurrence of z. We now conclude like in the base case h “ 0, by
applying Claim C.5 with λ “ z.

Induction case. Let u and v be two loopable words, and p and q be loopable states
respectively occurring in the accepting run of u and of v, and consider a sequence p “
q0, . . . , qh “ q witnessing this, with h ě 1. Let w be any word of S whose accepting path
goes via the state qh´1, which must exist, e.g., taking some path in the automaton that goes
via qh´1 and then repeating some loop of size ď k on the loopable state qh´1. By applying
the induction hypothesis, we know that u is d-connected to w. By applying the case h “ 1,
we know that w is d-connected to v. Thus, u is d-connected to v. This shows the claim, and
concludes the induction, proving Proposition 4.7.

A. Amarilli and M. Monet

31

D Proofs for Section 5 (Bounded-delay enumeration)

D.1 Details on the machine model

Throughout Section 5, we phrase our algorithms in a slight variant of the pointer machine
model presented in Section 2 and Appendix A.2. The modiﬁcation is that the number
of ﬁelds of records, and the number of possible data values, is no longer constant, but is
allowed to depend on the input automaton, speciﬁcally it can be exponential in the input
automaton. This allows us to talk, e.g., of records whose ﬁelds are indexed by edit scripts of
constant length, or of data values denoting integers that are exponential in the automaton;
and to allow arbitrary arithmetics on these. However, crucially, the number of ﬁelds and the
data values do not depend on the current length of words when we enumerate, or on the
current memory size. Intuitively, as our enumeration algorithm generally runs indeﬁnitely,
the unbounded memory size will eventually be much larger than the exponential quantity on
the input used to bound the number of ﬁelds and the number of data values; and our use of
a pointer machine model means that the resulting pointers cannot be used for arithmetic
operations.

Let us now explain why this diﬀerence in our deﬁnition of the pointer machine model
is in fact inessential. For any number x, it is easy to see that a pointer machine allowing
x ﬁelds per pointer and x diﬀerent data values can be emulated by a pointer machine in
the standard sense presented in Section 2, e.g., which allows only 2 ﬁelds per pointer and
2 diﬀerent data values. Indeed, the x diﬀerent data values can be coded, e.g., in unary as
linked lists of length up to x, and records with x ﬁelds can be coded as a linked list of records,
each of which contains one data ﬁeld and one continuation ﬁeld pointing to the next record.
Operations on data values can be emulated by a hardcoded program that allocates a new
linked list whose length is a function of the length of the two other lists, and accessing a ﬁeld
of a record can be emulated by a hardcoded program that follows the linked lists of records
to retrieve the correct ﬁeld. The overhead induced by the emulation is a multiplicative factor
in the number x.

Now, recall that our goal in Section 5 is to show Theorem 5.1, which claims an exponential
delay in the input automaton. Thus, we can work in the modiﬁed pointer machine model where
the quantity x is exponential in the input automaton, because the resulting algorithm can
be emulated by an algorithm on a pointer machine model in the standard sense, multiplying
only by an exponential factor in the automaton, and yielding an overall complexity which is
still exponential in the automaton.

D.2 Second component: Proof of Proposition 5.4

In this section we give the proofs for our bounded-delay enumeration algorithm (Theorem 5.1).
We start by presenting in more details what we called the second component of the amortized
linear time algorithm, namely:

§ Proposition 5.4. For i ě 1, given the stratum graph Γi and starting and ending nodes
vsi ‰ vei of Γi, we can compute in time Op|Γi|q a sequence of edit scripts σ1, . . . , σNi´1
such that, letting si “ u1, . . . , uNi be the successive results of applying σ1, . . . , σNi´1 starting
with si, then u1, . . . , uNi is a 3d-ordering of strat‘pL, iq starting at si and ending at ei.

Proof. First, observe that by Proposition 4.7, the undirected graph corresponding to Γi is
connected. We then compute in linear time in Γi a spanning tree T of Γi: this can indeed be
done in linear time in our pointer machine model using any standard linear-time algorithm

32

Enumerating Regular Languages with Bounded Delay

for computing spanning trees, e.g., following a DFS exploration, with the stack of the DFS
being implemented as a linked list. Notice that, importantly, the number of edges of Γi,
hence its size, is linear in Ni, since all nodes have bounded degree (namely, exponential
in |A|).

Next, we apply the enumeration technique described in the proof of Lemma C.1 on the
tree T and starting node vsi and exit node vei, which yields a sequence vsi “ n1, . . . , n|Ni| “
vei enumerating all nodes of T exactly once, and where the distance between any two
consecutive nodes (in T ) is at most 3. The traversal can easily be implemented by a recursive
algorithm: we prepare an output doubly linked list containing the nodes to enumerate, and
when we enumerate a node in the algorithm, we append it to the end of this linked list, so
that, at the end of the algorithm, the 3-ordering of the nodes of the graph is stored in that
linked list.

We must simply justify that the recursive algorithm can indeed be implemented in our
machine model. For this, we store the recursion stack in a linked list. To make this explicit,
we can say that the stack contains two kinds of pairs, where n is a node of T :

pexplore, nq, meaning “call the exploration function on node n”; and
penumerate, nq, meaning “append n to the output linked list”.

Initially, the stack contains only pexplore, vsiq for vsi the root of T . At each step, we pop the
ﬁrst pair of the stack and do what it says, namely:

If it is pexplore, nq, we call the exploration function on n;
If it is penumerate, nq, we append n to the output list;
If the stack is empty, the exploration is ﬁnished.

The recursive exploration function can then be implemented as follows:

1, . . . , n1

mq for n1

m the children of n;

When we explore a node n that is at even depth, we push at the beginning of the
recursion stack the following list of elements in order: penumerate, nq, pexplore, n1
1q, . . .,
pexplore, n1
When we explore a node n at odd depth that is not special, we push at the beginning of
the recursion stack the following list of elements in order: pexplore, n1
mq,
penumerate, nq for n1
When we explore a node n at odd depth that is special and has ě 1 children n1
1 . . . n1
we push at the beginning of the recursion stack the following list in order: pexplore, n1
. . ., pexplore, n1

m´1q, penumerate, nq, pexplore, n1

m the children of n;

1q, . . ., pexplore, n1

1, . . . , n1

m,
1q,

mq.

Once we have the sequence vsi “ n1, . . . , n|Ni| “ vei produced in the output doubly linked
list, the last step is to compute in linear time the sequence σ1, . . . , σNi´1 of edit scripts. We
determine each edit script by reading the edge labels of Γi; formally, to determine σj, we
simply re-explore Γi from nj at distance 3, which is in time exponential in |A|, and when we
ﬁnd nj`1 we concatenate the labels of the (directed) edges of Γi that lead from nj to nj`1.
This indeed gives us what we wanted and concludes the proof.
đ

D.3 First component: Proof of Proposition 5.5

In the rest of this appendix, we will present the formal details of the ﬁrst component of our
enumeration algorithm, that is:

§ Proposition 5.5. There is an integer C P N exponential in |A| such that we can produce a
stratum graph sequence pΓ1, vs1, ve1 q, pΓ2, vs2, ve2 q, . . . for L in amortized linear time, i.e., for
i
j“1 Nj steps, the algorithm is done preparing pΓi, vsi, veiq.
each i ě 1, after having run C

ř

To show Proposition 5.5, we ﬁrst prove Fact 5.7:

A. Amarilli and M. Monet

33

‚

Lpbq

Rpaq

‚

Lpaq

Rpaq

‚

‚

‚

‚

Lpbq

Rpbq

Rpbq

Lpaq

Lpbq

‚

‚

Rpaq

‚

Rpaq

Lpbq

Rpaq

Lpbq

Lpaq

Rpbq

Lpaq

Rpbq

root

root

Figure 3 Two example pre-word DAGs which are not word DAGs. The labels pushL and pushR
are abbreviated for legibility. In the left pre-word DAG, the four paths to the top node that start to
the left of the root all represent the word baa, whereas the four paths to that same node that start
to the right of the root all represent the word bba. In the right pre-word DAG, the left topmost
node represents ab and bb and the right topmost node represents aa and ba. The criteria of word
DAGs, and our construction to enlarge them, are designed to prevent these problems.

Rpbq

aa

Lpaq

aaba

Rpaq

aab

Lpaq

aba

Lpaq Rpaq

Lpaq

ab

Rpbq

Rpaq

a

Lpbq

Lpaq

ba

Rpaq

bb

Lpbq

Rpbq

b

Rpaq

Lpbq

Lpaq

Rpbq

root

Figure 4 An example word DAG. We annotate the nodes with the word that they represent, even
though in the memory representation the nodes are anonymous and the words are not represented.
The labels pushL and pushR are abbreviated for legibility.

34

Enumerating Regular Languages with Bounded Delay

§ Fact 5.7. There are no two diﬀerent nodes in a word DAG that represent the same word.

Proof. Let us write G “ pV, η, rootq. Assume by way of contradiction that some word w is
represented by two distinct nodes n1 ‰ n2 of the word DAG, and take |w| to be minimal.
Then w cannot be the empty word because root is the only node reachable by a empty path
from root, so that n1 ‰ root and n2 ‰ root. Thus, we can write w as w “ aw1 for a P Σ. Now,
consider the pushLpaq-predecessors n1
2 of n1 and n2, respectively (which must exist
because n1 and n2 are not root). Then, taking any path from root to n1
2) and
continuing it to a path to n1 (resp., to n2), we know that n1 (resp., n2) must represent w1.
As |w1| ă |w|, by minimality of w, we must have n1
2. But then this node has two
distinct successors n1 and n2 for the same label pushLpaq, a contradiction.
đ

1 (resp., to n1

1 and n1

1 “ n1

We then prove Proposition 5.8, whose statement we recall here:

§ Proposition 5.8. We can build a word DAG G representing the words of L in amortized
linear time: speciﬁcally, for some value C that is exponential in |A|, for all i, after C ˆ
ř
i
j“1 Nj computation steps, for each word w of Σ˚ whose push-pop distance to a word
Ť
i
of
j“1 strat‘pL, jq is no greater than d, then G contains a node that represents w. Moreover,
there is also a value D exponential in |A| such that any node that is eventually created in the
word DAG represents a word that is at push-pop distance at most D from a word of L.

At a high level, we will prove Proposition 5.8 by ﬁrst explaining how to enlarge word
DAGs, which would allow us to use them to represent all words of Σ˚; and then explaining
how we can speciﬁcally build them to eﬃciently reach the words that are close to L.

But before doing so, we ﬁrst make two simple observations about word DAGs that will

be useful for the rest of the proof. Here is the ﬁrst one:

§ Claim D.1. In a word DAG, we cannot have a node n with edges pushLpaq and pushRpbq
to the same node n1 with a ‰ b.

Proof. Letting v be the word captured by n, the node n1 would witness that we have av “ vb.
This equation is clearly unsatisﬁable as a ‰ b so the number of occurrences of a and b in the
left-hand-side and right-hand-side are clearly diﬀerent.
đ

Our second observation is that we can have a node n with edges pushLpaq and pushRpaq
to the same node n1, but this happens if and only if n (and therefore n1) captures a word of
the form ai for some i ě 0:

§ Claim D.2. If a node n has edges pushLpaq and pushRpaq to the same node n1, then n
(and therefore n1) capture a power of a.

Proof. Letting v the word captured by n, the node n1 witnesses that we have av “ va. We
show by induction on length that v “ ai. The base case is trivial for a length of 0, and for a
length of 1 indeed the only solution is a. For the induction, if we have a solution of length
n ` 1, then it starts and ends with a, i.e., it is of the form av2a with v2 of length n ´ 1.
Injecting this in the equation, we get a2v2a “ av2a2, and simplifying av2 “ v2a, i.e., v2
satisﬁes the same equation. By induction hypothesis we have v2 “ an´1, and then v “ an`1,
establishing the induction case and concluding the proof.
đ

We are ready to explain how we enlarge word DAGs.

A. Amarilli and M. Monet

35

‚

‚

‚

Rpaq

Lpcq

Lpbq

Rpbq

Lpaq

Rpcq

root

Figure 5 The initial word DAG for alphabet ta, b, cu, with pushL and pushR respectively shortened

to L and R for brevity.

Enlarging a word DAG. We will start from the initial word DAG, which we deﬁne to be
the word DAG whose nodes are trootu Y tva | a P Σu and where each va is both the pushRpaq
and the pushLpaq-successor of root for all a P Σ. For instance, the initial word DAG for
alphabet Σ “ ta, b, cu is shown in Figure 5. It is clear that this is indeed a word DAG.

§ Deﬁnition D.3. Given a word DAG G and a node n of G, we say that n is complete if it
has 2|Σ| outgoing edges; and incomplete otherwise.

For instance, the root of the initial word DAG is complete, but its children are not. We

next explain how to complete an incomplete node.

§ Deﬁnition D.4. For a node n of G which is incomplete, we deﬁne the completion G Ò n
of G on n as follows.

First, for each label pushLpaq with a P Σ for which n has no outgoing edge, we do what
follows, which is illustrated in Figure 6 for diﬀerent cases. We consider the successive strict
ancestors n1, . . . , nm of n for labels of the form pushRpbq for b P Σ which do not have an
pushLpaq-successor (there may be none, in which case m “ 0). We also consider nm`1 the
closest ancestor of n via labels of the form pushRpbq that has a pushLpaq child, and call this
child n1
m`1. Notice that nm`1 is well-deﬁned (because the root has a successor for each label).
Notice further that if m ě 1 then nm is a pushRpbq-child of nm`1, and if m “ 0 then n itself
is the pushRpbq-child of nm`1. We let pushRpb1q, . . . , pushRpbmq, pushRpbm`1q be the labels
of the edges to n, n1, . . . , nm along this path.
We then create pushLpaq-successors n1

m of n1, . . . , nm respectively, and set the
other predecessor of each of them to be, respectively, n2 with label pushRpb2q, . . ., n1
m`1 with
label pushRpbm`1q, We call these newly created nodes pillar nodes. Then we create a new
node n1, and set it as the pushLpaq-successor of n and the pushRpb1q-successor of n1
1. Note
that, in the case where n1
1 “ n (which can only happen when m “ 0 and b1 “ a by Claim D.1),
we handle two labels at once, i.e., we set n1 as the pushLpaq- and pushRpaq-child of n as
illustrated in Figure 6c.

1, . . . , n1

Second, for each label pushRpaq with a P Σ for which n has no outgoing edge, we do the
corresponding operation on a path of labels of the form pushLpbq, exchanging the role of the
two kinds of labels.

Thus, when completing an incomplete node n, two types of nodes are added to the word
DAG: children of n, and pillar nodes (possibly for several labels). Further, notice that during
this process, it is possible that another node has been completed; for instance, this could
happen to n1

1 from Figure 6c.

We prove next that this completion procedure does not break the properties of word
DAGs, and moreover that it can be performed in linear time with respect to the number of
newly created nodes (including pillar nodes). Note that the number of pillar nodes created
may be arbitrarily large, and it is the reason why the complexity in Proposition 5.8 is

36

Enumerating Regular Languages with Bounded Delay

n1

Rpb1q

Lpaq

n1
1

n

Rpb2q

Lpaq

Rpb1q

n1
2

n1

Rpb3q

Lpaq

Rpb2q

n1
3

n2

Lpaq

Rpb3q

n3

Lpaq

n1

n

Rpaq

Lpaq

Rpaq

n1

(a) Case m “ 2 and a ‰ b1. The pillar
2 and n1
nodes are n1
1.

n1

(c) Case m “ 0 and a “ b1.
There is no pillar node.

Rpb1q

Lpaq

n1
1

n

Lpaq

Rpb1q

n1

(b) Case m “ 0 and a ‰ b1.
There is no pillar node.

Figure 6 Figures illustrating the completion procedure from Deﬁnition D.4 for a label of the
form pushLpaq P Σ, in the diﬀerent possible cases. Only the nodes of the word DAG that are
mentioned in the procedure are drawn. Dotted edges, and nodes that are the successors of dotted
edges, are created by the completion procedure illustrated. The case m ě 1 and a “ b1 cannot occur
by deﬁnition of nm.

A. Amarilli and M. Monet

37

stated as amortized linear time. We do not see a way to avoid this, as the existence of all
predecessors of a node seems crucial to continue the construction and ensure that we do not
create multiple nodes for the same word (i.e., that we respect the properties of word DAGs).

Let us show these results:

§ Lemma D.5. For any word DAG G and incomplete node n of G, then G Ò n is a word
DAG. Further, G Ò n has at least one additional node, and the running time to build it is
proportional to the number of new nodes that are created.

Proof. We claim that the result is still a pre-word DAG. First, by deﬁnition all newly created
nodes have exactly two parents, one with a label of the form pushLpaq and one with a label
of the form pushRpbq for some a, b P Σ. Further, everything is still reachable from the root,
and the root is obviously still complete. Hence G Ò n is indeed a pre-word DAG.

We show next that it is also a word DAG, by checking that this holds after each step
where we handle one missing outgoing label on n, say pushLpaq. Since all new paths in the
word DAG after the operation end on one of the new nodes, it suﬃces to check that every
new node represents only one word. But it is clear by construction that any path from the
root to a new node must go through one of the nodes n1
m`1, nm, . . . , n1, n, which therefore
all capture a unique word. Speciﬁcally, letting w be the word captured by nm`1, we know
that n1
m`1 captures aw, that nm captures wbm`1, ..., that n1 captures wbm`1 ¨ ¨ ¨ b2, and n
captures wbm`1 ¨ ¨ ¨ b1, and the words captured by the new nodes are then deﬁned in the
expected way.

The claim about the running time is immediate, noting that the node was asserted to be
đ

incomplete so there is at least one successor to create.

Hence, one could in principle build the successive strata of L by starting from the initial
word DAG, keeping a FIFO queue (e.g., as a linked list) of the nodes that are still incomplete,
and progressively enlarging the word DAG with this operation, thus eventually reaching all
words of Σ˚. The problem with this naive exploration strategy is that, for most languages, it
will not yield the amortized linear time bound that we wanted. This is simply because the
strata of our language L can be much smaller than those of Σ˚.

For this reason, we explain next how to guide the construction of the word DAG to only

reach words that are at bounded push-pop distance from words of the language.

From now on, to alleviate notation, we will often identify the nodes of the word DAG
with the words that they represent — but we recall that the words are not really written in
the nodes of the word DAG, in particular algorithms on word DAGs do not consider them.

Word DAGs for our regular language L.
Intuitively, we would like to be able to eﬃciently
decide if a node n of the word DAG that we are building represents a word that is in the
language L “ LpAq or not. Indeed, informally, if we could do this, as we know that every
stratum is d-connected by Proposition 4.7 and has words that are close to words of adjacent
strata, it would be enough to only enlarge the word DAG by exploring the d-neighborhood
of words in L.

To do this, for each node n of G, letting w be the word represented by n, we will
annotate n by the state q that is reached in the partial run of w from the initial state of A
(or by K if this state is undeﬁned). We can annotate the newly created nodes eﬃciently when
we enlarge a word DAG: for instance, if we have a pushRpaq-successor n1 of n in the word
DAG, then we could simply annotate q1 by δpq, aq. The same situation does not work if n1 is
a pushLpaq-successor, but we will always be able to deﬁne the annotation of new nodes by
following the pushR-transitions from states with a known annotation.

38

Enumerating Regular Languages with Bounded Delay

We will also annotate each node of the word DAG by the distance to the closest word of
the language that is in the word DAG, if this distance is ă d, or by 8 otherwise. Moreover,
for a technical reason that will become clear later, we will also annotate every node of the
word DAG with the modulo ‘ of its depth, i.e., the length of the unique word that the node
represents. Note that these values are all bounded by an exponential function of the input,
so they can be represented in our machine model. We formalize this next.

§ Deﬁnition D.6. An A-word DAG is a word DAG where each node n is additionally
annotated with:

An element of Q Y tKu, called the state annotation;
An integer in t0, . . . , d ´ 1u Y t8u, called the distance annotation;
An integer in t0, . . . , ‘ ´ 1u, called the modulo annotation.

We then additionally require that:

The state annotation represents the state that is reached by reading w in A, where w is
the word represented by n (and K if we do not reach any state).
For every node n, letting i be the shortest distance of an undirected path between n and
a node representing a word of the language, the integer labeling n is i if i ă d and 8
otherwise.
For every node n, letting w be the word that it represents, then the modulo annotation
of n is congruent with |w| modulo ‘.

We say that a node is successful if m corresponds to a word accepted by the automaton, i.e.,
it is labeled by a ﬁnal state. Equivalently, its distance annotation is then 0.

The initial A-word DAG is deﬁned in the expected way by annotating the initial word
DAG with the correct elements. We show that these new annotations can be maintained
when we complete a node, in linear time in the number of newly created nodes:

§ Lemma D.7. If G is an A-word DAG, for an incomplete node n, we can extend G to an
A-word DAG also denoted G Ò n consisting of the word DAG G Ò n and correctly updating
the annotations. The running time is again proportional in the number of created nodes
(recall that A and d and ‘ are ﬁxed).

Note that the state annotation and modulo annotation will be assigned once when a node
is created and never modiﬁed afterwards, whereas the distance annotation can be modiﬁed
after the node is created, if we later discover a new word of L suﬃciently close to the node.
We prove the lemma:

Proof of Lemma D.7. For the state annotation, we consider two cases, depending on whether
the missing outgoing edge of node n that we are completing was a pushRpaq- or a pushLpaq-
successor. If it was a pushRpaq successor, then all newly created nodes have a pushRpaq-
predecessor that was already in the A-word DAG before the operation so we can easily
determine their annotation by looking at the transition function of A. If it was a pushLpaq-
successor (the case of Figure 6), then we can determine the state annotation of each new
node by starting from the state annotation of node n1
m`1 from the construction (which was
in the A-word DAG before the operation), again by simply reading the transition function
of A. This can clearly be done in linear time in the number of created nodes.

It is clear how to compute the modulo annotation of each newly created node, counting

from the modulo annotation of the expanded node, modulo ‘.

Next, we update the distance annotations. These distance annotations only need to be
updated on the nodes of the word DAG that are within a distance ă d of the newly created

A. Amarilli and M. Monet

39

nodes, so we recompute them there: for every new node, we explore its neighborhood in the
word DAG up to a distance of d ´ 1 and update the distances of nodes in that neighborhood.
For the running time claim, we use the fact that the degree of the word DAG is at most
2|Σ| ` 2, so the neighborhood at distance d ´ 1 of any node in G is of size exponential in |A|,
d´1
j“0p2|Σ| ` 2qj ď p2|Σ| ` 2qd. If we create N nodes, updating the
namely, it has size ď
distances is thus done in linear time in N .
đ

ř

We are now ready to describe the exploration algorithm of Proposition 5.8.

Initialization. First expand the initial A-word DAG by increasing length until the incomplete
nodes are exactly the nodes representing words of Σ˚ of length ‘, so that all words of Σ˚ of
length ď ‘ are in the A-word DAG. This can be done in time exponential in |A| as there
are ď |Σ|‘`1 such words. Then prepare a set of ‘ empty FIFO queues B0, . . . , B‘´1, each
being implemented as a doubly linked list in our machine model. Also initialize an empty
buﬀer B as a linked list, and ﬁll this buﬀer by adding a pointer to all the nodes of the word
DAG that are incomplete (their depth is ‘) and whose distance annotation is ă 8. Also add
for each such node a pointer to its (unique) occurrence in B.

Intuitively, all queues Bj and the buﬀer B will hold (nodes corresponding to) words that
are incomplete and whose distance annotation is ă d. Moreover, each Bj will hold nodes
whose length is j modulo ‘, and the buﬀer B will also hold words whose length is 0 modulo ‘
(intuitively corresponding to the nodes of the next strata). This is indeed true after the
initialization step, and we will prove that this will always hold, since the algorithm will
continue as follows:

Indeﬁnitely repeat the following operation:

If one of the queues Bj is not empty, consider the smallest j such that Bj is not empty,
pop a node n from Bj and expand it, i.e., build the A-word DAG G Ò n. When creating
new nodes with this operation, if their (updated) distance annotation is ă 8 (i.e., ă d),
then:

if the newly created node n1 is a child of the node n, then, letting t be the modulo
annotation of n1, we put n1 into the queue Bt if j ă ‘ ´ 1 and into the buﬀer B
otherwise (i.e., if j “ ‘ ´ 1);
if the newly created node n1 is a pillar node then we put n1 in queue Bt where t is its
modulo annotation.

Notice that all newly created nodes are incomplete by construction. Similarly, when
updating the distance annotation of nodes that already existed, if they are incomplete
and their updated distance goes from 8 to ă d then we put them in queue Bi where i is
their modulo annotation. Whenever we add a node to one of the lists or to the buﬀer,
we also add in the node a pointer to its occurrence in this list/buﬀer. Moreover, when
we make a node complete, either by expanding it or by attaching new edges to it when
completing another node (which can be detected in the construction), we remove this
node from the queue it was in (if any), using the pointer to ﬁnd this occurrence. All of
this can be performed in time exponential in |A|, for each new node added during this
expansion step.
If all queues Bj are empty, then transfer all elements of the buﬀer B to the queue B0 (in
a constant number of operations as we work with linked lists).

This ends the description of the algorithm. Next we analyse it.

40

Enumerating Regular Languages with Bounded Delay

We deﬁne the 1st phase of the algorithm as the initialization phase, and for i ě 2, we
deﬁne the i-th phase to be the i ´ 1-th execution of the above while loop. During the i-th
phase, we intuitively enlarge the word DAG with words of the i-th stratum, plus possibly
some words of lower strata because of pillar nodes.

We now show that this algorithm meets the requirements of Proposition 5.8. To this end,
we ﬁrst show that the algorithm only discovers nodes that are suﬃciently close to words of
the language, i.e., it does not “waste time” materializing nodes that are too far away and
will not yield words of the language. In other words, we ﬁrst show the claim corresponding
to the last sentence of Proposition 5.8. This is clear for the children of nodes that we expand:
when we expand a node n, its distance is ă d, so its new children n1 will be at distance ď d.
However, the expansion may create an unbounded number of pillar nodes and we have a
priori no distance bound on these. Fortunately, these nodes in fact represent factors of the
word represented by n1. Thus, we show that the distance bound on n1 extends to give a
weaker distance bound on these nodes:

§ Lemma D.8. For any word w of length ě 2d at push-pop distance ď d to a word of L, for
any factor w1 of w, then w1 is at push-pop distance ď 6d to a word of L.

Note that the distance bound in this result is weaker than what is used in the algorithm
to decide which nodes to expand, i.e., the distance is ď 6d and not ď d. So the pillar nodes
may have distance annotation 8. Still, the weaker bound will imply (in the next proposition)
that all words created in the word DAG obey some distance bound.

We can now prove the lemma:

Proof of Lemma D.8. As |w| ě 2d, we can write w “ sut with |s|, |t| ě d. Moreover, any
word v at push-pop distance ď d from w can be written as v “ ρuτ with |ρ|, |τ | ď 2d. Take v
to be such a word in L, which exists by hypothesis. Now, the factor w1 of w is either a
factor of u, or may include parts of s and/or of t. But in all cases we can write it w1 “ s1u1t1
with |s1|, |t1| ď d and u1 a factor of u. Now, by considering the accepting run of v and the
occurrence of u1 inside, considering the states ql and qr before and after this occurrence of u1
in the accepting run, we know we can take words ρ1 leading from the initial state to ql and τ 1
from qr to a ﬁnal state, having length ď |Q| ď d, so that v1 “ ρ1u1τ 1 is a word of L. But
then observe that w1 “ s1u1t1 is at push-pop distance at most 6d from v1 “ ρ1u1τ 1: we can
pop left s1 (at most d), push left ρ1 (at most 2d), pop right t1 (at most d) and then push
right τ 1 (at most 2d). This concludes since v1 is in L.
đ

This allows us to show that indeed, all words that will eventually be discovered by the

algorithm are within some bounded distance of a word of the language. Formally:

§ Proposition D.9. There is a value D exponential in |A| such that any (node corresponding
to a) word w that is added to the word DAG at some point is at push-pop distance at most D
from a word of the language.

Proof. To account for initialization phase, let D1 be the maximal push-pop distance of a
word (of Σ˚) of length ă 2d to a word of the language. Now, let D :“ maxpD1, 6dq, and let
us show that this value of D is suitable.

Let w be a word that is discovered by the algorithm. If |w| ă 2d then we are done by our
choice of D1, so assume |w| ď 2d. This implies that w has been discovered during some i-th
phase for i ě 2 (since the initialization phase only discovers words of size ď ‘ and d “ 2‘).
Now, at the moment where w was discovered, there are two cases. The ﬁrst case is when w
was created as a child of a node w1 that we have expanded, hence whose annotation was ă d,

A. Amarilli and M. Monet

41

so w itself is at distance at most d, hence at most D, from a word of the language. The
second case is when w is was created as an ancestor of a child of a node w2 that has been
expanded (i.e., w is a pillar node), hence a factor of w2: then Lemma D.8 applies because w2
is at push-pop distance at most d of a word of the language by the same reasoning and
moreover we have |w2| ě |w| since w2 is a factor of w, and since |w| ě 2d by hypothesis we
have indeed |w2| ě 2d, so our choice of D works.
đ

Notice that Proposition D.9 would hold no matter in which order we expand incomplete

nodes of the word DAG.

Next, we show that, thanks to our exploration strategy, the algorithm discovers all words
of the language, stratum by stratum. The proof is more involved. We start by proving
the property that we informally mentioned after describing the initialization step of the
algorithm.

§ Claim D.10. For all i ě 1, all the words in the word DAG that are discovered before or
during the i-th phase have length ď i‘. Moreover, during the i-th phase, each queue Bj only
ever contains words whose length is ď i‘ ´ 1 and is j modulo ‘, and the buﬀer B only ever
contains words whose length is 0 modulo ‘.

Proof. By induction on i.

Case i “ 1. This is trivially true for the 1st phase (the initialization phase).
Case i ą 1. By induction hypothesis, at the beginning of the i-th phase the word DAG
only contains nodes of size ď pi ´ 1q‘, hence ď i‘. Moreover, considering the last step
of the pi ´ 1q-th phase, we see that B0 only contains words of size pi ´ 1q‘, and this is
allowed.
Now, let n1, n2, . . . be the words that are discovered during the i-th phase of the algorithm,
considering that, e.g., when we expand a node n for some missing pushRpaq-label, we ﬁrst
“discover” its pushRpaq-child, and then discover the pillar nodes (if any) in descending
order. Then we show the following claim by induction on j: (‹) [the node nj has size ď i‘
and, if at some point we add it to some queue Bt then its size is ď i‘ ´ 1 and is t
modulo ‘ and if we add it to B its size is 0 modulo ‘]. This is clear of the ﬁrst word
that we discover: indeed, n1 is a child of the very ﬁrst node that we popped from B0
to expand, so the size of n1 is exactly pi ´ 1q‘ ` 1 and, since its modulo annotation is
then 1, the only queue to which we could ever add it is B1 and pi ´ 1q‘ ` 1 ď i‘ ´ 1
indeed as ‘ ě 2. Let now nj`1 be the pj ` 1q-th word discovered during the ﬁrst phase
and assume p‹q to hold for all previously discovered nodes of that phase. We ﬁrst show
that |nj`1| ď i‘. Consider indeed the moment that this node was discovered: it was
either the child of some node nρ for ρ ď j that we expanded, or it was a pillar node
of some node nρ that we expanded. Since p‹q is true for nρ and since we never expand
nodes of the buﬀer B during a phase, it follows that indeed |nj`1| ď i‘. We next show
the claim on the queues/buﬀer. Observe then that the only problematic case is if we
add nj`1 to the queue B0 and |nj`1| “ i‘; indeed, it is clear that when we add a node
to some queue its modulo is correct with respect to that queue, so the only constraint
that could be violated is that |nj`1| ď i‘ ´ 1. So let us assume that |nj`1| “ i‘ by way
of contradiction. But then, considering again how we have discovered nj`1, we see that
the only possibility is that it is a child of some node nρ that we have expanded (nj`1
cannot be a pillar node because all nodes nj1 with j1 ď j have size ď i‘ and we deﬁned
that in the discovering order we discover children of expanded nodes before their pillars),
but then by induction hypothesis nρ must have been popped from the B‘´1 queue and
then the algorithm must have added nj`1 into B and not into B0.

42

Enumerating Regular Languages with Bounded Delay

This concludes the proof.

đ

Next, we observe that every word of the pi ` 1q-th stratum can be obtained from some
word of the i-th stratum by a speciﬁc sequence of at most d push-pop edits. This claim is
reminiscent of the proof of Lemma 4.6, where we showed the existence of so-called ladders.
Let us formally state the result that we need:

§ Claim D.11. For any i ě 1, for any word w of strat‘pL, i ` 1q, there is a word w1 P
strat‘pL, iq with push-pop distance at most d to w and that can be built from w as follows:
ﬁrst remove a preﬁx of length at most ‘ ` |Q| to get a word w2 (not necessarily in L) of
length exactly i‘ ´ |Q| ´ 1, and then add back a preﬁx corresponding to some path of length
ď |Q| from the initial state to get w as desired.

Proof. The preﬁx removal and substitution is simply by replacing a preﬁx of an accepting
run with some simple path from the initial state that leads to the same state: note that the
exact same argument was already used in the proof of Lemma 5.2. The fact that w1 can be
chosen to have length exactly i‘ ´ |Q| ´ 1 is simply because ‘ ě |Q|. For the distance bound,
notice that ‘ ` 2|Q| ď d.
đ

These observations allow us to show that the algorithm discovers all words of the language,

stratum by stratum:

§ Proposition D.12. For i ě 1, at the end of the i-th phase, the algorithm has discovered all
words of

Ť

i
j“1 strat‘pL, jq.

Proof. We show this by induction on i.

Ť

Case i “ 1. This is trivially true for the initialization step.
Case i ą 1. By induction hypothesis we know that the algorithm has discovered all words
i´1
of
j“1 strat‘pL, jq. So, let w P strat‘pL, iq and we show that the algorithm will discover
w during the i-th phase, which would conclude the proof.
By Claim D.11 there is a word w1 P strat‘pL, i ´ 1q such that δpppw, w1q ď d and that
can be transformed into w by ﬁrst popping-left a preﬁx of size at most |Q|, obtaining a
word w2 whose size is exactly pi ´ 1q‘ ´ |Q| ´ 1, and then pushing-left a preﬁx of size at
most ‘ ` |Q| to obtain w. Thus, let us write w “ w2a1 . . . at with aj P Σ and t ď ‘ ` |Q|.
Now, by induction hypothesis, the algorithm has discovered w1 during the pi ´ 1q-th
phase, and by the properties of a word DAG the word w2 is stored at a node which is an
ancestor of w1 and has also been discovered. Crucially, notice that thanks to its size, w2
is actually in the pi ´ 1q-th stratum of L.
Now, this means that, at some point of the i´1-th phase, the algorithm has discovered w2.
Hence, at some point during the pi ´ 1q-th phase, the algorithm has already discovered
both w1 and w2, and in fact all nodes in some simple path from w1 to w2. But then at
that point, the distance annotation of w2 was ă d, since it is then at distance ď |Q| ă d
in the word DAG from w1. Thus, during the i ´ 1-th phase, all children of w2, and all its
descendants up to a depth of |Q| ` 1, must have been created, because 2|Q| ` 1 ă d, and
then the preﬁx w2a1 . . . a|Q|`1 of w of length pi ´ 1q‘ was added to the buﬀer B, and then
in queue B0 at the end of that phase. We now show that all longer preﬁxes of w, i.e.,
all preﬁxes of w which are of the form w2a1 . . . a|Q|`1a|Q|`1`1 ¨ ¨ ¨ at, including w itself,
were discovered. We know that the preﬁx w2a1 . . . a|Q|`1 was discovered and put in the
buﬀer B for the pi ´ 1q-th phase. Now, during the i-th phase, we have also completed
this node w2a1 . . . a|Q|`1, and its descendants up to depth ‘, because again, ‘ ` 2|Q| ď d.
Thus, indeed w was discovered, which concludes the proof.
đ

A. Amarilli and M. Monet

43

We point out that, when we are at the i-th phase, we can create arbitrarily long paths of
pillar nodes during a single expansion operation, including nodes representing words having
length ă pi ´ 1q‘. However, the above Proposition implies that no such node can represent a
word of the language, because all the words of L of the pi ´ 1q-th stratum have already been
discovered.

The last ingredient to show Proposition 5.8 is then to prove that by the end of the pi`2q-th
phase, the algorithm has discovered all words whose push-pop distance to a word of the ﬁrst
i-th strata is no greater than d. Formally:

§ Proposition D.13. For all i ě 1, by the end of the pi ` 2q-th phase, the algorithm has
discovered all words of Σ˚ whose push-pop distance to a word of the i ﬁrst strata is no greater
than d.

Ť

Proof. We know by Proposition D.12 that by the end of the i-th phase (hence by the end
i
of the pi ` 2q-th phase) the algorithm has discovered all words of
j“1 strat‘pL, jq. Let w
Ť
i
be a word that is at distance d from some word w1 of
j“1 strat‘pL, jq, and let us show
that the algorithm discovers it by the end of the pi ` 2q-th phase. We show it by induction
on t :“ δpppw, w1q. If t “ 0 then w “ w1 and we are done. For the inductive case let t ą 1
and assume this is true for all j ď t. As δpppw, w1q “ t, there is a sequence of t push/pop
operations that transform w into w1. Let w2 be the word just before w that this sequence
deﬁnes. By induction hypothesis we will have discovered w2 by the end of the pi ` 2q-th
phase. But then observe that |w2| ď pi ` 2q‘ ´ 2 because d ď 2‘. But then it is clear that we
will also discover w by the end of the pi ` 2q-th phase, because w2 will be made complete
before the end of the pi ` 2q-th phase. Indeed, the distance annotation of w2 is ă d as
witnessed by w1. Further, we have |w2| ď |w1| ` d, and as d “ 2‘ and |w1| ă i‘, we have
w2 ă pi ` 2q‘, so indeed we will complete it before the pi ` 2q-th phase concludes.
đ

We are now equipped to prove Proposition 5.8, which claims that the word DAG con-
struction is in amortized linear time and that all created nodes are within some bounded
distance to words of the language (keeping in mind that the latter was already established in
Proposition D.9):

ř

Proof of Proposition 5.8. By Proposition D.13 and Proposition D.9, it is enough to show
i
that there is a value C exponential in |A| such that for all i ě 1, after C ˆ
j“1 Ni, the
algorithm has ﬁnished the pi ` 2q-th phase. Consider then the point P in the execution
of the algorithm where it has ﬁnished the pi ` 2q-th phase, and let us ﬁnd out how much
time this has taken. By Claim D.10 at P the algorithm has not discovered any word of
length ą pi ` 2q‘. Moreover, we know by Proposition D.9 that all the words represented
by the nodes added to the word DAG are within some bounded push-pop distance D from
words of the language. Note that it could be the case that the witnessing words of the
language are not yet discovered, in particular that there are in higher strata. However, we
can let K “ rD{‘s, and then we know that any word discovered is at distance at most D
from a word of the ﬁrst i ` 2 ` K strata.

Letting as usual Ni be the size of the i-th ‘-stratum of L, we then have that the algorithm
p2|Σ| ` 2qDNj nodes. Moreover, for CA the bound from
has discovered at most
Lemma 5.2, we have for all i the inequality Ni`1 ď CANi, so that we can bound the number
of discovered nodes at follows:

i`2`K
j“1

ř

44

Enumerating Regular Languages with Bounded Delay

i`2`Kÿ

j“1

p2|Σ| ` 2qDNj “ p2|Σ| ` 2qD

“ p2|Σ| ` 2qD

ď p2|Σ| ` 2qD

ˆ

iÿ

ˆ

j“1
iÿ

ˆ

j“1
iÿ

j“1

˙

˙

K`2ÿ

Nj `

Ni`e

e“1
K`2ÿ

e“1

Nj `

C e

ANi

˙

Nj ` C K`3

A Ni

ď p1 ` C K`3

A

qp2|Σ| ` 2qD

iÿ

j“1

Nj.

ř

i
j“1 Nj for some value C 1 exponential in |A|. Thus, the total number
Hence, this value is C 1
ř
i
j“1 Nj. Now, as we only
of nodes added to the word DAG is indeed proportional to
add nodes by performing completions, and only consider nodes on which there is indeed a
completion to perform (in particular removing from the lists Bj the nodes that have become
complete), Lemma D.7 ensures that the running time satisﬁes a similar bound, which is what
we needed to show.
đ

Next, we show Proposition 5.5, which claims that we can produce the stratum graph

sequence in amortized linear time:

Proof of Proposition 5.5. For this, we will extend the algorithm from Proposition 5.8 to
compute the stratum graphs. Speciﬁcally: once we have ﬁnished the pi ` 2q-th phase (during
which we discover all nodes of the pi ` 2q-th stratum), and before entering the pi ` 3q-th phase,
we prepare in linear time in Ni the stratum graph Γi for stratum i and the corresponding
starting and exit nodes. Note that we can easily compute the ﬁrst stratum graph Γ1 during
the initialization, as well as starting and exit points for it; we pick an exit point for the 1st
stratum which is a word that is at distance ď d to a word of the second stratum, as can be
checked by naively testing all possible edit scripts of at most d operations. All of this can
be computed by a naive algorithm, and will only increase the delay by an amount that is
exponential in |A|. Also notice the following claim p‹q by the end of the pi ` 2q-th phase, we
Ť
i
have found all words of Σ˚ whose push-pop distance to a word of
j“1 strat‘pL, jq is ď d,
and these words are all of size at least pi ´ 3q‘ and at most pi ` 2q‘: this is by Proposition D.13
and because 2‘ ě d. We can moreover easily modify the algorithm of Proposition 5.8 so that
it keeps, when it is in the i-th phase, a pointer to some successful node wi of the i ´ 2-th
stratum (or a null pointer when i ă 3).

Let us explain how we compute the stratum graph Γi in linear time in Ni after the
pi ` 2q-th phase has concluded. We assume that we already know the ending point vei´1 of
the previous phase, and that it was picked to ensure that there was a word of strat‘pL, iq at
distance ď d (as we did above for the 1st stratum). We do the computation in four steps:

First, we explore the A-word DAG using a DFS (e.g., with a stack implemented as a
linked list, and without taking into account the orientation of the edges) starting from
the node wi from the i-th strata, only visiting nodes corresponding to words of length
ě pi ´ 3q‘ and ď pi ` 2q‘ (which can be detected with the modulo annotations). During
this exploration, whenever we see a successful node that is in the i-th strata, we store it
in a linked list (also marking it in the word DAG). This is in linear time in the number of
nodes created in this length interval, hence, by Lemma 5.2, in linear time in Ni. By p‹q,

A. Amarilli and M. Monet

45

and because each stratum is d-connected, we know that we will see every node that
corresponds to a word of the i-th stratum. So the linked list stores all nodes of the i-th
stratum. These form the vertices of Γi.
For each node n in the list corresponding to a word w, we do an exploration of all the (not
necessarily simple) undirected paths of length ď d that start from n in the word DAG,
where we remember the edit script corresponding to the current path (i.e., each time we
traverse an edge in the forward direction we append its operation to the script; each time
we traverse an edge pushL or pushR in the reverse direction we append popL and popR
respectively to the script). We consider all marked nodes seen in this exploration, and
add edges from the vertex for w in Γi to these other nodes with the label of these edges
being the edit script of the corresponding path. By p‹q, this indeed builds all the edges
of Γi. This process takes total time proportional in Ni, because d only depends on the
language and the degree of the word DAG also only depends on the language.
Last, we pick our starting and ending vertices for the current stratum i. Knowing the
ending point vei´1 of the pi ´ 1q-th stratum, we know that there was some word of the
current stratum at distance ď d of vei´1 , so we explore from vei´1 in the word DAG
and ﬁnd a witnessing node, which we pick as starting node vsi. For the ending node of
the i-th stratum, we consider all nodes of strat‘pL, iq again and explore all possible edit
scripts for them to check if there is a node of the next stratum at distance ď d; we know
that this must succeed by Claim D.11. We pick any one of them as vei
Last, we unmark all the nodes that we had previously marked in the word DAG, again in
linear time in the size of the current stratum.

This indeed computes in linear time the stratum graph Γi. The amortized linear time
bound for this whole algorithm can then be shown using the same reasoning as in the proof
of the amortized linear time bound for Proposition 5.8.
đ

Finally we show our bounded-delay enumerability result (Theorem 5.1):

Proof of Theorem 5.1. As we have already explained in Section 5, we simply have to
combine the two components of our enumeration algorithm: let us call A the algorithm from
Proposition 5.5 with KA the value in its amortized linear time complexity (called C in the
statement), and B that of Proposition 5.4, with KB the value in its (non-amortized) linear
time complexity (i.e., for each i it runs in time KB|Γi|). Last, let CA be the value from
Lemma 5.2.

We ﬁrst show that there is an algorithm C that produces the sequences of edit scripts
(and stores them in a FIFO implemented as a double-ended linked list, to be read later) in
ř
i
amortized linear time, i.e., for some KC, for all i ě 1, after KC
j“1 Ni computation steps,
the algorithm C has produced a sequence of edit scripts corresponding to all the words of
the ﬁrst ď i strata. To do this we simply start algorithm A, and whenever it has produced
pΓi, vsi, veiq we pause A and run algorithm B on it, resuming A afterwards. Notice that
the size |Γi| of Γi is ď CSNi for some value CS exponential in |A|. This is because the
nodes of Γi correspond to words of the i-th strata and its degree is bounded by deﬁnition.
i
j“1 KBCSNi “
Then it is clear that after a number of computation steps KA
ř
i
j“1 Ni we have indeed computed the sequence of edit scripts for strata ď i,
pKA ` KBCSq
so we can take KC :“ KA ` KBCS.

i
j“1 Ni `

ř

ř

Our bounded-delay algorithm C1 to enumerate LpAq is then as follows:
We initialize by launching C until it has produced the sequence of edit scripts for the ﬁrst
stratum. We store the sequence of edit scripts in a FIFO. Then, we continue the execution
of C to produce the rest of the sequence of edit scripts at the end of the FIFO. However,

46

Enumerating Regular Languages with Bounded Delay

every E :“ p1 ` CAqKC steps of C, we output one edit script from the prepared sequence,
i.e., from the beginning of the FIFO. We know that the edit script sequence thus produced
is correct (it is the one produced by C), and that algorithm C1 has bounded delay (namely,
delay E ` E1 for E1 the constant time needed to read one edit script from the FIFO and
output it), but the only point to show is that we never try to pop from the FIFO at a point
when it is empty, i.e., we never “run out” of prepared edit scripts.

To see why this is true, we know that C adds edit script sequences to the FIFO for each
stratum, i.e., once we have concluded the computation of the i-th stratum graph Γi and
the execution of algorithm B over Γi, we add precisely Ni edit scripts to the FIFO. So it
suﬃces to show that, for any i, the FIFO does not run out after we have enumerated the
edit scripts for the strata 1, . . . , i. Formally, we must show that for any i ě 1, after we have
ř
i
popped
j“1 Nj edit scripts from the FIFO, then algorithm C must have already added to
the FIFO the edit scripts for the pi ` 1q-th stratum. We know that the time needed for
i`1
1 Ni, which by
algorithm C to ﬁnish processing the pi ` 1q-th stratum is at most KC
ř
i
i
j“1 Nj. Now, by the deﬁnition of algorithm C1,
j“1 Nj ` CANi ď E
Lemma 5.2 is ď KC
ř
i
j“1 Nj edit scripts from the FIFO, then we have already run at least
if we have popped
ř
i
E
j“1 Nj steps of algorithm C. Hence, we know that algorithm C has ﬁnished producing
the edit scripts for the pi ` 1q-th stratum and the FIFO is not empty. This concludes the
proof.
đ

ř

ř

E

Proofs for Section 6 (Extensions)

E.1 Proof of the complexity results

§ Proposition 6.1 ([19]). For any ﬁxed t, d ě 1, it is NP-complete, given a DFA A with
LpAq ﬁnite, to decide if LpAq is pt, dq-partition-orderable (with the push-pop or Levenshtein
distance).

Proof. Our proof will show NP-hardness both for the push-pop distance and for the Leven-
shtein distance (both problems being a priori incomparable). We denote the distance by δ.
The membership in NP is immediate as a witnessing t-tuple of edit script sequences has
polynomial size.

Recall that a grid graph is a ﬁnite node-induced subgraph of the inﬁnite grid. Fix the
integers d ě 1 and t ě 1. We work on the alphabet Σ “ ta, bu. We reduce from the
Hamiltonian path problem on grid graphs, which is NP-hard [13]. Given a grid graph G,
letting n be its number of vertices, we assume without loss of generality that G is connected,
as otherwise it trivially does not have a Hamiltonian path. Hence, up to renormalizing, we
can see each vertex of G as a pair pi, jq such that two vertices pi, jq and pi1, j1q are adjacent
if and only if |i ´ i1| ` |j ´ j1| “ 1, with 0 ď i, j ď n. (Indeed, if some node is labeled p0, 0q
and the graph is connected, then any vertex must have values pi, jq with i ` j ď n.)

We code G as a set of words of size polynomial in G deﬁned as follows: for each vertex
pi, jq we have the word npi,jq :“ adibd`1adj. Let L1 be the language of these words. Then, let
L be L1 with the set of t ´ 1 words bpj`1qpd`1q for 1 ď j ď t ´ 1. This coding is in polynomial
time, and we can obtain from L1 a DFA recognizing it in polynomial time.

Let us show that the reduction is correct. For this, let us ﬁrst observe that L is pt, dq-
enumerable iﬀ L1 is p1, dq-enumerable. Indeed, if L1 is p1, dq-enumerable then we enumerate
L by adding one singleton sequence for each of the t ´ 1 words of LzL1. Conversely, if L is
pt, dq-enumerable then as the words of L1zL are at distance ą d from one another on from

A. Amarilli and M. Monet

47

the words of L1 (as each edit can only change the number of b’s by one), then each of the t ´ 1
words of L1zL must be enumerated in its own singleton sequence, and L1 is p1, dq-enumerable.
Now, we deﬁne G1 to be the graph whose nodes are the words of L1 and where we
connect two words if they are diﬀerent and the distance between them is ď d. Clearly
G1 has a Hamiltonian path iﬀ L1 is d-orderable. We claim that G1 is isomorphic to G,
which concludes the proof because then G has a Hamiltonian path iﬀ G1 does. So let us
take two distinct vertices pi, jq, pi1, j1q of G and show that they are adjacent in G1 (i.e.,
|i ´ i1| ` |j ´ j1| “ 1) iﬀ aidbd`1ajd is at distance ď d (for the Levenshtein or push/pop
distance) to ai1dbd`1aj1d. For the forward direction, it is clear that increasing/decreasing i
or j amounts to pushing/popping ad at the beginning or end of the word. For the backward
direction, proving the contrapositive, if the vertices are not adjacent then either |i ´ i1| ě 2
or |j ´ j1| ě 2 or both i ‰ i1 and j ‰ j1. In all three cases, noting that all words reachable
at Levenshtein edit distance ď d must include some b’s in the middle, if we edit one of the
words with ď d operations then the endpoints of the longest contiguous block of b’s cannot
have moved by more than d{2 relative to where they were in the original word, so the only
operations that can give a word of the right form amount to modifying the number of a’s to
the left or right of the block of b’s, and with d editions we cannot change both numbers nor
can we change a number by at least 2d.

We have shown that G and G1 are isomorphic, which establishes the correctness of the
đ

reduction and concludes the proof.

E.2 Proof of the results on the push-pop-right distance

We prove our result on the push-pop-right distance in this section:

§ Theorem 6.2. Given a DFA A, the language LpAq is pt, dq-partition-orderable for the
push-pop-right distance for some t, d P N if and only if LpAq is slender. Further, if LpAq is
slender, we can compute in PTIME the smallest t such that LpAq is pt, dq-partition-orderable
for some d P N for the push-pop-right distance.

In addition, there is an algorithm which, given a DFA A for which LpAq is slender and
t “ 1, enumerates the language LpAq with push-pop-right distance bound 2k and linear delay
in |A|. Further, the sequence of edit scripts produced by the algorithm is ultimately periodic.

We start with the proof of the characterization: LpAq is pt, dq-partition-orderable for the

push-pop-right distance iﬀ LpAq is slender.

We consider the inﬁnite tree T whose nodes are Σ˚ and where, for every w P Σ˚ and a P Σ,
the node w has an a-labeled edge to the node wa (i.e., wa is a child of w). A L-inﬁnite branch
of this tree is an inﬁnite branch of the tree such that there are inﬁnitely many nodes n on that
branch that have a descendant in L. Formally, there is an inﬁnite sequence w “ a1a2 ¨ ¨ ¨ (i.e.,
an inﬁnite word, corresponding to a branch) such that, for inﬁnitely many values i1, i2, . . .,
there are words xj such that a1 ¨ ¨ ¨ aij xj is a word of L.

We show that a pt, dq-partition-orderable regular language for the push-pop-right distance

must contain ﬁnitely many L-inﬁnite branches:

§ Claim E.1. If L is pt, dq-partition-orderable language for the push-pop-right distance and is
regular, then it must contain at most t L-inﬁnite branches.

Proof. We show that a language with ě t ` 1 many L-inﬁnite branches is not pt, dq-partition-
orderable. Indeed, assume by contradiction that it is, for some distance d. Consider a
depth m at which the t ` 1 L-inﬁnite branches have diverged, i.e., we have distinct nodes
n1, . . . , nt`1 at depth m that all have inﬁnitely many descendants in L. Consider a moment

48

Enumerating Regular Languages with Bounded Delay

at which all words of the language of length ď m ` d have been enumerated. Then by the
pigeonhole principle there must be some language in the partition that still has inﬁnitely
many words to enumerate from two diﬀerent branches, say descendants of ni and nj with
i ‰ j. Now, all descendants of ni at depth ą d are at distance ą d from all descendants
of nj at depth ą d, which contradicts the assumption that the language in the partition can
move from one to the other.
đ

And we show that a regular language having ﬁnitely many L-inﬁnite branches must be

slender.

§ Claim E.2. If a regular language L has ﬁnitely many L-inﬁnite branches, then it is slender.

This follows from an ancillary claim shown by pumping:

§ Claim E.3. For an inﬁnite regular language L, there is a value d P N such that, for each
word w, there is an L-inﬁnite branch in T such that w is at depth at most d from a node of
the branch.

Proof. Let d be the number of states of a DFA recognizing L. As L is inﬁnite, it clearly
has at least some L-inﬁnite branch obtained by considering rs˚t for s a simple loop on a
loopable state. Hence, the claim is trivial if w has length ă d because the root is a node of
this L-inﬁnite branch.

If |w| ě d, we know that there is some loopable state of w that occurs in the suﬃx of
length d, i.e., we can write w “ rt where the state q between r and t is loopable. Now, let s
be a loop on q of length at most d, and consider the word sequence wi “ rsit of L starting
at w1 “ w. All words of wi are in L. Further, let w1
1 is the least
common ancestor (LCA) of w1 and w2, w1
2 is the LCA of w2 and w3, and so on: clearly
w1
i`1,
so the inﬁnite sequence of the w1
i deﬁnes an inﬁnite branch in T . And is an L-inﬁnite branch,
because each w1
i has a descendant. Thus indeed w is at depth d from a node of this inﬁnite
branch.
đ

i “ rsi for all i (but they are not words of L), in particular each w1

i be the sequence where w1

i is an ancestor of w1

We can then prove Claim E.2:

Proof of Claim E.2. If L is ﬁnite then it is slender. Otherwise, letting d be the number of
states of a DFA recognizing L, we know by Claim E.3 that all words of the language must
be at depth ď d from a node of some L-inﬁnite branch, hence of one of the ﬁnite collection
of L-inﬁnite branches. This directly implies that L is slender, because for any length n ě d,
considering L X Σn, the number of nodes of T at depth n that can be in L are the descendants
of the nodes of the branches at depth between n and n ´ d, i.e., some number that only
depends on the language.
đ

Thanks to Claims E.1 and E.2, we know that, among the regular languages, only the
slender languages can be pt, dq-partition-orderable language for the push-pop-right distance.
Indeed, if a language is pt, dq-partition-orderable then it has ﬁnitely many L-inﬁnite branches
by the ﬁrst claim, which implies that it is slender by the second claim. So in the sequel it
suﬃces to focus on slender languages.

We will reﬁne a known characterization of slender languages. We know from [20, Chapter

XII, Theorem 4.23] the following characterization:

§ Theorem E.4 ([20]). The following are equivalent on a regular language L:

L is slender

A. Amarilli and M. Monet

49

L can be written as a union of regular expressions of the form xy˚z.
The minimal DFA for L does not have a connected pair of simple cycles.

This implies in particular that we can check in PTIME whether a language is slender given a
DFA A for the language, by computing in PTIME the minimal DFA equivalent to A, and
then checking if there are two connected simple cycles.

For t P N, a t-slender language is one that can be written as a disjoint union of a ﬁnite
language L1 and of t languages ris˚
i Li with ri and si words and Li a ﬁnite language, for
1 ď i ď t, and all ri are pairwise incomparable (i.e., they are pairwise distinct and none is a
strict preﬁx of another) and no word of ri is a preﬁx of a word of L1. Consider a trimmed
DFA A, its non-loopable preﬁxes are the words w such that reading w in A brings us to a
loopable state, and reading any strict preﬁx of w does not. We claim the following:

§ Proposition E.5. The following are equivalent on a regular language L:

L is recognized by a DFA without a connected pair of simple cycles and with exactly t
non-loopable preﬁxes.
L can be written as a t-slender language.

This proposition implies in particular that a slender language is necessarily t-slender for
some t, by considering its minimal DFA and counting the minimal inﬁnitely continuable
preﬁxes. Note that, given a DFA, we can compute in PTIME the equivalent minimal DFA
and count in PTIME the non-loopable preﬁxes. We prove the proposition:

Proof of Proposition E.5. If L is recognized by an automaton of the prescribed form, we can
write L as a disjoint union of the non-loopable words (a ﬁnite language) and the languages
riLqi where the ri are the non-loopable preﬁxes and the Lqi is the language accepted by
starting at the loopable state qi at which we get after reading the non-loopable preﬁx. (Some
of the qi may be identical.) Note that by construction the ri are pairwise incomparable and
none is a preﬁx of a non-loopable word. Now, each Lqi can be decomposed between the
words accepted without going to qi again, and those where we do. As L has no connected
pair of simple cycles, if we do not go to qi again, then we cannot complete the simple cycle
on qi and we cannot go to another cycle, so the possible words leading to a ﬁnite state form
a ﬁnite language Li. If we do, then the word starts with the (unique) label of the (unique)
simple cycle starting at qi, i.e., si, and then we have a word of Lqi. Thus, we can write L as
a disjoint union of the non-loopable words and of the ripsiq˚Li with Li ﬁnite.

Conversely, if L is written as a t-slender language then we obtain the automaton in the
obvious way: start with an acyclic DFA A for the ﬁnite language, construct DFAs Ai for
each s˚
i Li which have exactly one simple cycle on which the initial state is located, then for
each ri extend A with a path labeled by ri going from the initial state to the initial state
of Ai: some states of the path may already exist (because of the words of L1 or of the other
words of ri), but the condition on the ri and on L1 guarantee that we always create at least
one new transition. The resulting automaton accepts by construction the words of L1 and
the words of the ris˚
i Li, and for any accepting path in the automaton either it ends at a
state of A and the accepted word is a word of L1, or it goes into an Ai and the accepted
word is a word of some ris˚
đ

i Li

We now claim that the t-slender languages are precisely those that are pt, dq-partition-

orderable for some d:

§ Proposition E.6. A t-slender language is pt, dq-partition-orderable for some d. Conversely,
if a regular language is pt, dq-partition-orderable then it is t-slender.

50

Enumerating Regular Languages with Bounded Delay

Proof. For the ﬁrst claim, it suﬃces to show that a 1-slender language is d-orderable for the
push-pop-right distance for some d. This is easy: ﬁrst enumerate the words of L1 in some
naive way, and then enumerate the words of rL2, then rsL2, etc. The distance within each
sequence is bounded because L1 and L2 are ﬁnite, and the distance when going from the last
word of rsiL2 to the ﬁrst word of rsi`1L2 is bounded too.

For the second claim, we know by Claim E.1 that a regular language L that is pt, dq-
partition-orderable for some d must have at most t L-inﬁnite branches. Now, we know by
Claim E.2 that L must then be slender. Assuming by way of contradiction that L is not
t-slender, by Theorem E.4 together with Proposition E.5 we know L must be t1-slender for
some t1, implying that L is t1-slender for some t1 ą t. Now, being t1-slender implies that L
has t1 inﬁnite L-branches, namely, those starting at the ri which are pairwise incomparable.
This contradicts our assumption that L has at most t L-inﬁnite branches, and concludes the
proof.
đ

We have thus shown the ﬁrst part of Theorem 6.2: if the language LpAq is pt, dq-partition-
orderable for the push-pop-right distance for some t, d P N then it is t-slender by the
proposition, and conversely if it is slender then it is t-slender for some t by Theorem E.4
and Proposition E.5 and is then pt, dq-partition-orderable for some d P N by the proposition.
Further, if L is slender, using the characterization of Proposition E.5 we can compute in
PTIME the smallest t such that L is t-slender, and then we know that L is pt, dq-partition-
orderable for some d but not pt ´ 1, dq-partition-orderable, thanks to the proposition.

We last show that, if t “ 1, we can compute the description of an ultimately periodic
sequence of edit scripts that enumerates LpAq respecting a linear bound on the push-pop-right
distance and in linear delay:

§ Proposition E.7. There is an algorithm which, given a DFA A with k states representing a
1-slender language, computes a sequence of edit scripts that enumerates LpAq and is ultimately
periodic, with push-pop-right distance bound 2k and delay linear in |A|.

Note that this proposition admits a converse: the ultimately periodic sequences of edit

scripts can only achieve slender languages (see Proposition A.1).

We now prove the proposition:

Proof of Proposition E.7. Recall that we are given the input DFA in the way described in
Appendix A.2. We ﬁrst locate in the input DFA the unique simple cycle and path from
the initial state to that simple cycle: this can be performed in linear time by running a
depth-ﬁrst search (DFS) on the automaton from the initial state, marking states when the
DFS starts visiting them and is done visiting them. We stop at the moment where the
exploration reaches a vertex which is currently being visited: then the recursion stack gives
us (from bottom to top) the unique path from the initial state to a state of the loop, and
then the loop itself.

We now start the computation of the ultimately periodic sequence by producing scripts
to enumerate all words of the ﬁnite language L1 of the non-loopable words. For this, we
perform a DFS from the initial state where we do not mark vertices as visited (so as to
enumerate all paths). We do so on a slightly modiﬁed version of the automaton where we
remove the states of the loop (as we must produce only non-loopable words), and where we
trim the automaton to remove all states which no longer have a path to an accepting state.
Each time we reach a ﬁnal state in the DFS, we produce an edit sequence achieving the word
described by the recursion stack from the previously produced word; this is easy to do by
stacking push and pop operations which correspond to what the DFS does. Remember that

A. Amarilli and M. Monet

51

the automaton is trimmed, so we know that we achieve a new non-loopable word every k
steps at most, for k the number of automaton states.

Now, we continue the enumeration sequence with an edit script that goes from the last
produced word to the word corresponding to the unique path from an initial state to the
ﬁrst state q of the loop. We are now ready to start the computation of the periodic part of
the ultimately periodic sequence.

To do this, we ﬁrst enumerate all words that can be accepted from q without going back
to q. This can be done by a DFS in the same way that we did previously, with the same
distance and complexity bounds. Next, from the last word enumerated in this fashion, we
pop until we arrive to a state of the loop, then we complete the loop to reach the state q.
This completes the description of the periodic part.

Note that, in all cases, the distance between two words, hence the delay in producing the
corresponding elements of the sequence, is at most 2k. This is because, when going from
one word to the next, the algorithm is always popping a simple path of states, and pushing
a simple path, so each state can at most occur twice. The only exception is the previous
paragraph, where we pop until we arrive to a state of the loop, then push a completion of
the loop: the next produced word may then be pushing states that already occur in the
completion of the loop, but these states occur at most two times in total and did not occur
in the pop, so the overall bound still applies.
đ

