BelMan: An Information-Geometric Approach to
Stochastic Bandits
Debabrota Basu, Pierre Senellart, Stéphane Bressan

To cite this version:

Debabrota Basu, Pierre Senellart, Stéphane Bressan. BelMan: An Information-Geometric Approach
to Stochastic Bandits. ECML/PKDD - The European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases, Sep 2019, Würzburg, Germany. ￿hal-02195539￿

HAL Id: hal-02195539

https://inria.hal.science/hal-02195539

Submitted on 26 Jul 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

BelMan: An Information-Geometric Approach
to Stochastic Bandits

Debabrota Basu1, Pierre Senellart2,3, and St´ephane Bressan4

1 Data Science and AI Division, Chalmers University of Technology, G¨oteborg, Sweden
2 DI ENS, ENS, CNRS, PSL University, Paris, France
3 Inria, Paris, France
4 School of Computing, National University of Singapore, Singapore

Abstract. We propose a Bayesian information-geometric approach to
the exploration–exploitation trade-oﬀ in stochastic multi-armed bandits.
The uncertainty on reward generation and belief is represented using
the manifold of joint distributions of rewards and beliefs. Accumulated
information is summarised by the barycentre of joint distributions, the
pseudobelief-reward. While the pseudobelief-reward facilitates informa-
tion accumulation through exploration, another mechanism is needed
to increase exploitation by gradually focusing on higher rewards, the
pseudobelief-focal-reward. Our resulting algorithm, BelMan, alternates
between projection of the pseudobelief-focal-reward onto belief-reward
distributions to choose the arm to play, and projection of the updated
belief-reward distributions onto the pseudobelief-focal-reward. We the-
oretically prove BelMan to be asymptotically optimal and to incur a
sublinear regret growth. We instantiate BelMan to stochastic bandits
with Bernoulli and exponential rewards, and to a real-life application of
scheduling queueing bandits. Comparative evaluation with the state of
the art shows that BelMan is not only competitive for Bernoulli bandits
but in many cases also outperforms other approaches for exponential and
queueing bandits.

Introduction

1
The multi-armed bandit problem [30] is a sequential decision-making problem [11]
in which a gambler plays a set of arms to obtain a sequence of rewards. In the
stochastic bandit problem [7], the rewards are obtained from reward distributions
on arms. These reward distributions belong to the same family of distributions
but vary in the parameters. These parameters are unknown to the gambler. In the
classical setting, the gambler devises a strategy, choosing a sequence of arm draws,
that maximises the expected cumulative reward [30]. In an equivalent formulation,
the gambler devises a strategy that minimises the expected cumulative regret [26],
that is the expected cumulative deﬁcit of reward caused by the gambler not
always playing the optimal arm. In order to achieve this goal, the gambler must
simultaneously learn the parameters of the reward distributions of arms. Thus,
solving the stochastic bandit problem consists in devising strategies that combine
both the accumulation of information to reduce the uncertainty of decision
making, exploration, and the accumulation of rewards, exploitation [27]. We refer
to the stochastic bandit problem as the exploration–exploitation bandit problem
to highlight this trade-oﬀ. If a strategy relies on independent phases of exploration

2

D. Basu et al.

and exploitation, it necessarily yields a suboptimal regret bound [15]. Gambler
has to adaptively balance and intertwine exploration and exploitation [3].

In a variant of the stochastic bandit problem, called the pure exploration
bandit problem [8], the goal of the gambler is solely to accumulate information
about the arms. In another variant of the stochastic bandit problem, the gambler
interacts with the bandit in two consecutive phases of pure exploration and
exploration–exploitation. The authors of [29] named this variant the two-phase
reinforcement learning problem.

Although frequentist algorithms with optimism in the face of uncertainty
such as UCB [3] and KL-UCB [14] work considerably well for the exploration–
exploitation bandit problem, their frequentist nature prevents eﬀective assim-
ilation of a priori knowledge about the reward distributions of the arms [23].
Bayesian algorithms for the exploration–exploitation problem, such as Thompson
sampling [34] and Bayes-UCB [21], leverage a prior distribution that summarises
a priori knowledge. However, as argued in [22], there is a need for Bayesian
algorithms that also cater for pure exploration. Neither Thompson sampling nor
Bayes-UCB are able to do so.

Our contribution. We propose a uniﬁed Bayesian approach to address the
exploration–exploitation, pure exploration, and two-phase reinforcement learning
problems. We address these problems from the perspective of information repre-
sentation, accumulation, and balanced induction of bias. Here, the uncertainty is
two fold. Sampling reward from the reward distributions is inherently stochastic.
The other layer is due to the incomplete information about the true paramaters
of the reward distributions. Following Bayesian algorithms [34], we maintain a
parameterised belief distribution for each arm representing the uncertainty on
the parameter of its reward distribution. Extending this representation, we use a
joint distribution to express the two-fold uncertainty induced by both the belief
and the reward distributions of each arm. We refer to these joint distributions
as the belief-reward distributions of the arms. We set the learning problem in
the statistical manifold [2] of the belief-reward distributions, which we call the
belief-reward manifold. The belief-reward manifold provides a representation
for controlling pure exploration and exploration–exploitation, and to design a
unifying algorithmic framework.

The authors of [8] proved that, for Bernoulli bandits, if an exploration–
exploitation algorithm achieves an upper-bounded regret, it cannot reduce the
expected simple regret by more than a ﬁxed lower bound. This drives us to ﬁrst
devise a pure exploration algorithm, which requires a collective representation of
the accumulated knowledge about the arm. From an information-geometric point
of view [4,1], the barycentre of the belief-reward distributions in the belief-reward
manifolds serves as a succinct summary. We refer to this barycentre as the
pseudobelief-reward. We prove the pseudobelief-reward to be a unique representa-
tion in the manifold. Though pseudobelief-reward facilitates the accumulation
of knowledge, it is essential for the exploration–exploitation bandit problem to
also incorporate a mechanism that gradually concentrates on higher rewards [27].

BelMan: An Information-Geometric Approach to Stochastic Bandits

3

We introduce a distribution that induces such an increasing exploitative bias.
We refer to this distribution as the focal distribution. We incorporate it into the
deﬁnition of the pseudobelief-reward distribution to construct the pseudobelief-
focal-reward distribution. This pushes the summarised representation towards the
arms having higher expected rewards. We implement the focal distribution using
an exponential function of the form exp(X/τ (t)), where X is the reward, and a
parameter τ (t) dependent on time t and named as exposure. Exposure controls
the exploration–exploitation trade-oﬀ.

In Section 2, we apply these information-geometric constructions to develop
the BelMan algorithm. BelMan projects the pseudobelief-focal-reward onto belief-
rewards to select an arm. As it is played and a reward is collected, BelMan
updates the belief-reward distribution of the corresponding arm by projecting
of the updated belief-reward distributions onto the pseudobelief-focal-reward.
Information geometrically these two projections are studied as information (I-) and
reverse information (rI-) projections [10], respectively. BelMan alternates I- and rI-
projections between belief-reward distributions of the arms and the pseudobelief-
focal-reward distribution for arm selection and information accumulation. We
prove the law of convergence of the pseudobelief-focal-reward distribution for
BelMan, and that BelMan asymptotically converges to the choice of the optimal
arm. BelMan can be tuned, using the exposure, to support a continuum from
pure exploration to exploration–exploitation, as well as two-phase reinforcement
learning.

We instantiate BelMan for distributions of the exponential family [6]. These
distributions lead to analytical forms that allows derivation of well-deﬁned and
unique I- and rI-projections as well as to devise an eﬀective and fast computation.
In Section 3, we empirically evaluate the performance of BelMan on diﬀerent sets
of arms and parameters for Bernoulli and exponential distributions, thus showing
its applicability to both discrete and continuous rewards. Experimental results
validate that BelMan asymptotically achieves logarithmic regret. We compare
BelMan with state-of-the-art algorithms: UCB [3], KL-UCB, KL-UCB-Exp [14],
Bayes-UCB [21], Thompson sampling [34], and Gittins index [17], in these diﬀerent
settings. Results demonstrate that BelMan is not only competitive but also
outperforms existing algorithms for challenging setups such as those involving
many arms and continuous rewards. For the two-phase reinforcement learning,
results show that BelMan spontaneously adapts to the explored information,
improving the eﬃciency.

We also instantiate BelMan to the application of queueing bandits [24].
Queueing bandits represent the problem of scheduling jobs in a multi-server
queueing system with unknown service rates. The goal of the corresponding
scheduling algorithm is to minimise the number of jobs in hold while also learning
the service rates. A comparative performance evaluation for queueing systems
with Bernoulli service rates show that BelMan performs signiﬁcantly better than
2 Methodology
the existing algorithms, such as Q-UCB, Q-ThS, and Thompson sampling.
Bandit Problem. We consider a ﬁnite number K > 1 of independent arms.
An arm a corresponds to a reward distribution f a
θ (X). We assume that the

4

D. Basu et al.

a

form of the probability distribution f·(X) is known to the algorithm but the
parametrisation θ ∈ Θ is unknown. We assume the reward distributions of all
arms to be identical in form but to vary over the parametrisation θ. Thus,
we refer to f a
θ (X) as fθa (X) for speciﬁcity. The agent sequentially chooses an
arm at at each time step t that generates a sequence of rewards [xt]T
t=1, where
T ∈ N is the time horizon. The algorithm computes a policy or strategy that
sequentially draws a set of arms depending on her previous actions, observations
and intended goal. The algorithm does not know the ‘true’ parameters of the
arms {θtrue
}K
a=1 a priori. Thus, the uncertainty over the estimated parameters
{θa}K
a=1 is represented using a probability distribution B(θ1, . . . , θK). We call
B(θ1, . . . , θK) the belief distribution. In the Bayesian approach, the algorithm
starts with a prior belief distribution B0(θ1, . . . , θK) [19]. The actions taken and
rewards obtained by the algorithm till time t create the history of the bandit
process, Ht (cid:44) [(a1, x1), . . . , (at−1, xt−1)]. This history Ht is used to sequentially
update the belief distribution over the parameter vector as Bt(θ1, . . . , θK) (cid:44)
P(θ1, . . . , θK | Ht). We deﬁne the space consisting of all such distributions over
{θa}K
a=1 as the belief space B. Following the stochastic bandit literature, we
assume the arms to be independent, and perform Bayesian updates of beliefs.
Assumption 1 (Independence of Arms). The parameters {θa}K
independently from K belief distributions {ba
(cid:81)K

a=1 are drawn
a=1, such that Bt(θ1, . . . , θK) =

a=1 ba
Though Assumption 1 is followed throughout this paper, we note it is not
essential to develop the framework BelMan relies on, though it makes calculations
easier.
Assumption 2 (Bayesian Evolution). When conditioned over {θa}K
a=1 and
the choice of arm, the sequence of rewards [x1, . . . , xt] is jointly independent.
Thus, the Bayesian update at the t-th iteration is given by

t (θa) (cid:44) (cid:81)K

P(θa | Ht).

t (.)}K

a=1

t+1(θa) ∝ fθa (xt) × ba
ba

t (θa)

(1)

if at = a and a reward xt is obtained. For all other arms, the belief remains
unchanged.

Belief-reward Manifold. We use the joint distributions P(X, θ) on reward
X and parameter θ in order to represent the uncertainties of partial information
about the reward distributions along with the stochastic nature of reward.

Deﬁnition 1 (Belief-reward distribution). The joint distribution Pa
t (X, θ)
on reward X and parameter θa for the ath arm at the tth iteration is deﬁned as
the belief-reward distribution.

t (X, θ) (cid:44)
Pa

(cid:82)
X∈R

ba
t (θ)fθ(X)
ba
t (θ)fθ(X)dθdx

(cid:82)

=

1
Z

ba
t (θ)fθ(X).

θ∈Θ

If f·(X) is a smooth function of θa’s, the space of all reward distributions
constructs a smooth statistical manifold [2], R. We call R the reward manifold.
If belief B is a smooth function of its parameters, the belief space B constructs

BelMan: An Information-Geometric Approach to Stochastic Bandits

5

another statistical manifold. We call B the belief manifold of the multi-armed
bandit process. Assumption 1 implies that the belief manifold B is a product
of K manifolds Ba (cid:44) {ba(θa)}. Here, Ba is the statistical manifold of belief
distributions for the ath arm. Due to the identical parametrization, the Ba’s can
be represented by a single manifold Bθ.

Lemma 1 (Belief-Reward Manifold). If the belief-reward distributions P(X, θ)
have smooth probability density functions, their set deﬁnes a manifold BθR . We
refer to it as the belief-reward manifold. Belief-reward manifold is the product
manifold of the belief manifold and the reward manifold, i.e. BθR = Bθ × R.

The Bayesian belief update after each of the iteration is a movement on the
belief manifold from a point ba
t to another point ba
t+1 with maximum information
gain from the obtained reward. Thus, the belief-reward distributions of the played
arms evolve to create a set of trajectories on the belief-reward manifold. The
goal of pure exploration is to control such trajectories collectively such that after
a long enough time each of the belief-rewards accumulate enough information
to resemble the ‘true’ reward distributions well enough. The goal of exploration–
exploitation is to gain enough information about the ‘true’ reward distributions
while increasing the cumulative reward in the path, i.e, by inducing a bias towards
playing the arms with higher expected rewards.

Pseudobelief: Summary of Explored Knowledge. In order to control
the exploration, the algorithm has to construct a summary of the collective
knowledge on the belief-rewards of the arms. Since the belief-reward distribution
of each arm is a point on the belief-reward manifold, geometrically their barycentre
on the belief-reward manifold represents a valid summarisation of the uncertainty
over all the arms [1]. Since the belief-reward manifold is a statistical manifold,
we obtain from information geometry that this barycentre is the point on the
manifold that minimises the sum of KL-divergences from the belief-rewards of
all the arms [4,2]. We refer to this minimising belief-reward distribution as the
pseudobelief-reward distribution of all the arms.

Deﬁnition 2 (Pseudobelief-reward distribution). A pseudobelief-reward
distribution ¯Pt(X, θ) is a point in the belief-reward manifold that minimises the
sum of KL-divergences from the belief-reward distributions Pa
t (X, θ) of all the
arms.

¯Pt(X, θ) (cid:44) arg min
P∈BθR

K
(cid:88)

a=1

DKL (Pa

t (X, θ) (cid:107) P(X, θ)) .

(2)

We prove existence and uniqueness of the pseudobelief-reward for K given
belief-reward distributions. This proves the pseudobelief-reward to be an unam-
biguous representative of collective knowledge. We also prove that the pseudobelief-
reward distribution ¯Pt is the projection of the average belief-reward distribution
ˆPt(X, θ) = (cid:80)
t (X, θ) on the belief-reward manifold. This result validates the
claim of pseudobelief-reward as the summariser of the belief-rewards of all the
arms.

Pa

a

6

D. Basu et al.

Theorem 1. For given set of belief-reward distributions {Pa
a=1 deﬁned on the
same support set and having a ﬁnite expectation, ¯Pt is uniquely deﬁned, and is
such that its expectation parameter veriﬁes ˆµt(θ) = 1
K

a=1 µa

t (θ).

t }K

(cid:80)K

Hereby, we establish as a unique summariser of all the belief–reward distribu-
tions. Using this uniqueness proof, we can prove that the pseudobelief–reward
distribution ¯P is projection of the average belief–reward distribution ˆP on the
belief–reward manifold.
Corollary 1. The pseudobelief-reward distribution ¯Pt(X, θ) is the unique point
on the belief-reward manifold that has minimum KL-divergence from the distribu-
tion ˆPt(X, θ) (cid:44) 1
K

t (X, θ).

(cid:80)K

Pa

a=1

Focal Distribution: Inducing Exploitative Bias. Creating a succinct
pseudobelief-reward is essential for both pure exploration and exploration– ex-
ploitation but not suﬃcient for maximising the cumulative reward in case of
exploration–exploitation. If a reward distribution having such increasing bias
towards higher rewards is amalgamated with the pseudobelief-reward, the re-
sulting belief-reward distribution provides a representation in the belief-reward
manifold to balance the exploration–exploitation. Such a reward distribution
is called the focal distribution. The product of the pseudobelief-reward and the
focal distribution jointly represents the summary of explored knowledge and
exploitation bias using a single belief-reward distribution. We refer to this as the
pseudobelief-focal-reward distribution-reward distribution In this paper, we use
with a time dependent and controllable parameter τ (t) as the reward
exp
distribution inducing increasing exploitation bias.

(cid:16) X
τ (t)

(cid:17)

Deﬁnition 3 (Focal Distribution). A focal distribution is a reward distribu-
, where τ (t) is a decreasing function of t (cid:62) 1.
tion of the form Lt(X) ∝ exp
We term τ (t) the exposure of the focal distribution.

(cid:16) X
τ (t)

(cid:17)

Thus, the pseudobelief-focal-reward distribution-reward distribution is rep-
(cid:16) X
, where the normalisation factor
τ (t)

(cid:17)

resented as ¯Q(X, θ) (cid:44) 1
¯Zt
¯Zt = (cid:82)
(cid:82)
¯P(X, θ) exp
the pseudobelief-focal-reward distribution as

¯P(X, θ) exp
(cid:16) X
(cid:17)
τ (t)

X∈R

θ∈Θ

dθdx. Following Equation (2), we compute

¯Qt(X, θ) (cid:44) arg min

¯Q

K
(cid:88)

a=1

DKL

(cid:0)Pa

t−1(X, θ) (cid:107) ¯Q(X, θ)(cid:1) .

τ (t) has to grow in the order Ω( 1√

The focal distribution gradually concentrates on higher rewards as the exposure
τ (t) decreases with time. Thus, it constrains using KL-divergence to choose
distributions with higher rewards and induces the exploitive bias. From Theorem 3,
we obtain 1
) for exploration–exploitation bandit
problem independent of the family of reward distribution. Following the bounds
obtained in [14], we set the exposure τ (t) = [log(t) + C × log(log(t))]−1 for
experimental evaluation, where C is a constant (we choose the value C = 15 in
the experiments) . As the exposure τ (t) decreases with t, the focal distribution
gets more concentrated on higher reward values. For the pure exploration bandits,

t

BelMan: An Information-Geometric Approach to Stochastic Bandits

7

Algorithm 1 BelMan
1: Input: Time horizon T , Number of arms K, Prior on parameters B0, Reward

function f , Exposure τ (t).

2: for t = 1 to T do
3:
4:

/∗ I-projection ∗/
Draw arm at such that

at = arg min

DKL

a

(cid:0)Pa

t−1(X, θ) (cid:107) ¯Qt−1(X, θ)(cid:1) .

5:
6:
7:
8:
9:

/∗ Accumulation of observables ∗/
Sample a reward xt out of fθat
Update the belief-reward distribution of at to Pa
/∗ Reverse I-projection ∗/
Update the pseudobelief-reward distribution to

.

t (X, θ) using Bayes’ theorem.

¯Qt(X, θ) = arg min
¯Q∈Bθ R

K
(cid:88)

a=1

DKL

(cid:0)Pa

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1) .

10: end for

we set the exposure τ (t) = ∞ to remove any bias towards higher reward values
i.e, exploitation.

BelMan: An Alternating Projection Scheme. A bandit algorithm per-
forms three operations in each step– chooses an arm, samples from the reward
distribution of the chosen arm and incorporate the sampled reward to update the
knowledge-base. BelMan (Algorithm 1) performs the ﬁrst and the last operations
by alternately minimising the KL-divergence DKL(. (cid:107) .) [25] between the belief-
reward distributions of the arms and the pseudobelief-focal-reward distribution-
reward distribution. BelMan chooses to play the arm whose belief-reward incurs
minimum KL-divergence with respect to the pseudobelief-focal-reward distri-
bution. Following that, BelMan uses the reward collected from the played arm
to do Bayesian update of the belief-reward and to update the pseudobelief-
focal-reward distribution-reward distribution to the point minimising the sum
of KL-divergences from the belief-rewards of all the arms. [10] geometrically
formulated such minimisation of KL-divergence with respect to a participating
distribution as a projection to the set of the other distributions. For a given t, the
belief-reward distributions of all the arms Pa
t (X, θ) form a set P ⊂ BθR and the
pseudobelief-focal-reward distribution-reward distributions ¯Qt(X, θ) constitute
another set Q ⊂ BθR.

Deﬁnition 4 (I-projection). The information projection (or I-projection) of
a distribution ¯Q ∈ Q onto a non-empty, closed, convex set P of probability distri-
butions, Pa’s, deﬁned on a ﬁxed support set is deﬁned by the probability distribution
Pa∗ ∈ P that has minimum KL-divergence to q: Pa∗ (cid:44) arg minPa∈P DKL(Pa (cid:107) ¯Q).

BelMan decides which arm to pull by an I-projection of the pseudobelief-focal-
reward distribution onto the beliefs-rewards of each of the arms (Lines 3–4). This
operation amounts to computing

at (cid:44) arg min

a

DKL

(cid:0)Pa

t−1(X, θ) (cid:107) ¯Qt−1(X, θ)(cid:1)

8

D. Basu et al.

= arg max

a

(cid:18)

EPa

t−1(X,θ)

(cid:21)

(cid:20) X
τ (t)

− DKL

(cid:0)ba

t−1(θ) (cid:107) b ¯ηt−1(θ)(cid:1)

(cid:19)

The ﬁrst term symbolises the expected reward of arm a. Maximising this term
alone is analogous to greedily exploiting the present information about the arms.
The second term quantiﬁes the amount of uncertainty that can be decreased if
arm a is chosen on the basis of the present pseudobelief. The exposure τ (t) of the
focal distribution keeps a weighted balance between exploration and exploitation.
Decreasing τ (t) decreases the exploration with time which is quite an intended
property of an exploration–exploitation algorithm.

Following that (Line 5–7), the agent plays the chosen arm at and samples a
reward xt. This observation is incorporated in the belief of the arm using Bayes’
rule of Equation (1).
Deﬁnition 5 (rI-projection). The reverse information projection (or rI-
projection) of a distribution Pa ∈ P onto Q, which is also a non-empty, closed,
convex set of probability distributions on a ﬁxed support set, is deﬁned by the
distribution ¯Q∗ ∈ Q that has minimum KL-divergence from Pa: ¯Q∗ (cid:44) arg min ¯Q∈Q
DKL(Pa (cid:107) ¯Q).
Theorem 2 (Central limit theorem).

is estimator of
T (˜¯µT − ¯µ) converges
the expectation parameters of the pseudobelief distribution,
in distribution to a centered normal random vector in N (0, ¯Σ). The covariance
matrix ¯Σ = (cid:80)K

tends to λa as T → ∞.

(cid:80)K
a=1 ˜µa
ta
√
T

If ˜¯µT (cid:44) 1
K

a=1 λaΣa such that

Theorem 2 shows that the parameters of pseudobelief can be constantly
estimated and their estimation would depend on the accuracy of the estimators
of individual arms with a weight on the number of draws on the corresponding
arms. Thus, the uncertainty in the estimation of the parameter is more inﬂuenced
by the arm that is least drawn and less inﬂuenced by the arm most drawn. In
order to decrease the uncertainty corresponding to pseudobelief, we have to draw
the arms less explored.

T
K2ta
T

We need an additional assumption before moving into the asymptotic consis-

tency claim in Theorem 3.
Assumption 3 Bounded log-likelihood ratios. The log-likelihood of the
posterior belief distribution at time t with respect to the true posterior belief
distribution is bounded such that limt→∞

(cid:54) C < ∞ for all a.

(cid:12)
(cid:12)
(cid:12)log

Pa(X,θ)
Pa
t (X,θ)

(cid:12)
(cid:12)
(cid:12)

This assumption helps to control the convergence of sample KL divergences
in to the true KL-divergences as the number of samples grow inﬁnitely. This
is a relaxed version of Assumption 2 employed in [18] to bound the regret of
Thompson sampling. This is also often used in the statistics literature to control
the convergence rate of posterior distributions [33][35].
Theorem 3 (Asymptotic consistency). Given τ (t) =
log t+c×log log t for
any c (cid:62) 0, BelMan will asymptotically converge to choosing the optimal arm in
case of a bandit with bounded reward and ﬁnite arms. Mathematically, if there
exists µ∗ (cid:44) maxa µ(θa),

1

lim
T →∞

1
T

E

(cid:34) T

(cid:88)

t=1

(cid:35)

Xat

= µ∗.

(3)

BelMan: An Information-Geometric Approach to Stochastic Bandits

9

Fig. 1. Evolution of number of suboptimal draws for 2-arm Bernoulli bandit with
expected rewards 0.8 and 0.9 for 1000 iterations. The dark black line shows the average
over 25 runs. The grey area shows the 75 percentile.

Fig. 2. Evolution of number of suboptimal draws for 20-arm Bernoulli bandit with
expected rewards [0.25 0.22 0.2 0.17 0.17 0.2 0.13 0.13 0.1 0.07 0.07 0.05 0.05 0.05 0.02
0.02 0.02 0.01 0.01 0.01] for 1000 iterations.

Fig. 3. Evolution of number of suboptimal draws for 5-arm bounded exponential bandit
with expected rewards 0.2, 0.25, 0.33, 0.5, and 1.0 for 1000 iterations.

We intuitively validate this claim. We can show the KL-divergence between
t (X, θ) (cid:107) ¯Q(X, θ))
belief-reward of arm a and the pseudobelief-focal-reward is DKL(Pa
t ) − 1
= (1 − λa)h(ba
t ) denotes
the entropy of belief distribution ba
t of arm a at time t. As t → ∞, the entropy of
belief on each arm reduces to a constant dependent on its internal entropy. Thus,
when 1
τ (t) exceeds the entropy term for a large t, BelMan greedily chooses the
arm with highest expected reward. Hence, BelMan is asymptotically consistent.

t , for λa computed as per Theorem 2. Here, h(ba

τ (t) µa

BelMan is applicable to any belief-reward distribution for which KL-divergence
is computable and ﬁnite. Additionally for reward distributions belonging to the
exponential family of distributions, the belief distributions, being conjugate to
the reward distributions, also belong to the exponential family [6]. This makes
belief-reward distributions ﬂat with respect to KL-divergence. Thus, both I-and

05001000050100150Suboptimal drawsBelMan05001000050100150BayesUCB05001000Iterations050100150UCB05001000050100150KLUCB05001000050100150Gittins05001000050100150Random05001000050100150Thompson0500100001002003004005006007008009001000Random0500100001002003004005006007008009001000Gittins0500100001002003004005006007008009001000KLUCB05001000Iterations01002003004005006007008009001000UCB0500100001002003004005006007008009001000Thompson0500100001002003004005006007008009001000BayesUCB0500100001002003004005006007008009001000Suboptimal drawsBelMan05001000time050100150200250300350400450500Suboptimal drawsBelMan05001000time050100150200250300350400450500Thompson05001000time050100150200250300350400450500UCBtuned05001000time050100150200250300350400450500KLUCB05001000time050100150200250300350400450500KLUCBexp05001000time050100150200250300350400450500Random10

D. Basu et al.

rI-projections in BelMan are well-deﬁned and unique for exponential family
reward distributions. Furthermore, if we identify the belief-reward distributions
with expectation parameters, we obtain the pseudobelief as an aﬃne sum of
them. This allows us to compute belief-reward distribution directly instead of
computing its dependence on each belief-reward separately. The exponential
family includes the majority of the distributions found in the bandit literature
such as Bernoulli, beta, Gaussian, Poisson, exponential, and χ2.
3 Empirical Performance Analysis
Exploration–exploitation bandit problem. We evaluate the performance
of BelMan for two exponential family distributions – Bernoulli and exponential.
They stand for discrete and continuous rewards respectively. We use the pyma-
Bandits library [9] for implementation of all the algorithms except ours, and run
it on MATLAB 2014a. We plot the evolution of the mean and the 75 percentile
of cumulative regret and number of suboptimal draws. For each instance, we run
experiments for 25 runs each consisting of 1000 iterations. We begin with uniform
distribution over corresponding parameters as the initial prior distribution for all
the Bayesian algorithms.

We compare the performance of BelMan with frequentist methods like UCB [3]
and KL-UCB [14], and Bayesian methods like Thompson sampling [34] and Bayes-
UCB [21]. For Bernoulli bandits, we also compare with Gittins index [17] which
is the optimal algorithm for Markovian ﬁnite arm independent bandits with
discounted rewards. Though we are not speciﬁcally interested in the discounted
case, Gittins’ algorithm is indeed transferable to the ﬁnite horizon setting with
slight manipulation. Though it is often computationally intractable, we use it
as the optimal baseline for Bernoulli bandits. We also plot performance of the
uniform sampling method (Random), as a na¨ıve baseline.

From Figures 1, 2, and 3, we observe that at the very beginning the number
of suboptimal draws of BelMan grows linearly and then transitions to a state
of slow growth. This initial linear growth of suboptimal draws followed by a
logarithmic growth is an intended property of any optimal bandit algorithm as
can be seen in the performance of competing algorithms and also pointed out
by [16]: an initial phase dominated by exploration and a second phase dominated
by exploitation. The phase change indicates the ability of the algorithm to
reduce uncertainty by learning after a certain number of iterations, and to ﬁnd a
trade-oﬀ between exploration and exploitation. For the 2-arm Bernoulli bandit
(θ1 = 0.8, θ2 = 0.9), BelMan performs comparatively well with respect to the
contending algorithms, achieving the phase of exploitation faster than others,
with signiﬁcantly less variance. Figure 2 depicts similar features of BelMan for
20-arm Bernoulli bandits (with means 0.25, 0.22, 0.2, 0.17, 0.17, 0.2, 0.13, 0.13,
0.1, 0.07, 0.07, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, and 0.01). Since more
arms ask for more exploration and more suboptimal draws, all algorithms show
higher regret values. On all experiments performed, BelMan outperforms the
competing approaches. We also simulated BelMan on exponential bandits: 5 arms
with expected rewards {0.2, 0.25, 0.33, 0.5, 1.0}. Figure 3 shows that BelMan
performs more eﬃciently than state-of-the-art methods for exponential reward

BelMan: An Information-Geometric Approach to Stochastic Bandits

11

Fig. 4. Evolution of (mean) regret for
exploration–exploitation 20-arm Bernoulli
bandit setting of Figure 2 with hori-
zon=50,000.
distributions- Thompson sampling, UCBtuned [3], KL-UCB, and KL-UCB-exp,
a method tailored for exponential distribution of rewards [14]. This demonstrates
BelMan’s broad applicability and eﬃcient performance in complex scenarios.

Fig. 5. Evolution of (mean) cumulative
regret for two-phase 20-arm Bernoulli ban-
dits.

We have also run the experiments 50 times with horizon 50 000 for the 20
arm Bernoulli bandit setting of Figure 2 to verify the asymptotic behaviour of
BelMan. Figure 4 shows that BelMan’s regret gradually becomes linear with
respect to the logarithmic axis. Figure 4 empirically validates BelMan to achieve
logarithmic regret like the competitors which are theoretically proven to reach
logarithmic regret.

Two-phase reinforcement learning problem. In this experiment, we
simulate a two-phase setup, as in [29]: the agent ﬁrst does pure exploration
for a ﬁxed number of iterations, then move to exploration–exploitation. This
is possible since BelMan supports both modes and can transparently switch.
The setting is that of the 20-arm Bernoulli bandit in Figure 2. The two-phase
algorithm is exactly BelMan (Algorithm 1) with τ (t) = ∞ for an initial phase
of length TEXP followed by the decreasing function of t as indicated previously.
Thus, BelMan gives us a single algorithmic framework for three setups of bandit
problems– pure exploration, exploration–exploitation, and two-phase learning.
We only have to choose a diﬀerent τ (t) depending on the problem addressed.
This supports BelMan’s claim as a generalised, uniﬁed framework for stochastic
bandit problems.

We observe a sharp phase transition in Figure 5. While the pure exploration
version acts in the designated window length, it explores almost uniformly to
gain more information about the reward distributions. We know for such pure
exploration the cumulative regret grows linearly with iterations. Following this,
the growth of cumulative regret decreases and becomes sublinear. If we also
compare it with the initial growth in cumulative regret and suboptimal draws of
BelMan in Figure 2, we observe that the regret for the exploration–exploitation
phase is less than that of regular BelMan exploration–exploitation. Also, with
increase in the window length the phase transition becomes sharper as the growth

12

D. Basu et al.

(a) Q-ThS

(b) Q-UCB

(c) Thompson sampling

(d) BelMan

Fig. 6. Queue regret for single queue and 5 server setting with Poisson arrival with arrival
rate 0.35 and Bernoulli service distribution with service rates [0.5,0.33,0.33,0.33,0.25],
[0.33,0.5,0.25,0.33,0.25], and [0.25,0.33,0.5,0.25,0.25] respectively. Each experiment is
performed 50 times for a horizon of 10,000.
in regret becomes very small. In brief, there are three major lessons of this
experiment. First, Bayesian methods provide an inherent advantage in leveraging
prior knowledge (here, accumulated in the ﬁrst phase). Second, a pure exploration
phase helps in improving the performance during the exploration–exploitation
phase. Third, we can leverage the exposure to control the exploration–exploitation
trade-oﬀ.

4 Application to Queueing Bandits
We instantiate BelMan for the problem of scheduling jobs in a multiple-server
multiple-queue system with known arrival rates and unknown service rates. The
goal of the agent is to choose such a server for the given system such that the
total queue length, i.e. the jobs waiting in the queue, will be as less as possible.
This problem is referred as the queueing bandit [24].

We consider a discrete-time queueing system with 1 queue and K servers.
The servers are indexed by a ∈ {1, . . . , K}. Arrivals to the queue and service
oﬀered by the servers are assumed to be independent and identically distributed
across time. The mean arrival rate is λ ∈ R+. The mean service rates are denoted
by µ ∈ {µa}K
a=1, where µa is the service rate of server a. At a time, a server can
serve the jobs coming from a queue only. We assume the queue to be stable i.e,
µa. Now, the problem is to choose a server at each time t ∈ [T ] such
λ < max
a∈[K]

0200040006000800010000Time0255075100125150175200Queue regretQueue1Queue2Queue30200040006000800010000Time0255075100125150175200Queue regretQueue1Queue2Queue30200040006000800010000Time0255075100125150175200Queue regretQueue1Queue2Queue30200040006000800010000Time0255075100125150175200Queue regretQueue1Queue2Queue3BelMan: An Information-Geometric Approach to Stochastic Bandits

13

that the number of jobs waiting in queues is as less as possible. The number of
jobs waiting in queues is called the queue length of the system. If the number
of arrivals to the queues at time t is A(t) and S(t) is the number of jobs served,
the queue length at time t is deﬁned as Q(t) (cid:44) Q(t − 1) + A(t) − S(t), where
Q : [T ] → R(cid:62)0, A : [T ] → R(cid:62)0, and S : [T ] → R(cid:62)0. The agent, which is the
scheduling algorithm in this case, tries to minimise this queue length for a given
horizon T > 0. The arrival rates are known to the scheduling algorithm but the
service rates are unknown to it. This create the need to learn about the service
distributions, and in turn, engenders the exploration-exploitation dilemma.

Following the bandit literature, [24] proposed to use queue regret as the
performance measure of a queueing bandit algorithm. Queue regret is deﬁned
as the diﬀerence in the queue length if a bandit algorithm is used instead of an
optimal algorithm with full information about the arrival and service rates. Thus,
the optimal algorithm OPT knows all the arrival and service rates, and allocates
the queue to servers with the best service rate. Hence, we deﬁne the queue regret
of a queueing bandit algorithm Ψ (t) (cid:44) E (cid:2)Q(t) − QOPT(t)(cid:3) . In order to keep the
bandit structure, we assume that both the queue length Q(t) of algorithm A
and that of the optimal algorithm QOPT(t) starts with the same stationary state
distribution ν(λ, µ).

We show experimental results for the M/B/K queueing bandits. We assume
the arrival process to be Markovian, and the service process to be Bernoulli. The
arrival process being Markovian implies that the stochastic process describing the
number of arrivals is therefore A (t) have increments independent of time. This
makes the distribution of A(t) to be a Poisson distribution [12] with mean arrival
rate λ. We denote Ba(µa) is the Bernoulli distribution of the service time of
server a. It implies that the server processes a job with probability µa ∈ (0, 1) and
refuses to serve it with probability 1 − µa. The goal is to perform the scheduling
in such a way that the queue regret will be minimised. The experimental results
in Figure 6 depict that BelMan is more stable and eﬃcient than the competing
algorithms: Q-UCB, Q-Thompson sampling, and Thompson sampling. We observe
that in queues 2 and 3 the average service rates are lower than the corresponding
arrival rates. Due to this inherent constraint, the queue 2 and 3 can have unstable
queueing systems if the initial exploration of the algorithm does not damp fast
enough. Though the randomisation of Thompson sampling is good for exploration
but in this case playing the suboptimal servers can induce instability which aﬀects
the total performance in future.

5 Related Work
[5] posed the problem of discounted reward bandits with inﬁnite horizon as a
single-state Markov decision process [17] and proposed an algorithm for com-
puting deterministic Gittins indices to choose the arm to play. Though Gittins
index is proven to be optimal for discounted Bayesian bandits with Bernoulli
rewards [17], explicit computation of the indices is not always tractable and
does not provide clear insights into what they look like and how they change as
sampling proceeds [28]. This motivated researchers to design computationally
tractable algorithms [7] that still retain the asymptotic eﬃciency [26].

14

D. Basu et al.

These algorithms can be classiﬁed into two categories: frequentist and Bayesian.
Frequentist algorithms use the history obtained as the number of arm plays and
corresponding rewards obtained to compute point estimates of the ﬁtness index
to choose an arm. UCB [3], UCB-tuned [3], KL-UCB [14], KL-UCB-Exp [14],
KL-UCB+ [20] are examples of frequentist algorithms. These algorithms are
designed by the philosophy of optimism in face of uncertainty. This methodology
prescribes to act as if the empirically best choice is truly the best choice. Thus,
all these algorithms overestimate the expected reward of the corresponding arms
in form of frequentist indices.

Bayesian algorithms encode available information on the reward generation
process in form of a prior distribution. For stochastic bandits, this prior consists
of K belief distributions on the arms. The history obtained by playing the bandit
game is used to update the posterior distribution. This posterior distribution is
further used to choose the arm to play. Thompson sampling [34], information-
directed sampling [32], Bayes-UCB [20], and BelMan are Bayesian algorithms.

In a variant of the stochastic bandit problem, called the pure exploration
bandit problem [8], the goal of the gambler is solely to accumulate information
about the arms. In another variant of the stochastic bandit problem, the gambler
interacts with the bandit in two consecutive phases of pure exploration and
exploration–exploitation. [29] named this variant the two-phase reinforcement
learning problem. Two-phase reinforcement learning gives us a middle ground
between model-free and model-dependent approaches in decision making which is
often the path taken by a practitioner [13]. As frequentist methods are well-tuned
for exploration-exploitation bandits, a diﬀerent set of algorithms need to be
developed for pure exploration bandits [8]. [23] pointed out the lack of Bayesian
methods to do so. This motivated recent developments of Bayesian algorithms [31]
which are modiﬁcations of their exploration–exploitation counterparts such as
Thompson sampling. BelMan leverages its geometric insight to manage the pure
exploration bandits only by turning the exposure to inﬁnity. Thus, it provides a
single framework to manage the pure exploration, exploration–exploitation, and
two-phase reinforcement learning problems only by tuning the exposure.
6 Conclusion
BelMan implements a generic Bayesian information-geometric approach for
stochastic multi-armed bandit problems. It operates in a statistical manifold
constructed by the joint distributions of beliefs and rewards. Their barycentre,
the pseudobelief-reward, summaries the accumulated information and forms
the basis of the exploration component. The algorithm is further extended by
composing the pseudobelief-reward distribution with a reward distribution that
gradually concentrates on higher rewards by means of a time-dependent function,
the exposure. In short, BelMan addresses the issue of the adaptive balance of
exploration–exploitation from the perspective of information representation, ac-
cumulation, and balanced induction of exploitative bias. Consequently, BelMan
can be uniformly tuned to support pure exploration, exploration–exploitation,
and two-phase reinforcement learning problems. BelMan, when instantiated to
rewards modelled by any distribution of the exponential family, conveniently

BelMan: An Information-Geometric Approach to Stochastic Bandits

15

leads to analytical forms that allow derivation of a well-deﬁned and unique
projection as well as to devise an eﬀective and fast computation. In queueing
bandits, the agent tries and minimises the queue length while also learning the
unknown service rates of multiple servers. Comparative performance evaluation
shows BelMan to be more stable and eﬃcient than existing algorithms in the
queueing bandit literature.

We are investigating the analytical asymptotic eﬃciency and stability of
BelMan. We are also investigating how BelMan can be extended to other settings
such as dependent arms, non-parametric distributions and continuous arms.
Acknowledgement
We would like to thank Jonathan Scarlett for valuable discussions. This work is
partially supported by WASP-NTU grant, the National University of Singapore
Institute for Data Science project WATCHA, and Singapore Ministry of Education
project Janus.

References
1. Agueh, M., Carlier, G.: Barycenters in the wasserstein space. SIAM Journal on

Mathematical Analysis 43(2), 904–924 (2011)

2. Amari, S.I., Nagaoka, H.: Methods of information geometry, Translations of mathe-

matical monographs, vol. 191. American Mathematical Society (2007)

3. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed

bandit problem. Machine learning 47(2–3), 235–256 (2002)

4. Barbaresco, F.: Information geometry of covariance matrix: Cartan-siegel homoge-
neous bounded domains, mostow/berger ﬁbration and frechet median. In: Matrix
Information Geometry, pp. 199–255. Springer (2013)

5. Bellman, R.: A problem in the sequential design of experiments. Sankhy¯a: The

Indian Journal of Statistics (1933–1960) 16(3/4), 221–229 (1956)

6. Brown, L.D.: Fundamentals of Statistical Exponential Families: With Applications

in Statistical Decision Theory. Institute of Mathematical Statistics (1986)

7. Bubeck, S., Cesa-Bianchi, N., et al.: Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends in Machine Learning 5(1),
1–122 (2012)

8. Bubeck, S., Munos, R., Stoltz, G.: Pure exploration in multi-armed bandits problems.

In: ALT. pp. 23–37. Springer (2009)

9. Capp´e, O., Garivier, A., Kaufmann, ´E.: pymaBandits (2012), http://mloss.org/

software/view/415/

10. Csisz´ar, I.: Sanov property, generalized I-projection and a conditional limit theorem.

The Annals of Probability 12(3), 768–793 (1984)

11. DeGroot, M.H.: Optimal statistical decisions, Wiley Classics Library, vol. 82. John

Wiley & Sons (2005)

12. Durrett, R.: Probability: theory and examples. Cambridge University Press (2010)
13. Faheem, M., Senellart, P.: Adaptive web crawling through structure-based link

classiﬁcation. In: Proc. ICADL. pp. 39–51. Seoul, South Korea (Dec 2015)

14. Garivier, A., Capp´e, O.: The KL-UCB algorithm for bounded stochastic bandits

and beyond. In: COLT. pp. 359–376 (2011)

15. Garivier, A., Lattimore, T., Kaufmann, E.: On explore-then-commit strategies.
In: Advances in Neural Information Processing Systems 29, pp. 784–792. Curran
Associates, Inc. (2016)

16

D. Basu et al.

16. Garivier, A., M´enard, P., Stoltz, G.: Explore ﬁrst, exploit next: The true shape of

regret in bandit problems. arXiv preprint arXiv:1602.07182 (2016)

17. Gittins, J.C.: Bandit processes and dynamic allocation indices. Journal of the Royal

Statistical Society. Series B (Methodological) 41(2), 148–177 (1979)

18. Gopalan, A., Mannor, S.: Thompson sampling for learning parameterized markov

decision processes. In: Conference on Learning Theory. pp. 861–898 (2015)

19. Jaynes, E.T.: Prior probabilities. IEEE Transactions on Systems Science and

Cybernetics 4, 227–241 (1968)

20. Kaufmann, E.: On bayesian index policies for sequential resource allocation. Annals

of Statistics 46(2), 842–865 (April 2018)

21. Kaufmann, E., Capp´e, O., Garivier, A.: On Bayesian upper conﬁdence bounds for

bandit problems. In: AISTATS. pp. 592–600 (2012)

22. Kaufmann, E., Kalyanakrishnan, S.: Information complexity in bandit subset

selection. In: COLT. pp. 228–251 (2013)

23. Kawale, J., Bui, H.H., Kveton, B., Tran-Thanh, L., Chawla, S.: Eﬃcient Thompson
sampling for online matrix-factorization recommendation. In: NIPS. pp. 1297–1305
(2015)

24. Krishnasamy, S., Sen, R., Johari, R., Shakkottai, S.: Regret of queueing bandits.
In: Advances in Neural Information Processing Systems. pp. 1669–1677 (2016)

25. Kullback, S.: Information theory and statistics. Courier Corporation (1997)
26. Lai, T.L., Robbins, H.: Asymptotically eﬃcient adaptive allocation rules. Adv. Appl.

Math. 6(1), 4–22 (Mar 1985)

27. Macready, W.G., Wolpert, D.H.: Bandit problems and the exploration/exploitation
tradeoﬀ. IEEE Transactions on evolutionary computation 2(1), 2–22 (1998)
28. Nino-Mora, J.: Computing a classic index for ﬁnite-horizon bandits. INFORMS

Journal on Computing 23(2), 254–267 (2011)

29. Putta, S.R., Tulabandhula, T.: Pure exploration in episodic ﬁxed-horizon Markov

decision processes. In: AAMAS. pp. 1703–1704 (2017)

30. Robbins, H.: Some aspects of the sequential design of experiments. Bull. Amer.

Math. Soc. 58(5), 527–535 (09 1952)

31. Russo, D.: Simple bayesian algorithms for best arm identiﬁcation. In: Conference

on Learning Theory. pp. 1417–1418 (2016)

32. Russo, D., Van Roy, B.: An information-theoretic analysis of Thompson sampling.

Journal of Machine Learning Research (2014)

33. Shen, X., Wasserman, L., et al.: Rates of convergence of posterior distributions.

The Annals of Statistics 29(3), 687–714 (2001)

34. Thompson, W.R.: On the likelihood that one unknown probability exceeds another

in view of the evidence of two samples. Biometrika 25(3–4), 285 (1933)

35. Wong, W.H., Shen, X.: Probability inequalities for likelihood ratios and convergence

rates of sieve mles. The Annals of Statistics 23(2), 339–362 (1995)

BelMan: An Information-Geometric Approach to Stochastic Bandits

17

Supplementary Material: BelMan

We provide the following supplementary material:

– in Section A, an extended discussion of the related work and of the setting

of bandits, beyond what could ﬁt in the main paper;

– in Section A, proofs and technical details that complement the methodology

section (Section 2);

– in Section A.9, additional experiments in the exploration–exploitation setup.

A Extended Discussion of Related Work

In the exploration–exploitation
Exploration–exploitation bandit problem.
bandits, the agent searches for a policy that maximises the expected value of
cumulative reward ST (cid:44) E
as T → ∞. A policy is asymptotically con-
sistent [28] if it asymptotically tends to choose the arm with maximum expected
reward µ∗ (cid:44) max1(cid:54)a(cid:54)K µ(θa), i.e.,

t=1 Xat

(cid:104)(cid:80)T

(cid:105)

lim
T →∞

1
T

ST = µ∗.

(4)

The cumulative regret RT [24] is the amount of extra reward the gambler can
obtain if she knows the optimal arm a∗ and always plays it instead of the present
sequence:

RT (cid:44) T E [Xa∗ ] − E

(cid:35)

(Xat)

(cid:34) T

(cid:88)

t=1

= T µ∗ −

K
(cid:88)

E

(cid:34) T

(cid:88)

a=1

t=1

(cid:35)

(Xat × 1(at = a))

=

K
(cid:88)

a=1

[µ∗ − µa] E[ta

T ],

where ta
T is the number of times arm a is pulled till the T th iteration. [24]
proved that for all algorithms satisfying RT = o(T c) for a non-negative c, the
cumulative regret increases asymptotically in Ω(log T ). Such algorithms are
called asymptotically eﬃcient. The Lai–Robbins bound can be mathematically
formulated as

lim inf
T →∞

RT
log T

(cid:62)

[µ∗(θ) − µ(θa)]

(cid:80)
a:µ∗(θ)>µ(θa)
inf a DKL(fθa (x) (cid:107) fθ∗ (x))

,

(5)

where fθ∗ (x) is the reward distribution of the optimal arm. This states that the
best we can achieve is a logarithmic growth of cumulative regret. It also implies

18

D. Basu et al.

that this optimality is harder to achieve as the minimal KL-divergence between
the optimal arm and any other arm decreases. This is intuitive because in such
scenario the agent has to explore these two arms more to distinguish between
them and to choose the optimal arm. [24] also showed that for speciﬁc reward
distributions, the expected number of draws of any suboptimal arm a should
satisfy

(cid:18)

ta
T

(cid:54)

1
inf a DKL(fθa (x) (cid:107) fθ∗ (x))

(cid:19)

+ o(1)

log T.

(6)

Equation (5) and (6) together claim that the best achievable number of draws
of suboptimal arms is Θ(log T ). Based on this bound, [3] extensively studied
the upper conﬁdence bound (UCB) family of algorithms. These algorithms
operate on the philosophy of optimism in face of uncertainty. They compute the
upper conﬁdence bounds of each of the arm’s distributions in a frequentist way
and choose the one with the maximum upper conﬁdence bound optimistically
expecting that one to be the arm with maximum expected reward. Later on, this
family of algorithms was analysed and improved to propose algorithms such as
KL-UCB [15] and DMED [17].

1

, . . . , θtrue

Frequentist approaches implicitly assume a ‘true’ parametrization of reward
distributions (θtrue
K ). In contrast, Bayesians model the uncertainty
on the parameter using another probability distribution B (θ1, . . . , θK) [13, 29]
which is referred to as the belief distribution. Bayesian algorithms begin with
a prior B0 (θ1, . . . , θK) over the parameters and eventually try to ﬁnd out a
posterior distribution such that the Bayesian sum of rewards (cid:82) ST dB (θ1, . . . , θK)
is maximised, or equivalently the Bayesian risk (cid:82) RT dB (θ1, . . . , θK) is minimised.
Another variant of the Bayesian formulation was introduced by [4] with a
discounted reward setting. Unlike ST , the discounted sum of rewards Dγ (cid:44)
(cid:80)∞
t=0 [γtxt+1] is calculated over inﬁnite horizon. Here, γ ∈ [0, 1) ensures con-
vergence of the sequential sum of rewards for inﬁnite horizon. Intuitively, the
discounted sum implies the eﬀect of an action decay with each time step by the
discount factor γ. This setting assumes K independent priors on each of the arms
and also models the process of choosing the next arm as a Markov process. Thus,
the bandit problem is reformulated as maximising

(cid:90)

(cid:90)

. . .

Eθ[Dγ]db1(θ1) . . . dbK(θK)

where, ba is the independent prior distribution on the parameter θa for a =
1, . . . , K. [16] showed the agent can have an optimally indexed policy by sampling
from the arm with largest Gittins index

E

Ga(sa) (cid:44) sup
τ >0

(cid:20) τ
(cid:80)
t=0

E

(cid:20)τ −1
(cid:80)
t=0

γtxa(Sa

t ) | Sa

(cid:21)

0 = sa
(cid:21)

γt | Sa

0 = sa

where sa is the state of arm a and τ is referred to as the stopping time i.e, the
ﬁrst time when the index is no greater than its initial value. Though Gittins

BelMan: An Information-Geometric Approach to Stochastic Bandits

19

index [16] is proven to be optimal for discounted Bayesian bandits with Bernoulli
rewards, explicit computation of the indices is not always tractable and does not
provide clear insights into what they look like and how they change as sampling
proceeds [25].

Thus, researchers developed approximation algorithms [23] and sequential
sampling schemes like Thompson sampling [30]. At any iteration, the latter
samples K parameter values from the belief distributions and chooses the arm
that has maximum expected reward for them. [19] also proposed a Bayesian
analogue of the UCB algorithm. Unlike the original, it uses belief distributions to
keep track of arm uncertainty and update them using Bayes’ theorem, computes
UCBs for each arm using the belief distributions, and chooses the arm accordingly.

Pure exploration bandit problem.
In this variant of the bandit problem, the
agent aims to gain more information about the arms. [8] formulated this notion of
gaining information as minimisation of the simple regret rather than cumulative
regret. Simple regret rt(θ) at time t is the expected diﬀerence between the
maximum achievable reward Xa∗ and the sampled reward Xat. Unlike cumulative
regret, minimising simple regret depends only on exploration and the number of
available rounds to do so. [8] proved that, for Bernoulli bandits, if an exploration–
exploitation algorithm achieves an upper-bounded regret, it cannot reduce the
expected simple regret by more than a ﬁxed lower bound. This establishes
the fundamental diﬀerence between exploration–exploitation bandits and pure
exploration bandits. [2] identiﬁed the pure exploration problem as best arm
identiﬁcation and proposed the Successive Rejects algorithm under ﬁxed budget
constraints. [7] extended this algorithm for ﬁnding m-best arms and proposed
the Successive Accepts and Rejects algorithm. In another endeavour to adapt
the UCB family to pure exploration scenario, the LUCB family of frequentist
algorithms are proposed [20]. In the beginning, they sample all the arms. Following
that, they sample both the arm with maximum expected reward and the one with
maximum upper-conﬁdence bound till the algorithm can identify each of them
separately. Existing frequentist algorithms [2, 7, 20] do not provide an intuitive
and rigorous explanation of how a uniﬁed framework would work for both the pure
exploration and the exploration–exploitation scenario. As discussed in Section 1,
both Thompson sampling and Bayes-UCB also lack this feature of constructing a
single successful structure for both pure exploration and exploration–exploitation.

Two-Phase reinforcement learning. Two-phase reinforcement learning prob-
lems append the exploration–exploitation problem after the pure exploration
problem. The agent gets an initial phase of pure exploration for a given window.
In this phase, the agent collects more information about the underlying reward
distributions. Following this, the agent goes through the exploration–exploitation
phase. In this phase, it solves the exploration–exploitation problem and focuses
on maximising the cumulative reward. This setup is perceivable as an initial
online model building or ‘training’ phase followed by an online problem solving
or ‘testing’ phase. This problem setup often emerges in applications [14] where
the decision maker explores for an initial phase to create a knowledge base and

20

D. Basu et al.

another phase to take decisions by leveraging this pre-build knowledge base. In
applications, this way of beginning the exploration–exploitation is called a warm
start. Thus, two-phase reinforcement learning gives us a middle ground between
model-free and model-dependent approaches in decision making which is often
the path taken by a practitioner.

Formally, this knowledge-base is a prior distribution built from the agent’s
experience. Since Bayesian methods naturally accommodate and leverage prior
distributions, Bayesian formulation provide the scope to approach this problem
without any modiﬁcation. [27] approached this problem with a technique amalga-
mating a sampling technique, PSPE, and an extension of Thompson sampling,
PSRL [26], for episodic ﬁxed horizon Markov decision processes (MDPs) [11].
PSPE uses Bayesian update to create a posterior distribution for the reward
distribution of a policy. Then, PSPE samples from the distribution in order to
evaluate the policies. These two steps are performed iteratively for the initial
pure exploration phase. PSRL [26] is an extension of Thompson sampling for
episodic MDPs. Unlike Thompson sampling, they also use Markov chain Monte
Carlo method for creating the posteriors corresponding to each of the policies.
Though the amalgamation of these two methods for the two phase problems
in episodic MDPs perform reasonably, they lack a reasonable uniﬁed structure
attacking the problem and a natural cause to pipeline them.

A.1 KL-divergence on the Manifold.

Kullback-Liebler divergence (or KL-divergence) [22] is a pre-metric measure of
dissimilarity between two probability distributions.

Deﬁnition 6 (KL-divergence). If there exist two probability measures P and
Q deﬁned over a support set S and P is absolutely continuous with respect to Q,
we deﬁne the KL-divergence between them as

DKL(P (cid:107) Q) (cid:44)

(cid:90)

S

log

dP
dQ

dP .

dP
dQ is the Radon-Nikodym derivative of P with respect to Q.

Since it represents the expected information lost if P is encoded using Q,
it is also called relative entropy. Depending on the applications, P acts as the
representative of ‘true’ underlying distribution obtained from observations or
data or natural law, and Q represents the model or approximation of P . For
two probability density functions p(s) and q(s) deﬁned over a support set S, the
KL-divergence can be rewritten as

DKL(p(s) (cid:107) q(s)) =

(cid:90)

s∈S

p(s) log

p(s)
q(s)

ds = −h(p(s)) + H(p(s), q(s)).

(7)

Here, h(p(s)) is entropy of p and H(p(s), q(s)) is the mutual information between p
and q. Thus, from an information-theoretic perspective, we perceive KL-divergence

BelMan: An Information-Geometric Approach to Stochastic Bandits

21

as the natural divergence function on the belief-reward manifold when we analyse
the dynamics of the entropy function on it. Except that, any general α-divergence
function on the statistical manifold is a convex combination of ±1-divergences.
Mathematically, for α ∈ (−1, +1),
D(α)(p (cid:107) q) (cid:44) 1 + α
2
1 + α
2

1 − α
2
1 − α
2

D(+1)(p (cid:107) q) +

DKL(q (cid:107) p) +

D(−1)(p (cid:107) q)

DKL(p (cid:107) q).

(8)

=

From a manifold perspective, it seems that the divergence function for the
±1-connections on the belief-reward manifolds and a convex mixture of DKL
divergences form the general notion of movement on any such space. Thus, KL-
divergence between two belief-reward distributions is an eﬀective and natural
quantiﬁer of movement, and also of information accumulation during Bayesian
update. Hence, for updating the beliefs in an optimal manner, and to decrease
the uncertainty, we have to represent the observations using a knowledge-base,
and to minimise the KL-divergence between the knowledge-base and other dis-
tributions respectively. If P are the candidate belief-reward distributions of the
arms formed by accumulation of actions and rewards, and Q are the pseudobelief
or pseudobelief-focal-reward distribution-reward distributions, the alternating
minimisation scheme looks for the most succinct representation Q of the knowl-
edge and the exploitation bias while choosing such arms whose belief-reward
distributions resemble their true reward distributions as much as possible.

A.2 Exponential Family

Use of KL-divergence as a divergence measure on the statistical manifolds and also
the issue of representation of a random variable using suﬃcient statistics provoked
the study of the exponential family of distributions. Interesting properties of
exponential family distributions, such as existence of ﬁnite representation of
suﬃcient statistics, convenient mathematical form, and existence of moments,
provided them a central stage in the ﬁeld of mathematical statistics [6][12][18][21].

The exponential family [6] is a class of probability distributions which is
deﬁned by a set of natural parameters ω(θ) and a suﬃcient statistics T (X) of
the random variable X as follows:

fθ(X) (cid:44) g(X) exp ((cid:104)ω(θ), T (x)(cid:105) − A(θ)) .

Here, g(X) is the base measure on reward X and A(θ) is called the log-partition
function. The exponential family includes the majority of the distributions found
in the bandit literature such as Bernoulli, beta, Gaussian, Poisson, exponential,
and chi-squared. For T (X) = X, the log-partition function is logarithm of the
Laplace transform of the base measure.

Example 1. Bernoulli distribution with probability of success θ ∈ (0, 1) is deﬁned
as

fθ(X) (cid:44) Ber(θ) = θX (1 − θ)(1−X)

22

D. Basu et al.

(cid:18)

= exp

X log

(cid:18) θ

(cid:19)

1 − θ

(cid:19)

+ log(1 − θ)

for X ∈ {0, 1}. Here, the base measure g(x) is 1. The suﬃcient statistics is

T (X) = X. The natural parameter is ω(θ) = log

function is A(θ) = − log(1 − θ) = log(1 + exp(ω)).

(cid:17)

(cid:16) θ
1−θ

. The log-partition

We choose the exponential family to instantiate our framework not only
because of its wide range and applicability but also due to its well behaving
Bayesian and information geometric properties. From a sampling and uncertainty
representation point of view, the exponential family is useful because of its
ﬁnite representation of suﬃcient statistics. Speciﬁcally, suﬃcient statistics of
exponential family can represent any arbitrary number of independent identi-
cally distributed samples using a ﬁnite number of variables [21]. This keeps the
uncertainty representation tractable for exponential family distributions.

From a Bayesian point of view, the useful property of the exponential family is
the existence of conjugate distributions which also belong to this family [6]. Two
parametric distributions fθ(x) and bη(θ) are conjugate if the posterior distribution
P(θ|x) formed by multiplying them has the same form as bη(θ). Mathematically,
the conjugate distribution of the distribution of Equation A.2 is given by bη(θ) (cid:44)
P(θ|η, v) = f (η, v) exp((cid:104)η, θ(cid:105) − vA(θ)) = f (η, v)g(θ)v exp((cid:104)η, θ(cid:105)). Here, η is the
parameter of the conjugate prior and v > 0 corresponds to the eﬀective number of
observations that the prior contributes. Thus, if the reward distribution belongs
to the exponential family, the belief distribution is represented as: bη(θ) (cid:44)
h(θ) exp ((cid:104)η, T (θ)(cid:105) − A(η)) with the natural parameters η ∈ Rd(cid:48)

.

From information geometric point of view, exponential family distributions
are ﬂat with respect to KL-divergence [1]. Thus, both information and reverse
information projections [10] that we would use in BelMan are well-deﬁned and
unique. Thus, at each iteration, we obtain an optimal and unambiguous compu-
tation of the decision variables of BelMan. [1] also stated that the necessary and
suﬃcient condition for a parametric probability distribution to have an eﬃcient
estimator is that the distribution belongs to the exponential family and has an
expectation parametrisation. Thus, working with exponential family distributions
implicitly supports the well-deﬁned nature and possibility of getting an eﬃcient
estimation.

A.3 Pseudobelief–reward: Existence, Uniqueness and Consistency

In order to establish pseudobelief–reward as a valid knowledge-base for all
the arms, we have to prove that it exists uniquely and its parameters can be
consistently estimated.

The proofs require only two assumptions. Firstly, the belief–reward manifold
can be described by a unique chart. This implies that pdf of the belief–reward
distributions is a bijective function of parameters. Secondly, there exist unique

BelMan: An Information-Geometric Approach to Stochastic Bandits

23

geodesics between any two points of the belief–reward manifold. This implies that
the divergence function between any two belief–reward distributions is uniquely
deﬁned. Instead of having such modest requirement, we represent our proofs in
form of the exponential family distributions due to ease of presentation and our
limited interest.

Theorem 1. For given set of belief-reward distributions {Pa
a=1 deﬁned on the
same support set and having a ﬁnite expectation, ¯Pt is uniquely deﬁned, and is
such that its expectation parameter veriﬁes ˆµt(θ) = 1
K

a=1 µa

t (θ).

t }K

(cid:80)K

Proof. For belief–reward distributions Pa and P, the KL-divergence is deﬁned as

(cid:90)

(cid:90)

X

(cid:90)

X

DKL (Pa

t (cid:107) P) =

=

=

=

θ
(cid:90)

θ
(cid:90)

θ
(cid:90)

Pa

t (X, θ) log

fθ(X)ba
ξt

(θ) log

Pa
t (X, θ)
P(X, θ)
ba
(θ)
ξt
bξ(θ)

dxdθ

dxdθ

ba
ξt

(θ) log

ba
ξt

(θ) log

ba
(θ)
ξt
bξ(θ)
ba
(θ)
ξt
bξ(θ)

(cid:20)(cid:90)

(cid:21)
fθ(X)dx

dθ

X

dθ

θ
= Eba
= (cid:104)ξa

t

[(cid:104)ξa
t − ξ, µa

t , Θ(θ)(cid:105) − Ψ a
t (θ)(cid:105) − Ψ a

t (ξa
t (ξa

t ) − (cid:104)ξ, Θ(θ)(cid:105) + Ψ (ξ)]
t ) + Ψ (ξ).

Thus, the objective function that ¯P minimises is given by

F (P) (cid:44) 1
K

K
(cid:88)

a=1

DKL (Pa

t (cid:107) P) =

1
K

K
(cid:88)

(cid:104)ξa

t − ξ, µa(θ)(cid:105) −

a=1

1
K

K
(cid:88)

a=1

t (ξa
Ψ a

t ) + Ψ (ξ).

(9)

Since the exponential family distributions are dually ﬂat [1], we get a unique
expectation parametrisation µ(θ) of the belief distributions for a given natural
parametrisation ξ. The expectation parameter is deﬁned as µ(θ) (cid:44) Eb[Θ(θ)] =
∇ξΨ (ξ). µ(θ) dually expresses a natural parametrisation as its dual. Mathe-
matically, ξ = ∇µ((cid:104)ξ, µ(cid:105) − Ψ (ξ)) = ∇µΦ(µ). Ψ (ξ) and Φ(µ) are log-normalisers
under two parametrisations and are convex conjugate to each other. If we deﬁne
t , we get a unique natural parameter ˆξt (cid:44) ξ(ˆµt). This allows
ˆµt(θ) (cid:44) 1
K
us to rewrite Equation 9 as

a=1 µa

(cid:80)K

F (P) =

(cid:105)
(cid:104)
(cid:104) ˆξt − ξ, ˆµt(θ)(cid:105) − Ψ ( ˆξt) + Ψ (ξ)

+

1
K

K
(cid:88)

a=1

((cid:104)ξa

t , µa

t (θ)(cid:105) − Ψ a

t (ξ)) − ((cid:104)ξ(ˆµt), ˆµt(θ)(cid:105) − Ψ (ξ(ˆµt))

= DKL (Pˆµt (cid:107) P) +

1
K

K
(cid:88)

a=1

Φ(µa

t ) − Φ(ˆµt) (cid:62) 1
K

K
(cid:88)

a=1

Φ(µa

t ) − Φ(ˆµt).

24

D. Basu et al.

Fig. 7. Evolution of the focal distribution over X ∈ [0, 1] for t = 1, 10, 100 and 1000.
Since DKL (Pˆµt (cid:107) P) = 0 for P = Pˆµt, F (P) reaches unique minimum F (Pˆµ) for the
belief–reward distribution with expectation parameter ˆµt(θ) (cid:44) 1
t . Thus,
K
for a given set of belief–reward distributions the pseudobelief–reward distribution
¯Pt(X, θ) (cid:44) Pˆµt(X, θ) is a unique distribution in belief–reward manifold.

a=1 µa

(cid:80)K

Corollary 1. The pseudobelief-reward distribution ¯Pt(X, θ) is the unique point on
the belief-reward manifold that has minimum KL-divergence from the distribution
ˆPt(X, θ) (cid:44) 1
K

t (X, θ).

(cid:80)K

Pa

a=1

Proof. KL-divergence from ˆPt(X, θ) to any pseudobelief–reward distribution
P(X, θ)is

DKL

(cid:17)
(cid:16)ˆPt (cid:107) P

= DKL

(cid:16)ˆPt (cid:107) ¯Pt

(cid:17)

+ (cid:104) ˆξt − ξ, ˆµt(cid:105) − Ψ ( ˆξt) + Ψ (ξ) = DKL

(cid:17)
(cid:16)ˆP (cid:107) ¯P

+ DKL

(cid:0)¯P (cid:107) P(cid:1) .

Here, ¯Pt is the pseudobelief distribution with ˆξt and ˆµt as deﬁned in Theorem 1.
Since ˆPt is a mixture of belief–reward distributions, it does not belong to the belief–
(cid:17)
(cid:16)ˆPt (cid:107) P
reward manifold. Thus, ˆPt (cid:54)= ¯Pt and DKL
attends unique minimum for P = ¯Pt.

> 0. Hence, DKL

(cid:16)ˆPt (cid:107) ¯Pt

(cid:17)

A.4 Focal Distribution: Visualisation

The focal distribution gradually concentrates on higher rewards as the exposure
τ (t) decreases with time. We see this feature in Figure 7. Thus, it constrains
using KL-divergence to choose distributions with higher rewards and induces the
exploitive bias.

A.5 Condition for Existence of Alternating Projection Scheme

Both I- and rI-projections are valid and well-deﬁned if the KL-divergence between
any two distributions in P and Q is deﬁned and ﬁnite.

00.51Reward(R)00.20.40.60.81Lt(R)(1) > (10) > (100) > (1000)(1)(10)(100)(1000)BelMan: An Information-Geometric Approach to Stochastic Bandits

25

Assumption 4 (Absence of singularities). The distribution families P and
Q are deﬁned over the sets Supp(P) (cid:44) {a : p(a) > 0, ∀p ∈ P} and Supp(Q) (cid:44)
{a : q(a) > 0, ∀p ∈ P} respectively. Moreover, none of the supports are empty
and Supp(P) ⊆ Supp(Q).

A.6 Implications of Alternating Projections

Deﬁnition 4 (I-projection). The information projection (or I-projection) of a
distribution ¯Q ∈ Q onto a non-empty, closed, convex set P of probability distribu-
tions, Pa’s, deﬁned on a ﬁxed support set is deﬁned by the probability distribution
Pa∗ ∈ P that has minimum KL-divergence to q: Pa∗ (cid:44) arg minPa∈P DKL(Pa (cid:107) ¯Q).

Since DKL(p(s) (cid:107) q(s)) = −h(p(s)) + H(p(s), q(s)), we observe that the I-
projection p∗ is the distribution in P that maximises the entropy h(p) of P, while
minimising the mutual information H(p, q): it is the distribution in P which is
most similar to q. This implies that the I-projection p∗ captures at least the ﬁrst
moment, i.e., the expectation of the ﬁxed distribution q.

In the last part (Lines 8–9), the updated beliefs are used to obtain the
pseudobelief-focal-reward distribution using rI-projection. Following Theorem 1,
rI-projection would lead to a unique pseudobelief-focal-reward distribution for
a given set of belief-rewards and exposure τ (t). Here, BelMan is inducing the
exploitative bias. It keeps the pseudobelief-focal-reward distribution away from
the ‘actual’ barycentre of the belief-reward distributions and pushes it towards
the arms with higher expected reward. Increasing exploitative bias eventually
merges the pseudobelief-focal-reward distribution to the distribution of the arm
having the highest expected reward.

Deﬁnition 5 (rI-projection). The reverse information projection (or rI-projection)
of a distribution Pa ∈ P onto Q, which is also a non-empty, closed, convex set
of probability distributions on a ﬁxed support set, is deﬁned by the distribu-
tion ¯Q∗ ∈ Q that has minimum KL-divergence from Pa: ¯Q∗ (cid:44) arg min ¯Q∈Q
DKL(Pa (cid:107) ¯Q).

The rI-projection ﬁnds the distribution q∗ from a space of candidate distribu-
tions Q that encodes maximum information of the distribution p. If the set of
candidate distributions is engendered by a statistical model, the rI-projection
of the empirical distribution formed from samples to the model is equivalent to
ﬁnding the maximum likelihood estimate. Since rI-projection aims to maximise the
complete likelihood rather than ﬁnding a distribution with similar entropy, q∗ also
captures higher moments of the ﬁxed distribution p. Thus, it is computationally
more demanding but more informative than I-projection.

Due to the underlying minimisation operation, if we begin from p0 ∈ P and
q0 ∈ Q and alternately perform I-projection and reverse I-projection, it will lead
to two distributions p∗ and q∗ for which the KL-divergence between sets P and
Q are minimum [10].

26

D. Basu et al.

A.7 Law of Convergence for the Pseudobelief-reward Distribution

We are simultaneously approximating the belief–reward parameters as well as
the pseudobelief–reward parameters. If we look into the belief update step
(Equation 1), we observe that the belief distribution of each arm ba
(θ) is updated
ξt
by incorporating i.i.d samples obtained from the reward distribution of that
arm. Let us assume that BelMan has played total T times and any arm a for
ta
T times. Since we are doing na¨ıve Bayesian updates with i.i.d. samples, the
belief distributions will follow central limit theorem. This means that if ˜µa
ta is
the estimate of the expectation parameters of the belief distribution of arm a
constructed from samples {X a
− µa) converges in distribution to
a centered normal random vector in N (0, Σa). In Theorem 2, we show that the
estimator of the mean parameters of pseudobelief is also consistent with these
estimators and satisﬁes central limit theorem.

i=1, (cid:112)ta

T (˜µa
ta
T

i }ta

T

Theorem 2 (Central limit theorem). If ˜¯µT (cid:44) 1
is estimator of
K
T (˜¯µT − ¯µ) converges
the expectation parameters of the pseudobelief distribution,
in distribution to a centered normal random vector in N (0, ¯Σ). The covariance
matrix ¯Σ = (cid:80)K

tends to λa as T → ∞.

a=1 ˜µa
ta
√
T

a=1 λaΣa such that

(cid:80)K

T
K2ta
T

Proof. The characteristics function for

√

Φ√

T (˜¯µT −¯µ)(t) = E

(cid:104)

exp(ι(cid:104)t,

(cid:34)

= E

exp(ι(cid:104)t,

√

N (˜¯µN − ¯µ) is
(cid:105)

T (˜¯µT − ¯µ)(cid:105))
√

T
K

K
(cid:88)

a=1
√

T
K
√

(˜µa
ta
T

− µa)(cid:105))

(˜µa
ta
T

− µa)(cid:105))

t, (cid:112)ta

T (˜µa
ta
T

T
K(cid:112)ta
T
(cid:32) √

(cid:33)

t

T
K(cid:112)ta
T

(cid:34)

(cid:34)

K
(cid:89)

E

=

exp(ι(cid:104)t,

a=1

K
(cid:89)

a=1

K
(cid:89)

a=1

=

=

E

exp(ι(cid:104)

Φ√

T (˜µa
ta
ta
T

−µa)

(cid:35)

(cid:35)

(cid:35)

− µa)(cid:105))

Since each of the (cid:112)ta
T (˜µa
ta
T
tor that follows N (0, Σa), the covariance matrix for

− µa) converges in distribution to a random vec-
T (˜¯µT − ¯µ) would be

√

limT →∞

(cid:80)K

a=1

(cid:19)2

(cid:18) √
T
√
ta
T

K

Σa = (cid:80)K

a=1 λaΣa (cid:44) ¯Σ.

A.8 Proof of Theorem 3

Theorem 3 (Asymptotic consistency). Given τ (t) =
log t+c×log log t for any
c (cid:62) 0, BelMan will asymptotically converge to choosing the optimal arm in case

1

BelMan: An Information-Geometric Approach to Stochastic Bandits

27

of a bandit with bounded reward and ﬁnite arms. Mathematically, if there exists
µ∗ (cid:44) maxa µ(θa),

(cid:35)

Xat

= µ∗.

(3)

lim
T →∞

1
T

(cid:34) T

(cid:88)

E

t=1

We reformulate this result more precisely using Lemma 2.

Lemma 2.
expected reward µ∗ (cid:44) maxa µ(θa), and the exposure satisﬁes limt→∞ τ (t) (cid:54) 1√
2C
then BelMan would satisfy asymptotic consistency

If Assumption 3 is true and there exists at least an optimal arm with
,

lim
T →∞

E

1
T

(cid:34) T

(cid:88)

t=1

(cid:35)

(XAt)

= µ∗.

(10)

Proof. Without loss of generality, let us consider that there exists at least one
optimal arm and it is identiﬁed as the arm a = 1. At the I-projection step, we
t (X, θ) (cid:107) ¯Q(X, θ)(cid:1) from
choose the arm that has minimum KL-divergence DKL
the pseudobelief–focal distribution. Thus, we have to prove that for large t and
for all a (cid:54)= 1,

(cid:0)Pa

P(DKL

(cid:0)P1

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1) − DKL

(cid:0)Pa

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1) < 0) = 1.

lim
t→∞

This is equivalent to proving that almost surely

(cid:0)DKL

(cid:0)P1

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1) − DKL

(cid:0)Pa

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1)(cid:1) < 0.

(11)

lim
t→∞

We begin as follows,

(cid:0)P1
DKL
(cid:90)
(cid:90)

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1) − DKL
t (X, θ) log P1
P1

=

(cid:0)Pa

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1)
(cid:90)
t (X, θ) log Pa
Pa

(cid:90)

t (X, θ) dθ dX −
(cid:123)(cid:122)
T1

X

θ

t (X, θ) dθ dX
(cid:125)

X

θ

(cid:124)

+

(cid:90)

(cid:90)

X

θ

(cid:124)

(cid:2)Pa

t (X, θ) − P1

t (X, θ)(cid:3) log ¯Q(X, θ) dθ dX
(cid:125)

(cid:123)(cid:122)
T2

The ﬁrst term T1 is the diﬀerence in entropy in two of the arms.

(cid:90)

(cid:90)

X

(cid:90)

θ
(cid:90)

X

(cid:90)

θ
(cid:90)

X

(cid:90)

θ
(cid:90)

X

θ

T1 =

=

(cid:54)
(a)

(cid:54)
(b)

t (X, θ) log P1
P1

t (X, θ) dθ dX −

(cid:90)

(cid:90)

X

θ

t (X, θ) log Pa
Pa

t (X, θ) dθ dX

(cid:2)Pa

t (X, θ) − P1

t (X, θ)(cid:3) log P1

t (X, θ) dθ dX − DKL

(cid:0)Pa

t (X, θ) (cid:107) P1

t (X, θ)(cid:1)

(cid:2)Pa

t (X, θ) − P1

t (X, θ)(cid:3) log P1

t (X, θ) dθ dX

(cid:2)Pa

(cid:12)
(cid:12)

t (X, θ) − P1

t (X, θ)(cid:3) log P1

t (X, θ)(cid:12)

(cid:12) dθ dX

28

D. Basu et al.

(cid:54)
(c)

sup
X,θ

(cid:54)
(d)

sup
X,θ

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)
(cid:12)

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)
(cid:12)

(cid:90)

(cid:90)

θ

X
(cid:114)

log 2
2

(cid:12)
(cid:12)Pa

t (X, θ) − P1

t (X, θ)(cid:12)

(cid:12) dθ dX

DKL (Pa

t (X, θ) (cid:107) P1

t (X, θ))

The inequality (a) is due to the non-negativity of KL-divergence. Inequality (b)
is derived from the monotonicity of integrals. This means that if f (cid:54) g for all
w ∈ W then (cid:82)
w∈W g(w) dw. Boundedness of the logarithmic
density function of the pseudobelief-reward as stated in Proposition 3 results to
inequality (c). Inequality (d) is obtained from Pinsker’s inequality [9].

w∈W f (w) dw (cid:54) (cid:82)

Similarly, we get for the second term T2:

T2 =

=

−

(cid:90)

(cid:90)

X

(cid:90)

θ
(cid:90)

θ

X
1
τ (t)

(cid:2)Pa

t (X, θ) − P1

t (X, θ)(cid:3) log ¯Q(X, θ) dθ dX)
(cid:32)

(cid:2)Pa

t (X, θ) − P1

t (X, θ)(cid:3) log

t (X, θ)λa
Pa

t

(cid:89)

a

(cid:33)

dθ dX

EP1

t (X,θ)−Pa

t (X,θ) [X] + log ¯Zt × EP1

t (X,θ)−Pa

t (X,θ) [1]

(cid:54)
(e)

sup
X,θ

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)
(cid:12)

(cid:114)

log 2
2

(cid:113)

DKL (Pa

t (X, θ) (cid:107) P1

t (X, θ)) −

∆a
t
τ (t)

.

(cid:44) µ1

t − µa

Here, ∆a
t , which means the diﬀerence between the expected reward
t
of the optimal arm and the suboptimal arm a. Inequality (e) is obtained by
applying AM-GM inequality, inequalities (a), (b), (c), and (d) in sequence. Thus,

(cid:112)2 log 2

(cid:113)

DKL (Pa

t (X, θ) (cid:107) P1

t (X, θ)) −

∆a
t
τ (t)

T1 + T2 (cid:54) sup
X,θ

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)
(cid:12)
(cid:113)

= (cid:112)2 log 2
(cid:32)

DKL (Pa

t (X, θ) (cid:107) P1

t (X, θ))

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)

(cid:12) −

sup
X,θ

1
τ (t)

(cid:112)DKL (Pa

(cid:33)

∆a
t
t (X, θ) (cid:107) P1
(cid:32)

t (X, θ))

(cid:54) (cid:112)2 log 2

(cid:113)

DKL (Pa

t (X, θ) (cid:107) P1

t (X, θ))

sup
X,θ

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)

(cid:12) −

(cid:33)

√

1
2τ (t)

If we consider limt→∞ for both sides of the inequality, we observe Equation 11 is
true if

(cid:32)

lim
t→∞

sup
X,θ

(cid:12)
(cid:12)log P1

t (X, θ)(cid:12)

(cid:12) −

√

1
2τ (t)

(cid:33)

< 0.

(cid:0)Pa
t (X, θ) (cid:107) P1
This holds as DKL
(cid:12)
(cid:12)log P1
we get limt→∞ supX,θ
in order to satisfy the inequality limt→∞ τ (t) < 1√

t (X, θ)(cid:1) > 0 for all a and t. By Assumption 4,
t (X, θ) = C (cid:48)(say). Thus, we get
2C(cid:48) which is in our premise.

(cid:12) (cid:54) C + log P1

t (X, θ)(cid:12)

BelMan: An Information-Geometric Approach to Stochastic Bandits

29

Lemma 3. For τ (t) =
C < ∞.

log t+c×log log t with c (cid:62) 0, limt→∞ τ (t) < 1

1

C for any

Proof. Since limt→∞

1

log t+c log log t = 0, the aforementioned claim holds true.

Lemma 2 and 3 together prove Theorem 3. This proves that BelMan is

asymptotically consistent for ﬁnite-arm stochastic bandit problems.

For exploration–exploitation bandit problem, we observe that τ (t) has to be a
positive valued function of time t that asymptotically decreases with time. Such
decay in the value of exposure τ (t) adaptively increases the importance of reward
maximisation over minimising the KL-divergence between the belief-reward of
selected arm and the pseudobelief-reward. This mechanism allows BelMan to
adaptively balance between the exploration and exploitation components.

The growth rate proposed for exposure, O( 1

log t ), is a loose bound. Beside
this, it is also distribution independent. Thus, we observe a gap between the
bound on exposure growth obtained here, and the one used in practice. It would
be interesting to ﬁnd out tighter bounds with more speciﬁc constants for given
reward distributions.

A.9 BelMan for Exponential Family Distributions

As mentioned in Section B.2, exponential family [6] is a class of probability
distributions which can be deﬁned using a set of natural parameters ω(θ) and a
given natural suﬃcient statistics T (X) as follows:

fθ(X) (cid:44) h(X) exp ((cid:104)ω(θ), T (X)(cid:105) − A(θ)) .

Here, h(X) is the base measure on reward X and A(θ) is called the log-partition
function. The exponential family includes the majority of the distributions found
in the bandit literature such as Bernoulli, beta, Gaussian, Poisson, exponential,
and chi-squared.

We choose the exponential family to instantiate our framework not only
because of its wide range and applicability but also due to its well behaving
Bayesian and information geometric properties. From a Bayesian point of view,
the most useful property of the exponential family is the existence of conjugate
distributions which also belong to this family [6]. Two parametric distributions
fθ(X) and bη(θ) are conjugate if the posterior distribution P(θ|X) formed by
multiplying them has the same form as bη(θ). Thus, if the reward distribution
belongs to the exponential family, the belief distribution is represented as: bη(θ) (cid:44)
h(θ) exp ((cid:104)η, T (θ)(cid:105) − A(η)) with the natural parameters η.

Since exponential family distributions are ﬂat with respect to KL-divergence
[1], both I-and rI-projections in BelMan are well-deﬁned and unique. Thus,
at each iteration, we obtain an optimal and unambiguous choice of the arm
and pseudobelief respectively. [1] also stated that the necessary and suﬃcient
condition for a parametric probability distribution to have an eﬃcient estimator

1
t , βa
B(αa
t )
t and βa

30

D. Basu et al.

is that the distribution belongs to the exponential family and has an expectation
parametrisation. Thus, working with exponential family distributions implicitly
supports the well-deﬁned nature and possibility of getting an eﬃcient estimation.
Being a member of the exponential family, the belief distributions bη(θ) construct
a statistical manifold with local co-ordinates η [1]. Theorem 1 and 2 validate
these claims in case of BelMan.

Bernoulli Bandits. In the case of Bernoulli bandits, we assume that drawing
an arm returns the rewards 1 and 0 with probability θ and 1 − θ respectively.
Thus, the reward distribution of the ath arm is fθa (X) (cid:44) Ber(θa). Following
the Bayesian approach, we choose the conjugate prior to begin with. Thus, we
keep the prior belief over each arm as a beta distribution with shape parameters
{αa}K
a=1. After t-iterations the prior over the probability of success
of the ath arm is

a=1 and {βa}K

t (θa) (cid:44) Beta(θa; αa
ba

t , βa

t ) =

t −1

θαa

a

(1 − θa)βa

t −1,

t , βa

t > 0 and θa ∈ (0, 1). Here, αa

for αa
t are the number of successes and
failures, respectively, for the arm a till iteration t. We begin with both αa
0 and
βa
0 to be 1 for all arms. This amounts to the uniform distribution over 0 and 1.
This initialisation allows us to choose all the arms with equal probability and
without any initial bias. We update this belief eventually as we further draw the
arms and compute it using BelMan. Under this speciﬁc setting of beta prior and
Bernoulli reward, we compute the targeted KL-divergence of BelMan as

K
(cid:88)

a=1

DKL

(cid:0)Pa

t (X, θ) (cid:107) ¯Qt−1(X, θ)(cid:1)

=

K
(cid:88)

a=1

[−

1
τ (t)

αa
t
N a
t

− log (B (αa

t , βa

t )) + (αa

t − ¯αt−1)Ψ (αa

t ) + (βa

t − ¯βt−1)Ψ (βa

t )−

(N a

t − ¯Nt−1)Ψ (N a

t )] + K log

(cid:32) ¯αt−1 exp( 1
τ (t) ) + ¯βt−1
¯Nt−1

(cid:33)

+ K log (cid:0)B (cid:0)¯αt−1, ¯βt−1

(cid:1)(cid:1) .

t + βa

t = αa

Here, N a
t is the total number of times the jth arm is played till the nth
iteration, ¯N = ¯α + ¯β and Ψ is the digamma function [5] deﬁned as the derivative
of the logarithm of gamma function, i.e. d

da (log Γ (a)).

In Line 4 of Algorithm 1, we ﬁrst perform the I-projection to decide which
arm at to draw to minimize the KL-divergence. Following this, we update the
pseudobelief using I-projection in Line 9 of Algorithm 1. In order to perform this
update, we ﬁnd out such ¯α and ¯β that minimize the objective and update the
pseudobelief accordingly. The presence of pseudobelief oﬀers BelMan a chance
to explore the less successful arms to minimize the entropy, while the Focal
distribution creates the scope of exploiting the present information of the best
arm.

Exponential Bandits. The exponential distribution is another member
of the exponential family. For a given positive rate parameter θa, the reward

BelMan: An Information-Geometric Approach to Stochastic Bandits

31

distribution of arm a of exponential bandit is fθa (X) (cid:44) θa exp(−θaX) for X ∈
[0, ∞). Following the structure of Sections A.9 and the previous Bernoulli case,
we obtain the gamma distribution, another member of the exponential family, as
the conjugate prior. After the tth iteration, the belief distribution corresponding
to ath arm is expressed as

t (θa) (cid:44) Gamma(θa; αa
ba

t , βa

t ) =

αa
t

βa
t
Γ (αa
t )

θa

αa
t −1 exp(−θaβa

t ),

t , βa
for both shape and rate parameters αa
t are, respectively,
the number of times the arm a is played and sum of the rewards obtained by
playing the arm till iteration t. As we update using Equation (1), we get gamma
distributions with parameters αa
t + xt if the arm a is
played and a reward xt is obtained. Under this speciﬁc setting of gamma prior
and exponential reward, we compute the targeted KL-divergence of BelMan as

t > 0. Here, αa

t + 1, and βa

t+1 = αa

t+1 = βa

t and βa

K
(cid:88)

a=1

DKL

(cid:0)Pa

t (X, θ) (cid:107) ¯Q(X, θ)(cid:1)

K
(cid:88)

[−

=

a=1

1
τ (t)

+ ¯αt−1 log βa

− log (Γ (αa

αa
t
βa
t
t ] + K log ¯Zt + K log (Γ (¯αt−1)) − K ¯αt−1 log ¯βt−1.

t − ¯αt−1)Ψ (αa

t )) + (αa

αa
t
βa
t

t ) −

(βa

t − ¯βt−1)

We incorporate this analytical form in Algorithm 1 and update it as mentioned
in the Bernoulli case. Figure 8, 9, and 10 show the evolution of cumulative
regret with number of iterations for the three cases whose number of suboptimal
arm draws are reported in Figure 1, 2, and 3, respectively.

We also experimented on another 2-arm bandit scenario with means 0.45
and 0.55. Figures 11 depicts the evolution of cumulative regret and suboptimal
draws for BelMan and the other competing algorithms. Similar to Figure 11,
we observe the cumulative regret of BelMan grows at ﬁrst linearly and then it
transits to a state of slow growth. Except showing this ideal behaviour, BelMan
performs competitively with the contending algorithms. This shows its eﬃciency
as a candidate solution to the exploration–exploitation bandit.

Figure 12 shows performance for 10-arm Bernoulli bandit. For this setup,
BelMan outperforms other algorithms. We also observe though the number of
arms increases from Figure 11 to Figure 12 that performance of all algorithms is
comparatively better in the ﬁrst case. This is explainable from the fact that hard-
ness of minimising cumulative regret increases as the number of arms increases.
Beside that, as more arms with identical or almost identical distributions appear,
the algorithm requires more exploration to separate them and to determine which
one is optimal. The diﬀerence in performance between Figure 11 and 1 indicates
this.

We ﬁnally tested BelMan on an exponential bandit consisting of 5-arms with
expected rewards {0.2, 0.25, 0.33, 0.5, 1.0}. We compare performance of BelMan

32

D. Basu et al.

Fig. 8. Evolution of cumulative regret (top), and number of suboptimal draws (bottom)
for 2-arm Bernoulli bandit with expected rewards 0.8 and 0.9 for 1000 iterations. The
dark black line shows the average over 25 runs. The grey area shows the 75 percentile.

Fig. 9. Evolution of cumulative regret (top), and number of suboptimal draws (bottom)
for 20-arm Bernoulli bandit with expected rewards [0.25 0.22 0.2 0.17 0.17 0.2 0.13 0.13
0.1 0.07 0.07 0.05 0.05 0.05 0.02 0.02 0.02 0.01 0.01 0.01] for 1000 iterations.

Fig. 10. Evolution of cumulative regret (top), and number of suboptimal draws (bottom)
for 5-arm bounded exponential bandit with expected rewards 0.2, 0.25, 0.33, 0.5, and
1.0 for 1000 iterations.

Fig. 11. Evolution of cumulative regret (top), and number of suboptimal draws (bottom)
for 500 iterations for 2-arm Bernoulli bandit with means 0.45 and 0.55.

05001000051015202530Cumulative regretBelMan05001000051015202530BayesUCB05001000051015202530Thompson05001000051015202530UCB05001000051015202530KLUCB05001000051015202530Gittins05001000051015202530Random050010000102030405060708090100Cumulative regretBelMan050010000102030405060708090100BayesUCB050010000102030405060708090100Thompson050010000102030405060708090100UCB050010000102030405060708090100KLUCB050010000102030405060708090100Gittins050010000102030405060708090100Random05001000time010203040506070Random05001000time010203040506070KLUCBexp05001000time010203040506070KLUCB05001000time010203040506070UCBtuned05001000time010203040506070Cumulative regretBelMan05001000time010203040506070Thompson0500100005101520253035404550timeCumulative regretBelMan05001000050100150200250timeSuboptimal drawsBelMan0500100005101520253035404550timeBayesUCB05001000050100150200250timeBayesUCB0500100005101520253035404550timeThompson05001000050100150200250timeThompson0500100005101520253035404550timeUCB05001000050100150200250timeUCB0500100005101520253035404550timeKLUCB05001000050100150200250timeKLUCB0500100005101520253035404550timeRandom05001000050100150200250timeRandomBelMan: An Information-Geometric Approach to Stochastic Bandits

33

Fig. 12. Evolution of cumulative regret
suboptimal
draws (bottom) for 500 iterations for 10-arm Bernoulli bandit with means
{0.1, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01}. The dark black line shows the
average. The grey area shows 75 percentile.

(top), and number of

Fig. 13. Evolution of cumulative regret (top), and number of suboptimal draws (bot-
tom) for 1000 iterations for 5-arm unbounded exponential bandit with parameters
{0.2, 0.25, 0.33, 0.5, 1.0}.

Fig. 14. Evolution of cumulative regret (top), and number of suboptimal draws (bot-
tom) for 1000 iterations for 5-arm unbounded exponential bandit with parameters
{1, 2, 3, 4, 5}.

05001000010203040506070timeCumulative regretBelMan0500100001002003004005006007008009001000timeSuboptimal drawsBelMan05001000010203040506070timeBayesUCB0500100001002003004005006007008009001000timeBayesUCB05001000010203040506070timeThompson0500100001002003004005006007008009001000timeThompson05001000010203040506070timeUCB0500100001002003004005006007008009001000timeUCB05001000010203040506070timeKLUCB0500100001002003004005006007008009001000timeKLUCB05001000010203040506070timeRandom0500100001002003004005006007008009001000timeRandom020040060080010000102030405060timeBelManExp020040060080010000100200300400500600700800timeSuboptimal drawsBelManExp020040060080010000102030405060timeThompson020040060080010000100200300400500600700800timeThompson020040060080010000102030405060timeUCBtuned020040060080010000100200300400500600700800timeUCBtuned020040060080010000102030405060timeKLUCBexp020040060080010000100200300400500600700800timeKLUCBexp020040060080010000102030405060timeRandom020040060080010000100200300400500600700800timeRandom0200400600800100005001000150020002500timeCumulative regretBelMan0200400600800100001002003004005006007008009001000timeSuboptimal drawsBelMan0200400600800100005001000150020002500timeThompson0200400600800100001002003004005006007008009001000timeThompson0200400600800100005001000150020002500timeUCBtuned0200400600800100001002003004005006007008009001000timeUCBtuned0200400600800100005001000150020002500timeKLUCBexp0200400600800100001002003004005006007008009001000timeKLUCBexp0200400600800100005001000150020002500timeRandom0200400600800100001002003004005006007008009001000timeRandom34

D. Basu et al.

with state-of-the-art frequentist method tailored for exponential distribution of
rewards, called KL-UCBExp [15]. We also compare it with Thompson sampling,
UCBtuned and uniform sampling method (Random). The results are shown
in Figure 13 and 14. Since the formulation is oblivious to boundedness of the
distribution, we choose to validate also on unbounded rewards. In Figure 13, it
outperforms all the other algorithms. In Figure 14, though KL-UCBexp performs
the best, performance of BelMan is still competitive with it.

These results validate BelMan’s claim as a generic solution to a wide range

of bandit problems.

References for the Appendix

1. Amari, S.I., Nagaoka, H.: Methods of information geometry, Translations of mathe-

matical monographs, vol. 191. American Mathematical Society (2007)

2. Audibert, J.Y., Bubeck, S.: Best arm identiﬁcation in multi-armed bandits. In:

COLT. pp. 41–53 (2010)

3. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed

bandit problem. Machine learning 47(2–3), 235–256 (2002)

4. Bellman, R.: A problem in the sequential design of experiments. Sankhy¯a: The

Indian Journal of Statistics (1933–1960) 16(3/4), 221–229 (1956)

5. Bernardo, J.M.: Algorithm AS 103: Psi (digamma) function. Journal of the Royal

Statistical Society. Series C (Applied Statistics) 25(3), 315–317 (1976)

6. Brown, L.D.: Fundamentals of Statistical Exponential Families: With Applications

in Statistical Decision Theory. Institute of Mathematical Statistics (1986)

7. Bubeck, S., Wang, T., Viswanathan, N.: Multiple identiﬁcations in multi-armed

bandits. In: ICML. pp. 258–265 (2013)

8. Bubeck, S., Munos, R., Stoltz, G.: Pure exploration in multi-armed bandits problems.

In: ALT. pp. 23–37. Springer (2009)

9. Cover, T.M., Thomas, J.A.: Elements of information theory. John Wiley & Sons

(2012)

10. Csisz´ar, I.: Sanov property, generalized I-projection and a conditional limit theorem.

The Annals of Probability 12(3), 768–793 (1984)

11. Dann, C., Brunskill, E.: Sample complexity of episodic ﬁxed-horizon reinforcement

learning. In: NIPS. pp. 2818–2826 (2015)

12. Darmois, G.: Sur les lois de probabilites a estimation exhaustive. C. R. Acad. Sci.

Paris 200, 1265–1266 (1935)

13. DeGroot, M.H.: Optimal statistical decisions, Wiley Classics Library, vol. 82. John

Wiley & Sons (2005)

14. Faheem, M., Senellart, P.: Adaptive web crawling through structure-based link

classiﬁcation. In: Proc. ICADL. pp. 39–51. Seoul, South Korea (Dec 2015)

15. Garivier, A., Capp´e, O.: The KL-UCB algorithm for bounded stochastic bandits

and beyond. In: COLT. pp. 359–376 (2011)

16. Gittins, J.C.: Bandit processes and dynamic allocation indices. Journal of the Royal

Statistical Society. Series B (Methodological) 41(2), 148–177 (1979)

17. Honda, J., Takemura, A.: An asymptotically optimal policy for ﬁnite support models
in the multiarmed bandit problem. Machine Learning 85(3), 361–391 (2011)
18. Kaufmann, E.: On bayesian index policies for sequential resource allocation. Annals

of Statistics 46(2), 842–865 (April 2018)

BelMan: An Information-Geometric Approach to Stochastic Bandits

35

19. Kaufmann, E., Capp´e, O., Garivier, A.: On Bayesian upper conﬁdence bounds for

bandit problems. In: AISTATS. pp. 592–600 (2012)

20. Kaufmann, E., Kalyanakrishnan, S.: Information complexity in bandit subset

selection. In: COLT. pp. 228–251 (2013)

21. Koopman, B.O.: On distributions admitting a suﬃcient statistic. Transactions of

the American Mathematical society 39(3), 399–409 (1936)

22. Kullback, S.: Information theory and statistics. Courier Corporation (1997)
23. Lai, T.L.: Asymptotic solutions of bandit problems. In: Fleming, W., Lions, P.L.
(eds.) Stochastic diﬀerential systems, stochastic control theory and applications, pp.
275–292. Springer (1988)

24. Lai, T.L., Robbins, H.: Asymptotically eﬃcient adaptive allocation rules. Adv. Appl.

Math. 6(1), 4–22 (Mar 1985)

25. Nino-Mora, J.: Computing a classic index for ﬁnite-horizon bandits. INFORMS

Journal on Computing 23(2), 254–267 (2011)

26. Osband, I., Russo, D., Van Roy, B.: (More) eﬃcient reinforcement learning via

posterior sampling. In: NIPS. pp. 3003–3011 (2013)

27. Putta, S.R., Tulabandhula, T.: Pure exploration in episodic ﬁxed-horizon Markov

decision processes. In: AAMAS. pp. 1703–1704 (2017)

28. Robbins, H.: Some aspects of the sequential design of experiments. Bull. Amer.

Math. Soc. 58(5), 527–535 (09 1952)

29. Scott, S.L.: A modern Bayesian look at the multi-armed bandit. Applied Stochastic

Models in Business and Industry 26(6), 639–658 (2010)

30. Thompson, W.R.: On the likelihood that one unknown probability exceeds another

in view of the evidence of two samples. Biometrika 25(3–4), 285 (1933)

