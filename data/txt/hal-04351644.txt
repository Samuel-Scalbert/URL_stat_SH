Unmasking the Hidden Meaning: Bridging Implicit and
Explicit Hate Speech Embedding Representations
Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata

To cite this version:

Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata. Unmasking the Hidden Meaning: Bridg-
ing Implicit and Explicit Hate Speech Embedding Representations. EMNLP 2023 - Conference on
Empirical Methods in Natural Language Processing, Dec 2023, Singapore, France. pp.6626-6637,
￿10.18653/v1/2023.findings-emnlp.441￿. ￿hal-04351644￿

HAL Id: hal-04351644

https://hal.science/hal-04351644

Submitted on 19 Dec 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

6626
Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6626–6637
December 6-10, 2023 ©2023 Association for Computational Linguistics

UnmaskingtheHiddenMeaning:BridgingImplicitandExplicitHateSpeechEmbeddingRepresentationsNicolasOcampoandElenaCabrioandSerenaVillataUniversiteCôted’Azur,CNRS,Inria,I3S,France{nicolas-benjamin.ocampo,elena.cabrio,serena.villata}@univ-cotedazur.frAbstractResearchonautomatichatespeech(HS)detec-tionhasmainlyfocusedonidentifyingexplicitformsofhatefulexpressionsonuser-generatedcontent.Recently,afewworkshavestartedtoinvestigatemethodstoaddressmoreimplicitandsubtleabusivecontent.However,despitetheseefforts,automatedsystemsstillstruggletocorrectlyrecognizeimplicitandmoreveiledformsofHS.Asthesesystemsheavilyrelyonpropertextualrepresentationsforclassification,itiscrucialtoinvestigatethedifferencesinem-beddingimplicitandexplicitmessages.Ourcontributiontoaddressthischallengingtaskisfourfold.First,wepresentacomparativeanal-ysisoftransformer-basedmodels,evaluatingtheirperformanceacrossfivedatasetscontain-ingimplicitHSmessages.Second,weexaminetheembeddingrepresentationsofimplicitmes-sagesacrossdifferenttargets,gaininginsightintohowveiledcasesareencoded.Third,wecompareandlinkexplicitandimplicithatefulmessagesacrossthesedatasetsthroughtheirtargets,enforcingtherelationbetweenexplicit-nessandimplicitnessandobtainingmoremean-ingfulembeddingrepresentations.Lastly,weshowhowthesenewerrepresentationmaintainshighperformanceonHSlabels,whileimprov-ingclassificationinborderlinecases.1IntroductionTheproliferationofhatespeech(HS)onsocialmediaplatformshasbecomeapressingconcerninonlinesocialcommunities.WhilesignificantprogresshasbeenmadeinthedevelopmentofHSdetectionmethods,currentSOTAmodelsfocusondetectingexplicitHS,leavingimplicithatecasesundetected(ElSheriefetal.,2021;Ocampoetal.,2023).Thisissueisaggravatedbythesheervolumeofimplicithatespeechcontentbeingspreadacrossvariousonlineplatforms,necessitatingautomatedapproachestodetectthemeffectively.ImplicitHSdetectionposesuniquechallengescomparedtoitsexplicitcounterpart:itcontainscoded,ambiguousorindirectlanguagethatdoesnotimmediatelydenotehate,butstilldisparagesapersonoragroupbasedonprotectedcharac-teristicssuchasrace,gender,culturalidentity,orreligion(e.g.,“Ithinkitisabitlatetothinktolookafterthesafetyandthefutureofwhitepeo-pleinSouth-Africa"-theWhiteSupremacyForumDataset(deGibertetal.,2018)).TheperformanceofcurrentHSsystemsheavilyreliesonhowcodedlanguageisrepresentedandhowwellclassifierscancapturetheunderlyingsemanticmeaningofmessagesthroughembeddings.Hence,obtainingbettertextrepresentationbecomescrucialineffec-tivelyidentifyingimplicitHSmessages.Inthisdirection,thegoalofourworkistobridgethegapbetweenexplicitandimplicitmessages,aimingtoenhancetheembeddingrepresentationsofSOTAmodels.Ourcontributionisfourfold:i)Weanalyzetheembeddingrepresentationsoffivebenchmarkdatasetswithveiledhatefulcontent,examiningthelevelsofexplicitnessandimplicit-ness,throughcross-evaluationusingstate-of-the-arttransformermodels.ii)Weexaminetheembed-dingrepresentationsofimplicitmessagesacrossdifferenttargetgroups.Throughthisanalysis,wegaininsightsintohowimplicitHSmessagesareen-codedbasedontheirtargetgroups.iii)WeproposeanovelapproachtolinkexplicitandimplicitHSmessagesintherepresentationspace.iv)Weillus-tratethatthenewerrepresentationspacepreservesstrongefficacyforHSlabels,whilealsorefiningclassificationinborderlineinstances.Usingcon-trastivelearningtechniques(Guneletal.,2020;RethmeierandAugenstein,2021;Kimetal.,2022;Tianetal.,2020),weaimtopushexplicitandim-plicitmessageseffectivelyenforcingtheuncoveredrelationbetweenthesetwonotionsandtherebyob-tainingmoremeaningfulrepresentationsthanthoseobtainedthroughfine-tuninglearningmethods.11Theaccompanyingsoftwarecanbefoundat:https://github.com/benjaminocampo/bridging_ie_hs_embs.6627

NOTE:Thispapercontainsexamplesoflanguagewhichmaybeoffensivetosomereaders.Theydonotrepresenttheviewsoftheauthors.2RelatedWorkHSdetectionhasbeenextensivelystudiedbytheresearchcommunityprovidingmultipleresources,suchaslexicons(Wiegandetal.,2018;Bassig-nanaetal.,2018),datasets(Zampierietal.,2019;Basileetal.,2019;Davidsonetal.,2017;Fountaetal.,2018),andsupervisedmethods(ParkandFung,2017;GambäckandSikdar,2017;Wangetal.,2020;Leeetal.,2019)(forasurvey,see(Polettoetal.,2021)).Thesestudiesprovideasolidstartingpointtoexaminetheproblemofabu-sivelanguage,especiallyinsocialmediamessages.Lately,therehasbeengrowinginterestinthedetec-tionofimplicitHS,whichprovidesadditionalchal-lenges.DatasetsspecificallydesignedforimplicitHS(Ocampoetal.,2023;Hartvigsenetal.,2022;ElSheriefetal.,2021;Vidgenetal.,2021;Sapetal.,2020),moresolidveileddetectors(HanandTsvetkov,2020),guidedaugmentationstrategies(Nejadgholietal.,2022;RoychowdhuryandGupta,2023),andtheoreticalanalysis(Jurgensetal.,2019;Waseemetal.,2017;Wiegandetal.,2021)havebeenrecentlyproposedtoadvanceinthisdirection.However,littleattentionhasbeendedicatedtoeffectivelyrepresentimplicitmessagesthroughembeddingsonthesebenchmarks.Embeddingsplayacrucialroleintheperformanceofclassi-fiers(Pavlopoulosetal.,2017;Kshirsagaretal.,2018;Ocampoetal.,2023),yettheirapplicationtocapturetheimplicitnatureofHShasbeenunder-investigated.Inthisdirection,(Kimetal.,2022)tacklescross-datasetunderperformingissuesonHSclassifiersandproposesacontrastivelearningmethodthatencodesahatefulpostanditscorre-spondingimplicationcloseinrepresentationspace,closelydependingontheannotatedimplicationsandwithoutcontrapositioningexplicitnesswithimplicitness.(Bourgeadeetal.,2023)capturestopic-genericandtopic-specificknowledgewhentrainedondifferentdatatoimprovegeneralization.3ImplicitandExplicitHSEmbeddings3.1ResearchQuestionsWewillfocusonthebehaviorofSOTAmodelsincross-evaluationsettings,specificallyondatasetscontainingimplicithate.Thestudyexploresthemodels’behaviorondifferentHSclasses,includingbothexplicitandimplicithate.Inparticular,wetargetthefollowingresearchquestions(RQ):RQ1:Howdothemodels’embeddingscapturetheHSclasses?Areexplicitandimplicithatefulmes-sagesencodeddifferentlyacrossdifferentdatasets?Whatistheextentofthisvariation?RQ2:DoesgroupingthetestsetsbytargetresultinsimilarencodingpatternsforexplicitHSanddistinctencodingpatternsforimplicitHSintheembeddings?RQ2buildsupontheanalysiscon-ductedinRQ1,butwithafocusontargetgroups.RQ3:Canwelinkandbringexplicitandimplicitembeddingrepresentationsclosertogetherwithinthelearnedembeddingspacethroughtheirtargetgroups?RQ4:Howdothesenewerembeddingrepresenta-tionscaptureHSclassesincomparisonwithRQ1?3.2DatasetsWecarriedoutouranalysisonthefollowingstan-darddatasets,containingimplicitHSmessages:Im-plicitSubtleHate(ISHate)(Ocampoetal.,2023),SocialBiasInference(SBIC)(Sapetal.,2020),ImplicitHateCorpus(IHC)(ElSheriefetal.,2021),Dynahate(DYNA)(Vidgenetal.,2021),andToxi-gen(TOX)(Hartvigsenetal.,2022).WeensuredthatthedefinitionsofHSwereconsistentacrossthedatasets.Specifically,fortheSBICdataset,mes-sagesareconsideredasHSiflabeledasoffensiveandtargetaspecificgroup.Asfortheexplicit-implicitHSlabelingacrossalldatasets,thepro-videdimplicitlabelsareusedforIHCandISHatedatasets.ForSBIC,DYNA,andTOX,wecom-putedthepercentageofHSimplicitmessagesastheoneswherenoneofthewordsoftheGooglepro-fanitywordsresourcewaspresent2.Thedatasetsweredividedintotrain,dev,andtestsets.Existingdatasetsplitswereretained,whiledatasetswith-outpredefinedsplitsweredividedusingastratifiedsplittingmethodwitha60%train,20%dev,and20%testratio.Table4inAppendixshowstheper-centageofimplicit/explicitinstancesperdataset.3.3ExperimentalSettingsConcerningourresearchquestions(Section3.1),toanswertoRQ1weperformedfine-tuningontwostate-of-the-artmodelscommonlyusedforHSde-tection:BERTandHateBERT(Casellietal.,2021).Bothmodelswerefine-tunedoneachdatasetusing2ListofswearwordsbannedbyGoogle:https://github.com/RobertJGabriel/Google-profanity-words6628

atwo-labelclassificationapproach,distinguishingbetweennon-HSandHSmessages.Toensurero-bustnessandaccountforrandomnessinthetrain-ingprocess,werepeatedthefine-tuningprocedurefivetimes,eachtimeemployingadifferentrandomseed.Thisallowedustoevaluatetheperformanceofthemodelsconsistency.Toassesstheperfor-manceofthemodels,wecross-evaluatethebench-markscalculatingtheaverageF1-scoreacrossallfine-tuningruns.Additionally,wecalculatedthestandarddeviationtoquantifythevariabilityinperformanceobservedacrossthedifferentruns.Fi-nally,wecalculatedtheembeddingsofthesemod-elsusingTSNEhighlightinghowexplicitandim-plicitmessageswereencoded.Weusedthebaseversionssizeofthesemodelswithbatchsizeof32,weightdecayof0.01,4epochs,andalearningrateof2e-5.AsforTSNE,weuseperplexityof30,and1000maximumiterationsforconvergence.ForRQ2,wegroupedtheembeddingspertargetintheplots.Toensureconsistencyacrossdatasets,westandardizedthetargetnames,addressingla-belvariations,e.g.,weresolveddifferenceslike"asian"and"asianpeople"byusingaunifiedla-bel.Moreover,whenamessagetargetedmultipleoffensivegroups(e.g.AsiansandMigrants),weselectedthelabelcorrespondingtothepredomi-nanttarget(amongMUSLIMS,WOMEN,JEWS,LGBTQ+,BLACKPEOPLE,WHITEPEOPLE,IMMIGRANTS,ASIAN,andDISEASE).ForRQ3,weaimtovalidatethepotentiallink-agebetweenexplicitandimplicitmessagesthroughtheirtargetgroups.Toachievethis,weemploycon-trastivelearningtechniquesonthepre-trainedandfine-tunedmodels.Contrastivelearninginvolvesdefiningpairsofpositiveandnegativesamplesandtrainingthemodelusingamodifiedlossfunction.Inourexperimentalsettings,wedesignatepairsofimplicitandexplicitmessageswiththesametar-getaspositivesamples.Foreachimplicitones,arandomlyselectedexplicitmessagewiththesametargetispaired.Incaseswheretheyareunavail-ableorwhentheimplicitinstancelacksatargetlabel,werandomlyassignanyexplicitmessage.NegativesamplesconsistofpairsofHSandNon-HSinstances.ForeveryNon-HSinstance,oneHSinstanceisrandomlyselected.Usingcontrastivelearningfacilitatesthetrainingprocessbypushingpositivepairsclosertogetherwhilepushingnega-tivepairsfurtherapartwithintheembeddingspace.Thecontrastivelossisdefinedasfollows:loss_cont=mean(cid:0)(1−l)·s2+l·(max(0,m−s))2(cid:17)(1)Wherelrepresentsthelabelpair(1forpositivepairs,0fornegativepairs),sisthecosinesimi-laritybetweenpairedmessages,andmisthemar-ginhyper-parameter.Forclassification,thecross-entropylossis:loss_clf=−N−1Xi=0(gilog(pi)+(1−gi)log(1−pi))(2)Wheregisthegoldlabel(labelsofthedatasetonwhichthemodelisfine-tuned)andpisthepredic-tion.Thefinallossis:total_loss=loss_cont+loss_clf(3)Bycombiningthem,weoptimizeboththemodel’sunderstandingofembeddingsandclassification.ForRQ4,wefine-tunedbothBERTandHate-BERTusingourenhancedembeddings(sameset-tingsofourinitialRQs).Additionally,togainmoretargeteddiagnosticinsights,models’accuracywasevaluatedonthreecategoriesdefinedontheSBICdataset(Non-HS,ExplicitHS,andImplicitHS),andtheHateCheckdataset(Röttgeretal.,2021),asuiteoffunctionaltestsforHSdetectionmodels.3.4ObtainedResultsandDiscussionFigure1:RQ1:TSNEembeddingsoftheSBICtestsetusingHateBERTfine-tunedonDYNA.RegardingRQ1,Table1ashowsthattrainingandevaluatingHateBERT(BERTresultscanbefoundintheAppendix)onthesamedatasetyields6629

TrainTestIHCSBICDYNAISHateTOXIHC0,7618±0,00270,6824±0,01290,5679±0,05220,6959±0,02380,5896±0,0119SBIC0,6385±0,03100,8632±0,00990,6355±0,03990,7522±0,01530,6998±0,0086DYNA0,6717±0,03100,7473±0,03570,7860±0,01110,7602±0,00250,7526±0,0075ISHate0,6188±0,00520,7209±0,04000,6190±0,00640,8684±0,00350,6034±0,0045TOX0,5063±0,01400,5428±0,01330,4952±0,01970,5900±0,01950,7650±0,0102(a)Cross-evaluationresultswithHateBERT.TrainTestIHCSBICDYNAISHateTOXIHC0,7433±0,01070,6618±0,01730,5415±0,00920,6678±0,00580,5843±0,0153SBIC0,6593±0,00950,8620±0,00640,6591±0,00690,7583±0,00860,6742±0,0208DYNA0,6520±0,01020,7323±0,01160,7831±0,00300,7566±0,01350,7147±0,0075ISHate0,6253±0,00600,6753±0,04420,6165±0,01160,8394±0,00640,5973±0,0210TOX0,5354±0,02100,5637±0,03460,5202±0,00980,6180±0,02560,7610±0,0103(b)Cross-evaluationresultswithContrastiveHateBERT.BoldvaluesindicateimprovementscomparedtoTable1a.Table1:HateBERTandContrastiveHateBERTcross-evaluationresultswithfivedifferentrunseeds.Figure2:RQ2:TSNEembeddingsoftheSBICtestsetusingHateBERTfine-tunedonDYNAbasedontheirtargetgroups.Non-HSareexcludedfromtheplot.betterresultsoverall,ascouldbeexpected.How-ever,evenincross-evaluationscenarios,reasonableperformancesareobserved.Notably,amongthemostgeneralizablemodels,HateBERTtrainedonDYNAexhibitsbettergeneralization.WethereforeselectedHateBERTtrainedonDYNAasthebestconfigurationandweplottheembeddingsforthetestsetsofallthedatasets,applyingtheTSNEalgo-rithm.Figure1showshowexplicitHSandnon-HSmessagesareencodedwithclearseparation,result-inginanoticeabledistancebetweenthem.3Ontheotherhand,implicitHSinstancestendtobeinter-twinedwithbothnon-HSandexplicitHSmessages.Thispatternholdstrueacrossall5datasets.AsfortheresultsforRQ2,Figure2showsex-3Duetospaceconstraints,weshowonlytheplotsoftheTSNEembeddingsoftheSBICtestsetusingHateBERTfine-tunedonDYNA.TheplotsshowingtheembeddingsforalltheothertestsetscanbefoundintheAppendix.plicitandimplicittextrepresentationspertargetgrouphighlightinghow,ingeneral,embeddingsofexplicitandimplicitmessagestendtobelinkedbytheirtargetgroupsinrepresentationspaces.Fi-nally,asforRQ3,Figure3demonstratesthattheembeddingrepresentationsofexplicitandimplicitinstancesstartstooverlapacrossalldatasetswhenusingHateBERTtrainedonDYNA.Additionally,Figure4highlightsthatbyleveragingthetargetsofHSusingcontrastivelearning,explicitandimplicitmessagesexhibitasimilarrepresentation.Figure3:RQ3:TSNEembeddingsoftheSBICtestsetusingHateBERTfine-tunedonDYNAandlinkingexplicitandimplicitinstances.AsforRQ4,Table1billustratesthatthenovelrepresentationenhancestheF1-scoreforcertaindatasets,suchasSBIC,TOX,andISHate.Conversely,forotherdatasetslikeIHCandDYNA,theperformanceremainscomparabletothatofthenon-contrastiveapproach.Table2showshighercapabilityofthecontrastive6630

TestCaseHateBERTIHCContrastiveIHCHateBERTSBICContrastiveSBICHateBERTDYNAContrastiveDYNAHateBERTISHateContrastiveISHateHateBERTTOXContrastiveTOXcounter_quote_nh.486±.082.499±.085.276±.213.422±.114.857±.048.962±.027.379±.079.524±.1600±0.010±.009counter_ref_nh.576±.045.617±.086.240±.168.393±.144.882±.018.902±.037.387±.038.521±.187.177±.043.295±.117ident_pos_nh.441±.020.446±.136.301±.076.361±.038.849±.021.767±.099.272±.071.350±.101.470±.126.575±.133negate_neg_nh.502±.054.517±.134.120±.060.161±.104.448±.037.486±.040.164±.030.211±.106.087±.036.194±.077profanity_nh.796±.036.774±.076.922±.099.994±.0051±01±0.992±.004.996±.005.292±.101.456±.105slur_homonym_nh.353±.061.413±.084.393±.162.513±.099.813±.030.787±.073.680±.056.827±.101.320±.051.473±.064slur_reclaimed_nh.217±.045.277±.084.472±.196.711±.117.891±.018.879±.044.741±.070.802±.113.272±.031.346±.067target_group_nh.710±.036.700±.027.623±.328.810±.072.968±0.971±.021.448±.027.561±.0850±0.006±.014target_indiv_nh.538±.049.572±.104.655±.451.951±.0281±01±0.782±.032.809±.061.003±.007.003±.007target_obj_nh.637±.042.622±.105.923±.086.966±.037.969±.011.985±.015.735±.013.757±.073.006±.008.034±.035Table2:ComparativeaccuracyperformanceofHateBERTvsContrastiveHateBERTtrainedineachdatasetandevaluatedacrossvarioustestcasesonHateCheck.TrainExplicitImplicitNon-HSIHC0.88320.88420.4659SBIC0.89710.84400.8568DYNA0.60710.66650.8314ISHate0.55150.46760.8768TOX0.79380.86170.3439Table3:ContrastiveHateBERTavgaccuracyacrossExplicit,Implicit,andNon-HS(SBICtestset).Figure4:RQ3:TSNEembeddingsoftheSBICtestsetusingHateBERTfine-tunedonDYNAbasedontheirtargetgroups.Non-HSareexcludedfromtheplot.HateBERTinaccuratelyclassifyingchallengingNon-HSmessagesacrossallfivedatasets.AsignificantreductioninfalsepositivesisalsoobservedinHateCheckcategoriessuchasquotedannouncements(counter_quote_nh),directreferences(counter_ref_nh),pos-itiveidentifiers(ident_pos_nh),negatedhatefulremarks(negate_neg_nh),non-hatefulprofanity(profanity_nh),reclaimedslurs(slur_reclaimed_nh),homonymslurs(slur_homonym_nh),aswellastargetedabusedirectedatindividuals(target_indiv_nh),objects(target_obj_nh),andnon-protectedgroups(target_group_nh).Additionally,Table3indicatesthatbothExplicitandImplicitcat-egoriesexhibitsimilarlyhighaccuracylevels,highlightingtheirnearlyindistinguishableimpactonthemodel’saggregateperformance.Also,theimportanceoftheNon-HScategoryisunderscored,varyingwithdifferenttrainingdatasets,yetremainingacriticalcomponent.Hence,ourexperimentsemphasizetheimpor-tanceofstudyingimplicitrepresentations,asclassi-caltrainingstrategiescannotencodethemproperly(RQ1).Weshowedthatimplicitandexplicitmes-sagesshareaconnectionconveyingsimilarmes-sagestothesametarget(RQ2)andhowcontrastivelearningeffectivelyforcesthatpropertybybridg-ingexplicitandimplicitinstancesthroughtheirtargets(RQ3),therebyobtainingmoremeaningfulrepresentationsthattheonesobtainedthroughfine-tuning.Finally,wereducedbiasesinnon-hatefulimplicitcasesoftenmisclassifiedduetotriggerwordsornuancedcontent.OurenhancedmethodmaintainshighperformanceonHSlabelswhileim-provingclassificationinborderlinecases,provingitsrobustnessandprecision(RQ4).4ConclusionsOurcontributioninthisstudyisfourfold:i)Westudiedhowmodels’embeddingscaptureHSw.r.t.explicitnessandimplicitness,ii)WeshowedhowexplicitandimplicitHSmessagesresultinsimilarencodingsifgroupedbytheirprotectedtarget,iii)Weanalyzedacontrastivelearningmethodtoforcethispropertywhenrepresentingimplicittext.Weproveourresearchhypothesison5HSbenchmarks,movingastepforwardinbridgingthegapbetweenexplicitnessandimplicitness,andiv)Weshowhowthenewerrepresentationspacemaintainshighper-formanceonHSlabelswhileimprovingclassifi-cationinborderlinecases.Infuturework,we’llrefinecontrastivelearning,delvingintocontextualpairingbasedonothersemanticdependenciesbe-tweenexplicitandimplicitcues,aimingtosharpennuancedhatespeechdetection.6631

LimitationsInthisstudy,weareawareofsomekeyissues,oneofwhichpertainstotheselectionofpositiveandnegativesamplesincontrastivelearning.Theeffec-tivenessofthealgorithmheavilyreliesonthecare-fulselectionofthesepairs.Whileourinvestigationdemonstratesthatexplicitandimplicitmessagesexhibitarelationshipthroughtheirtargetgroupsacrossfivedistinctdatasets,itisimportanttoac-knowledgethatthisassumptionmaynotalwaysholdtrue.Additionally,ensuringaclearseparationbetweennon-hatefulandHSinstancescanbechal-lengingduetotheheterogeneityofeachcategory.Moreover,theefficacyofourapproachiscon-tingentupontheavailabilityandalignmentoftar-getinformationacrossthedatasets.Whiletargetinformationiscommonlyprovidedinbenchmarkdatasets,differentdatasetsmayaddressvariouspro-tectedcharacteristics.Ourapproachassumesthatthereissomedegreeofoverlapintermsoftargetgroupsamongtheselecteddatasets.Furthermore,theselectionofpairswhenlinkingexplicitandimplicitmessagescanvaryintermsofthenumberofcombinations.However,itisimportanttonotethatasthenumberofpairsin-creases,thetrainingrequirementstendtogrowsig-nificantly,resultinginslowertrainingprocesses.Thistrade-offbetweenthenumberofpairsandtrainingefficiencyshouldbecarefullyconsideredwhenimplementingtheapproach.EthicsStatementThispaperusesacollectionofHSexamplesex-tractedfromlinguisticresourcescommonlyem-ployedforHSdetection,ensuringtheirindepen-dencefromtheauthors’personalopinions.Thedatasetsusedinthisstudyhavebeenmeticulouslyhandledtoaddressprivacyconcernsassociatedwithuserdata.Whileweacknowledgethepoten-tialformisuse,wefirmlybelievethatdevelopingrobustHSclassifiersisessentialincombatingtheproliferationofharmfulcontent.Inthisregard,ourworkrepresentsasignificantcontributiontowardsthisobjectiveandencouragesfurtherexplorationandinvestigationwithinthescientificcommunity.AcknowledgementsThisworkhasbeensupportedbytheFrenchgov-ernment,throughthe3IACôted’AzurInvestmentsintheFutureprojectmanagedbytheNationalRe-searchAgency(ANR)withthereferencenumberANR-19-P3IA-0002.ReferencesValerioBasile,CristinaBosco,ElisabettaFersini,DeboraNozza,VivianaPatti,FranciscoManuelRangelPardo,PaoloRosso,andManuelaSanguinetti.2019.SemEval-2019task5:MultilingualdetectionofhatespeechagainstimmigrantsandwomeninTwitter.InProceedingsofthe13thInternationalWorkshoponSemanticEvaluation,pages54–63,Min-neapolis,Minnesota,USA.AssociationforCompu-tationalLinguistics.ElisaBassignana,ValerioBasile,andVivianaPatti.2018.Hurtlex:AMultilingualLexiconofWordstoHurt.InElenaCabrio,AlessandroMazzei,andFabioTamburini,editors,ProceedingsoftheFifthItalianConferenceonComputationalLinguisticsCLiC-it2018,pages51–56.AccademiaUniversityPress.TomBourgeade,PatriciaChiril,FarahBenamara,andVéroniqueMoriceau.2023.Whatdidyoulearntohate?atopic-orientedanalysisofgeneralizationinhatespeechdetection.InProceedingsofthe17thConferenceoftheEuropeanChapteroftheAsso-ciationforComputationalLinguistics,pages3495–3508,Dubrovnik,Croatia.AssociationforComputa-tionalLinguistics.TommasoCaselli,ValerioBasile,JelenaMitrovi´c,andMichaelGranitzer.2021.HateBERT:RetrainingBERTforabusivelanguagedetectioninEnglish.InProceedingsofthe5thWorkshoponOnlineAbuseandHarms(WOAH2021),pages17–25,Online.As-sociationforComputationalLinguistics.ThomasDavidson,DanaWarmsley,MichaelMacy,andIngmarWeber.2017.AutomatedHateSpeechDe-tectionandtheProblemofOffensiveLanguage.Pro-ceedingsoftheInternationalAAAIConferenceonWebandSocialMedia,11(1):512–515.Number:1.OnadeGibert,NaiaraPerez,AitorGarcía-Pablos,andMontseCuadros.2018.Hatespeechdatasetfromawhitesupremacyforum.InProceedingsofthe2ndWorkshoponAbusiveLanguageOnline(ALW2),pages11–20,Brussels,Belgium.AssociationforComputationalLinguistics.MaiElSherief,CalebZiems,DavidMuchlinski,Vaish-naviAnupindi,JordynSeybolt,MunmunDeChoud-hury,andDiyiYang.2021.Latenthatred:Abench-markforunderstandingimplicithatespeech.InPro-ceedingsofthe2021ConferenceonEmpiricalMeth-odsinNaturalLanguageProcessing,pages345–363,OnlineandPuntaCana,DominicanRepublic.Asso-ciationforComputationalLinguistics.AntigoniFounta,ConstantinosDjouvas,DespoinaChatzakou,IliasLeontiadis,JeremyBlackburn,Gi-anlucaStringhini,AthenaVakali,MichaelSirivianos,6632

andNicolasKourtellis.2018.LargeScaleCrowd-sourcingandCharacterizationofTwitterAbusiveBehavior.ProceedingsoftheInternationalAAAIConferenceonWebandSocialMedia,12(1).Num-ber:1.BjörnGambäckandUtpalKumarSikdar.2017.Usingconvolutionalneuralnetworkstoclassifyhate-speech.InProceedingsoftheFirstWorkshoponAbusiveLan-guageOnline,pages85–90,Vancouver,BC,Canada.AssociationforComputationalLinguistics.BelizGunel,JingfeiDu,AlexisConneau,andVesStoyanov.2020.Supervisedcontrastivelearningforpre-trainedlanguagemodelfine-tuning.CoRR,abs/2011.01403.XiaochuangHanandYuliaTsvetkov.2020.Fortify-ingtoxicspeechdetectorsagainstveiledtoxicity.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages7732–7739,Online.AssociationforComputa-tionalLinguistics.ThomasHartvigsen,SaadiaGabriel,HamidPalangi,MaartenSap,DipankarRay,andEceKamar.2022.ToxiGen:Alarge-scalemachine-generateddatasetforadversarialandimplicithatespeechdetection.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages3309–3326,Dublin,Ireland.AssociationforComputationalLinguistics.DavidJurgens,LibbyHemphill,andEshwarChan-drasekharan.2019.Ajustandcomprehensivestrat-egyforusingNLPtoaddressonlineabuse.InPro-ceedingsofthe57thAnnualMeetingoftheAsso-ciationforComputationalLinguistics,pages3658–3666,Florence,Italy.AssociationforComputationalLinguistics.YoungwookKim,ShinwooPark,andYo-SubHan.2022.Generalizableimplicithatespeechdetectionusingcontrastivelearning.InProceedingsofthe29thInter-nationalConferenceonComputationalLinguistics,pages6667–6679,Gyeongju,RepublicofKorea.In-ternationalCommitteeonComputationalLinguistics.RohanKshirsagar,TyrusCukuvac,KathyMcKeown,andSusanMcGregor.2018.PredictiveembeddingsforhatespeechdetectiononTwitter.InProceedingsofthe2ndWorkshoponAbusiveLanguageOnline(ALW2),pages26–32,Brussels,Belgium.Associa-tionforComputationalLinguistics.Ju-HyoungLee,Jun-UPark,Jeong-WonCha,andYo-SubHan.2019.Detectingcontextabusivenessusinghierarchicaldeeplearning.InProceedingsoftheSecondWorkshoponNaturalLanguageProcessingforInternetFreedom:Censorship,Disinformation,andPropaganda,pages10–19,HongKong,China.AssociationforComputationalLinguistics.IsarNejadgholi,KathleenFraser,andSvetlanaKir-itchenko.2022.Improvinggeneralizabilityinim-plicitlyabusivelanguagedetectionwithconceptac-tivationvectors.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLin-guistics(Volume1:LongPapers),pages5517–5529,Dublin,Ireland.AssociationforComputationalLin-guistics.NicolasOcampo,EkaterinaSviridova,ElenaCabrio,andSerenaVillata.2023.Anin-depthanalysisofimplicitandsubtlehatespeechmessages.InProceed-ingsofthe17thConferenceoftheEuropeanChap-teroftheAssociationforComputationalLinguistics,pages1997–2013,Dubrovnik,Croatia.AssociationforComputationalLinguistics.JiHoParkandPascaleFung.2017.One-stepandtwo-stepclassificationforabusivelanguagedetectiononTwitter.InProceedingsoftheFirstWorkshoponAbusiveLanguageOnline,pages41–45,Vancouver,BC,Canada.AssociationforComputationalLinguis-tics.JohnPavlopoulos,ProdromosMalakasiotis,JuliBaka-gianni,andIonAndroutsopoulos.2017.Improvedabusivecommentmoderationwithuserembeddings.InProceedingsofthe2017EMNLPWorkshop:Nat-uralLanguageProcessingmeetsJournalism,pages51–55,Copenhagen,Denmark.AssociationforCom-putationalLinguistics.FabioPoletto,ValerioBasile,ManuelaSanguinetti,CristinaBosco,andVivianaPatti.2021.Resourcesandbenchmarkcorporaforhatespeechdetection:asystematicreview.LanguageResourcesandEvalua-tion,55(2):477–523.NilsRethmeierandIsabelleAugenstein.2021.Aprimeroncontrastivepretraininginlanguageprocessing:Methods,lessonslearnedandperspectives.CoRR,abs/2102.12982.PaulRöttger,BertieVidgen,DongNguyen,ZeerakWaseem,HelenMargetts,andJanetPierrehumbert.2021.HateCheck:Functionaltestsforhatespeechdetectionmodels.InProceedingsofthe59thAn-nualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConfer-enceonNaturalLanguageProcessing(Volume1:LongPapers),pages41–58,Online.AssociationforComputationalLinguistics.SumeghRoychowdhuryandVikramGupta.2023.Data-efficientmethodsforimprovinghatespeechdetec-tion.InFindingsoftheAssociationforCompu-tationalLinguistics:EACL2023,pages125–132,Dubrovnik,Croatia.AssociationforComputationalLinguistics.MaartenSap,SaadiaGabriel,LianhuiQin,DanJuraf-sky,NoahA.Smith,andYejinChoi.2020.Socialbiasframes:Reasoningaboutsocialandpowerim-plicationsoflanguage.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages5477–5490,Online.AssociationforComputationalLinguistics.6633

YonglongTian,ChenSun,BenPoole,DilipKrishnan,CordeliaSchmid,andPhillipIsola.2020.Whatmakesforgoodviewsforcontrastivelearning.CoRR,abs/2005.10243.BertieVidgen,TristanThrush,ZeerakWaseem,andDouweKiela.2021.Learningfromtheworst:Dy-namicallygenerateddatasetstoimproveonlinehatedetection.InProceedingsofthe59thAnnualMeet-ingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNatu-ralLanguageProcessing(Volume1:LongPapers),pages1667–1682,Online.AssociationforComputa-tionalLinguistics.KunzeWang,DongLu,CarenHan,SiquLong,andJosiahPoon.2020.Detectallabuse!towarduniver-salabusivelanguagedetectionmodels.InProceed-ingsofthe28thInternationalConferenceonCom-putationalLinguistics,pages6366–6376,Barcelona,Spain(Online).InternationalCommitteeonCompu-tationalLinguistics.ZeerakWaseem,ThomasDavidson,DanaWarmsley,andIngmarWeber.2017.Understandingabuse:Atypologyofabusivelanguagedetectionsubtasks.InProceedingsoftheFirstWorkshoponAbusiveLan-guageOnline,pages78–84,Vancouver,BC,Canada.AssociationforComputationalLinguistics.MichaelWiegand,JosefRuppenhofer,andElisabethEder.2021.Implicitlyabusivelanguage–whatdoesitactuallylooklikeandwhyarewenotgettingthere?InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages576–587,Online.AssociationforComputa-tionalLinguistics.MichaelWiegand,JosefRuppenhofer,AnnaSchmidt,andClaytonGreenberg.2018.Inducingalexiconofabusivewords–afeature-basedapproach.InPro-ceedingsofthe2018ConferenceoftheNorthAmer-icanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages1046–1056,NewOrleans,Louisiana.AssociationforComputationalLinguis-tics.MarcosZampieri,ShervinMalmasi,PreslavNakov,SaraRosenthal,NouraFarra,andRiteshKumar.2019.SemEval-2019task6:Identifyingandcat-egorizingoffensivelanguageinsocialmedia(Of-fensEval).InProceedingsofthe13thInternationalWorkshoponSemanticEvaluation,pages75–86,Min-neapolis,Minnesota,USA.AssociationforCompu-tationalLinguistics.6634

ADatasetsStatisticsandDetailsTable4displaysstatisticsrelatedtothedatasetsusedinourstudy,includingIHC,SBIC,DYNA,TOX,andISHate.Thetablepresentsthepercent-ageofimplicitandexplicitinstancesperdataset,alongwiththeirdistributionacrosssetpartitions,andtargetgroupsdistribution.BEvaluationresultswithBERTInthissectionweshowtheevaluationoftheBERTandContrastiveBERTmodelsforRQ1andRQ4specifiedinsections3.1and3.3.Table5ademonstratesBERT’sefficacyincross-evaluationcontexts,mirroringtheresultsseenwithHateBERT.Amongthemodels,SBICstandsout,displayingsuperiorgeneralizationcapabilities.Conversely,Table5billustratesthatwhilemodelslikeSBIC,IHC,andTOXreapadvantagesfromcontrastivelearning,othersexperienceaslightdipinperformance,thoughmaintaininganoverallhigh-qualityoutput.MovingontoTable6,it’sevidentthataseg-mentoftheenhancementisattributedtotheprecisecategorizationofchallengingNon-HSmessagesprevalentacrossallfivedatasets.ThisprecisionunderscoresamoreconservativeandmeticulousapproachinclassifyingamessageasHateful.Finally,Table7highlightsBERT’sconsistentperformance,boastinghighaccuracyinhandlingNon-HSinstancesforeachdataset.ThisisachievedwithoutcompromisingtheemphasisondiscerningbetweenExplicitandImplicitlabels,therebyensuringthatthemodelmaintainsabal-ancedfocusonvariedcontentnuances.CTSNEembeddingsforRQ1,RQ2,andRQ3inalldatasetsThissectionpresentstheTSNEresultsforeachresearchquestionsRQ1,RQ2,andRQ3,illustratedinFigures5,6,7,and8.ThesevisualizationsaregeneratedfromtheembeddingscapturedbyHate-BERT,specificallytrainedontheDYNAdatasetandevaluatedonalldatasets.6635

DatasetsSourceSize%Implicit%Explicit%HateClassIHCTwitter2148086,701713,298338,1238SBICSocialMedia14464958,958641,041438,9667DYNAHuman-MachineAdv.4114458,065441,934653,8961ISHateSocialMedia5307371,568628,431466,3313TOXGPT-3986642,38157,61948,956Table4:Comparingtoxiclanguagedatasets.%HateClass,%Implicit,and%Explicitarethepercentlabeledashate,implicithate,andexplicithate,respectively.TrainTestIHCSBICDYNAISHateTOXIHC0,7625±0,00630,6891±0,00720,5511±0,00770,6824±0,00680,6074±0,0329SBIC0,6603±0,03010,8568±0,00920,6500±0,03100,7581±0,01670,6939±0,0086DYNA0,6660±0,00460,7412±0,00980,7831±0,00270,7515±0,00390,7501±0,0036ISHate0,6214±0,00400,7480±0,00580,6279±0,00560,8635±0,00290,6012±0,0090TOX0,5455±0,00910,5855±0,03120,5193±0,02040,6140±0,02750,7824±0,0094(a)Cross-evaluationresultswithBERT.TrainTestIHCSBICDYNAISHateTOXIHC0.7418±0.00660.6877±0.01570.5463±0.01830.6748±0.01170.6141±0.0100SBIC0.6630±0.00490.8602±0.00340.6390±0.00560.7493±0.00930.6574±0.0145DYNA0.6431±0.00740.6973±0.02410.6979±0.00800.7414±0.01240.7001±0.0145ISHate0.6202±0.01560.6808±0.03980.6233±0.00780.8350±0.00970.5797±0.0206TOX0.5501±0.03960.5741±0.04100.5611±0.02640.6086±0.03490.7645±0.0103(b)Cross-evaluationresultswithContrastiveBERT.BoldvaluesindicateimprovementscomparedtoTable5a.Table5:BERTandContrastiveBERTcross-evaluationresultswithfivedifferentrunseeds.TestCaseBERTIHCContrastiveIHCBERTSBICContrastiveSBICBERTDynaHateContrastiveDynaHateBERTISHateContrastiveISHateBERTToxiGenContrastiveToxiGencounter_quote_nh.297±.095.410±.070.282±.143.414±.095.843±.057.870±.082.319±.044.434±.050.097±.051.109±.121counter_ref_nh.329±.055.467±.049.201±.093.264±.073.848±.033.908±.018.410±.033.555±.072.380±.152.359±.193ident_pos_nh.340±.039.406±.088.233±.085.361±.165.846±.044.808±.137.317±.060.454±.046.556±.130.624±.194negate_neg_nh.346±.054.427±.132.048±.031.128±.077.406±.055.502±.062.093±.022.194±.098.218±.146.262±.176profanity_nh.810±.054.822±.052.946±.1151±01±01±01±01±0.780±.203.688±.168slur_homonym_nh.513±.045.500±.120.520±.090.513±.112.860±.037.875±.057.880±.038.953±.038.667±.100.667±.113slur_reclaimed_nh.165±.060.215±.094.398±.113.481±.081.815±.045.815±.035.694±.064.790±.049.430±.050.420±.156target_group_nh.684±.031.716±.049.732±.267.771±.070.987±.013.992±.016.416±.031.529±.080.048±.036.084±.119target_indiv_nh.557±.063.498±.135.797±.351.938±.0381±01±0.695±.028.818±.077.142±.127.074±.073target_obj_nh.677±.067.711±.111.988±.028.978±.0181±01±0.785±.031.855±.056.228±.182.218±.175Table6:ComparativeaccuracyperformanceofBERTvsContrastiveBERTtrainedineachdatasetandevaluatedacrossvarioustestcasesonHateCheck.TrainExplicitImplicitNon-HSIHC0.87540.86780.5206SBIC0.89200.83480.8607DYNA0.57960.61130.8068ISHate0.55420.43870.9067TOX0.68920.77270.4408Table7:ContrastiveBERTavgaccuracyacrossExplicit,Implicit,andNon-HS(SBICtestset).6636

(a)DYNA→IHC(b)DYNA→SBIC(c)DYNA→DYNA(d)DYNA→ISHate(e)DYNA→TOXFigure5:RQ1:TSNEembeddingsofthetestsetsforallthedatasetsusingHateBERTfine-tunedonDYNA.(a)DYNA→IHC(b)DYNA→SBIC(c)DYNA→DYNA(d)DYNA→ISHate(e)DYNA→TOXFigure6:RQ2:TSNEembeddingsofthetestsetsforallthedatasetsusingHateBERTfine-tunedonDYNAbasedontheirtargetgroups.Non-hatefulinstancesareexcludedfromtheseplots.6637

(a)DYNA→IHC(b)DYNA→SBIC(c)DYNA→DYNA(d)DYNA→ISHate(e)DYNA→TOXFigure7:RQ3:TSNEembeddingsofthetestsetsforallthedatasetsusingHateBERTfine-tunedonDYNAandlinkingexplicitandimplicitinstances(a)DYNA→IHC(b)DYNA→SBIC(c)DYNA→DYNA(d)DYNA→ISHate(e)DYNA→TOXFigure8:RQ3:TSNEembeddingsofthetestsetsforallthedatasetsusingHateBERTfine-tunedonDYNAbasedontheirtargetgroups.Non-hatefulinstancesareexcludedfromtheseplots.