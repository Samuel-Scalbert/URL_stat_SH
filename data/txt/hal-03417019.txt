PROTECT: A Pipeline for Propaganda Detection and
Classification
Vorakit Vorakitphan, Elena Cabrio, Serena Villata

To cite this version:

Vorakit Vorakitphan, Elena Cabrio, Serena Villata. PROTECT: A Pipeline for Propaganda Detection
and Classification. CLiC-it 2021- Italian Conference on Computational Linguistics, Jan 2022, Milan,
Italy. pp.352-358, ￿10.4000/books.aaccademia.10884￿. ￿hal-03417019v2￿

HAL Id: hal-03417019

https://hal.science/hal-03417019v2

Submitted on 28 Feb 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

PROTECT
A Pipeline for Propaganda Detection and Classiﬁcation

Vorakit Vorakitphan, Elena Cabrio, Serena Villata
Universit´e Cˆote d’Azur, Inria, CNRS, I3S, France
vorakit.vorakitphan@inria.fr,
elena.cabrio@univ-cotedazur.fr, villata@i3s.unice.fr

Abstract

English. Propaganda is a rhetorical tech-
nique to present opinions with the deliber-
ate goal of inﬂuencing the opinions and the
actions of other (groups of) individuals for
predetermined misleading ends. The em-
ployment of such manipulation techniques
in politics and news articles, as well as
its subsequent spread on social networks,
may lead to threatening consequences for
the society and its more vulnerable mem-
In this paper, we present PRO-
bers.
TECT (PROpaganda Text dEteCTion), a
new system to automatically detect propa-
gandist messages and classify them along
with the propaganda techniques employed.
PROTECT is designed as a full pipeline
to ﬁrstly detect propaganda text snippets
from the input text, and then classify the
technique of propaganda,
taking advan-
tage of semantic and argumentation fea-
tures. A video demo of the PROTECT sys-
tem is also provided to show its main func-
tionalities.

Italiano. La propaganda `e una tecnica re-
torica per presentare determinate opinioni
con l’obiettivo deliberato di inﬂuenzare le
opinioni e le azioni di altri (gruppi di) in-
dividui per ﬁni predeterminati e tenden-
zialmente fuorvianti. L’impiego di tale tec-
nica di manipolazione in politica e nella
stampa, cos`ı come la sua diffusione sulle
reti sociali, pu`o portare a conseguenze
disastrose per la societ`a e per i suoi mem-
bri pi`u vulnerabili. In questo articolo pre-
sentiamo PROTECT (PROpaganda Text

Copyright ©2021 for this paper by its authors. Use per-
mitted under Creative Commons License Attribution 4.0 In-
ternational (CC BY 4.0).

dEteCTion), un nuovo sistema per identi-
ﬁcare automaticamente i messaggi propa-
gandistici e classiﬁcarli rispetto alle tec-
niche di propaganda utilizzate. PROTECT
`e un sistema progettato come una pipeline
completa per rilevare in primo luogo i
frammenti di testo propagandistici dato il
testo proposto, e successivamente classi-
ﬁcare tali frammenti secondo la tecnica
di propaganda usata, sfruttando le carat-
teristiche semantiche e argomentative del
testo. Questo articolo presenta anche un
video dimostrativo del sistema PROTECT
per mostrare le principali funzionalit`a for-
nite all’utente.

1

Introduction

Propaganda represents an effective but often mis-
leading communication strategy which is em-
ployed to promote a certain viewpoint, for in-
stance in the political context (Lasswell, 1938;
Koppang, 2009; Dillard and Pfau, 2009; Long-
pre et al., 2019). The goal of this communica-
tion strategy is to persuade the audience about the
goodness of such a viewpoint by means of mis-
leading and/or partial arguments, which is particu-
larly harmful for the more vulnerable public in the
society (e.g., young or elder people). Therefore
the ability to detect the occurrences of propaganda
in political discourse and newspaper articles is of
main importance, and Natural Language Process-
ing methods and technologies play a main role in
this context addressing the propaganda detection
and classiﬁcation task (Da San Martino et al.,
2019; Da San Martino et al., 2020a). It is, in par-
ticular, important to make this vulnerable public
aware of the problem and provide them tools able
to raise their awareness and develop their critical
thinking.

To achieve this ambitious goal, we present in

this paper a new tool called PROTECT (PROpa-
ganda Text dEteCTion) to automatically identify
and classify propaganda in texts.
In the current
version, only English text is processed. This tool
has been designed with an easy-to-access user in-
terface and a web-service API to ensure a wide
public use of PROTECT online. To the best of
our knowledge, PROTECT is the ﬁrst online tool
for propagandist text identiﬁcation and classiﬁca-
tion with an interface allowing the user to submit
his/her own text to be analysed.1

PROTECT presents two main functionalities: i)
the automatic propaganda detection and classiﬁca-
tion service, which allows the user to paste or up-
load a text and returns the text where the propagan-
dist text snippets are highlighted in different colors
depending on the propaganda technique which is
employed, and ii) the propaganda word clouds, to
show in a easy to catch visualisation the identiﬁed
propagandist text snippets. PROTECT is deployed
as a web-service API, allowing users to download
the output (the text annotated with the identiﬁed
propaganda technique) as a json ﬁle. The PRO-
TECT tool relies on a pipeline architecture to ﬁrst
detect the propaganda text snippets, and second to
classify the propaganda text snippets with respect
to a speciﬁc propaganda technique. We cast this
task as a sentence-span classiﬁcation problem and
we address it relying on a transformer architec-
ture. Results reach SoTA systems performances
on the tasks of propaganda detection and classi-
ﬁcation (for a comparison with SoTA algorithms,
we refer to (Vorakitphan et al., 2021)).

The paper is structured as follows: ﬁrst, Section
2 discusses the state of the art in propaganda de-
tection and classiﬁcation and compares our contri-
bution to the literature. Then Section 3 describes
the pipeline for the detection and classiﬁcation of
propaganda text snippets as well as the data sets
used for the evaluation and the obtained results.
Section 4 describes the functionalities of the web
interface, followed by the Conclusions.

2 Related Work

In the last years, there has been an increasing
interest in investigating methods for textual pro-
paganda detection and classiﬁcation. Among
them, (Barr´on-Cede˜no et al., 2019) present a sys-

1The video demonstrating the PROTECT tool is available
here https://1drv.ms/u/s!Ao-qMrhQAfYtkzD69
JPAYY3nSFub?e=oUQbxQ

tem to organize news events according to the level
of propagandist content in the articles, and in-
troduces a new corpus (QProp) annotated with
the propaganda vs.
trustworthy classes, provid-
ing information about the source of the news
articles. Recently, a web demo named Prta
(Da San Martino et al., 2020b) has been pro-
posed, trained on disinformation articles. This
demo allows a user to enter a plain text or a URL,
but it does not allow users to download such re-
sults. Similarly to PROTECT, Prta shows the
propagandist messages at the snippet level with
an option to ﬁlter the propaganda techniques to be
shown based on the conﬁdence rate, and also ana-
lyzes the usage of propaganda technique on deter-
mined topics. The implementation of this system
relies on the approach proposed in (Da San Mar-
tino et al., 2019).

The most recent approaches for propaganda de-
tection are based on language models that mostly
involve transformer-based architectures. The ap-
proach that performed best on the NLP4IF’19
sentence-level classiﬁcation task relies on the
BERT architecture with hyperparameters tun-
ing without activation function (Mapes et al.,
2019). (Yoosuf and Yang, 2019) focused ﬁrst on
the pre-processing steps to provide more informa-
tion regarding the language model along with ex-
isting propaganda techniques, then they employ
the BERT architecture casting the task as a se-
quence labeling problem. The systems that took
part in the SemEval 2020 Challenge - Task 11 rep-
resent the most recent approaches to identify pro-
paganda techniques based on given propagandist
spans. The most interesting and successful ap-
proach (Jurkiewicz et al., 2020) proposes ﬁrst to
extend the training data from a free text corpus as
a silver dataset, and second, an ensemble model
that exploits both the gold and silver datasets dur-
ing the training steps to achieve the highest scores.

As most of the above mentioned systems, also
PROTECT relies on language model architectures
for the detection and classiﬁcation of propaganda
messages, empowering them with a rich set of
features we identiﬁed as pivotal in propagandist
text from computational social science literature
(Vorakitphan et al., 2021). In particular, (Morris,
2012) discusses how emotional markers and af-
fect at word- or phrase-level are employed in pro-
paganda text, whilst (Ahmad et al., 2019) show
that the most effective technique to extract senti-

ment for the propaganda detection task is to rely
on lexicon-based tailored dictionaries. (Li et al.,
2017) show how to detect degrees of strength from
calmness to exaggeration in press releases. Fi-
nally, (Troiano et al., 2018) focus on feature ex-
traction of text exaggeration and show that main
factors include imageability, unexpectedness, and
the polarity of a sentence.

3 Propaganda Detection and

Classiﬁcation

PROTECT addresses the task of propaganda tech-
nique detection and classiﬁcation at fragment-
level, meaning that both the spans and the type
of propaganda technique are identiﬁed and high-
lighted in the input sentences. In the following, we
describe the datasets used to train and test PRO-
TECT, and the approach implemented in the sys-
tem to address the task.

3.1 Datasets

To evaluate the approach on which PROTECT
relies, we use two standard benchmarks for
Propaganda Detection and Classiﬁcation, namely
the NLP4IF’19 (Da San Martino et al., 2019)
and SemEval’20 datasets (Da
San Martino
et al., 2020a). The former was made available
for the shared task NLP4IF’19 on ﬁne-grained
propaganda detection. 18 propaganda techniques
are annotated on 469 articles (293 in the training
set, 75 in the development set, and 101 in the test
set).2 As a follow up, in 2020 SemEval proposed
a shared task (T11)3 reducing the number of
propaganda categories with respect to NLP4IF’19
(14 categories, 371 articles in the training set and
75 in the development set). PROTECT detects
and classiﬁes the same list of 14 propaganda
task, namely:
techniques as in the SemEval
Appeal to Authority,
Appeal to fear-prejudice,
Bandwagon,
Black-
and-White Fallacy,
Causal Oversimpliﬁcation,
Doubt, Exaggeration Minimisation, Flag-Waving,
Loaded-Language, Name-Calling Labeling, Rep-
etition, Slogans, Thought-terminating Cliches,
Whataboutism Straw-Men Red-Herring.

Reductio ad hitlerum,

Those classes are not uniformly distributed
in the data
and
Name-Calling Labeling are the classes with the

Loaded-Language

sets.

2https://propaganda.qcri.org/nlp4if-s

hared-task/

3https://propaganda.qcri.org/semeval2

020-task11/

higher number of
instances (representing re-
the propagan-
spectively 32% and 15% of
dist messages on all above-mentioned datasets).
The classes with the lower number of
in-
stances are Whataboutism, Red-Herring, Band-
wagon, Straw-Men,
respectively occurring in
1%,
0.23% in NLP4IF’19
0.87%,
datasets.
In SemEval’20T11 such labels where
merged, and the classes Whataboutism Straw-
Men Red-Herring, Bandwagon respectively rep-
resent 1.33% and 1.29% of the propagandist mes-
sages.

0.29%,

3.2 PROTECT Architecture

Given a textual document or a paragraph as input,
the system performs two steps. First, it performs
a binary classiﬁcation at token level, to label a to-
ken as propagandist or not. Then, it classiﬁes pro-
pagandist tokens according to the 14 propaganda
categories from SemEval task (T11).

For

instance, given the following example
“Manchin says Democrats acted like babies at the
SOTU (video) Personal Liberty Poll Exercise your
right to vote.” the snippets “babies” is ﬁrst classi-
ﬁed as propaganda (step 1), and then more speciﬁ-
cally as an instance of the Name-Calling Labeling
propaganda technique (step 2).

Step 1: Propaganda Snippet Detection. To
train PROTECT, we merge the training, develop-
ment and test sets from NLP4IF, and the training
set from Semeval’20 T11. The development set
from Semeval’20 T11 is instead used to evaluate
the system performances.4
In the preprocessing
phase, each sentence is tokenized and tagged with
a label per token according to the IOB format.

For the binary classiﬁcation, we adopt Pre-
trained Language Model (PLM) based on BERT
(bert-base-uncased model) (Devlin et al., 2019)
architecture. The hyperparameters are a learning
rate of 5e-5, a batch of 8, max len of 128. For
the evaluation, we compute standard classiﬁca-
tion metrics5 at the token-level. The results ob-
tained by the binary classiﬁer (macro average over
5 runs) on SemEval’20 T11 development set are
0.71 precision, 0.77 recall and 0.72 F-measure (us-

4The gold annotations of Semeval’20 test set are not avail-
able, this is why we selected the development set for evalua-
tion.

5https://scikit-learn.org/stable/modu
les/generated/sklearn.metrics.precisio
n recall fscore support.html

Propaganda Technique

Appeal to Authority
Appeal to fear-prejudice
Bandwagon,Reductio ad hit.
Black-White-Fallacy
Casual-Oversimpliﬁcation
Doubt
Exaggeration,Minimisation
Flag-Waving
Loaded Language
Name Calling,Labeling
Repetition
Slogans
Thought-terminating Cliches
Whatab.,Straw Men,Red Her.
Average

PLM:
RoBERTa
0.48
0.57
0.72
0.38
0.70
0.74
0.67
0.88
0.88
0.85
0.70
0.72
0.52
0.55
0.67

Table 1: Results on sentence-span classiﬁcation on
SemEval’20 T11 dev set (micro-F1) using span-
pattern produced by the binary classiﬁcation step
(Step 1).

ing Softmax as activation function6).

We then perform a post-processing step to auto-
matically join tokens labelled with the same pro-
paganda technique into the same textual span.

Given that PLM is applied at token-level, each
token is processed into sub-words (e.g., “running”
is tokenized and cut into two tokens: “run” and
“##ing”). Such sub-words can mislead the classi-
ﬁer. For instance, in the following sentence: “The
next day, Biden said, he was informed by Indian
press that there were at least a few Bidens in In-
dia.”, our system detects least a few Bidens in
as a propagandist snippet, but it misclassiﬁes one
sub-word (“at” was not considered as part of “at
least”, and therefore excluded from the propagan-
dist snippet).

Step 2: Propaganda Technique Classiﬁcation.
We cast this task as a sentence-span multi-class
classiﬁcation problem. More speciﬁcally, both the
tokenized sentence and the span are used to feed
the transformer-based model RoBERTa (roberta-
base pre-trained model)7 (Liu et al., 2019) to per-

6We are aware that sigmoid function is usually used as
default activation function in binary classiﬁcation. However,
in our setting we tested both functions and we obtained better
performances with Softmax as activation function (+0.04 F1
with respect to sigmoid).

7https://huggingface.co/transformers/

model doc/roberta.html

form both a sentence classiﬁcation and a span clas-
siﬁcation. More precisely: i) we input a sentence
to the tokenizer where max length is set to 128
with padding; ii) we input the span provided by
the propaganda span-template from SemEval T11
dataset, and we set max length value of 20 with
padding. RoBERTa tokenizer is applied in both
cases. If a sentence does not contain propaganda
spans, it is labeled as “none-propaganda”.

To take into account context

features at
sentence-level, a BiLSTM is introduced. For each
sentence, semantic and argumentation features are
extracted following the methodology proposed in
(Vorakitphan et al., 2021) and given in input to
the BiLSTM model (hyper-parameters: 256 hid-
den size, 1 hidden layer, drop out of 0.1 with
ReLU function at the last layer before the joint
loss function). Such features proved to be use-
ful to improve the performances of our approach
on propagandist messages classiﬁcation, obtaining
SoTA results on some categories (in (Vorakitphan
et al., 2021) we provide a comparison of our model
with SoTA systems on both NLP4IF and SemEval
datasets).

To combine the results from sentence-span
based RoBERTa with the feature-based BiLSTM
loss strategy proposed in
we apply the joint
(Vorakitphan et al., 2021). Each model produces
a loss per batch using CrossEntropy loss function
L. Following the function: lossjoint loss = α ×
(losssentence+lossspan+losssemantic argumentation features)
N loss
where each loss value is produced from CrossEn-
tropy function of its classiﬁer (e.g., losssentence
and lossspan from RoBERTa models of sentence
and span, losssemantic argumentation features from the
BiLSTM model.)

To train the above mentioned methods for
the propaganda technique classiﬁcation task, we
merged the data sets of NLP4IF’19 and Se-
mEval’20 T11 (same setting as in Step 1). Then
we tested the full pipeline of PROTECT on the de-
velopment set from Semeval’20 T11. The output
of the snippet detection task (Step 1) are provided
as a span-pattern to the models performing Step 2.
Table 1 reports on the obtained results of the full
pipeline (Step 1+Step 2) averaged over 5 runs (we
cannot provide a fair comparison of those results
with SoTA systems, given that in SemEval the two
tasks are separately evaluated and no pipeline re-
sults are provided). We can notice however, that
our results in a pipeline are comparable with the

Figure 1: PROTECT Interface: Propaganda Techniques Classiﬁcation

ones obtained in (Vorakitphan et al., 2021) on the
two separate tasks.

Given the high complexity of the propaganda
technique classiﬁcation task and the classes’ un-
balance, some examples are miss-classiﬁed by the
system. For instance, in the following sentence
“The Mueller probe saw several within Trump’s
orbit indicted, but not Trump
as family or Trump
’
himself”, the system annotated the snippet in ital-
ics as “Name Calling,Labeling”, while the correct
labels would have been “Repetition”.

4 PROTECT Functionalities

As previously introduced, PROTECT allows a
user to input plain text and retrieve the propagan-
dist spans in the message as output by the system.
In the current version of the system, two services
are provided through the web interface (and the
API), described in the following.

4.1 Service 1: Propaganda Techniques

Classiﬁcation

The system accepts an input plain text in English,
and then the architecture described in Section 3.2
is run over such text. The output consists of
an annotated version of the input text, where the
different propagandist techniques detected by the
system are highlighted in different colours. The
colour of the highlighted snippet is distinctive of
a certain propaganda technique:
the darker the
color, the higher the conﬁdence score of the sys-
tem in assigning the label to a textual snippet. Fig-
ure 1 shows an example of PROTECT web inter-

face. Checkboxes on the right side of the page pro-
vide the key to interpret the colors, and allow the
user to check or un-check (i.e. highlight or not)
the different propagandist snippets in the text, ﬁl-
tering the results. Faded to dark colours represent
the conﬁdence level of the prediction (the darker
the colour, the higher the system conﬁdence). The
snippets in bold contain multiple propaganda tech-
niques in the same text spans, that can be unveiled
hovering with the mouse over the snippets.

As said before, PROTECT can be used through
the provided API, and annotated text can be down-
loaded as a JSON ﬁle with the detected propagan-
dist snippet(s) at character indices (start to end in-
dices of a snippet) based on individual sentence,
propaganda technique(s) used, and the conﬁdence
score(s)).

4.2 Service 2: Propaganda Word Clouds

The propagandist snippets output by the system
can also be displayed as word clouds, where the
size of the words represents the system conﬁdence
score in assigning the labels (see Figure 2). The
different sizes represent the conﬁdence score of
the prediction, and the colors the propaganda tech-
nique (as in Service 1). If multiple techniques are
found in the same snippet, it is duplicated in the
word cloud. As for the ﬁrst service, a checkbox on
the right side of the word clouds allows the user
to select the propagandist techniques to be visual-
ized. Also in this case, a json ﬁle can be down-
loaded with the system prediction.

The word cloud service has been added to PRO-

Figure 2: PROTECT Interface: Word Cloud

TECT in addition to the standard visualization, to
provide a different and informative way to sum-
marise propaganda techniques on a topic, and to
facilitate their identiﬁcation.

5 Conclusions

In this paper, we presented PROTECT, a propa-
ganda detection and classiﬁcation tool. PROTECT
relies on a pipeline to detect propaganda snip-
pets from plain text. We evaluated the proposed
pipeline on standard benchmarks achieving state-
of-the-art results. PROTECT is deployed as a
web-service API that accepts a plain text input,
returning downloadable annotated text for further
usage. In addition, a propaganda word clouds ser-
vice allows to gain further insights from such text.

Acknowledgments

This work is partially supported by the AN-
SWER project PIA FSN2 n.
P159564-
2661789/DOS0060094 between Inria and Qwant.
This work has also been supported by the French
government, through the 3IA Cˆote d’Azur Invest-
ments in the Future project managed by the Na-
tional Research Agency (ANR) with the reference
number ANR-19-P3IA-0002.

References

Siti Rohaidah Ahmad, Muhammad Zakwan Muham-
mad Rodzi, Nurlaila Syaﬁra Shapiei, Nurhaﬁzah
Moziyana Mohd Yusop, and Suhaila Ismail. 2019.
A review of feature selection and sentiment analy-
sis technique in issues of propaganda. International
Journal of Advanced Computer Science and Appli-
cations, 10(11).

Alberto Barr´on-Cede˜no, Israa Jaradat, Giovanni Mar-
tino, and Preslav Nakov. 2019. Proppy: Organizing
the news based on their propagandistic content. In-
formation Processing Management, 56, 05.

Giovanni Da San Martino, Seunghak Yu, Alberto
Barr´on-Cede˜no, Rostislav Petrov,
and Preslav
Nakov. 2019. Fine-grained analysis of propaganda
In Proceedings of the 2019 Con-
in news article.
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 5636–5646, Hong Kong, China,
November. Association for Computational Linguis-
tics.

Giovanni Da San Martino, Alberto Barr´on-Cede˜no,
Henning Wachsmuth, Rostislav Petrov, and Preslav
Nakov. 2020a. SemEval-2020 task 11: Detection of
propaganda techniques in news articles. In Proceed-
ings of the Fourteenth Workshop on Semantic Eval-
uation, pages 1377–1414, Barcelona (online), De-
cember. International Committee for Computational
Linguistics.

Giovanni Da San Martino, Shaden Shaar, Yifan Zhang,
Seunghak Yu, Alberto Barr´on-Cede˜no, and Preslav
Nakov. 2020b. Prta: A system to support the anal-
ysis of propaganda techniques in the news. In Pro-
ceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics: System Demon-
strations, pages 287–293, Online, July. Association
for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova.
2019. BERT: pre-training
of deep bidirectional transformers for language un-
In Jill Burstein, Christy Doran, and
derstanding.
Thamar Solorio, editors, Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2019, Min-
neapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
and Short Papers), pages 4171–4186. Association
for Computational Linguistics.

Vorakit Vorakitphan, Elena Cabrio, and Serena Vil-
lata. 2021. ”Don’t discuss”: Investigating Semantic
and Argumentative Features for Supervised Propa-
gandist Message Detection and Classiﬁcation.
In
Recent Advances in Natural Language Processing
(RANLP 2021), Varna (Online), Bulgaria, Septem-
ber.

Shehel Yoosuf and Yin Yang. 2019. Fine-grained pro-
In Pro-
paganda detection with ﬁne-tuned BERT.
ceedings of the Second Workshop on Natural Lan-
guage Processing for Internet Freedom: Censor-
ship, Disinformation, and Propaganda, pages 87–
91, Hong Kong, China, November. Association for
Computational Linguistics.

James Price Dillard and Michael Pfau. 2009. The
Persuasion Handbook: Developments in Theory and
Practice. Sage Publications, Inc.

Dawid Jurkiewicz, Łukasz Borchmann, Izabela Kos-
mala, and Filip Grali´nski.
2020. ApplicaAI at
SemEval-2020 task 11: On RoBERTa-CRF, span
CLS and whether self-training helps them. In Pro-
ceedings of the Fourteenth Workshop on Semantic
Evaluation, pages 1415–1424, Barcelona (online),
December. International Committee for Computa-
tional Linguistics.

Haavard Koppang. 2009. Social inﬂuence by manipu-
lation: A deﬁnition and case of propaganda. Middle
East Critique, 18:117 – 143.

Harold Dwight Lasswell. 1938. Propaganda technique

in the world war.

Yingya Li, Jieke Zhang, and Bei Yu. 2017. An NLP
analysis of exaggerated claims in science news. In
Proceedings of the 2017 EMNLP Workshop: Nat-
ural Language Processing meets Journalism, pages
106–111, Copenhagen, Denmark, September. Asso-
ciation for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov.
2019.
RoBERTa: A Robustly Optimized BERT Pretrain-
ing Approach. CoRR, abs/1907.11692.
eprint:
1907.11692.

Liane Longpre, Esin Durmus, and Claire Cardie. 2019.
Persuasion of the undecided: Language vs. the lis-
In Proceedings of the 6th Workshop on Ar-
tener.
gument Mining, pages 167–176, Florence, Italy, Au-
gust. Association for Computational Linguistics.

Norman Mapes, Anna White, Radhika Medury, and
Sumeet Dua. 2019. Divisive language and pro-
paganda detection using multi-head attention trans-
formers with deep learning BERT-based language
models for binary classiﬁcation. In Proceedings of
the Second Workshop on Natural Language Process-
ing for Internet Freedom: Censorship, Disinforma-
tion, and Propaganda, pages 103–106, Hong Kong,
China, November. Association for Computational
Linguistics.

Travis Morris. 2012. Extracting and networking emo-
In 2012 European
tions in extremist propaganda.
Intelligence and Security Informatics Conference,
pages 53–59.

Enrica Troiano, Carlo Strapparava, G¨ozde ¨Ozbal, and
Serra Sinem Tekiro˘glu. 2018. A computational
exploration of exaggeration. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 3296–3304, Brussels,
Belgium, October-November. Association for Com-
putational Linguistics.

