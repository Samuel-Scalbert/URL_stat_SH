Modal sense classification with task-specific context
embeddings
Bo Li, Mathieu Dehouck, Pascal Denis

To cite this version:

Bo Li, Mathieu Dehouck, Pascal Denis. Modal sense classification with task-specific context em-
beddings. ESANN 2019 - 27th European Symposium on Artificial Neural Networks, Computational
Intelligence and Machine Learning, Apr 2019, Bruges, Belgium. ￿hal-02143762￿

HAL Id: hal-02143762

https://hal.science/hal-02143762

Submitted on 29 May 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Modal sense classiﬁcation with task-speciﬁc
context embeddings

Bo Li1,2, Mathieu Dehouck2 and Pascal Denis2 ∗

1- Univ. Lille, CNRS, UMR 8163 - STL - Savoirs Textes Langage
59000 Lille, France

2- Magnet, INRIA Lille - Nord Europe, 59650 Villeneuve d’Ascq, France
{bo.li, pascal.denis, mathieu.dehouck}@inria.fr

Abstract.
Sense disambiguation of modal constructions is a crucial
part of natural language understanding. Framed as a supervised learning
task, this problem heavily depends on an adequate feature representation
of the modal verb context. Inspired by recent work on general word sense
disambiguation, we propose a simple approach of modal sense classiﬁcation
in which standard shallow features are enhanced with task-speciﬁc context
embedding features. Comprehensive experiments show that these enriched
contextual representations fed into a simple SVM model lead to signiﬁcant
classiﬁcation gains over shallow feature sets.

1

Introduction

Modal sense analysis plays an important role in NLP tasks such as sentiment
analysis [1], hedge detection [2] and factuality recognition [3]. As an important
part of modality analysis, sense classiﬁcation of modal verbs has attracted no-
table interest of researchers in recent years. Modal sense classiﬁcation (MSC)
can be treated as a special type of Word Sense Disambiguation (WSD) [4], but
diﬀers from general WSD tasks in that MSC deals with a small set of modal
verbs with a limited set of senses. For instance, the English modal verb can has
a limited sense set containing dynamic, deontic, and epistemic senses.

MSC is typically modeled in a supervised learning framework, which in turn
raises the question of how to build the most eﬀective feature representation of the
modal context. Existing studies have proposed various context representations.
A ﬁrst set of approaches rely on manually engineered features derived from so-
phisticated linguistic pre-processing. For instance, [5] uses syntax-based features
in addition to shallow features such as POS and lemma. Pushing this line of
research even further, [6, 7] propose to use semantic-rich features that are re-
lated to lexical, proposition-level and discourse-level semantic factors. Taking a
drastically diﬀerent approach, [8] employs Convolution Neural Networks (CNNs)
to automatically extract modal sense features, and show that their approach is
competitive with hand-crafted feature-based classiﬁers.

In this paper, we explore an intermediate line of research, attempting to
leverage both shallow feature sets and pre-trained embeddings for the context
words in a very simple learning architecture. That is, we propose to enrich a very

∗This work was supported by the ANR-16-CE93-0009 REM project.

simple feature set (basically, POS tags, uni-, bi- and tri-grams), known to be
very eﬀective for WSD, with embeddings for the context words. Inspired by the
recent work [9] for general WSD, we propose several weighting schemes, including
a new task-speciﬁc one, for constructing context embeddings from pre-trained
word embeddings. Experimental results prove that this approach outperforms
state-of-the-art methods over two standard MSC datasets.

2 Related work

Modal sense classiﬁcation. Similar to WSD, MSC has been framed as a
supervised learning problem. [5] presents an annotation schema for modal verbs
in the MPQA corpus and represent modal senses with word-based and syntactic
features. On top of feature sets, they perform machine learning with a maximum
entropy classiﬁer. Since the corpus used in [5] is relatively small and unbalanced,
[6, 7] develop a paraphrase-driven, cross-lingual projection approach to create
large and balanced corpora automatically. In addition, they develop semantically
grounded features such as lexical features of the modiﬁed verb and subject-
related features for sense representation. [8] casts modal sense classiﬁcation as
a novel semantic sentence classiﬁcation task using CNNs to model propositions.
Word sense disambiguation. Word sense disambiguation is a long-standing
challenge in natural language processing [4]. We focus here on most recent ad-
vances in WSD which come from the use of neural networks. One category of
neural WSD works use neural networks to learn word embeddings [10, 9] or
context representation [11, 12] that can be further used separately or as addi-
tional features to shallow sense features. The other category of approaches [13]
model WSD as an end-to-end sequence learning problem. These two categories
of approaches show similar performance as reported in [13]. Our work is in-
spired by [9] which has shown state-of-the-art performance in WSD and which
is convenient for adaptation to MSC.

3 Approach

Following [9] , we will use word embeddings trained on unlabeled data to enrich
shallow features used in general WSD tasks. As far as we can tell, such a
combination has never been investigated for MSC.

3.1 Shallow features (SF)

We refer to standard features used in WSD tasks and make use of three shallow
feature sets as general features for MSC. For all feature sets, we compute with
the same context window surrounding a target modal verb with size 2n (n left,
n right). The ﬁrst feature set is POS tags of context words. The second feature
set is context words excluding stop words. The third feature set is the local
collocations of a target word which are ordered sequences of words appearing in
the context window. We use several uni-gram, bi-gram and tri-gram patterns as

the collocation patterns in the context window. We use these simple yet eﬃcient
features to do away with heavy feature engineering.

3.2 Word embedding features

Embeddings of context words can be combined in a way such that a compre-
hensive context embedding vector can be obtained to represent the modal con-
text. The context vector can then be combined with those shallow features.
Inspired by [9], we obtain context representation by taking a weighted sum of
embedding vectors of all context words. Given a target modal verb w0 and a
context window size 2n, let us denote the context word at the relative position
i(i ∈ [−n, 0) ∪ (0, n]) as wi. The context embedding of w0 (denoted as wc ∈ Rd)
can be written as the weighted sum of embedding vector wi ∈ Rd of each context
word wi, which is:

n
(cid:88)

wc =

f (wi)wi

(1)

i=−n&i(cid:54)=0

where f (wi) is a weighting function measuring the importance of each context
word wi w.r.t. the modal verb w0. For simplicity, we assume that f (wi) only
depends on i (i.e., the relative position of wi w.r.t. w0).

Average weighting (WEav). The most intuitive weighting schema simply

treats all the context words equally, which corresponds to:

f (wi) =

1
2n

(2)

Linear weighting (WEli). In a linear manner, the importance of a word is
assumed to be inversely proportional to its distance from the target word, which
can be formulated as:

f (wi) = 1 −

|i − 1|
n

(3)

Exponential weighting (WEex). Compared to linear manner, the expo-
nential weighting schema put more emphasis on the nearest context words. The
importance of a word decreases exponentially w.r.t. its distance from the target
word, which is:

f (wi) = α

i−1
n−1

(4)

where α is a hyper-parameter. The above weighting schema have been investi-
gated in [9].

Modal-speciﬁc weighting. In order to adapt the weighting function f (wi)
to MSC, we want to assign more weights to context words which have closer
connections to modal senses. Inspired by ﬁndings in [6], we note that the em-
bedded verb in the scope of the modal and the subject noun phrase are two
components acting as strong hints to determine the modal verb sense. Let us
consider as an example the sentence the springbok can jump very high. When
the embedded verb jump appears, we prefer a dynamic reading of can. Based
on the intuition above, we assign more weights to context word which is the

embedded verb or which is the head in the subject noun phrase. Formally, we
deﬁne a modal-speciﬁc weighting function g such that:

g(wi) =






β1f (wi), wi is embedded verb
β2f (wi), wi is the head in the subj noun phrase
f (wi),

else

where β1 and β2 are hyper-parameters indicating that words that are more rele-
vant to modal verb senses take more responsibility, and f is one of the weighting
schema deﬁned above. We can obtain a context representation wc that is opti-
mized for MSC task by replacing f in equation 1 with g deﬁned here.

4 Experiments

In this section, we perform comprehensive experiments in order to evaluate the
usefulness of word embedding features in MSC.

Datasets. We make use here of two representative corpora for MSC. One
is the small and unbalanced corpus annotated manually based on the MPQA
corpus in [5]. The other one is the larger and balanced corpus EPOS which
is constructed automatically via paraphrase-driven modal sense projection in
[6]. We use two training/testing settings: (1) both training and testing sets
picked from MPQA. (2) training set from EPOS and testing set from MPQA.
Characteristics of training/testing sets are not given here due to space reasons.
Experimental settings. We have used Stanford parser to pre-process the
original corpora. For classiﬁcation, we employ SVM implemented in LibSVM
[14]. The context window size 2n is picked from {2, 6, 10, 20}. The parameter
α is set to {0.1, 0.2, 0.5, 0.8}, and the parameters β1 and β2 are selected from
{1, 2, 5, 10}. The word embeddings are trained with word2vec[15] on Wikipedia.
The hyper-parameters are optimized via 5-fold cross validation. We employ
accuracy as the performance measure and McNemar’s test (p < 0.05) to test
signiﬁcance.

Competing Systems. we use max frequent sense (MFS) baseline for un-
balanced classiﬁer and random sense (RS) baseline for balanced classiﬁer. The
other baseline systems are RR [5] and ZH [6]. In addition, we compare diﬀerent
combinations of features presented in section 3. SF is the shallow features, and
+WE∗∗ denotes the concatenation of word embedding features WE∗∗ to SF.
+WE∗∗(cid:48) stands for the modal speciﬁc weighting version.

Results on unbalanced corpus. We ﬁrstly show the results on the un-
balanced MPQA corpus in table 1. The performance of SF is comparable to
RR and slightly inferior to ZH, given that SF contains similar features as RR.
Furthermore, when we consider word embeddings as extra features, we obtain re-
sults that are at least as good as SF. The best performance comes from +WEex(cid:48)
where, compared to SF, we have gotten signiﬁcant improvement on can and in-
signiﬁcant improvement on could, and slightly decrease that is not signiﬁcant on
should. However, with any of +WEav, +WEli and +WEex, we cannot achieve
any signiﬁcant improvement. We further note that MFS is a strong baseline on

the unbalanced corpus. Neither RR nor ZH is able to beat the MFS baseline
with ZH being slightly better than RR, which is coincident with conclusions in
previous work [6].

Table 1: Accuracy with various features on MPQA. + or - indicates that the
improvements or degradations with respect to SF are statistically signiﬁcant.
The highest value in each row is marked in bold.

can
could
may
must
shall
should

MFS
69.9
65.0
93.6
94.3
84.6
90.8

RR
66.6
62.5
93.6
94.3
83.3
90.8

ZH
66.1
67.9
93.6
94.3
83.3
92.9

SF
63.5
67.5
93.6
94.3
83.3
91.1

+WEav +WEli +WEex +WEex(cid:48)
70.1+
70.5
93.6
94.3
83.3
90.8

66.7
68.9
93.6
94.3
83.3
90.8

67.1
70.4
93.6
94.3
83.3
90.8

66.2
67.8
93.6
94.3
83.3
90.9

Results on balanced corpus. Since the MPQA training set is small in
size and unbalanced in sense distribution, machine learning algorithms tend to
over ﬁt the training set. To circumvent this problem, we perfom additional
experiments and train on the balanced EPOS corpus. The results are reported
in table 2. Not surprisingly, the RS baseline is inferior to any other system in the
table. When word embedding features are combined to SF, we note improvement
on most of the six modal verbs, and the best results are obtained with +WEex(cid:48).
The improvements of +WEex(cid:48) over SF are signiﬁcant on can. Both results on
balanced and unlabeled corpora reveal that task-speciﬁc embedding features on
top of WEex are superior to their counterparts without task-speciﬁc information,
which shows the best performance overall.

Table 2: Accuracy with various features on EPOS+MPQA. + or - indicates that
the improvements or degradations with respect to SF are statistically signiﬁcant.
The highest value in each row is marked in bold.

can
could
may
must
shall
should

RS
33.3
33.3
50.0
50.0
50.0
50.0

RR
57.8
49.2
92.1
71.7
53.9
76.3

ZH
60.4
56.3
92.1
85.6
53.9
88.3

SF
65.6
56.7
93.6
76.8
61.5
90.8

5 Conclusion

+WEav +WEli +WEex +WEex(cid:48)
67.3+
59.6
95.0
80.6
53.8
90.8

64.7
60.0
94.2
79.9
46.2
90.8

64.4
57.9
94.2
78.7
46.2
90.8

64.7
57.9
93.6
79.3
46.2
90.8

In this paper, we make use of word embeddings learned from unlabeled data
as additional features to more adequately represent the context of modal verbs.
Our main experimental result is that the best weighting scheme for combining

pre-trained word embeddings into context embeddings is a corrected version of
exponentially decaying weighting which attributes higher weights to the verb
modiﬁed by the modal and its subject.

References

[1] Yang Liu, Xiaohui Yu, Zhongshuai Chen, and Bing Liu. Sentiment analysis of sentences
with modalities. In Proceedings of the 2013 International Workshop on Mining Unstruc-
tured Big Data Using Natural Language Processing, UnstructureNLP ’13, pages 39–44,
2013.

[2] Roser Morante and Walter Daelemans. Learning the scope of hedge cues in biomedical
texts. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language
Processing, BioNLP ’09, pages 28–36, 2009.

[3] Roser Sauri and James Pustejovsky. Are you sure that this happened? assessing the
factuality degree of events in text. Computational Linguistics, 38(2):261–299, 2012.
[4] Roberto Navigli. Word sense disambiguation: A survey. ACM Computing Surveys,

41(2):10:1–10:69, February 2009.

[5] Josef Ruppenhofer and Ines Rehbein. Yes we can!? annotating english modal verbs. In
Proceedings of the Eight International Conference on Language Resources and Evaluation
(LREC), pages 1538–1545, Istanbul, Turkey, 2012.

[6] Mengfei Zhou, Anette Frank, Annemarie Friedrich, and Alexis Palmer. Semantically
Enriched Models for Modal Sense Classiﬁcation. In Proceedings of the EMNLP Workshop
LSDSem: Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 44–
53, Lisbon, Portugal, 2015.

[7] Ana Marasovic, Mengfei Zhou, Alexis Palmer, and Anette Frank. Modal sense classiﬁ-
cation at large: Paraphrase-driven sense projection, semantically enriched classiﬁcation
models and cross-genre evaluations. Linguistic Issues in Language Technology, Special
issue on Modality in Natural Language Understanding, 14(3):1–58, 2016.

[8] Ana Marasovic and Anette Frank. Multilingual modal sense classiﬁcation using a convo-
lutional neural network. In Proceedings of the 1st Workshop on Representation Learning
for NLP, pages 111–120, Berlin, Germany, 2016.

[9] Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. Embeddings for word
sense disambiguation: An evaluation study. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics, ACL ’16, pages 897–907, 2016.

[10] Kaveh Taghipour and Hwee Tou Ng. Semi-supervised word sense disambiguation using
word embeddings in general and speciﬁc domains.
In Proceedings of the 2015 Annual
Conference of the North American Chapter of the ACL, NAACL ’15, pages 314–323,
2015.

[11] Dayu Yuan, Julian Richardson, Ryan Doherty, Colin Evans, and Eric Altendorf. Semi-
supervised word sense disambiguation with neural models.
In Proceedings of the 26th
International Conference on Computational Linguistics, COLING ’16, pages 1374–1385,
2016.

[12] Mikael K˚ageb¨ack and Hans Salomonsson. Word sense disambiguation using a bidirectional
lstm. In Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon, pages 51–
56, 2016.

[13] Alessandro Raganato, Claudio Delli Bovi, and Roberto Navigli. Neural sequence learn-
ing models for word sense disambiguation.
In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing, EMNLP ’17, pages 1156–1167, 2017.
[14] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines.

ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27, May 2011.

[15] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed
representations of words and phrases and their compositionality. In Advances in Neural
Information Processing Systems, pages 3111–3119. 2013.

