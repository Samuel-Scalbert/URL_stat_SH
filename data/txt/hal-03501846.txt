Probabilistic End-to-End Graph-based Semi-Supervised
Learning
Mariana Vargas Vieyra, Aurélien Bellet, Pascal Denis

To cite this version:

Mariana Vargas Vieyra, Aurélien Bellet, Pascal Denis. Probabilistic End-to-End Graph-based Semi-
Supervised Learning. Graph Representation Learning workshop, NeurIPS, 2019, Vancouver, Canada.
￿hal-03501846￿

HAL Id: hal-03501846

https://hal.science/hal-03501846

Submitted on 23 Dec 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Probabilistic End-to-End Graph-based
Semi-Supervised Learning

Mariana Vargas Vieyra, Aurélien Bellet, Pascal Denis
Inria Lille Nord Europe, France
first.last@inria.fr

Abstract

In this paper we address the problem of graph-based semi-supervised learning
in tasks where a graph describing the relationships between data points is not
available. We propose a method to jointly learn the graph and the parameters of a
semi-supervised model using a probabilistic framework. We empirically show our
proposal achieves competitive results in a variety of datasets.

1

Introduction

Graph-based semi-supervised models are good at leveraging unannotated data when small amounts
of labels are available: they take as input a graph whose nodes are data points (both labeled and
unlabeled) and edges describe how points are related to each other. There exists a variety of such
models which propagate labels based on a smoothness criterion [1, 2, 3, 4, 5]. A more recent approach
uses Graph Convolutional Networks (GCN) to learn node representations based on all the input data
while backpropagating the error on the labeled data [6]. Unfortunately, in many applications the
graph structure is not readily available.

A standard solution is to compute graphs using classical heuristics such as k-nn or (cid:15)-graphs [7], but
those choices poorly adapt to the underlying data manifold and disregard label information, thus
yielding suboptimal results. Dhillon et al. [8] propose a metric learning based framework in which a
graph is constructed via a metric that maximizes the conﬁdence of label assignments. Following a
different route, Alexandrescu et al. [9] use a supervised model on the labeled subset to transform
the data into a new space consisting in soft label predictions where the graph is constructed. These
two approaches still rely on the classic heuristics for graph construction, and are not able to learn
complex data representations. The recent work of Franceschi et al. [10] represents edges as Bernoulli
random variables and uses a bilevel programming framework to ﬁt the parameters of the graph and a
GCN for semi-supervised classiﬁcation.

In this paper, we present a probabilistic model to learn the parameters of a semi-supervised classiﬁca-
tion model and the graph jointly. We model edges as latent variables, and we learn by minimizing a
reconstruction error over the predicted labels. Our choice of a probabilistic framework allow us to
explicitly deﬁne a prior over the graph. This enables the model to account for prior knowledge and
provides a principled mechanism to impose speciﬁc structures (such as sparsity) upon the graph.

2 Model

Let us assume a training set of the form D = DL ∪ DU where DL = {(xi, yi)}l
i=1 is the set of
labeled data points with labels in some discrete set Y, and DU = {xi}l+u
i=l+1 the set of unlabeled
points. Let X = [x1; . . . ; xl+u]T be the design matrix and yL = [y1, . . . , yl] the label vector, Wij a
random variable representing an edge between xi and xj. Our goal is to ﬁnd labels {yl+1, . . . , yl+u}.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

We start by assuming labels y are generated by a random process that depends on the data X and
unknown parameters W that encode how data points are connected. That is, Wij represents an edge
between xi and xj. We can describe this process through the conditional probability pθ(y|X, W )
parameterized by θ that gives the likelihood of labels y provided the dataset X and the graph W . In a
variational Bayesian context parameters W are latent variables with prior distribution pθ(W ) and
approximate posterior qφ(Wij|X, y) parameterized by φ, on which we can make inference.

We therefore aim to ﬁnd the parameters [θ, φ] so as to maximize the likelihood of the labels while
keeping the distribution qφ close to the prior. To do this, we maximize the evidence lower bound
(ELBO) given by:

L(θ, φ) = Eqφ(W |X,y)[log pθ(y|W, X)] − KL[qφ(W |X, y)||pθ(W )]
This model is similar to a variational autoencoder [11] where qφ is the encoder and pθ the decoder.
Maximizing this bound is equivalent to minimizing the reconstruction error of the estimations the
decoder produces while the encoder remains as close to the prior as possible.

(1)

To instantiate our model, we choose pθ(y|W, X) to be a categorical distribution over the labels Y
parametrized by a GCN [6], and qφ(W |X, y) to model edges as a collection of Bernoulli distributions
parameterized by a Graph Neural Network (GNN) [12]. Each Bernoulli distribution represents the
probability of an edge connecting two nodes i and j, hence Wij is a binary variable.

We took inspiration from [13] to use a message passing Graph Neural Network (GNN) [14] that
computes node and edge embeddings as follows:

i = MLP(1)
h(1)
ij = MLP(1)
h(1)

node(xi),
edge([h(1)
i
(cid:18)(cid:20)

i = MLP(2)
h(2)

node

h(1)
i

, h(1)
j

]),

(cid:21)(cid:19)

,

h(1)
ij

(cid:88)

,

j∈N (i)

ij = MLP(2)
h(2)

edge([h(2)

i

, h(2)
j

]),

(2)

(3)

(4)

(5)

and ﬁnally gφ(X) = Softmax(h(2)
ij ). N (i) denotes the neighbors of point i in the GNN (in practice
we simply take them to be the closest points to xi, or even all other points). Finally, we deﬁne
gφ(X) = Softmax(h(2)
ij ). Here [xi, xj] denotes concatenation and MLP is short for multilayer
perceptron.

We also pick the prior distribution pθ(W ) to be a collection of Bernoulli distributions.

Training details. We train our model by backpropagation. We ﬁrst run the encoder qφ, which is
a distribution taking discrete values. This prevent us from being able to directly backpropagate
the error through its reparametrized samples. We then use the concrete distribution [15] to get a
continuous approximation of qφ and apply the reparametrization trick to compute the gradients. More
speciﬁcally, we draw samples W as follows: we draw a vector ξ from a Gumbel(0, 1) distribution
and then we compute Wij = Softmax((h(2)
ij + ξ)/τ ), where τ is a parameter controlling how smooth
the resulting distribution is (the bigger τ is, the more it will resemble a uniform distribution). To
control the variability we take several samples this way, W (1), . . . , W (r) and feed the decoder to
get ˆy(1), . . . , ˆy(r) where ˆyi = GCN (X, W (i)). We then backpropagate the error with respect to yL
through the mean ˆy = 1

r (ˆy(1) + · · · + ˆy(r)).

The reconstruction error that corresponds to the ﬁrst term of Equation 1 is the average cross-entropy
over the labeled examples:

Lreconstruction =

1
l

CrossEnt(cid:0)yL, ˆy(cid:12)

(cid:12)DL

(cid:1).

(6)

The KL-divergence of qφ that corresponds to the second term of Equation 1, given a Bernoulli prior
ρ, is given by:

LKL-divergence =

l+u
(cid:88)

(cid:88)

i=1

j:ρij (cid:54)=0

Wij log

Wij
ρij

+ (1 − Wij) log

1 − Wij
1 − ρij

.

(7)

2

Dataset

Wine
20news3
Cora

Size

178
2756
2708

Table 1: Statistics of datasets.

Dimension Nb. of classes Train/Val/Test

13
229
1433

3
3
7

10/20/158
20/40/2696
140/300/1000

Table 2: Mean and standard deviation of test accuracy over ﬁve random splits.

Wine

20news3

Cora

LogReg
SVM
FFNN

GCN+knn
GCN+Sknn
GCN+RBF

.95 ± .02
.94 ± .03
.93 ± .01

.95 ± .03
.93 ± .03
.94 ± .03

.77 ± .01
.76 ± .00
.77 ± .01

.77 ± .02
.66 ± .05
.76 ± .01

.53 ± .00
.50 ± .00
.55 ± .01

.61 ± .01
.31 ± .00
.51 ± .01

PSSL

.95 ± .01

.83 ± .01

.65 ± .04

3 Experiments and Results

We carried out experiments to compare our probabilistic approach (denoted by PSSL) with supervised
algorithms such as logistic regression (LogReg), support vector machines with a radial kernel (SVM)
and feed-forward neural networks (FFNN),1 and with the state-of-the-art semi-supervised method
based on GCNs, which we fed with different types of heuristically computed graphs. We use three
strategies for building a graph: k nearest neighbors (knn), radial kernel (RBF), and a random variant
of the k-NN graph constructed as follows: denoting by K the regular k-nn graph, an edge eij between
xi and xj is sampled according to a Bernoulli distribution with some high probability α if Kij = 1,
or with probability 1 − α otherwise (Sknn). The prior distribution for our model PSSL is constructed
in the same way as Sknn. We also speciﬁed different sparsity patterns over the prior.

We evaluate the baselines and our method on three datasets: Cora [16], a subset of 20 Newsgroups
with three classes and a TFIDF feature space, and Wine, a benchmark dataset available in scikit-learn
[17].

We used Adam to optimize our objective function. For all methods we tune the main hyperparameters
over ﬁve random splits with train, test, and validation sizes as described in Table 1.

Results are shown in Table 2. We can observe that we achieve competitive results in Wine, and
outperform the baselines by a considerable margin in 20news3 and Cora.

4 Conclusion and Discussion

We presented preliminary work on a framework based on autoencoding variational bayes that
learns the parameters of a semi-supervised model and the underlying graph structure of the data
simultaneously. We empirically showed that our model can achieve considerable gains over different
baselines in different semi-supervised datasets.

We plan to run experiments on other semi-supervised datasets, and to compare this method empirically
with that of Franceschi et al. [10]. We believe our proposal exhibits two advantages. First, we can
explicitly specify a prior over the graph, which allow us to bias towards speciﬁc structures and sparsity
patterns. Second, [10] requires two separate validation sets while we require only one: we therefore
have access to more training data.

An interesting future research line is to extend this work to an inductive setting in order to be able to
elegantly handle unseen test examples.

1This model is equivalent to a GCN with no graph.

3

References

[1] Xiaojin Zhu and Zoubin Ghahramani. Learning from Labeled and Unlabeled Data with Label

Propagation. Technical Report CMU-CALD-02-107, 2002.

[2] Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised Learning Using Gaussian
Fields and Harmonic Functions. In Proceedings of the Twentieth International Conference on
International Conference on Machine Learning, ICML’03, pages 912–919. AAAI Press, 2003.

[3] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf.
Learning with local and global consistency. In Advances in Neural Information Processing
Systems 16, pages 321–328. MIT Press, 2004.

[4] Thorsten Joachims. Transductive Learning via Spectral Graph Partitioning. In Proceedings
of the Twentieth International Conference on International Conference on Machine Learning,
ICML’03, pages 290–297. AAAI Press, 2003.

[5] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold Regularization: A Geometric
Framework for Learning from Labeled and Unlabeled Examples. J. Mach. Learn. Res., 7:2399–
2434, December 2006.

[6] Thomas N. Kipf and Max Welling. Semi-Supervised Classiﬁcation with Graph Convolutional
Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings, 2017.

[7] Amarnag Subramanya and Partha Pratim Talukdar. Graph-Based Semi-Supervised Learning.

Morgan & Claypool Publishers, 2014.

[8] Paramveer Dhillon, Partha Talukdar, and Koby Crammer. Inference Driven Metric Learning

(IDML) for Graph Construction. Technical Reports (CIS), 07 2010.

[9] Andrei Alexandrescu and Katrin Kirchhoff. Data-Driven Graph Construction for Semi-
Supervised Graph-Based Learning in NLP. In Human Language Technologies 2007: The
Conference of the North American Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 204–211, Rochester, New York, April 2007.
Association for Computational Linguistics.

[10] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning Discrete
Structures for Graph Neural Networks. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages
1972–1982, 2019.

[11] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.

[12] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.

The Graph Neural Network Model. Trans. Neur. Netw., 20(1):61–80, January 2009.

[13] Thomas N. Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard S. Zemel.
Neural Relational Inference for Interacting Systems. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July
10-15, 2018, pages 2693–2702, 2018.

[14] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.

Neural Message Passing for Quantum Chemistry. CoRR, abs/1704.01212, 2017.

[15] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A Continuous
In 5th International Conference on Learning
Relaxation of Discrete Random Variables.
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017.

[16] Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina
Eliassi-Rad. Collective Classiﬁcation in Network Data. AI Magazine, 29(3):93–106, 2008.

4

[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

5

