Making Computer Music on the Web with JSPatcher
Michel Buffa, Laurent Pottier, Shihong Ren, Yang Yu

To cite this version:

Michel Buffa, Laurent Pottier, Shihong Ren, Yang Yu. Making Computer Music on the Web with
￿hal-
JSPatcher. SMC 2022 - Sound and Music Computing 2022, Jun 2022, Saint-Etienne, France.
03812972￿

HAL Id: hal-03812972

https://inria.hal.science/hal-03812972

Submitted on 13 Oct 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

Making Computer Music on the Web with JSPatcher

Shihong Ren
Shanghai Conservatory of
Music, SKLMA, China
UniversitÂe Jean Monnet,
ECLLA, France
shihong.ren
@univ-st-etienne.fr

Laurent Pottier
UniversitÂe Jean Monnet,
ECLLA, France
laurent.pottier
@univ-st-etienne.fr

Michel Buffa
UniversitÂe Cˆote d’Azur,
CNRS, INRIA, France
michel.buffa
@univ-cotedazur.fr

Yang Yu
Shanghai Conservatory of
Music, SKLMA, China
yuyang
@shcmusic.edu.cn

ABSTRACT

Web technology through the internet and a web browser, is
attractive to modern computer music artists as it provides
high accessibility and interactivity and opens to possibil-
ities. Since the development of JSPatcher [1], an online
visual programming language (VPL) for audio process-
ing and interactive web-based programs, we have recently
added various music-related features to the application, in-
cluding important features that common music computing
practices need, such as file management, audio buffer han-
dling, audio plugin support, and computer-assisted com-
position functions. It aims to make it easier for musicians,
composers or designers of multimedia projects who might
be familiar with VPLs on native platforms like Max [2],
PureData [3] or OpenMusic [4], to bring or create their
work on the web with less effort. This paper presents
the concept, implementation details and examples of these
newly added features.

1. INTRODUCTION

JSPatcher is a web-based visual programming language
(VPL) originally designed for providing a user interface
(UI) for Web Audio API. 1 Since the API describes the
audio processing flow as a graph of DSP nodes, it is con-
venient to have a patcher editing system [5] to manipulate
the audio graph. JSPatcher is initially a WebAudio patcher
editor that runs in a browser, where users can create boxes
representing the DSP nodes and cables representing the
connections in a canvas.

Since the whole JSPatcher platform is mainly developed
in TypeScript and compiled to JavaScript which is the
scripting language for the Web API, we have the possibility
to fully import the language it self to the patcher system.
Thus, in addition to the audio connection layer, in order
to control DSP parameters with non-audio data, we added
a dataflow layer that can distribute events between func-
tions in real time. In this layer, various functions imported

1 https://www.w3.org/TR/webaudio/

Copyright: © 2022 Shihong Ren et al. This is an open-access article distributed

under the terms of the Creative Commons Attribution 3.0 Unported License, which

permits unrestricted use, distribution, and reproduction in any medium, provided

the original author and source are credited.

from the web environment, including JavaScript language
built-ins and Web APIs, are available. These can be used
to access the browser’s high-level APIs, such as those for
managing mouse and keyboard events, battery life or com-
puter peripherals.

With these two layers in a single imperative patcher, the
usage becomes similar to some VPLs available on native
platforms like Max or PureData, while offering more flexi-
ble computational possibilities, taking advantages from the
web community, as most of the JavaScript packages can be
imported and used as functional boxes in a patcher.

An additional patcher interpretation mode, FAUST com-
piled patcher, has also been developed to facilitate DSP de-
sign under the WebAudio AudioWorklet specification [6].
The FAUST [7] language ecosystem is used to transform
patchers under a specific mode into FAUST code which
can be compiled to a customized DSP. This kind of can
be used in imperative patchers as a special WebAudio node
called AudioWorkletNode that can be connected with other
WebAudio nodes. With this interpreter, some Max’s Gen
patchers can also be interpreted to FAUST code and be
compiled in the same way in JSPatcher.

These new features make JSPatcher not only a utility to
interface graphs from the Web Audio API, but also an in-
tegrated development environment (IDE) for data compu-
tation and audio processing on the web. Various possibili-
ties related to music composition and performance are thus
open for exploration on the platform.

The following sections will present the improvements
made to JSPatcher that contain important features for
modern computer music practices, including a file man-
ager, audio buffer manipulation, audio plugin support and
computer-assisted composition functions.

2. FILE MANAGER

2.1 Concept

One major limitation of the web environment is the access
to the device’s local files due to security reasons. Accord-
ing to the current web standards, web applications do not
have permissions by default to read local files unless users
explicitly allow them. The lack of a proper file manager
API makes any web application difficult to maintain the
structure of a file-based project.

To support sub-patchers and audio file manipulation in

516

Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

517

Figure1.JSPatcherwithafilemanagerJSPatcher,wehadtodesignavirtualfilesysteminthebrowserthatallowsthefollowingoperations:1.Makethefilesystempersistentafterclosingtheap-plication,2.Uploadfilesfromtheuser’smachine,3.Createnewfiles,4.Deletefiles,5.Copyormovefiles,6.Accessfiledatausingitspath.Designingandimplementingapersistentfilestoragesys-temisachallengeinthisworkastherearefewwaystokeepreusabledatainthebrowser.OursolutionreliesonIndexedDB,2atechnologyforclient-sidestorageofsig-nificantamountsofstructureddata.Normally,in2022andonadesktopbrowser,uptotwogigabytesofstoragequotaareavailableintheIndexedDBpersitegroup,3.Itissuffi-cientforanylightweightproject.WealsouseBrowserFS,aJavaScriptmodulethatcanstorepersistentlyawholestructuredfilesystemintotheIndexedDBandprovideaJavaScriptAPIforfilemanagement[8,9].Toacceleratefilereadingandwritinginruntime,wealsobuiltahigher-levelfilesystemthatcachesthefiletreestructureanddatainmemory.ItactsasabridgebetweentheBrowserFSandtheUI,whenachangeismadebytheusertothefilesystem,itcallstheBrowserFStostorenewchangeinIndexedDB;meanwhile,alleventsubscriberstothechangedfile,suchasthefilemanagerUI(Figure1),fileeditorsoraudioplayerswillbealertedandtheycanreactinrealtime.2.2TemporaryFileSystemAlongwiththepersistentfilesysteminIndexedDB,wealsoimplementedanothermemory-onlyfilesystemfortemporaryentries.Itisinternallyalabelandfilecontentmapwitheventemittersandanobservationmechanism.Thepurposeofthetemporaryfilesystemistoallowuserstocreatememoryspacesandusethemwithanassociatednamejustlikeapersistentprojectfile.AsimilarbehaviorexistsinMax.Forexample,whenauserdefinesanaudiobufferusinganamethatdoesn’trefertoafilefromtheharddisk,thesystemwillallocateapartofthememorythatistemporarilyaccessiblewiththisname.Anyobject2https://www.w3.org/TR/IndexedDB/3https://tinyurl.com/muc8cmyxthatusesthesamenamewillthenrefertothesamepartofthememory.Wheneveryobjectassociatedwiththisnameisremoved,thememorywillbefreedanderased.InJSPatcher,thisbehaviorisimplementedbythetempo-raryfilesysteminwhicheveryfileisundertherootdirec-tory.WehaveprovidedanAPIforobjectstocreatetempo-raryfileswhenanamecannotberesolvedasapersistentfile.Meanwhile,theobjectbecomesanobserverofthetemporaryfile.Whenthelastobservergetsremoved,thetemporaryfilewillautomaticallyfreethememoryspaceandremoveitself.2.3FileStructureofaProjectInJSPatcher,aprojectisafolderthatcontainssubfoldersorfilesinanyformatasinnativeoperatingsystems.Whenasessionisstarted,theprojectfolderwillautomaticallybeinitializedwithahiddenfilenamed.jspatprojundertherootfolder.Thisfilecontainsmetadatarelatedtotheprojectsuchasitsname,itsauthor,itsdependenciesandwillbesavedwiththewholeproject.Typically,aprojectwillhaveoneormultiplepatcherswithsomeassetfiles.Todistinguishpatchersunderdif-ferentmodes,weuse.jspatastheextensionforregu-larimperativepatcher,and.dsppatforFAUSTcompiledpatcher.Patcherfilescanbeuploadedtothefilesystemordownloaded(exported)fromit.Userscanalsouploadordownloadthewholeprojectforfurtherexchangeunderthe.zipformatwitheveryfilecompressedinside.2.4UsageinPatchersSometypesofprojectfilescanberecognizedbyJSPatcherandcanbeloadedintopatchers.Typically,theywillbeusedinaJavaScriptpatcher(usingimperativeandWebAu-diolayers)bydifferentboxobjects.Toloadaspecificfile,theusershouldputthefilepathinUnixformat(usingslashforsub-directory)relativetotheprojectrootfolder.Table1showsalistoffiletypesthatcanbeloadedbyboxobjectsinapatcher.Figure2showsasub-patcherfile,anaudiofile,atextfileandanimagefileinapatcher.Figure2.FilesinapatcherProceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

Loader
Description
patcher Sub-patcher
pfaust

gen

that can
compiled to an Au-

Faust sub-patcher that can
compiled to an Au-
be
dioWorklet DSP
Gen sub-patcher
be
dioWorklet DSP
buffer~ Audio files that can be de-
coded to in-memory audio
buffers for further manipula-
tions
Display the image in the
patcher

img

ptext

Edit the text in the patcher

Extension
jspat
dsppat

gendsp

Audio files
(wav, aif,
mp3, etc.)

Image files
(jpg, png,
gif, etc.)
Text
(txt,
json, etc.)

files

Table 1. A list of file types that can be loaded by box ob-
jects in a patcher.

3. AUDIO EDITING

3.1 Concept

Different kinds of web-based audio editors or digital audio
workstations (DAW) have been developed by the commu-
nity in recent years to bring desktop audio production ex-
perience on the web platform. However, for a long period,
due to the inconsistent implementation of the Web Audio
API between different web browsers, it has been hard to
maintain related projects and ensure the same user experi-
ence across browsers and devices.

Support for the AudioWorklet API on Safari Desktop and
iOS browsers was added in April 2021, which as the final
step in providing full Web Audio API coverage on all ma-
jor browsers. Also, the Web Audio API version 1.0 be-
came, after ten years of maturation, a W3C recommenda-
tion (a frozen standard) in June 2021. These two events
mark a milestone of the standard and its implementation
process. It motivated developers to adapt Web Audio ap-
plications to the recommended standard without worrying
about compatibility issues. The AudioWorklet API is im-
portant for audio editors or DAWs as buffer recording,
looping, editing regions require sample-accurate controls
over the input signal and can be done in a customized DSP
through the API.

In JSPatcher, using the recent API, two features related
to audio files were added. Audio files can now be edited in
non-real time from a single-track audio editor and can be
manipulated as an audio buffer in real time in a patcher.

3.2 Audio Editor

In the workflow for some genres of electronic music, espe-
cially musique concr`ete or interactive electroacoustic mu-
sic, raw audio materials need to be cut and ªcleaned upº
before being put into a DAW project or a program. This

518

process requires a tool that can directly modify the audio
file with the following features:

1. Visualize any part of the waveform with rulers,
2. Play any part of the audio clip,
3. Select on the waveform with sample-accuracy,
4. Record audio in-place or after the clip,
5. Cut, copy and paste any part of the clip,
6. Adjust the volume of the selected section,
7. Silent the selected section or insert silence of any

length,

8. Perform a phase inversion or reverse the selected

section,

9. Fade-in and fade-out of any length with a flexible

curve,

10. Resample the audio to any sample rate,
11. Create, delete, reorder channels, up-mix and down-

mix (i.e., mix a stereo audio to a mono one),

12. Apply audio effects,
13. Export the audio in some formats.
Its implementation in JSPatcher is a dedicated window
that can be opened by double-clicking an audio file listed
in the file manager. (Figure 1)

When the user needs to open the editor, the system will
firstly decode the given file to raw PCM data. Then, the
data will be grouped into different levels of zoom that con-
tain minimum and maximum values of each 16, 256, 4096
samples, etc.
to optimize the waveform display for long
audio files.

The layout of the editor’s UI is similar to some desktop
audio editors. A navigation bar with the waveform of the
whole file is shown at the top of the window with a range
of currently displayed and selected sections. Below the
bar, a larger waveform viewer shows a section of the au-
dio. Conventional features were implemented such as the
cursor, the auto-scalable timing grid (vertical), energy (in
dB) grid (horizontal), buttons to enable or disable channels
for replay, fade-in and fade-out handlers, and an additional
popup handler to adjust the gain when a section is selected.
Below the main viewer, a playback toolbar is presented.
Users can play, pause or stop the playback from the cur-
sor’s position. Loop can also be enabled or disabled here.
To record audio, users can choose an input device from a
menu, enable the monitoring, and start recording using the
red button.

At the bottom, a meter shows the current volume of the
playing or the monitoring audio. A table with adjustable
numbers shows the current displayed and selected section,
and allows users to change it manually.

In the menu, a set of options are available to perform sim-
ple processing like silence, reverse, phase inversion, insert
silence, resample, remix channels and export. To perform
more complex processing using third-party DSPs, it is pos-
sible to load WebAudioModule (WAM) plugins [10, 11]
into a plugin rack. Users can choose to process the whole
audio file or only the selected part. We will present the
WAM support in the next sections of the paper.

In the ªRemix channelsº option, users have access in a
popup window to an I/O matrix to decide the number of
outputs and how much volume of signals coming from

Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

original channels for the resulting audio. (Figure 3)

Figure 4. Audio buffer manipulation

4. AUDIO PLUGIN SUPPORT

Figure 3. ªRemix channelsº and ªBounceº options

4.1 WebAudioModule Plugin Standard

The audio export (ªBounceº option) supports .wav, .mp3
and .aac formats with different sample rates or bit-rates
thanks to a web implementation of ffmpeg 4 through Em-
7 (Figure 3)
scripten 5 compilation and WebAssembly. 6

3.3 Audio Buffer Manipulation in a Patcher

Audio buffer manipulation is a key feature for any audio
programming environment like Max or JSPatcher, as real
time audio projects like some interactive music pieces of-
ten need to record and replay audio clips during the perfor-
mance.

In the previous section, we mentioned that the buffer~
object can refer to any audio file in the project or can create
any temporary audio clip in the memory. The audio editor
is also available when the user double-clicks the object. In
addition, the editor can be ªdockedº beside the patcher.

The buffer~ object accepts 4 possible arguments. The
first argument is the identifier of the audio file or a tem-
porary audio clip. If it is temporary, users can initialize
it by specifying the buffer’s number of channels, length in
samples and its sample rate.

In fact, the object is associated with a PatcherAudio API
in which the audio buffer is stored with its waveform data,
with a set of methods related to the buffer editing. A Bang
(a triggering event) from the first inlet of the buffer~ ob-
ject will trigger output of the PatcherAudio API.

Users can connect the buffer~ object to some other ob-
jects to use the PatcherAudio API. For example, the wave-
form~ object can display the waveform of the audio clip,
bufferSource~ object is a player that wraps the Buffer-
SourceNode from the Web Audio API that accepts the
PatcherAudio as the input and can play the audio with loop
and different playback speed.

The PatcherAudio API provides necessary information
getters like its number of channels, length and sample rate;
and manipulation methods like split, concatenate, pick,
paste, remove, insert, etc.

Figure 4 shows an example of picking a section (from
sample 24000 to 25000) of the existing buffer with 48000
samples.

4 https://www.ffmpeg.org/
5 https://emscripten.org/
6 https://webassembly.org/
7 https://github.com/ffmpegwasm/ffmpeg.wasm

In the music production industry, most hardware devices
have been substituted by software solutions over the past
decades. Using DAWs with third-party audio plugins that
act as synthesizers or audio effects is a common workflow
for modern audio projects. VST (Virtual Studio Technol-
ogy), introduced by Steinberg in 1996, is one of the most
used cross-platform native plug-in formats based on C++.
It provides a public SDK and API documents to allow plu-
gin providers and host developers to implement them cor-
rectly.

Due to the limitation of the web platform, native audio
plugins such as VSTs cannot be used in a web applica-
tion without installing additional native software. Thus,
for web-based DAWs, the need arose for a new standard of
WebAudio plugins, offering similar functionality to their
native counterparts.

In 2015, the first version of the Web Audio Modules
(WAM) [10] standard is created, primarily for native plug-
ins developers to port their existing plugins to the web. In
2018, they joined forces with other groups of people work-
ing on interoperable Web Audio plugins and plugin hosts
to synchronize their efforts toward the beginnings of an
open standard called Web Audio Plugins (WAP) [11, 12],
covering a wider range of use cases. Recently, taking into
account previous developments and the feedback received
from developers over the past few years, a new version of
the WAM standard has been created by emerging recent
technological developments.

Now, a WAM plugin can be fetched from the web us-
ing a URI, and be initialized using the current WebAudio
context. The plugin comes with a standardized API that
can create a UI, get or adjust parameters, schedule events
like parameter automation or MIDI messages, and connect
with other WAMs or native WebAudio nodes.

4.2 WAMs in the Audio Editor

The Audio Editor in JSPatcher has a dedicated effect
ªrackº for WAMs. When the audio is loaded, the ªrackº
is empty so that users can add WAMs from their URIs.
The audio will be played and processed through a pre-gain,
then through the ordered WAM effects, finally through a
post-gain. Users can use the rack UI to add or delete
WAMs or display the WAM’s own UI. (Figure 5)

Web Audio’s OfflineAudioContext is an audio context

519

Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

that, instead of process audio in real time, generates au-
dio data from a WebAudio graph as fast as it can. When a
user needs to export the entire audio file rendered with ef-
fects, or to apply the effects in-place, we will use a copy of
the current ªrackº in a new OfflineAudioContext to render
the audio.

Figure 5. WAM plugins in the audio editor

4.3 WAMs in Patchers

It is also important to be able to create interactive audio
programs with WAMs in JSPatcher. Using the plugin~
object, users can load WAMs via a URI into a patcher.

When the plugin~ receives a text string (as URI), it will
remotely download the code from the URI and initialize
it as a WAM. Then, it will automatically wrap the WAM
and load its UI. Its inlets and outlets will be connectable
with other WebAudio node objects. Additional inlets will
be created to accept real time parameter change messages.
As WAMs support MIDI messages as input, the first inlet
of the object accepts numbered arrays as MIDI event mes-
sages to the WAM inside. In addition, as WAMs have the
ability to transmit non-audio events between each other,
the patcher connection between two WAMs will be treated
as a special one and make the necessary connection.

Figure 6 is an example of a WAM loaded in a patcher

with an audio player.

Figure 6. WAM with an audio player in a patcher

520

5. COMPUTER-AIDED COMPOSITION

5.1 Context

Algorithmic composition often needs computer programs
to calculate its musical representation. To design such a
computer-aided composition (CAC) system for composers,
visual programming and musical score display are two key
features. OpenMusic, developed at IRCAM, shows the
power of such a VPL in algorithmic composition with its
composer-oriented design. The web environment, with its
strong accessibility from device to device and flexibility
from the UI perspective, is already a common platform for
the low-code development system. It is likely to be highly
suitable for a CAC system.

High-quality digital musical scores can be dynami-
cally rendered and displayed on a computer using the
SVG (Scalable Vector Graphics) format. Libraries like
VexFlow, 8 Guido, 9 Verovio 10 or abcjs 11 can render mu-
sic scores on the web. However, not all of them provide an
API to interactively deal with the musical structure (model)
behind the score.

The challenge we are facing is a need for a JavaScript-

compatible musical model system that is able to:

1. Calculate and compose abstract music (like a MIDI

file),

2. Play via WAM synthesizers,
3. Be displayed as a musical score.

Therefore, we created a JavaScript library called Sol for
the calculation of different musical concepts. 12 It aims to
deal with the musical model issue in the web-based CAC
system. Then, a WebAssembly version of Guido 13 is used
to render the score as it provides the AR (Abstract Repre-
sentation) API to easily convert the musical model to the
score. Finally, these feature are integrated as a package 14
into JSPatcher.

5.2 Musical Model

Musical notation is basically a representation of the mu-
sical structure based on a set of different musical concepts
and a composition of instances of these concepts. To facili-
tate the CAC, the modeling of these concepts, or generally
music theory, should be implemented in a digital way to
allow the calculation between the musical concepts.

Yet, modern music theory is so complex and diverse that
its modeling cannot be all-inclusive. The Sol library is only
a proof of concept with a covering of very basic but neces-
sary concepts in a common CAC workflow.

5.2.1 Pitch and Note

The difference between a pitch and a note can be ambigu-
ous. In our library, ªnoteº means different ªpitch classesº
in an octave. A note can be A, B, C, D, E, F and G with
any number of sharp or flat accidentals.

8 https://www.vexflow.com/
9 https://guido.grame.fr/
10 https://www.verovio.org/
11 https://www.abcjs.net/
12 Open-sourced on https://github.com/fr0stbyter/sol
13 https://github.com/grame-cncm/guidolib
14 https://github.com/jspatcher/package-cac

Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

Text strings can be used to construct a Note object. For
example, ªC###º ªDbº or ªBxº are recognizable as ªnote
C with 3 sharpsº ªnote D with 1 flatº and ªnote B with dou-
ble sharps.º Internally, the note name and the accidentals
will be preserved and can be used for further calculation.
Integer numbers can also be used to construct a note as the
offset in semitones from note C, in this case, the note name
and the accidentals will be determined automatically.

Pitch is an extended Note object with additional octave
information. It is then a more concrete concept as the fre-
quency can be calculated. It is also possible to get a pitch
from a frequency with an approximation of a semitone.
The pitch’s offset follows the C4 = 60 standard.

Both Note and Pitch object supports some kinds of math-
ematical calculations. Adding or subtracting a number
from a Pitch will change by semitones its offset. The offset
difference can also be calculated between two Pitches For
example, the following code creates a C4 pitch, by adding
12 to get a C5 pitch, then by subtracting a C4 pitch to get
12.

const pitch = new Pitch("C4");
pitch.add(12).toString(); // => "C5"
// Now pitch is C5
pitch.sub(new Pitch("C4")); // => 12

The multiplication between a pitch and a number is sup-
ported to calculate an approximated pitch based on the cur-
rent pitch as the fundamental frequency and a multiplica-
tion ratio. The division is also possible between the pitch
and a number or another pitch to calculate a new pitch or
the frequency ratio between them. For example, the fol-
lowing code creates a C3 pitch, by multiplying 4 (to its
frequency) to get a 2 octaves higher pitch C5, then by mul-
tiplying 1.5 to get a perfect fifth higher.

const pitch = new Pitch("C3");
pitch.mul(4).toString(); // => "C5"
// Now pitch is C5
pitch.mul(1.5).toString(); // => "G5"

5.2.2 Interval

Interval is the distance between two pitches, but it is more
complex than just the number of semitones. With different
note names and accidentals, the same distance, in equal
temperament, can have different intervals such as dimin-
ished fourth and major third. Its calculation involves three
properties: quality, degree and octave.

In Sol, an interval can be created with a text string, a fre-
quency ratio, or from these properties. ªA4º ªd5-1º ªP5º
ªM9º ªm10+1º are usable strings for ªaugmented fourthº
ªdiminished fifth with one octave lowerº ªperfect fifthº
ªmajor ninth (internally will be transformed to a major sec-
ond with one octave higher)º ªmajor tenth with one octave
higher (internally a major third with two octaves higher).º
Here, the distance between two pitches can be ªnegative.º
In one octave, the interval between note F and note B is
augmented fourth, and the interval between note B and
note F will be ªdiminished fifth with one octave lower.º
The operation can be executed using the following code:

const b = new Note("B");
const f = new Note("F");
const i1 = f.getInterval(b);
i1.toString(); // => "A4"
const i2 = b.getInterval(f);
i2.toString(); // => "d5-1"

i1 and i2 from the code above are two intervals. They
can be used for addition and subtraction from notes and
pitches.

5.2.3 Chord

A chord is basically a set of pitches aligned vertically and
executed at the same time. In our library, the Chord inter-
face includes a base note or pitch and an array of intervals.
It has methods such as inverse up and down, move up and
down, etc. This model is chosen to easily move the chords,
and also to solve the following question more easily:

“How to ﬁnd the missing fundamental frequency of a

given chord, if any?”

CAC is often used for spectral music composition al-
gorithms, which involves the calculation of harmonics or
the fundamental frequency. The missing fundamental of
a sound [13] is an interesting topic in the scope.
It has
been demonstrated that the frequency of the pitch heard
in response to a set of two or more successive harmon-
ics corresponds to the greatest common divisor (GCD) of
the harmonic set, even when there is no spectral energy
at that frequency [14]. From a signal perspective, the au-
tocorrelation algorithm can be used to find the periodicity
of a sound and to determine the missing fundamental fre-
quency. As we already have the frequencies from every
pitch of the given chord, we can simply calculate the GCD
of the frequencies.

However, the missing fundamental frequency should be
an approximated value, as we are under the equal tempera-
ment, and human ears has a limited frequency discrimina-
tion capacity [15].

Therefore, we calculate a commonly approximated ratio
between pitches from multiple ratios given by the inter-
vals, we choose 1
3 of a semitone, which is nearly a 2%
difference, as a threshold of the frequency discrimination
and the temperament compensation.

As a result, the algorithm is able to recognize that a dom-
inant seventh chord has a ratio of 4 : 5 : 6 : 7 and a missing
fundamental on the two octaves lower dominant degree.

const chord = new Chord(

new Pitch("C4"), new Pitch("E4"),
new Pitch("G4"), new Pitch("Bb4")

);
chord.ratio; // => [4,5,6,7]
chord.phantomBase.toString(); // => "C2"

The code above calculates a dominant seventh chord on
C4, it returns that its missing fundamental (phantomBase)
is C2.

5.2.4 Duration

A musical value (duration) in fractions of beats can be ex-
pressed using the Duration object. Its constructor accepts a

521

Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

522

fraction(numeratoranddenominator)oratextstringsuchasª4nºª8ndºorª16ntºforªaquarterºªadotted8thºandªatriplet16th.ºMathematicaloperationslikecompare,add,subtract,multiplyanddivisionarealsoavailable.Forexample,wecreateinthefollowingcodethelengthofaquarternote(onebeat),thenmultiplyitby1.5togetadot-tedquarter,divideitby9togetatriplet16th,finallyaddaquartertoittogetalengthof724beat.constdur=newDuration(1,4);//1beatdur.mul(1.5);//3/8beatdur.div(9);//1/24beatdur.add(newDuration(1,4));//7/24beat5.2.5SequenceCombiningthepresentedmodels,asequencecanbeformedwithanarrayofchordsornullvaluesforarest,andtheirduration.Thesequenceinterfacecontainsminimumdatatorenderascoresegment.ItcanalsobecompiledtoaMIDIfilewithatimesignatureandBPM(beatsperminute)information.5.2.6OthersOthermusicalconceptssuchasVelocity,Scale,Tonality,Track,Instrument,Genre,etc.areaddedtothelibraryaswell.However,theseconceptsaremorecomplexandstillneedtobeimprovedinfutureworks.5.3ScoreRenderingTheGuidoengine[16]isoriginallyaC++libraryaimedtocompileplaintextasGMN(GuidoMusicNotation)andtoanSVGformatfileforthescore.Butitalsohastwointermediatesteps,AR(AbstractRepresentation)andGR(GraphicalRepresentation)thatallowrenderingascorefromitsAPI[17].ThroughanEmscriptentoolchain,theenginehasbeencompiledtoWebAssemblywithmostofitsAPIandre-leasedasaJavaScriptlibraryatNPM(NodePackagesManager)online.TousetheSollibrarywiththeGuidoAPI,weaddedamethodtoGuidoARtotheNote,Pitch,ChordandSequencetoperformfunctioncalls,constructthemusicalstructureintheGuidoAR.InJSPatcher,theGuidoenginerunsinaseparatethreadusingtheWebWorkerAPI.Aguido.viewobjectisavail-abletoacceptaGuidoARinstanceandshowsdirectlythecompiledSVGscoreintheobjectUI.5.4OtherCAC-relatedObjectsManipulationsofarrays(lists)ofpitches,durations,veloc-itiesarecommontasksinaCACwork.Likethenumerousarray-relatedfunctionsinOpenMusic,wealsoneedtopro-videtheminJSPatcher.ArrayversionsofJavaScriptoperatorsareavailableasobjectswiththeprefixª[]ºlike[]+,[]*,[]&&,whicharearrayversionsofªaddºªmultiplyºandªlogicaland.ºTheyacceptanarrayasthefirstinput.Whenthesecondinputisanarray,valuesintwoarrayswillbeappliedtothefunc-tionrespectivelyaccordingtotheirindex;otherwise,everyvalueinthefirstarraywillbeappliedtothefunctionswiththesecondvalue.WealsoaddedsomefunctionsexistinginOpenMusiclikex2dx(calculatethedifferencebetweenvaluesinanar-ray),dx2x(integrateanarray),permute,combinations,arithmSer(createanarithmeticseries),fiboSer(createaFibonacciseries),etc.6.EXAMPLEThefollowingpatcher(Figure7)showsaCACprojectcombiningthepresentedfeatures.Figure7.AnexampleofCACwithaMIDIplayerThefirstpartofthepatcherstartswithabuttonthatwillcreateaBangonclick.TheBanggeneratesanarithmeticseriesbetween60and71,thenrandomlypermutesthear-raytogettheprimeformofadodecaphonicseries.Itsinversionisthencalculatedusingthefirstvalueandtheintervalsbetweeneachvalue.Thereversionsofthetwoformsarecalculatedusingacopyofthearrays.UsingtheSequenceandtheSequencesfunctions,wenowhaveafour-voicemusicsegment.Thescoreisdisplayedus-ingthecac2guidoobject,anditsMIDIfileisgeneratedandpassedtoasequencer.Belowthesequencer,aWAMsynthesizerisloadedandwaitingforrealtimeMIDImes-sages.Itproducesaudiodatathatisconnectedtothephys-icaloutput.Withthetoggle,userscanplayorstopthesequence.7.CONCLUSIONSThescopeofsoundandmusiccomputingbecomeswidertodayduetovariouspossibilitiestocreatemusicrapidlyoreveninrealtime.ThedevelopmentofmachinelearningandAImakesautomaticcompositionpossibleandcontin-uouslyinspiresmusicianstocooperatewiththecomputerinthecompositionprocess.TheaccessibilityofferedbymodernwebtechnologywithitshighlyactiveecosystemisanopportunityforustobringmusicprogrammingandProceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)

CAC systems to a new stage. For instance, an artwork cre-
ated on JSPatcher can be easily shared with people by de-
signing a presentation mode of the patcher in which only
selected boxes is showing with specific position and size,
and specifying in the URI the location of the project file
and the patcher filename, with additional options such as
hiding the editor UI or make the patcher read-only. 15
In
addition, UI components in patchers can also be interacted
using touch devices which could make the design work-
flow simpler than existing VPLs on native platforms.

Today, audio processing in the web do have some limi-
tations, especially the performance cost and the reliability
compared to native C/C++ programs due to the implemen-
tation of different browsers, i.e.
lack of optimization for
multi-core CPUs. Even though, the web platform is still
attractive to both developers and users. It has minimized
the barrier for the deployment and use of any computer
program from any device.

The described additions and improvements on JSPatcher
are just the first steps towards a complete online IDE for
sound and music computing. Feedbacks given by musi-
cians from the community shows high interest in the future
possibilities of JSPatcher such as supports for INScore, 16
collaborative editing and messaging via the network. As
the project is open-sourced 17 with the examples and SDK
(Software development kit) provided, contributions from
third-party are feasible and welcome.

8. REFERENCES

[1] S. Ren, L. Pottier, and M. Buffa, ªBuild webaudio and
javascript web applications using jspatcher: A web-
based visual programming editor,º in Proceedings of
the International Web Audio Conference, ser. WAC
’21, L. Joglar-Ongay, X. Serra, F. Font, P. Tovstogan,
A. Stolfi, A. A. Correya, A. Ramires, D. Bogdanov,
A. Faraldo, and X. Favory, Eds.
Barcelona, Spain:
UPF, July 2021.

[2] M. Puckette and D. e. a. Zicarelli, ªMax/msp,º Cycling,

1990.

[3] M. Puckette, ªPure Data,º in Proceedings of the Inter-
national Computer Music Conference. Thessaloniki,
Hellas: The International Computer Music Associa-
tion, Sep. 1997, pp. 224±227.

cisco, United States: Computer Music Association,
1986, pp. 420±429.

[6] H. Choi, ªAudioworklet: the future of web audio.º in
Proceedings of the International Computer Music Con-
ference, Daegu, South Korea, Aug. 2018, pp. 110±116.

[7] Y. Orlarey, D. Fober, and S. Letz, ªFAUST : an Effi-
cient Functional Approach to DSP Programming,º in
New Computational Paradigms for Computer Music,
E. D. France, Ed. Paris, France: Delatour, Jan. 2009,
pp. 65±96.

[8] J. Vilk and E. D. Berger, ªDoppio: Breaking the
Browser Language Barrier,º in Proceedings of the 35th
ACM SIGPLAN Conference on Programming Lan-
guage Design and Implementation, 2014, pp. 508±518.

[9] B. Powers, J. Vilk, and E. D. Berger, ªBrowsix: Bridg-
ing the Gap Between Unix and the Browser,º in Pro-
ceedings of the Twenty-Second International Confer-
ence on Architectural Support for Programming Lan-
guages and Operating Systems, 2017, pp. 253±266.

[10] J. Kleimola and O. Larkin, ªWeb audio modules,º in
Proceedings of the Sound and Music Computing Con-
ference, Jul. 2015.

[11] M. Buffa, J. Lebrun, J. Kleimola, O. Larkin, and
S. Letz, ªTowards an open web audio plugin standard,º
in Companion Proceedings of the The Web Conference
2018, Lyon, France, Apr. 2018, pp. 759±766.

[12] M. Buffa, J. Lebrun, S. Ren, S. Letz, Y. Orlarey, R. Mi-
chon, and D. Fober, ªEmerging w3c apis opened up
commercial opportunities for computer music applica-
tions,º in The Web Conference 2020 DevTrack, Taipei,
Apr. 2020.

[13] A. Seebeck, ªBeobachtungen Èuber einige bedingungen
der entstehung von tÈonen,º Annalen der Physik, 2nd
ser., vol. 53, pp. 417±436, 1841.

[14] D. A. Schwartz and D. Purves, ªPitch is determined
by naturally occurring periodic sounds,º Hearing Re-
search, vol. 194, no. 1, pp. 31±46, 2004.

[15] B. C. J. Moore, ªFrequency difference limens for short-
duration tones,º The Journal of the Acoustical Society
of America, vol. 54, no. 3, pp. 610±619, 1973.

[4] J. Bresson, C. Agon, and G. Assayag, ªOpenmusic: Vi-
sual programming environment for music composition,
analysis and research,º in Proceedings of the 19th ACM
International Conference on Multimedia, ser. MM ’11.
New York, NY, USA: Association for Computing Ma-
chinery, 2011, pp. 743±746.

[16] H. H. Hoos, K. Hamel, K. Renz, and J. Kilian, ªThe
GUIDO notation format: A novel approach for ade-
quately representing score-level music,º in Proceed-
ings of the 1998 International Computer Music Con-
ference, ICMC 1998, Ann Arbor, Michigan, USA, Oc-
tober 1-6, 1998. Michigan Publishing, 1998.

[5] M. Puckette, ªThe patcher,º in Proceedings of the In-
San Fran-

ternational Computer Music Conference.

15 The URI

share could be like https://fr0stbyter.
github.io/jspatcher/dist/?projectZip=../../soundcraft/
Soundcraft6.zip&file=020.jspat&runtime=1

for

16 https://inscore.grame.fr/
17 https://github.com/Fr0stbyteR/jspatcher

[17] D. Fober, Y. Orlarey, and S. Letz, ªScores level com-
position based on the GUIDO music notation,º in Non-
Cochlear Sound: Proceedings of the 38th International
Computer Music Conference, ICMC 2012, Ljubljana,
Slovenia, September 9-14, 2012. Michigan Publish-
ing, 2012.

523

