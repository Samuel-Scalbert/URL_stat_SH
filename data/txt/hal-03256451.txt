Relative Positional Encoding for Transformers with
Linear Complexity
Antoine Liutkus, Ondřej Cífka, Shih-Lun Wu, Umut Şimşekli, Yi-Hsuan Yang,

Gael Richard

To cite this version:

Antoine Liutkus, Ondřej Cífka, Shih-Lun Wu, Umut Şimşekli, Yi-Hsuan Yang, et al.. Relative Posi-
tional Encoding for Transformers with Linear Complexity. ICML 2021 - 38th International Conference
on Machine Learning, Jul 2021, Virtual Only, United States. pp.7067-7079. ￿hal-03256451￿

HAL Id: hal-03256451

https://telecom-paris.hal.science/hal-03256451

Submitted on 10 Jun 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Relative Positional Encoding for Transformers with Linear Complexity

Antoine Liutkus * 1 Ondˇrej C´ıfka * 2 Shih-Lun Wu 3 4 5 Umut S¸ ims¸ekli 6 Yi-Hsuan Yang 3 5 Ga¨el Richard 2

Abstract

Recent advances in Transformer models allow
for unprecedented sequence lengths, due to linear
space and time complexity. In the meantime, rela-
tive positional encoding (RPE) was proposed as
beneﬁcial for classical Transformers and consists
in exploiting lags instead of absolute positions
for inference. Still, RPE is not available for the
recent linear-variants of the Transformer, because
it requires the explicit computation of the atten-
tion matrix, which is precisely what is avoided by
such methods. In this paper, we bridge this gap
and present Stochastic Positional Encoding as a
way to generate PE that can be used as a replace-
ment to the classical additive (sinusoidal) PE and
provably behaves like RPE. The main theoretical
contribution is to make a connection between posi-
tional encoding and cross-covariance structures of
correlated Gaussian processes. We illustrate the
performance of our approach on the Long-Range
Arena benchmark and on music generation.

1. Introduction

1.1. Linear Complexity Transformers

The Transformer model (Vaswani et al., 2017) is a new
kind of neural network that quickly became state-of-the-
art in many application domains, including the processing
of natural language (He et al., 2020), images (Dosovitskiy
et al., 2020), audio (Huang et al., 2018; Pham et al., 2020)
or bioinformatics (AlQuraishi, 2019) to mention just a few.

The core, novel component of the Transformer is the at-
tention layer. It computes M output values ym from N
input values vn, all being vectors of an arbitrary dimen-

*Equal contribution 1Inria, Zenith Team, UMR LIRMM, Univ.
Montpellier, France 2LTCI, T´el´ecom Paris, Institut Polytechnique
de Paris, France 3Research Center for IT Innovation, Academia
Sinica, Taiwan 4National Taiwan University, Taiwan 5Taiwan AI
Labs, Taiwan 6INRIA – D´epartement d’Informatique de l’ ´Ecole
Normale Sup´erieure – PSL Research University, Paris, France.
Correspondence to: Liutkus Antoine <antoine.liutkus@inria.fr>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

Figure 1. Examples of attention patterns observed in the Perform-
ers trained for pop piano music generation (section 3.2) at infer-
ence time, for sequence length M = N = 3 072 while training
sequences have length 2 048.
(left) Absolute PE. (middle) Si-
nusoidal SPE. (right) Convolutional SPE. Note that SPE never
requires computing these full attention patterns.

sion. Following classical non-parametric regression princi-
ples (Nadaraya, 1964; Watson, 1964), it consists in a simple
weighted sum:

ym =

(cid:80)

n amnvn
(cid:80)
n amn

,

(1)

where each attention coefﬁcient amn ∈ R+ – gathered in
the M × N matrix A – indicates how important the value
vn is in the computation of the output ym.

One of the main contributions of the Transformer is an orig-
inal method to compute these coefﬁcients. D-dimensional
feature vectors kn and qm are attached to all items of the
input and output sequences and are called keys and queries,
respectively. Gathering them in the N × D and M × D
matrices K and Q, we get softmax dot-product attention as:

(cid:16)

A = exp

QK(cid:62)(cid:46)√

D

(cid:17)

≡ [amn = K(qm, kn)]mn ,

(2)

where the function exp is applied element-wise. The right-
hand side in (2) is a generalization introduced by Tsai et al.
(2019) and Choromanski et al. (2020), where K is a kernel
function. Parameters pertain to how keys kn, values vn and
queries qm are obtained from the raw sequences, usually by
time-distributed fully connected layers.

The original Transformer architecture (Vaswani et al., 2017)
explicitly computes the attention matrix A, leading to a
O(M N ) complexity that prevents it from scaling to very
long sequence lengths. Although this is not necessarily a
problem when sequence lengths are barely on the order of
a few hundreds, as in some language processing tasks, it is

Relative Positional Encoding for Transformers with Linear Complexity

prohibitive for very large signals like high-resolution images
or audio.

Focusing on this scalability issue, several approaches have
been recently investigated to allow for long sequences:
• Attention clustering schemes group items among which
dependencies are computed through regular attention.
This is either done by using simple proximity rules within
the sequences, leading to chunking strategies (Dai et al.,
2019), or by clustering the keys and values (Roy et al.,
2020). Inter-cluster dependencies are either ignored or
summarized via ﬁxed-length context vectors that are
coined in as memory (Wu et al., 2020).

• Assuming the attention matrix to be sparse. In this case,

only a few amn are nonzero (Child et al., 2019).

• Assuming A has a particular (low-rank) structure and can
be decomposed as the product of two smaller matrices.
A prototypical example is the Linformer (Wang et al.,
2020b), which is limited to ﬁxed-length inputs. Another
very recent line of research in this same vein takes:

A ≈ φ(Q)φ(K)(cid:62),
(3)
where φ : RD → RR is a non-linear feature map applied
to each key kn and query qm, and R (cid:28) min(M, N )
(Shen et al., 2020; Katharopoulos et al., 2020).

• When K in (2) is a positive (semi)deﬁnite kernel, the
Performer (Choromanski et al., 2020) leverages reproduc-
ing kernel Hilbert spaces to show that a random φ may
be used to exploit this convenient decomposition (3) on
average, even when A is not low rank:

K (cid:23) 0 ⇔ A = Eφ

(cid:104)

φ(Q)φ(K)(cid:62)(cid:105)

,

(4)

where φ is drawn from a distribution that depends on K.
A simple example is φW(kn) = max(0, Wkn), with a
random W ∈ RR×D for some R ∈ N.

Whenever an efﬁcient scheme like (3) or (4) is used, the
outputs can be obtained without computing the attention
coefﬁcients amn, as in (10).1

1.2. Positional Encoding

In Transformer networks, the outputs ym are computed as
linear combinations of all input values vn, weighted by
attention coefﬁcients amn. In sequence modeling, it is rea-
sonable to assume that the actual positions m and n should
play a role in the computation, in addition to the content at
these locations; otherwise, any permutation of the sequence
would lead to the same output. Two core approaches were
undertaken to incorporate position information:
• The original Transformer (Vaswani et al., 2017) adds this

1A somewhat related strategy is used by the recent LambdaNet-
works (Bello, 2020), which encapsulate the key-value information
as a so-called lambda function to be applied query-wise, hence
also avoiding the computation of a full attention matrix.

information to the inputs of the network, i.e. before the
ﬁrst attention layer. This can be equivalently understood
as augmenting the keys, values and queries:

kn ← kn + kn, vn ← vn + vn, qm ← qm + qm, (5)

where we write kn ∈ RD for the keys positional encoding
(PE; Sukhbaatar et al., 2015) at position n ∈ N and anal-
ogously for values and queries. Vaswani et al. propose a
deterministic scheme based on trigonometric functions,
which is shown to work as well as trainable embeddings.
• As an example of positional encoding in the attention
domain, a relative positional encoding (RPE) was pro-
posed by Shaw et al. (2018), building on the idea that
time lags m − n are more important than absolute po-
sitional encoding (APE) for prediction. It is written as:
(cid:16)(cid:0)QK(cid:62) + Ω(cid:1)(cid:46)√

A = exp

, with:

(6)

D

(cid:17)

(cid:34)

Ω ≡

ωmn =

D
(cid:88)

d=1

(cid:35)

qmdPd(m − n)

.

(7)

mn

The terms Pd now act as D different encodings for time
lags selected based on the queries. This change is advo-
cated as bringing important performance gains in many
application areas and has enjoyed a widespread use ever
since.

Although writing down the positional encoding in the at-
tention domain is beneﬁcial for performance (Shaw et al.,
2018; Dai et al., 2019; Tsai et al., 2019), we are only aware
of implementations that either require the computation of
A, or clustered attention schemes, which in ﬁne decompose
A into smaller attention matrices, and compute them. This
is in sharp contrast to (3) and (4), which never compute the
attention matrix.

Our contributions can be summarized as follows:
• We propose Stochastic Positional Encoding (SPE) as a
general PE scheme in the keys domain, that enforces a par-
ticular attention pattern devised in the attention domain.
This enables RPE without explicit computation of atten-
tion. To our knowledge, it is the ﬁrst RPE strategy that is
compatible with O(N ) Transformers like Choromanski
et al. (2020) and Katharopoulos et al. (2020).

• We study the impact of SPE on performance on the Long-
Range Arena benchmark (Tay et al., 2021) and two music
generation tasks. Since RPE was so far limited to short
sequences, we believe this is the ﬁrst study of its advan-
tages on long-range predictions. Our results demonstrate
better validation losses and extrapolation ability.

• We provide additional resources on our companion web-
site,2 including Python implementations of SPE for Py-
Torch and JAX/Flax.

2https://cifkao.github.io/spe/

Relative Positional Encoding for Transformers with Linear Complexity

Algorithm 1 Stochastic Positional Encoding.
Input
• position kernel P(m, n), number of replicas R.
• initial M × D and N × D queries Q and keys K.
Positional encoding:
• Draw the D independent couples {Qd, Kd}d of M × R

and N × R matrices as in section 2.1

• Set (cid:98)Q and (cid:98)K as in (16) and (17)
Inference compute outputs Y with the O(N ) Transformer:

Y ← diag(d)−1

(cid:20)

φ

(cid:16)

(cid:98)Q

(cid:17) (cid:20)
φ

(cid:16)

(cid:98)K

(cid:17)(cid:62)

V

(cid:21)(cid:21)

(10)

(cid:20)
with d = φ( (cid:98)Q)

φ

(cid:16)

(cid:98)K

(cid:17)(cid:62)

1N

(cid:21)

and φ discussed in (3)/(4).

2. Stochastic Positional Encoding

Index set and notation. We assume that the input/output
sequences are indexed by n, m ∈ T, where T is the index
set. For regularly sampled sequences, we have T = N, but
more settings are possible, like irregularly sampled time
series (T = R) or images (T = N2). In any case, the partic-
ular lists of input / output locations under consideration are
written: N and M, with respective sizes N and M (the case
N = M is called self-attention). The corresponding keys
and values are hence indexed as {kn}n∈N and {vn}n∈N ,
while queries are {qm}m∈M. For convenience, we write
amn for the entries of the M × N attention matrix A.
We use bold uppercase for matrices, bold lowercase for vec-
tors and a NumPy-like notation: if Xk is a I × J matrix, xk,i
and xk,:,j stand for its ith row and jth column, respectively.

Assumptions. In the remainder of this paper, we will seek
an attention matrix A given by:

(cid:32)(cid:34) D
(cid:88)

A = exp

d=1

qmdPd(m, n)knd

(cid:35)

(cid:44)√

(cid:33)
,

D

(8)

mn

d=1 are position kernels. Deﬁning Pd ≡

where {Pd}D
[Pd(m, n)]mn, this can be written in matrix form as:
(cid:33)
,

(cid:1)Pd diag(k:,d)

(cid:32) D
(cid:88)

A = exp

diag(cid:0)q:,d

(cid:46)√

(9)

D

d=1

which is understood as having D positional attention tem-
plates Pd jointly activated by the queries q:,d and keys k:,d.
Original RPE (7) can be seen as a special case, where some
entries are kept constant.

Positional attention as covariance. The key idea for SPE
is to see the attention kernel Pd(m, n) as a covariance:
(∀M, N ) (∀m, n) Pd(m, n) = E (cid:2)Qd(m)K d(n)(cid:3) , (11)

where Qd(m) and K d(n) are two real and zero-mean ran-

dom variables, which will be chosen with the single condi-
tion that their covariance function matches Pd. Semantically,
they should be understood as (randomly) encoding position
m for queries and position n for keys, respectively. When
multiplied together as in dot-product attention, they yield
the desired attention template Pd(m, n) on average. The
central intuition is that the actual positional encodings do
not matter as much as their dot-product.

In what follows, we will impose speciﬁc structures on
the cross-covariance Pd(m, n), which will in turn allow
us to design random processes Qd = {Qd(m)}m∈M and
K d = {K d(n)}n∈N such that (11) holds. The core advan-
tage of this construction is to allow for Pd to be factorized.
Let us for now assume that we construct the distributions of
{Qd(m), K d(n)}d in such a way that we can sample from
them (we will see how in section 2.1) and consider R inde-
pendent realizations of them for given M and N , gathered
in the M × R and N × R matrices Qd and Kd:

Qd ≡ [qd,m,r ∼ Qd(m)]mr, Kd ≡ [kd,n,r ∼ K d(n)]nr.
(12)

For large R, by the law of large numbers, we obtain:

Pd ≈

(cid:104)
(cid:62)
QdK
d

(cid:105)

/R.

(13)

This leads A in (9) to be given by:

(cid:32) D
(cid:88)

A ≈ exp

diag(cid:0)q:,d

(cid:62)
d

(cid:1) QdK
R

√

diag(k:,d)/

(cid:33)

D

(14)

d=1
(cid:18) D
(cid:80)
d=1

≈ exp

diag(cid:0)q:,d

(cid:1)Qd

(cid:19)(cid:18) D
(cid:80)
d=1
D

√

R

(cid:19)(cid:62)

diag(k:,d)Kd

.

(15)

Here, a crucial observation is that for large R, the cross-
(cid:62)
terms QdK
d(cid:48)(cid:54)=d are negligible due to independence, pro-
vided that the means of the processes are selected to be zero.
Finally, picking queries and keys as:

(cid:98)Q ←

(cid:98)K ←

D
(cid:88)

d=1

D
(cid:88)

d=1

diag(cid:0)q:,d

√
(cid:1)Qd/ 4

DR ,

√
diag(k:,d)Kd/ 4

DR ,

(16)

(17)

we see from (15-17) that we get back to the usual multi-
plicative scheme (2) with A = exp( (cid:98)Q (cid:98)K
R), where the
queries/keys now have dimension R and can be used in (10)
to directly get outputs without computing A.

√

/

(cid:62)

The procedure is summarized in Algorithm 1: we provide a
way (16-17) to achieve PE in the keys domain, such that the
desired model (8) is enforced in the attention domain, pa-

Relative Positional Encoding for Transformers with Linear Complexity

rameterized by the attention kernels Pd. Interestingly, this is
done without ever computing attention matrices, complying
with O(N ) Transformers. The remaining challenge, which
we discuss next, is to generate Qd and Kd enforcing (13).

2K, with entries (0-based indexing):

[Ω (I, a, b)]nl =

(cid:40)

cos(2πakn + bk)
sin(2πakn + bk)

if l = 2k
if l = 2k + 1

2.1. Drawing Stochastic Positional Encodings

Inspecting (11), we notice that our objective is to draw
samples from D pairs of centered random processes
(cid:8)Qd, K d
(cid:9)
d, with a prescribed cross-covariance structure
Pd.
It is reasonable to use Gaussian processes for this
purpose (Williams & Rasmussen, 2006), which have the
maximum entropy for known mean and covariance. Such
distributions are frequently encountered in geophysics in the
co-kriging literature (Matheron, 1963; Genton & Kleiber,
2015), where scientists routinely handle correlated random
ﬁelds. The particular twists of our setup are: we have a gen-
erative problem, e.g. as in Voˇrechovsk´y (2008); however, as
opposed to their setting, we are not directly interested in the
marginal covariance function of each output, provided that
the desired cross-covariance structure holds.

The most straightforward application of SPE arises when
we pick Pd(m, n) = Pd(m − n), i.e. a stationary posi-
tion kernel, which was coined in as choosing relative at-
tention in Shaw et al. (2018) and boils down to enforc-
ing a Toeplitz structure for the cross-covariance matrix
Pd ≡ [Pd(m − n)]m,n between Qd and K d.
We propose two variants of SPE to handle this important
special case, illustrated in Figure 2. The ﬁrst variant yields
periodic covariance functions. It can be beneﬁcial when-
ever attention should not vanish with large lags, as in trafﬁc
prediction (Xue & Salim, 2020) or, as we show, in music
generation. The second variant generates vanishing covari-
ance functions; a concept which has recently been shown
useful (Wang et al., 2021), and notably yields smaller vali-
dation losses in some of our experiments.

Variant I. Relative and periodic attention (sineSPE).
In our ﬁrst approach, we consider the case where Pd is
periodic, which gets a convenient treatment. We assume:

Pd(m, n) =

K
(cid:88)

k=1

λ2
kd cos(2πfkd (m − n) + θkd) ,

(18)

where K ∈ N is the number of sinusoidal components and
fd ∈ [0 1]K, θd ∈ [−π π]K and λd ∈ RK gather their K
frequencies, phases, and weights, respectively. By using the
matrix notation, we can rewrite (18) as:

Pd = Ω(M, f d, θd) diag

(cid:17)2

(cid:16) ¨λd

Ω(N , f d, 0)(cid:62),

(19)

where ¨v ≡ (cid:2)v(cid:98)p/2(cid:99)
(cid:3)
p ∈ R2K denotes a twice upsampled
version of a vector v ∈ RK, (cid:98)·(cid:99) denotes the ﬂoor operation,
and for an index set I, Ω(I, a, b) is a matrix of size |I| ×

It can be shown that if θd = 0 and M = N , we get back to
the (unique) Vandermonde decomposition for positive deﬁ-
nite Toeplitz matrices3 (Yang et al., 2016), which boils down
in our context to assuming that ∀τ, Pd(0) ≥ Pd(τ ). Since
this is not always desirable, we keep the more general (19).

At this point, we can easily build Qd and Kd. We draw a
2K × R matrix Zd with independent and identically dis-
tributed (i.i.d.) Gaussian entries of unit variance, and deﬁne:

Qd ← Ω(M, f d, θd) diag

Kd ← Ω(N , f d, 0) diag

√

2K ,

(cid:17)

(cid:16) ¨λd
(cid:17)
(cid:16) ¨λd

Zd/
√

Zd/

2K .

(20)

(21)

It is easy to check that such a construction leads to (13). Its
parameters are {fd, θd, Λd}d, which can be trained through
stochastic gradient descent (SGD) as usual.

Variant II. Relative (vanishing) attention with regu-
lar sampling (convSPE). Due to their periodic structure,
the covariance functions generated by Variant I are non-
vanishing. Yet, our framework is ﬂexible enough to allow
for vanishing covariance structures, which may be more
desirable depending on the application (Wang et al., 2021).

As opposed to Variant I, where we imposed a speciﬁc struc-
ture on Pd, we will now follow an indirect approach, where
Pd will be implicitly deﬁned based on our algorithmic con-
struction. In this case, we assume that the signals are regu-
larly sampled (typical in e.g. text, images, audio), and we
will exploit the structure of Gaussian random matrices and
basic properties of the convolution operation.

d , ΦK

For ease of notation, we assume self attention, i.e. M =
N . Let {ΦQ
d }d denote a collection of ﬁlters, which
will ultimately be learned from training data. The size and
the dimension of these ﬁlters can be chosen according to
the input data (i.e. can be vectors, matrices, tensors). We
then propose the following procedure, which leads to a
Toeplitz Pd by means of convolutions:

• We ﬁrst draw an M × R random matrix Zd with i.i.d.
standard Gaussian entries. For multidimensional signals,
Zd gathers R random vectors, matrices, cubes, etc.

• The desired Qd and Kd are obtained by convolving Zd

with respective ﬁlters ΦQ

d and ΦK
d :
d , Kd = Zd ∗ ΦK
d ,

Qd = Zd ∗ ΦQ

(22)

where ∗ denotes convolution with appropriate dimension
(e.g. 1D, 2D or 3D). Using convolutions with ﬁnite ﬁlters

3If Pd (cid:23) 0 and K ≥ N , (19) still holds but is not unique.

Relative Positional Encoding for Transformers with Linear Complexity

Figure 2. (left) Generation of Q and K in SPE, which approximate the templates Pd when multiplied together. (right) Q and K can be
shared across layers. At each layer l, different gating is (optionally) used, before applying (16-17) to generate new queries Q and keys K.

ensures vanishing covariance, as proven in the appendix.

Due to the independence of the entries of Zd, for large R,
the product ZdZ(cid:62)
d /R will tend to the identity matrix. Given
the fact the convolution operations in (22) can be equiva-
lently expressed as a multiplication by triangular Toeplitz
matrices constructed from the respective ﬁlters, it can be
(cid:62)
shown that, as R → ∞, 1
R QdK
d tends to the product of
two triangular Toeplitz matrices. Hence, by using the prop-
erties of triangular Toeplitz matrices (cf. Kucerovsky et al.
2016, Theorem 2.4), we conclude that, as R → ∞, our
construction yields a Toeplitz matrix Pd as desired.
This approach is parameterized by the ﬁlters{ΦQ
d , ΦK
which will be learned from training data through SGD.

d }d,

The variety of attention patterns P(m − n) that can be ob-
tained directly depends on the kernel sizes, which is a classi-
cal result from signal processing (Vetterli et al., 2014). Cas-
cading several convolutions as in the VGGNet (Simonyan
& Zisserman, 2014) may be a convenient way to augment
the expressive power of this convolutional SPE variant.

From a more general perspective, the two operations in (22)
can be understood as producing PE through ﬁltering white
noise, which is the core idea we introduce for PE. Other
classical signal processing techniques may be used like
using inﬁnite impulse response ﬁlters. Such considerations
are close to the ideas proposed in (Engel et al., 2020).

To summarize, the core difference between the two pro-
posed constructions (20-21) and (22) lies in the behaviour
of RPE beyond a maximum lag, implicitly deﬁned through
the frequencies fd for (20-21) and through the sizes of the
ﬁlters for (22). While the sinusoidal construction leads to a

periodic RPE, the ﬁltering construction leads to a vanishing
RPE, which is called monotonic in (Wang et al., 2021). Both
may be the desired option depending on the application.

2.2. Gated SPE

Although RPE and the generalization (9) we propose are
novel and efﬁcient strategies to handle position information,
it may be beneﬁcial to also allow for attention coefﬁcients
that are computed without positional considerations, simply
through (cid:104)qm, kn(cid:105). As a general gating mechanism, we
propose to weight between positional and non-positional
attention through a gate parameter δd ∈ [0 1]:

Pd ≡ [δd + (1 − δd)Pd(m, n)]m,n .

(23)

This gating scheme can be implemented simply by augment-
ing Qd and Kd generated as above through:
(cid:112)

(cid:112)

qd,m ←

1 − δdqd,m +

δd(cid:15)d ,

(24)

kd,m ←

(cid:112)

1 − δdkd,m +

(cid:112)

δd(cid:15)d ,

(25)

where (cid:15)d ∈ RR in (24) and (25) is the same and has i.i.d.
standard Gaussian entries.

In practice, we can share some SPE parameters across the
network, notably across layers, to strongly reduce comput-
ing time and memory usage. In our implementation, sharing
means generating a single instance of Q and K for each head,
on which a layer-wise gating is applied, before achieving
PE through (16-17). This is illustrated in Figure 2.

sinusoidalconvolutionalgatinggatinglayer lRelative Positional Encoding for Transformers with Linear Complexity

Table 1. Long-Range Arena results (higher scores are better). Mean and standard deviation of accuracy over three runs is reported, except
for Performer with convolutional SPE, where only a single run was completed. For comparison, the best result reported by Tay et al.
(2021), along with the name of the best-performing model (in parentheses), is included.

Best result from Tay et al. (2021)

Linear Transformer-ReLU from Tay et al.

Performer-softmax (APE)
Performer-softmax + sineSPE
Performer-softmax + convSPE
Linear Transformer-ReLU (APE)
Linear Transformer-ReLU + sineSPE
Linear Transformer-ReLU + convSPE

ListOps

Text

Retrieval

Image

37.27
(Reformer)
18.01

17.80 ± 0.00
17.43 ± 0.32
17.80
17.58 ± 1.01
17.80 ± 0.00
9.50 ± 1.17

65.90
(Linear Trans.)
65.40

59.59
(Sparse Trans.)
53.82

44.24
(Sparse Trans.)
42.77

62.58 ± 0.22
62.60 ± 0.50
60.94
63.98 ± 0.05
64.09 ± 0.62
63.23 ± 1.31

59.84 ± 1.46
60.00 ± 1.20
57.22
58.78 ± 0.93
62.39 ± 0.59
61.00 ± 1.34

41.81 ± 1.16
41.12 ± 1.70
40.06
42.25 ± 0.01
41.21 ± 1.18
39.96 ± 1.31

3. Experiments

3.1. Long-Range Arena

Experimental setup. We evaluate the proposed method in
the Long-Range Arena (LRA; Tay et al., 2021), a benchmark
for efﬁcient Transformers, consisting of sequence classiﬁca-
tion tasks with a focus on long-range dependencies. We use
the following tasks from this benchmark:
• ListOps: parsing and evaluation of hierarchical expres-
sions. a longer variant of (Nangia & Bowman, 2018);
• Text: movie review sentiment analysis on the IMDB cor-

pus (Maas et al., 2011);

• Retrieval: article similarity classiﬁcation on the All About

NLP (AAN) corpus (Radev et al., 2013);

• Image: object recognition on the CIFAR10 dataset

(Krizhevsky, 2009) represented as pixel sequences.

The tasks are challenging due to the large sequence lengths,
deliberately increased by choosing a character-/pixel-level
representation. An overview of the tasks can be found in the
appendix. We do not include Pathﬁnder (a synthetic image
classiﬁcation task) as we were unable to reproduce the re-
sults of Tay et al. on this task, even through correspondence
with the authors.

We evaluate SPE (the gated variant) on two efﬁcient Trans-
former models:
the (softmax) Performer (Choromanski
et al., 2020), and a Linear Transformer (Katharopoulos
i.e. choosing
et al., 2020) with a ReLU feature map,
φ(·) = max(0, ·) element-wise in (3).4 It should be noted
that the ReLU feature map does not approximate the soft-
max kernel, which SPE is designed for (see assumption 8).
Nevertheless, it is possible to use SPE with any feature
map in practice, allowing us to include Linear Transformer-
ReLU as an interesting test of generalization to alternative
kernels.

4A model named ‘Performer’ is reported by Tay et al., but com-
munication with the authors revealed it to be in fact equivalent to
our Linear Transformer-ReLU, as it does not use random features.
To avoid confusion, we refer to this model as such herein.

We adopt the conﬁguration of Tay et al., only changing the
PE and the batch sizes/learning rates to allow training on
limited hardware with similar results. All other hyperpa-
rameters are kept identical to the original LRA. It is worth
noting that the Image models are different from the rest
in that they employ a single-layer network and only use
the ﬁrst position for prediction, dramatically limiting their
ability to beneﬁt from relative positional information.

Since we observe some variation between different runs, we
train and evaluate each model 3 times (except for Performer
with convolutional SPE, which is computationally more
costly) and report the mean and standard deviation of the
results.

The results of the benchmark are given in Table 1. The
accuracies achieved by the baseline Linear Transformer-
ReLU (APE) are similar to or surpass those reported by Tay
et al., which is a clear validation of our experimental setup.

Discussion. Results on ListOps are poor overall, with ac-
curacies around 17 %. This complies with Tay et al. (2021),
who reasoned that “kernel-based models [e.g. Performer,
Linear Transformers] are possibly not as effective on hier-
archically structured data,” leaving room for improvement.
We also hypothesize this is largely due to some known is-
sues with the training data for this task, which unfortunately
have not been ﬁxed at the time of this writing.5

Regarding performance of SPE, we ﬁrst notice that the
sineSPE variant yields the best results on three tasks,
which is a strong achievement and validates our approach,
especially considering the difﬁculty of this evaluation bench-
mark. While it is only marginally better than APE for
ListOps and Text, it is worth mentioning that sineSPE
combined with the Linear Transformer-ReLU yields an ac-
curacy improvement of ∼3 % on Retrieval compared to the
best result obtained by Tay et al. (2021).

5Currently, the ofﬁcial data loader for ListOps inadvertently

strips some characters from the input sequences.

Relative Positional Encoding for Transformers with Linear Complexity

Regarding convSPE, its performance in the LRA is not
as remarkable as it is for the music generation experiment
reported later in section 3.2. This mitigated result appears
somewhat in contradiction with the discussion found in
Wang et al. (2021), which presents vanishing attention as a
desirable property of PE. On the contrary, we empirically ob-
serve that our non-vanishing sinusoidal version sineSPE
does behave better in these particular tasks.

Finally, the superior results of APE on Image are not unex-
pected, given the limited ability of these models to exploit
relative positions. On the contrary, the relatively good per-
formance of SPE on this task is in fact remarkable, espe-
cially considering that the baseline systems for this task use
learnable APE.

As we will see later in our music generation experiments,
there are tasks where our proposed SPE clearly yields re-
markable improvements. Here in the LRA, we notice that
it does not result in an obvious and systematic boost in
performance. This raises interesting considerations:

(i) The variance of the Monte Carlo estimator might be prob-
lematic. We are enthusiastic about the elegant formulation
of stochastic feature maps as in the Performer, which was
a strong inspiration. Still, we must acknowledge that their
computation relies on a Monte Carlo estimator (15). We
suspect that the variance of the estimator might play a role
in the ﬁnal performance in large dimensions, which opens
up the direction of exploring variance-reduced estimation
methods, rather than plain Monte Carlo.

(ii) LRA tasks might not beneﬁt from strong (R)PE schemes.
The LRA was designed to compare Transformer architec-
tures, ﬁlling a gap in this domain and standing as the de
facto standard, justifying our choice. Still, although PE
is known to be important in many cases, it is not known
whether it is so in the LRA tasks. We feel that there is room
for such a specialized comparison, which is scheduled in
our future work, possibly leading to new long-range tasks
where PE is critical.

3.2. Pop Piano Music Generation

In our music generation experiments (this subsection and
section 3.3), music is represented as sequences of symbols
(tokens) and a Performer (Choromanski et al., 2020) is used
as an autoregressive language model, which predicts a prob-
ability distribution over the next token given the past context.
At test time, a new sequence is generated by iteratively sam-
pling the next token, as commonly done in text generation.

Experimental setup. We train Performers for music gen-
eration, with 24 layers and 8 heads per layer on a dataset
composed of 1 747 pop piano tracks, encoded using the re-
cently proposed Revamped MIDI-derived format (REMI;
Huang & Yang, 2020). The sequences are composed of

Figure 3. Validation cross-entropy vs. token position on pop piano
music generation task. (lower is better; the black vertical line
indicates the maximum position to which the models are trained.)

metrical tokens: bar, subbeat, and tempo, which rep-
resent musical timing; and note tokens: chord, pitch,
duration, and volume, which describe the musical con-
tent (see the appendix for more details). We hold out 5% of
the songs as the validation set.

We train the models with sequence length N = 2 048, cor-
responding to ∼1 minute of music. The only difference
between our models is the PE strategy. We consider base-
line APE, as well as SPE: sinusoidal or convolutional, with
or without gating, resulting in 5 different models.

Results and discussion. For qualitative assessment, we
ﬁrst display in Figure 1 one attention pattern for each PE
model: APE and (gated) sineSPE/convSPE, obtained as
an average over 20 from-scratch generations for a chosen
(layer, head). More similar plots can be found in appendix.
Interestingly, we notice that for early layers, APE attention
does not go much beyond training sequence length. This
behaviour is not found in SPE variants, which consistently
attend to all positions. Another remarkable feature of the
proposed model (only displayed in the appendix) is that
gating as described in section 2.2 visually disables PE al-
together for some layers/heads, in which case attention is
global.

Since the literature suggests that RPE improves general-
ization performance (Shaw et al., 2018; Zhou et al., 2019;
Rosendahl et al., 2019), we display validation cross-entropy
computed with teacher forcing (Williams & Zipser, 1989)
in Figure 3, as a function of the target token position. The
values would indicate how well the models predict the token
at a certain position given the preceding tokens, for tracks
in the validation set. We notice that all SPE variants, espe-
cially convSPE, behave much better than APE for token
positions beyond 2 048. This suggests that SPE inherits this
celebrated advantage of RPE (Huang et al., 2018) while
being applicable to much longer sequences.

Recently, Wang et al. (2021) deﬁned metrics for the eval-
uation of PE, suggesting that translation invariance and
monotonicity are desirable properties. The former states that
the distances of two arbitrary τ -offset position embeddings
should be identical, while the latter states that neighboring

Relative Positional Encoding for Transformers with Linear Complexity

example has a uniform style (‘groove’), we prime the model
with a short prompt (2-bar musical fragment) and let it gen-
erate a continuation. We then observe whether the generated
continuation matches the style of the prompt.

Experimental setup. The models (24-layer Performers
with 8 attention heads) are trained on an accompaniment
dataset comprising 5 522 samples in 2 761 different musical
styles, encoded in a token-based format adopted from C´ıfka
et al. (2020) and detailed in the appendix. All SPE-based
models use gating in this experiment. Unlike the previous
experiment, which leverages long training sequences, we
consider training sequences of length N = 512, correspond-
ing to 2–10 bars. At test time, the model is prompted with 2
bars in a style not seen during training and new tokens are
sampled to complete the sequence to a length of 1 024, i.e.
twice the training length.

We use two musically motivated style similarity metrics –
time-pitch and onset-duration proposed by C´ıfka et al.
(2019; 2020) – to quantify the similarity of the generated
continuation to the prompt. When listening to the generated
music, we perceptually notice a drift in quality along time.
For this reason, we divide each generated sample into four
successive chunks of identical duration and evaluate them
independently. The results are displayed in Figure 5.

Discussion. We clearly see that SPE substantially outper-
forms APE in both metrics. Although APE visibly does
manage to generate close to the desired style at the begin-
ning of the sequence, this similarity strongly degrades over
time. Both sineSPE and convSPE are much more stable
in this regard, conﬁrming the result from section 3.2 that
SPE extrapolates better beyond the training sequence length.
This matches our informal perceptual evaluation.6

This experiment suggests that exploiting a local neighbor-
hood is a robust way to process long sequences. This could
appear as contradicting the use of long-range Transformers,
but we highlight that gating is used here, enabling some
heads to exploit long term-attention independently from po-
sition. Further comparisons with local attention schemes
(e.g. Dai et al., 2019; Hofst¨atter et al., 2020) could be in-
teresting, although they were not included here due to Tay
et al. (2021) suggesting that they are clearly inferior, at least
in the LRA setting.

4. Related Work

This paper is concerned with PE (Sukhbaatar et al., 2015),
as a way to embed the position of each token as part of its
features. This idea is a core ingredient for many subsequent
groundbreaking studies (Gehring et al., 2017; Vaswani et al.,
2017), and has been the actual topic of many investigations.

6Examples: https://cifkao.github.io/spe/

Figure 4. PE evaluation metrics (Wang et al., 2021) for the pop
piano music generation task in the 1st layer (lower is better), w.r.t.
query positions. Training sequence length is 2 048. Only query-
key offsets <128 are considered here. See appendix for details.

Figure 5. Musical style similarity for groove continuation (higher
is better) between output and initial prompt through two musically-
motivated metrics, as a function of time in the output. Each data
point corresponds to a single musical style.

positions should be assigned with position embeddings that
are closer than faraway ones. Following their identical word
probing methodology, we report these metrics in Figure 4.
As expected, SPE variants greatly outperform APE in terms
of translation invariance. However, monotonicity does not
seem a very relevant criterion in our music application, as
can be seen when comparing scores in Figures 3 and 4. It
seems that music modeling can beneﬁt from non-vanishing
attention patterns. In any case, SPE scores are remarkably
stable across positions, contrarily to APE, which rapidly
degrades beyond the training length.

3.3. Groove Continuation

In this experiment, we evaluate Performers on a groove
continuation task. After training on a dataset where each

Relative Positional Encoding for Transformers with Linear Complexity

Absolute Positional Encoding (APE) based on sinusoids
from Vaswani et al. (2017) is the most widely used for
Transformer-like architectures. However, PE q(n) and k(n)
in (5) can also be trained as in BERT (Devlin et al., 2019;
Liu et al., 2019). Although the original Transformer only
includes PE at the input layer, it may be included at all
layers (Dehghani et al., 2019; Lan et al., 2020).

Relative positional encoding (RPE; Shaw et al., 2018) is a
way to leverage relative positions. It came with a O(N 2D)
space complexity, which was reduced to O(N 2) in Huang
et al. (2018); He et al. (2020). Considering log-distances
was proposed in Raffel et al. (2020). Several variants for
RPE were introduced (Huang et al., 2020; Wang et al., 2021).
They all apply learned RPE in the attention domain. Using
ﬁxed embedding functions was also considered for RPE
(Pham et al., 2020), and masking RPE is used in Kim et al.
(2020) to promote local attention.

Keys-domain vs attention domain. Doing PE in the keys
domain introduces position-content cross terms that are ad-
vocated as noisy and not beneﬁcial in Ke et al. (2020) and
replaced by Untied attention, i.e. PE in the attention domain.
This is also called disantangled attention in He et al. (2020)
and already proposed in Tsai et al. (2019) through separa-
ble content-position attention kernels. All of these studies
require the explicit computation and storage of A.

Non-integer positions were considered for structured in-
puts. Tree-based PE was proposed both for APE (Shiv &
Quirk, 2019; Xiao et al., 2019; Ma et al., 2019) and RPE
(Omote et al., 2019). Positional encoding of robots within
arbitrary polygons is found in Bose et al. (2019).

Dynamical models for PE. Attention for machine transla-
tion was introduced in Bahdanau et al. (2016), which was
retrospectively understood in Ke et al. (2020) as using re-
current neural nets (RNN) for PE. In Chen et al. (2018), the
hidden states of encoder RNNs are said to contain enough
position information to skip explicit PE. Neishi & Yoshinaga
(2019) builds on this view, but explicitly describes the idea
for the ﬁrst time. Their contribution is to replace the additive
PE in (5) by an RNN. In the same vein, Liu et al. (2020)
generates PE using (neural) ordinary differential equations.

Convolutional contexts. Our convSPE variant involves
convolving random noise. First, this can be related to Mo-
hamed et al. (2019), who use convolutional neural networks
for queries and keys computation. Second, the connections
between convolutions and stationary processes have recently
been highlighted by Xu et al. (2020) as enforcing PE.

Multiplicative PE. Various levels of content-position inter-
actions are formalized in (Tsai et al., 2019). Multiplicative
strategies were proposed for both RPE (Huang et al., 2020)
and APE (Dai et al., 2019). The latter was generalized in
Tsai et al. (2019). All these require the explicit computa-

tion of the attention matrix. Wang et al. (2020a) presents a
scheme that is close to our sinusoidal variant, but without
the stochastic part that is the key to go from (14) to (15).

The limits of APE and RPE were highlighted by some au-
thors. In Wang & Chen (2020), the best performing models
exploit both absolute and relative positions. In Irie et al.
(2019) and Tsai et al. (2019), it is found that removing APE
altogether in the causal decoder part of Transformer-based
architectures leads to comparable/better performance. It is
also not clear which one is best between incorporating PE
in the raw input signal (and hence propagating it through
the value entries) or using it anew on the queries and keys
only, as we do. Our choice is backed by Tsai et al. (2019).

5. Conclusion

We propose a new Stochastic Positional Encoding (SPE),
based on ﬁltering random noise. As we show, the proce-
dure generalizes relative PE and is a principled means to
enforce any prescribed (but trained) cross-covariance struc-
ture, which we demonstrated should be the central concern
in dot-product attention. In our experiments, we show that
SPE brings an interesting gain in performance for large-scale
transformer models (Choromanski et al., 2020; Katharopou-
los et al., 2020), as compared to classical (sinusoidal) PE.
This was expected, because RPE (Shaw et al., 2018) is often
advocated as beneﬁcial. However, no way to incorporate it
for long sequences was available so far and this is the core
contribution of this paper. The natural future directions for
our study are (i) Signal-dependent PE that incorporates the
input sequence as an additional input for SPE, (ii) Nonsta-
tionary PE that utilizes both relative and absolute positions,
(iii) Extending our approach to arbitrary attention kernels,
e.g. deﬁned implicitly through their (random) mappings as
in (4). Indeed, SPE as it is presented here holds theoretically
for dot-product attention kernels only, but our results given
in Table 1 suggest that this generalizes, asking an interesting
research question.

Acknowledgements

This work was supported by the European Union’s Hori-
zon 2020 research and innovation programme under the
Marie Skłodowska-Curie grant agreement No. 765068 (MIP-
Frontiers) and in part by the French government under man-
agement of Agence Nationale de la Recherche as part of
the “Investissements d’avenir” program, reference ANR-19-
P3IA-0001 (PRAIRIE 3IA Institute).

We would like to thank Yi Tay, Mostafa Dehghani and Philip
Pham for their help with troubleshooting the Long-Range
Arena, and Krzysztof Choromanski for clariﬁcations about
the Performer.

Relative Positional Encoding for Transformers with Linear Complexity

References

AlQuraishi, M. AlphaFold at CASP13. Bioinformatics, 35

(22):4862–4865, 2019.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
translation by jointly learning to align and translate.
arXiv:1409.0473 [cs, stat], May 2016. URL http://
arxiv.org/abs/1409.0473. arXiv: 1409.0473.

Bello, I. LambdaNetworks: Modeling long-range inter-
actions without attention. In Proc. Int. Conf. Learning
Representations, 2020.

B¨ock, S., Korzeniowski, F., Schl¨uter, J., Krebs, F., and
Widmer, G. Madmom: A new Python audio and music
signal processing library. In Proc. ACM International
Multimedia Conf., pp. 1174–1178, 2016.

Bose, K., Adhikary, R., Kundu, M. K., and Sau, B.
Positional encoding by robots with non-rigid move-
arXiv:1905.09786 [cs], May 2019. URL
ments.
http://arxiv.org/abs/1905.09786.
arXiv:
1905.09786.

Chen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey,
W., Foster, G., Jones, L., Parmar, N., Schuster, M.,
Chen, Z., Wu, Y., and Hughes, M. The best of both
worlds: Combining recent advances in neural machine
translation. arXiv:1804.09849 [cs], April 2018. URL
http://arxiv.org/abs/1804.09849.
arXiv:
1804.09849.

Child, R., Gray, S., Radford, A., and Sutskever,

I.
Generating long sequences with sparse Transform-
ers. arXiv:1904.10509 [cs, stat], April 2019. URL
http://arxiv.org/abs/1904.10509.
arXiv:
1904.10509.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X.,
Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,
A., Kaiser, L., Belanger, D., Colwell, L., and Weller, A.
Rethinking attention with Performers. arXiv:2009.14794
[cs, stat], September 2020. URL http://arxiv.
org/abs/2009.14794. arXiv: 2009.14794.

C´ıfka, O., S¸ ims¸ekli, U., and Richard, G. Supervised sym-
In
bolic music style translation using synthetic data.
Proc. International Society for Music Information Re-
trieval Conf., pp. 588–595, 2019. doi: 10.5281/zenodo.
URL https://doi.org/10.5281/
3527878.
zenodo.3527878.

Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and
Salakhutdinov, R. Transformer-XL: Attentive language
models beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860, 2019.

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and
Kaiser, L. Universal Transformers. arXiv:1807.03819
[cs, stat], March 2019. URL http://arxiv.org/
abs/1807.03819. arXiv: 1807.03819.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
BERT: Pre-training of deep bidirectional Transformers
for language understanding. arXiv:1810.04805 [cs],
May 2019. URL http://arxiv.org/abs/1810.
04805. arXiv: 1810.04805.

Donahue, C., Mao, H. H., Li, Y. E., Cottrell, G. W., and
McAuley, J. Lakhnes: Improving multi-instrumental mu-
sic generation with cross-domain pre-training. In ISMIR,
2019.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020.

Engel, J., Hantrakul, L., Gu, C., and Roberts, A. Ddsp:
Differentiable digital signal processing. arXiv preprint
arXiv:2001.04643, 2020.

Gehring, J., Auli, M., Grangier, D., Yarats, D., and
Dauphin, Y. N. Convolutional sequence to sequence
arXiv:1705.03122 [cs], July 2017. URL
learning.
http://arxiv.org/abs/1705.03122.
arXiv:
1705.03122.

Genton, M. G. and Kleiber, W. Cross-covariance functions
for multivariate geostatistics. Statistical Science, pp. 147–
163, 2015.

Hawthorne, C., Elsen, E., Song, J., Roberts, A., Simon, I.,
Raffel, C., Engel, J., Oore, S., and Eck, D. Onsets and
Frames: Dual-objective piano transcription. In Proc. Int.
Society for Music Information Retrieval Conf., 2018.

He, P., Liu, X., Gao, J., and Chen, W. DeBERTa:
Decoding-enhanced BERT with disentangled atten-
arXiv:2006.03654 [cs], June 2020.
URL
tion.
http://arxiv.org/abs/2006.03654.
arXiv:
2006.03654.

C´ıfka, O., S¸ ims¸ekli, U., and Richard, G. Groove2Groove:
One-shot music style transfer with supervision from syn-
thetic data. IEEE/ACM Transactions on Audio, Speech
and Language Processing, 28:2638–2650, 2020. doi:
10.1109/TASLP.2020.3019642. URL https://hal.
archives-ouvertes.fr/hal-02923548.

Hofst¨atter, S., Zamani, H., Mitra, B., Craswell, N., and
Hanbury, A. Local self-attention over long text for ef-
ﬁcient document retrieval. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pp. 2021–2024,
2020.

Relative Positional Encoding for Transformers with Linear Complexity

Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
The curious case of neural text degeneration. In Proc.
International Conference on Learning Representations,
2019.

Hsiao, W.-Y., Liu, J.-Y., Yeh, Y.-C., and Yang, Y.-H. Com-
pound Word Transformer: Learning to compose full-song
music over dynamic directed hypergraphs. In Proc. AAAI
Conf. Artiﬁcial Intelligence, 2021.

Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer,
N., Simon, I., Hawthorne, C., Dai, A. M., Hoffman,
M. D., Dinculescu, M., and Eck, D. Music Transformer.
arXiv:1809.04281 [cs, eess, stat], December 2018. URL
http://arxiv.org/abs/1809.04281.
arXiv:
1809.04281.

Huang, Y.-S. and Yang, Y.-H. Pop Music Transformer:
Generating music with rhythm and harmony. In Proc.
ACM International Multimedia Conf., 2020.

Huang, Z., Liang, D., Xu, P., and Xiang, B. Improve Trans-
former models with better relative position embeddings.
arXiv preprint arXiv:2009.13658, 2020.

Irie, K., Zeyer, A., Schl¨uter, R., and Ney, H. Lan-
guage modeling with deep Transformers. Proc. Inter-
speech, pp. 3905–3909, 2019. doi: 10.21437/Interspeech.
2019-2225. URL http://arxiv.org/abs/1905.
04226. arXiv: 1905.04226.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are RNNs: Fast autoregressive Transform-
In Proc. Int. Conf. Machine
ers with linear attention.
Learning, pp. 5156–5165, 2020.

Ke, G., He, D., and Liu, T.-Y. Rethinking positional en-
coding in language pre-training. arXiv:2006.15595 [cs],
July 2020. URL http://arxiv.org/abs/2006.
15595.

Kim, J., El-Khamy, M., and Lee, J. T-GSA: Trans-
former with Gaussian-weighted self-attention for speech
arXiv:1910.06762 [cs, eess], Febru-
enhancement.
ary 2020. URL http://arxiv.org/abs/1910.
06762. arXiv: 1910.06762.

Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, University of Toronto,
2009.

Kucerovsky, D., Mousavand, K., and Sarraf, A. On
some properties of toeplitz matrices. Cogent Math-
ematics, 3(1), 2016.
10.1080/23311835.
2016.1154705. URL http://doi.org/10.1080/
23311835.2016.1154705.

doi:

Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,
and Soricut, R. ALBERT: A lite BERT for self-supervised
learning of language representations. arXiv:1909.11942
[cs], February 2020. URL http://arxiv.org/
abs/1909.11942. arXiv: 1909.11942.

Liu, X., Yu, H.-F., Dhillon, I., and Hsieh, C.-J. Learn-
ing to encode position for Transformer with continu-
arXiv:2003.09229 [cs, stat],
ous dynamical model.
URL http://arxiv.org/abs/
March 2020.
2003.09229. arXiv: 2003.09229.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,
V. RoBERTa: A robustly optimized BERT pretrain-
ing approach. arXiv:1907.11692 [cs], July 2019. URL
http://arxiv.org/abs/1907.11692.
arXiv:
1907.11692.

Ma, C., Tamura, A., Utiyama, M., Sumita, E., and Zhao,
T.
Improving neural machine translation with neural
syntactic distance. In Proc. Conf. North American Chap-
ter of the Association for Computational Linguistics, pp.
2032–2037, 2019. doi: 10.18653/v1/N19-1205. URL
http://aclweb.org/anthology/N19-1205.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,
A. Y., and Potts, C. Learning word vectors for sen-
In Proc. Annual Meeting of the As-
timent analysis.
sociation for Computational Linguistics: Human Lan-
guage Technologies, pp. 142–150, 2011. URL https:
//www.aclweb.org/anthology/P11-1015.

Matheron, G. Principles of geostatistics. Economic geology,

58(8):1246–1266, 1963.

Mohamed, A., Okhonko, D., and Zettlemoyer, L. Trans-
formers with convolutional context for asr. arXiv preprint
arXiv:1904.11660, 2019.

Nadaraya, E. A. On estimating regression. Theory of Prob-

ability & Its Applications, 9(1):141–142, 1964.

Nangia, N. and Bowman, S. ListOps: A diagnostic dataset
for latent tree learning. In Proc. Conf. North American
Chapter of the Association for Computational Linguistics:
Student Research Workshop, pp. 92–99, 2018. doi: 10.
18653/v1/N18-4013. URL https://www.aclweb.
org/anthology/N18-4013.

Neishi, M. and Yoshinaga, N. On the relation between
position information and sentence length in neural ma-
chine translation. In Proc. Conf. Computational Natural
Language Learning, pp. 328–338, 2019. doi: 10.18653/
v1/K19-1031. URL https://www.aclweb.org/
anthology/K19-1031.

Relative Positional Encoding for Transformers with Linear Complexity

Omote, Y., Tamura, A., and Ninomiya, T. Dependency-
based relative positional encoding for Transformer
NMT. In Proc. Natural Language Processing in a Deep
Learning World, pp. 854–861, 2019. ISBN 978-954-452-
056-4. doi: 10.26615/978-954-452-056-4 099. URL
https://acl-bg.org/proceedings/2019/
RANLP2019/pdf/RANLP099.pdf.

Pham, N.-Q., Ha, T.-L., Nguyen, T.-N., Nguyen, T.-S.,
Salesky, E., Stueker, S., Niehues, J., and Waibel, A. Rela-
tive positional encoding for speech recognition and direct
translation. arXiv:2005.09940 [cs, eess], May 2020. URL
http://arxiv.org/abs/2005.09940.

Radev, D. R., Muthukrishnan, P., Qazvinian, V., and Abu-
Jbara, A. The ACL anthology network corpus. Lan-
guage Resources and Evaluation, 47(4):919–944, January
2013. doi: 10.1007/s10579-012-9211-2. URL https:
//doi.org/10.1007/s10579-012-9211-2.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the
limits of transfer learning with a uniﬁed text-to-text Trans-
former. arXiv:1910.10683 [cs, stat], July 2020. URL
http://arxiv.org/abs/1910.10683.
arXiv:
1910.10683.

Rosendahl, J., Tran, V. A. K., Wang, W., and Ney, H. Analy-
sis of positional encodings for neural machine translation.
In Proc. IWSLT, 2019.

Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient
content-based sparse attention with routing Transformers.
arXiv:2003.05997 [cs, eess, stat], October 2020. URL
http://arxiv.org/abs/2003.05997.
arXiv:
2003.05997.

Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with
relative position representations. arXiv:1803.02155 [cs],
April 2018. URL http://arxiv.org/abs/1803.
02155. arXiv: 1803.02155.

Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to-
end memory networks. arXiv:1503.08895 [cs], Novem-
ber 2015. URL http://arxiv.org/abs/1503.
08895. arXiv: 1503.08895.

Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D.,
Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.
Long Range Arena: A benchmark for efﬁcient Trans-
formers. In Proc. Int. Conf. Learning Representations,
2021. URL https://openreview.net/forum?
id=qVyeW-grC2k.

Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and
Salakhutdinov, R. Transformer dissection: An uniﬁed
understanding for Transformer’s attention via the lens of
kernel. In Proc. Conf. Empirical Methods in Natural Lan-
guage Processing and Int. Joint Conf. Natural Language
Processing, pp. 4335–4344, 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is all you need. In Proc. Advances in neural information
processing systems, pp. 5998–6008, 2017.

Vetterli, M., Kovaˇcevi´c, J., and Goyal, V. K. Foundations of
signal processing. Cambridge University Press, 2014.

Voˇrechovsk´y, M. Simulation of simply cross correlated
random ﬁelds by series expansion methods. Structural
safety, 30(4):337–363, 2008.

Wang, B., Zhao, D., Lioma, C., Li, Q., Zhang, P., and
Simonsen, J. G. Encoding word order in complex em-
beddings. arXiv:1912.12333 [cs], June 2020a. URL
http://arxiv.org/abs/1912.12333.
arXiv:
1912.12333.

Wang, B., Shang, L., Lioma, C., Jiang, X., Yang, H., Liu,
Q., and Simonsen, J. G. On position embeddings in
In Proc. Int. Conf. Learning Representations,
BERT.
2021. URL https://openreview.net/forum?
id=onxoVA9FxMw.

Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H.
Efﬁcient attention: Attention with linear complexi-
ties. arXiv:1812.01243 [cs], November 2020. URL
http://arxiv.org/abs/1812.01243.
arXiv:
1812.01243.

Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma,
H. Linformer: Self-attention with linear complex-
ity. arXiv:2006.04768 [cs, stat], June 2020b. URL
http://arxiv.org/abs/2006.04768.
arXiv:
2006.04768.

Shiv, V. and Quirk, C. Novel positional encodings to enable
tree-based Transformers. In Proc. Advances in neural
information processing systems, 2019.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

Wang, Y.-A. and Chen, Y.-N. What do position embeddings
learn? An empirical study of pre-trained language model
positional encoding. In Proc. Conf. Empirical Methods
in Natural Language Processing, 2020.

Watson, G. S. Smooth regression analysis. Sankhy¯a: The
Indian Journal of Statistics, Series A, pp. 359–372, 1964.

Relative Positional Encoding for Transformers with Linear Complexity

Williams, C. K. and Rasmussen, C. E. Gaussian processes
for machine learning, volume 2. MIT press Cambridge,
MA, 2006.

Williams, R. J. and Zipser, D. A learning algorithm for con-
tinually running fully recurrent neural networks. Neural
computation, 1(2):270–280, 1989.

Wu, Q., Lan, Z., Gu, J., and Yu, Z. Memformer: The
arXiv:2010.06891
memory-augmented Transformer.
[cs], October 2020. URL http://arxiv.org/abs/
2010.06891. arXiv: 2010.06891.

Xiao, F., Li, J., Zhao, H., Wang, R., and Chen, K.
Lattice-Based Transformer encoder for neural machine
translation. arXiv:1906.01282 [cs], June 2019. URL
http://arxiv.org/abs/1906.01282.
arXiv:
1906.01282.

Xu, R., Wang, X., Chen, K., Zhou, B., and Loy, C. C. Posi-
tional encoding as spatial inductive bias in gans. arXiv
preprint arXiv:2012.05217, 2020.

Xue, H. and Salim, F. D. Trailer: Transformer-based time-
wise long term relation modeling for citywide trafﬁc ﬂow
prediction. arXiv preprint arXiv:2011.05554, 2020.

Yang, Z., Xie, L., and Stoica, P. Vandermonde decomposi-
tion of multilevel Toeplitz matrices with application to
multidimensional super-resolution. IEEE Transactions
on Information Theory, 62(6):3685–3701, 2016.

Zhou, P., Fan, R., Chen, W., and Jia, J. Improving gen-
eralization of transformer for speech recognition with
parallel schedule sampling and relative positional embed-
ding. arXiv preprint arXiv:1911.00203, 2019.

Relative Positional Encoding for Transformers with Linear Complexity

Supplementary Material

Introduction

This document comprises additional information that could
not be included in the paper due to space constraints. It
is structured as follows. In appendix A, we provide some
further theoretical developments. In appendix B, we de-
tail the experimental setup on the Long Range Arena. In
appendix C, we detail our music generation experiments.
Finally, we provide additional results in appendix D.

Our source code is available at:

https://github.com/aliutkus/spe/

See also the companion website:

https://cifkao.github.io/spe/

Qd(m, r) =

K d(n, r) =

P
(cid:88)

p=0

P
(cid:88)

p=0

Zd(m − p, r)φQ

d (p),

Zd(n − p, r)φK

d (p),

where Zd is a white Gaussian noise process,
i.e.
E[Zd(m, r)Zd(m(cid:48), r)] = δmm(cid:48). Omitting the dependency
on r for notational convenience (all realizations are indepen-
dent), we can compute the positional attention as:

Pd(m, n) = E (cid:2)Qd(m)K d(n)(cid:3)
(cid:34) P

= E

= E

(cid:88)

zd(m − τ )zd(n − p)φQ

d (τ )φK

p,τ =0

zd(n − p)2φQ

d (p + m − n)φK

(cid:35)
d (p)

(cid:34) P

(cid:88)

p=0

(cid:35)
d (p)

A. Theory

A.1. Convolutional SPE Leads to Vanishing Attention

=

P
(cid:88)

p=0

φQ
d (p + m − n)φK

d (p),

where only the (p, τ ) values such that n−p = m−τ remain,
all other cross terms E[Zd(m)Zd(m(cid:48) (cid:54)= m)] disappearing
due to whiteness of Zd. Filters are taken as 0-valued outside
of [0 : P ]. As can be seen, whenever |m − n| > P , we get
Pd(m, n) = 0, because φK

d (p + (m − n)) = 0.

A.2. Complexity

In this section, we detail the additional complexity caused
by the proposed SPE method.

• Sinusoidal SPE ﬁrst requires the computation of the
modulation matrices Ω for each feature dimension d =
1 . . . D, which has a O(2N K) complexity. Then, this
matrix must be multiplied by the noise matrix Zd with
shape 2K × R, leading to an overall complexity of
O(DRN K 2). Since K is typically very small in our
experiments, SineSPE can be seen as quite light in terms
of both time and space complexity.

• Convolutional SPE involves drawing a new noise signal
zd,:,r of length N for each d and r, and convolving it with
the ﬁlters φQ
In the 1D case, this leads to an overall time complexity of
O(DRN P ), which can be replaced by O(DRN log N )

d , whose length is written P .

d and φK

d and ΦK

Figure 6. If ΦQ
d have length P , Qd and K d for convolu-
tional SPE depend on the noise Zd over the intervals [m − P : m]
and [n − P : n], respectively. Their correlation depends only on
the shaded area, due to whiteness of Zd. Whenever |m − n| > P ,
the two signals are uncorrelated.

In the main document, we claim that the convolutional vari-
ant leads to vanishing attention. We shortly prove this claim
here. For ease of notation, the proof is given in the 1D case,
but extends trivially to higher dimensions. The core idea is
illustrated in Figure 6. Convolutional SPE yields:

Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

higher

when operating the convolutions in the frequency domain,
which is advantageous for long ﬁlters.
becomes
In
2D,
dimensions,
O(DRN1N2P1P2)
in the original domain and
O(DRN1N2 log N1 log N2) in the frequency domain,
where (N1, N2) and (P1, P2) are the shapes of noise and
ﬁlters, respectively.

this

say

• The bottleneck of gating is the generation of random

noise (cid:15)d, which has complexity O(DR).

Note that this complexities of course must be multiplied by
the number of heads considered, up to 8 in our experiments.

As can be seen, the complexities of the sinusoidal and con-
volutional variants are similar, depending on the length P
of the ﬁlters and the number K of sinusoids.
Still, other aspects come into the play. First, the convolu-
tional version requires generating noise whose size scales
as N , while the sinusoidal version requires much smaller
2K-large noise matrices. Second, only a very small number
of sinusoids was required in our experiments, whereas the
convolutional version required longer contexts, so that we
often had 2K (cid:28) P in practice. Finally, although this may
change in the near future, deep learning frameworks like Py-
Torch do not easily integrate convolutions in the frequency
domain.

Sample-wise noise sharing. In practice, SPEs do not need
to be drawn anew for each example. The most straightfor-
ward trick to reduce memory and computational footprint of
the method is to share Q and K among all examples in each
mini-batch, as we do in all our experiments. This can bring
signiﬁcant memory savings when SPE is used as a drop-in
addition to networks trained with large batch sizes.

B. Experimental Setup: Long-Range Arena

An overview of the Long-Range Arena (Tay et al., 2021)
tasks is given in table 2. We do not include Pathﬁnder
(a synthetic image classiﬁcation task) or its harder variant
Pathﬁnder-X in this paper as we were unable to reproduce
the results of Tay et al. on this task. All the datasets are
described in detail in Tay et al. and available from the ofﬁcial
LRA repository.7

In all LRA experiments, we employ gated SPE with R ∈
{32, 64}. We consistently use K = 10 for sinusoidal (peri-
odic) SPE and ﬁlters of length 128 for convolutional SPE.
For convolutional SPE, we share Q and K across all layers
(but not across attention heads); for sinusoidal SPE, Q and
K are unique to each layer and head; in both cases, layer-
speciﬁc gating is employed. Baseline experiments employ
the same absolute positional encodings as Tay et al. (learn-

able APE for Image and sinusoidal APE for the remaining
tasks). In models employing SPE, APE is removed.

The numbers of parameters of the models presented in the
main document are shown in Table 3. We can see that
SPE-based models have at most 3.1 % more parameters than
the baselines. In the Image column, the numbers for SPE-
based models are about 50 % lower due to the fact that the
baselines on this task employ learnable APE.

We use code from the ofﬁcial LRA repository, including
the authors’ Transformer implementation, modiﬁed as nec-
essary to incorporate SPE. We keep the same training con-
ﬁguration as provided by the LRA authors, but decrease
the batch sizes (from 256 to 96 for Image and from 32 to 8
for the rest) and learning rates so as to ﬁt within 16 GB of
GPU memory. Our modiﬁed code and conﬁguration ﬁles
are available in our source code repository.

B.1. Resource usage

The typical training times of the LRA models are displayed
in Table 4. Note that the times may not be comparable
across models or tasks due to evaluation (which may be
time-consuming) being done more frequently in some runs
than others.

The total training time was 1 405 h (189 runs in total), out of
which 273 h (61 runs) were spent on attempts to reproduce
the results of Tay et al. (2021) using Performer-softmax,
Linear Transformer-ReLU and vanilla Transformer. Some
of these preliminary experiments were distributed over 1–3
Tesla V100 GPUs with 32 GB of memory each. The ﬁnal
models were all trained on a single Tesla V100 or P100
GPU with 16 GB of memory.

C. Experimental Setup: Music Generation

Our music Performers are implemented using the
pytorch-fast-transformers package,8 modiﬁed
as necessary to incorporate SPE. The modiﬁed code and
conﬁguration ﬁles are available in our code repository.

All models have 24 layers with model dimension 512, 8
attention heads and 2 048 feed-forward units, which amount
to ∼80 million trainable parameters. In models that use SPE,
Q and K are shared across all layers (but not across attention
heads); layer-speciﬁc gating is employed for models trained
with gated SPE.

The models are trained with the Adam optimizer. We sched-
ule the learning rate with linear warmup, followed by cosine
decay. Full details of hyperparameters can be found in the
provided conﬁguration ﬁles.

7https://github.com/google-research/

long-range-arena

8https://github.com/idiap/

fast-transformers

Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Table 2. Long-Range Arena classiﬁcation tasks used in this paper.

Name

Dataset

Input

ListOps
ListOps
IMDB
Text
Retrieval AAN
Image

CIFAR10

expression with operations on lists of digits
movie review as byte string
pair of articles as byte strings
8-bit gray-scale 32 × 32 image as byte string

Length

2 k
8 k
2 × 4 k
1 k

Goal

# classes

evaluate expression
classify sentiment
detect citation link
recognize object

10
2
2
10

Table 3. Numbers of parameters of LRA models, identical for both Performer-softmax and Linear Transformer-ReLU.

ListOps

Text

Retrieval

Image

Baseline (APE)
+ sineSPE
+ convSPE

19 982 858
20 078 090
20 117 002

3 486 722
3 518 466
3 553 282

1 087 618
1 103 490
1 120 898

248 458
119 242
133 706

C.1. Pop Piano Music Generation

The note tokens are:

Training data. The pop piano MIDI dataset we use is
derived from the one provided in Hsiao et al. (2021), open-
sourced on GitHub.9 It consists of 1,747 pure piano per-
formances of various Japanese, Korean, and Western pop
songs, amounting to a total duration of ∼100 hours. All the
songs are in 4/4 time signature, namely four beats per bar
(measure). We leave 5% (87 songs) as the validation set.

According to Hsiao et al. (2021), the piano performances
are originally collected from the Internet in the MP3 (audio)
format. Hsiao et al. further employed Onsets and Frames
piano transcription (Hawthorne et al., 2018), madmom beat
tracking tool (B¨ock et al., 2016), and chorder rule-based
chord detection10 to transcribe the audio into MIDI format
with tempo, beat, and chord information.

Data representation. The representation adopted here is
largely identical to the Revamped MIDI-derived (REMI)
encoding by Huang & Yang (2020), except that an extended
set of chord tokens (described below) is used. REMI
encodes a piano piece into a sequence composed of two
types, metrical and note, of tokens. The metrical tokens are:

• bar: Marks the start of a musical bar.
• subbeat: Marks the musical timing within a bar. A bar
is divided into 16 subbeats, which is equivalent to 4
beats. This symbolic timing provides an explicit time grid
for sequence models to model music.

• tempo: Determines the pace (in beats per minute, or
bpm) at which the piece is played, varied per bar. The
range of tempo tokens is [32, 224] bpm, in steps of 3
bpm for quantization.

9https://github.com/YatingMusic/

compound-word-transformer

10https://github.com/joshuachang2311/

chorder

• pitch: Marks a note played. The 88 pitch-es corre-

spond to each key on the piano.

• duration: Denotes the length of a played note, ranging

from 1/2 to 16 subbeats, in steps of 1/2 subbeat.

• volume (or, velocity): Denotes how loud a note is played.

A total of 24 volume levels are considered.

• chord: Marks a change on the accompanying chord.
Each chord is described by its root note and quality,
e.g., C-Maj7, E-min. A total of 133 distinct chord
tokens are found in the dataset.

Please note that a single note played is represented by a co-
occurring triple of (pitch, duration, volume). The
aforementioned tokens constitute a vocabulary of size ∼340
for our REMI encoding. On average, we need a sequence
with 5 300 tokens to represent a song.

Training and inference.
In each training epoch, we ran-
domly crop a segment of length 2 048 from each sample,
and shift the pitches of the entire segment by −6 to 6 semi-
tones randomly (this is called transposition in music) as
data augmentation. We use batch size = 4, and set the learn-
ing rate to 0.0001 for APE and 0.0002 for all SPE models.
For sineSPE, we choose the number of sines K = 5; for
convSPE, the convolutional ﬁlter size is set to be 128, 512
for the gated and ungated variants respectively.

Detailed resource usage of each model is shown in Table 5.

During inference, we employ nucleus sampling (Holtzman
et al., 2019) with p = 0.9 and softmax temperature t = 1.2.
No post-processing on enforcing the grammatical correct-
ness of the generated sequence is done.

Validation loss of the models trained on this task is listed in
Table 6. On this metric, our convSPE variant performs the

Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Table 4. Training times for LRA models (hours). Numbers in parentheses are from Tesla P100 GPUs, the rest from Tesla V100 GPUs.

ListOps Text Retrieval

Image

Performer-softmax
Performer-softmax + sineSPE
Performer-softmax + convSPE
Linear Transformer-ReLU
Linear Transformer-ReLU + sineSPE
Linear Transformer-ReLU + convSPE

1.1
4.8
(4.2) 11.7
23.2
8.9
(3.2)
0.6
6.8
2.0
18.6
15.0

1.2
2.9
21.9
0.7
2.1
19.0

4.8
5.0
5.3
4.8
5.0
5.3

Table 5. Resource usage of models trained on pop piano music
generation, on a Tesla V100 GPU with 32GB of memory. # of
epochs and time to the checkpoint with the lowest validation loss
are displayed. (ug: trained without SPE gating.)

# epochs

Time

Memory

APE
sineSPE
sineSPE (ug)
convSPE
convSPE (ug)

72
78
78
80
68

9.74 h
17.92 h
16.31 h
28.02 h
24.76 h

14.34 GB
29.80 GB
18.29 GB
30.01 GB
18.99 GB

Table 6. Validation cross-entropy for models trained for pop piano
music generation (mean and standard deviation) over all sequences.
(ug: trained without SPE gating). Trained: pos ≤ 2 048, Extrapo-
lation: 2 048 < pos ≤ 3 072.

Positions

Trained

Extrapolation

APE
sineSPE
sineSPE (ug)
convSPE
convSPE (ug)

1.721 ± 0.148
1.694 ± 0.148
1.754 ± 0.146
1.685 ± 0.151
1.733 ± 0.145

3.215 ± 0.200
2.396 ± 0.359
1.965 ± 0.170
1.932 ± 0.225
1.805 ± 0.163

best both within the trained positions and on extrapolation.

C.2. Groove Continuation

Training data. The Groove2Groove MIDI dataset11 con-
sists of accompaniments generated by the Band-in-a-Box
software (BIAB).12 We only use the training section of the
Groove2Groove MIDI dataset and perform a custom train-
ing/validation/test split such that each section contains a
unique set of BIAB styles (2 761 for training and 50 each
for validation and testing). The code necessary to download,
pre-process and split the dataset is included in the repository.

We convert each accompaniment to a trio consisting of bass,

11http://doi.org/10.5281/zenodo.3958000
12https://www.pgmusic.com/

drums and another randomly selected accompaniment track
(e.g. piano, guitar). We then perform random data augmenta-
tion by skipping measures at the beginning, dropping some
of the instruments, and transposition (pitch-shifting by −5
to +5 semitones). All randomization is done anew in each
epoch.

Data representation. We use a representation similar to
the one proposed by C´ıfka et al. (2020), but adapted to
a multi-track (multi-instrument) setting. Speciﬁcally, we
encode a piece of music as a sequence of the following types
of event tokens, each with two integer arguments:

• note on(track, pitch): Begins a new note at the

given pitch (0–127).

• note off(track, pitch): Ends the note at the

given pitch (0–127).

• time shift(beats, offset): Advances current
time by a given number of beats and then sets the offset
within the beat, given as the number of ticks from its
beginning (0–11). Maximum possible shift is (2, 0).

The track numbers range from 1 to 3, where 1 is always bass
and 2 is always drums. The vocabulary of the model then
consists of 794 tokens (3×128 note-ons, 3×128 note-offs,
24 time shifts, and 2 beginning-/end-of-sequence markers).

The main differences to the representation described in Sec-
tion C.1 are a more compact encoding of timing, no repre-
sentation of musical dynamics (for simplicity), and support
for multiple tracks (not originally proposed by C´ıfka et al.,
2020 but introduced here inspired by Donahue et al., 2019).

Training and inference. During training, each example
is pre-processed and encoded as described above and the
resulting token sequence is truncated to a length of 512. We
train each model for a total of 24 epochs.

At test time, we sample with a softmax temperature of 0.6.
We disallow sampling tokens that would result in invalid
sequences (i.e. spurious note-offs, backward time shifts) in
order to ensure that the generated sequence can be correctly
decoded.

Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Various training details. Hyperparameter tuning was
mostly performed in preliminary experiments (∼100 runs);
these were mostly done on other variants of the dataset and
with different sequence lengths (ranging from 256 to 20 k);
this includes experiments discarded due to bugs discovered
during or after training. Learning rates between 0.0001 and
0.0008 and batch sizes between 1 and 24 were considered.
For SPE, we considered both the gated and ungated vari-
ants with as many realizations as ﬁt in memory (between
16 and 64). Model selection was based on validation loss
and informal perceptual evaluation. Only a minimal attempt
at further learning rate tuning was made for the ﬁnal set of
models with length 512, which did not appear to be particu-
larly sensitive to it, and we chose to keep the initial learning
rate 0.0004, which was found to perform well in all cases.

The models included in the main document – APE,
sineSPE and convSPE – all use a batch size of 10 and
ﬁnished training in about 3 h, 5 h and 6 h, respectively, using
9.7 GB, 14.4 GB and 14.8 GB of GPU memory. The total
training time including all preliminary experiments was 852
hours.

Evaluation metrics. We use the objective metrics pro-
posed by C´ıfka et al. (2019; 2020) to measure the style
similarity between the generated continuation and the ﬁle
from which the prompt was extracted. Given two pieces
of music, each metric gathers musical event statistics of
the two pieces in histograms called style proﬁles, and then
computes the cosine similarity between them.

The two metrics used here, onset-duration and time-pitch,
differ in what kind of events they use to construct the style
proﬁle:

• The onset-duration proﬁle is deﬁned as a 2D histogram
relating note onset positions to note durations. More
precisely, for all notes a in a piece of music, it records a
tuple of the form

(start(a) mod 4, end(a) − start(a)) ∈ [0, 4) × [0, 2),

where start(a) and end(a) refer to the onset and offset
time of a in beats. The expression start(a) mod 4 then
represents the position of the note onset relative to the
current bar, since all examples in the dataset are in a 4-
beat meter. These tuples are gathered in 24×12 histogram
bins (24 for onset time and 12 for duration).

• The time-pitch proﬁle is also obtained as a 2D histogram,
this time capturing time differences and pitch differences
(intervals) between notes. The tuples it considers have the
form

(start(b) − start(a), pitch(b) − pitch(a))
∈ [0, 4) × {−20, −19, . . . , 20}, a (cid:54)= b,

where a, b is a pair of notes and pitch(·) represents the
pitch of a note as its MIDI note number (the number of
semitones from C−1). The histogram has 24 × 41 bins
(24 for time lags between 0 and 4 beats and 41 bins for
intervals between −20 and 20 semitones).

In both cases, the 2D histograms are ﬂattened to vectors
before computing cosine similarities.

D. Additional Results

D.1. Attention Visualization: Music Generation

In this section, we display attention patterns produced by
our pop piano music generation models.

Learned positional templates. We share the SPE mod-
ules across all layers of the Performer, but not across the
attention heads, resulting in 512 learned positional kernels
Pd) (number of heads × key dimensions per head. In Figure
7, we display 16 randomly picked resulting templates Pd for
both sineSPE and convSPE, trained with gating. Details
of the two variants are:

• sineSPE: We set the number of sines K = 5.
• convSPE: We use ﬁlters of size 128.

In accordance with the deﬁnition, all of the visualizations
(cid:62)
are plotted with the equation Pd = QdK
d , which we never
need to explicitly compute for linear transformers. From
Figure 7, we can observe that sineSPE learns to exploit a
wide range of frequencies, and that convSPE is effective
within small query-key offsets corresponding to the ﬁlter
size, as expected.

Full Attention. Although the full attention matrix A is
not computed in linear transformers, we can still obtain
it ofﬂine by multiplying queries and keys through either
A = exp(QK(cid:62)/
D) (in the case of APE, where D is the
R) (in the
key dimensions per head), or A = exp( (cid:98)Q (cid:98)K
case of SPEs); then apply row-wise softmax operation on A
as normalization.

√
/

√

(cid:62)

Here, we present the (softmax-ed) attention matrices in the
1st, 3rd, 12th, 20th, and 24th (last) layers of all the ﬁve
models trained on pop piano music generation in Figures 8–
12. These are computed from one of each model’s random
from-scratch music generations. To examine the models’
extrapolation ability, we let them generate a sequence of
length 3 072, while the training sequence length is only
2 048. The attention matrices are lower-triangular due to
causal masking. For better visualization, the color of each
0.4/0.020.4} in the
pixel is adjusted through min{1, amn
plots, where amn ∈ [0, 1] is the softmax-ed attention score.

Figure 8 reveals a major drawback of APE: the attention of

Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

(a) sineSPE

(b) convSPE

Figure 7. Examples of Pd learned by SPE. X- and Y-axes are key and query positions respectively. Max position = 2 048.

Figure 8. Full attention matrices of APE (baseline). X- and Y-axes are key and query positions respectively. Max position = 3 072.

Head 1, Dim 3Head 3, Dim 24Head 5, Dim 41Head 7, Dim 7Head 1, Dim 22Head 3, Dim 50Head 5, Dim 48Head 7, Dim 49Head 2, Dim 33Head 4, Dim 4Head 6, Dim 22Head 8, Dim 42Head 2, Dim 47Head 4, Dim 29Head 6, Dim 49Head 8, Dim 44Head 1, Dim 13Head 3, Dim 51Head 5, Dim 24Head 7, Dim 39Head 1, Dim 60Head 3, Dim 54Head 5, Dim 46Head 7, Dim 53Head 2, Dim 13Head 4, Dim 5Head 6, Dim 22Head 8, Dim 7Head 2, Dim 28Head 4, Dim 60Head 6, Dim 43Head 8, Dim 50Layer 1, Head 1Layer 1, Head 2Layer 1, Head 3Layer 1, Head 4Layer 1, Head 5Layer 1, Head 6Layer 1, Head 7Layer 1, Head 8Layer 3, Head 1Layer 3, Head 2Layer 3, Head 3Layer 3, Head 4Layer 3, Head 5Layer 3, Head 6Layer 3, Head 7Layer 3, Head 8Layer 12, Head 1Layer 12, Head 2Layer 12, Head 3Layer 12, Head 4Layer 12, Head 5Layer 12, Head 6Layer 12, Head 7Layer 12, Head 8Layer 20, Head 1Layer 20, Head 2Layer 20, Head 3Layer 20, Head 4Layer 20, Head 5Layer 20, Head 6Layer 20, Head 7Layer 20, Head 8Layer 24, Head 1Layer 24, Head 2Layer 24, Head 3Layer 24, Head 4Layer 24, Head 5Layer 24, Head 6Layer 24, Head 7Layer 24, Head 8Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Figure 9. Full attention matrices of sineSPE (with gated SPE). Max token position = 3 072.

Figure 10. Full attention matrices of sineSPE (without SPE gating). Max token position = 3 072.

Layer 1, Head 1Layer 1, Head 2Layer 1, Head 3Layer 1, Head 4Layer 1, Head 5Layer 1, Head 6Layer 1, Head 7Layer 1, Head 8Layer 3, Head 1Layer 3, Head 2Layer 3, Head 3Layer 3, Head 4Layer 3, Head 5Layer 3, Head 6Layer 3, Head 7Layer 3, Head 8Layer 12, Head 1Layer 12, Head 2Layer 12, Head 3Layer 12, Head 4Layer 12, Head 5Layer 12, Head 6Layer 12, Head 7Layer 12, Head 8Layer 20, Head 1Layer 20, Head 2Layer 20, Head 3Layer 20, Head 4Layer 20, Head 5Layer 20, Head 6Layer 20, Head 7Layer 20, Head 8Layer 24, Head 1Layer 24, Head 2Layer 24, Head 3Layer 24, Head 4Layer 24, Head 5Layer 24, Head 6Layer 24, Head 7Layer 24, Head 8Layer 1, Head 1Layer 1, Head 2Layer 1, Head 3Layer 1, Head 4Layer 1, Head 5Layer 1, Head 6Layer 1, Head 7Layer 1, Head 8Layer 3, Head 1Layer 3, Head 2Layer 3, Head 3Layer 3, Head 4Layer 3, Head 5Layer 3, Head 6Layer 3, Head 7Layer 3, Head 8Layer 12, Head 1Layer 12, Head 2Layer 12, Head 3Layer 12, Head 4Layer 12, Head 5Layer 12, Head 6Layer 12, Head 7Layer 12, Head 8Layer 20, Head 1Layer 20, Head 2Layer 20, Head 3Layer 20, Head 4Layer 20, Head 5Layer 20, Head 6Layer 20, Head 7Layer 20, Head 8Layer 24, Head 1Layer 24, Head 2Layer 24, Head 3Layer 24, Head 4Layer 24, Head 5Layer 24, Head 6Layer 24, Head 7Layer 24, Head 8Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Figure 11. Full attention matrices of convSPE (with SPE gating, conv ﬁlter size = 128). Max token position = 3 072.

Figure 12. Full attention matrices of convSPE (without SPE gating, conv ﬁlter size = 512). Max token position = 3 072.

Layer 1, Head 1Layer 1, Head 2Layer 1, Head 3Layer 1, Head 4Layer 1, Head 5Layer 1, Head 6Layer 1, Head 7Layer 1, Head 8Layer 3, Head 1Layer 3, Head 2Layer 3, Head 3Layer 3, Head 4Layer 3, Head 5Layer 3, Head 6Layer 3, Head 7Layer 3, Head 8Layer 12, Head 1Layer 12, Head 2Layer 12, Head 3Layer 12, Head 4Layer 12, Head 5Layer 12, Head 6Layer 12, Head 7Layer 12, Head 8Layer 20, Head 1Layer 20, Head 2Layer 20, Head 3Layer 20, Head 4Layer 20, Head 5Layer 20, Head 6Layer 20, Head 7Layer 20, Head 8Layer 24, Head 1Layer 24, Head 2Layer 24, Head 3Layer 24, Head 4Layer 24, Head 5Layer 24, Head 6Layer 24, Head 7Layer 24, Head 8Layer 1, Head 1Layer 1, Head 2Layer 1, Head 3Layer 1, Head 4Layer 1, Head 5Layer 1, Head 6Layer 1, Head 7Layer 1, Head 8Layer 3, Head 1Layer 3, Head 2Layer 3, Head 3Layer 3, Head 4Layer 3, Head 5Layer 3, Head 6Layer 3, Head 7Layer 3, Head 8Layer 12, Head 1Layer 12, Head 2Layer 12, Head 3Layer 12, Head 4Layer 12, Head 5Layer 12, Head 6Layer 12, Head 7Layer 12, Head 8Layer 20, Head 1Layer 20, Head 2Layer 20, Head 3Layer 20, Head 4Layer 20, Head 5Layer 20, Head 6Layer 20, Head 7Layer 20, Head 8Layer 24, Head 1Layer 24, Head 2Layer 24, Head 3Layer 24, Head 4Layer 24, Head 5Layer 24, Head 6Layer 24, Head 7Layer 24, Head 8Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

We report the scores of the best-performing (i.e., lowest-
scoring) head of each model in Table 7. From the table,
we can notice that the PE properties of APE often deterio-
rate drastically in cases of extrapolation. On the contrary,
the scores of ungated SPE models, i.e., models in which
we enforce the incorporation of positional information in
every layer, remain remarkably consistent throughout the
positions. The evaluation here provides additional evidence
for the extrapolation capability of SPEs.

D.4. Impact of the Number R of Realizations

In the main document, we discussed how SPE asymptoti-
cally leads to the desired cross-covariance structure as R
grows to inﬁnity. In this section, we empirically study how
performance is affected by that parameter in practice. A
ﬁrst thing to highlight is that each training batch yields a
new set of realizations for the noise Zd, so that the network
sees the right attention pattern on average.

However, we may wonder whether how the number of real-
izations R impacts training and test performance. One can
indeed notice that R may totally be set differently during
training and inference, since it has no impact on the shape
of the actual parameters/structure of the model. For this
reason, we performed an ablation study where we use differ-
ent values for Rtrain at training time, resulting in a trained
model, and then evaluate its performance using a possibly
different value Rtest. The results are displayed in Figure 14.

We can notice that the result achieved with Rtest = Rtrain
(highlighted in bold) is consistently close to the best result
for the same Rtrain, and conversely, choosing Rtest (cid:54)= Rtrain
often leads to a poor result. In other words, training and test-
ing with the same R appears to be favorable for consistently
good performance.

Another remarkable fact is that a higher R does not seem to
imply better performance, even when Rtest = Rtrain. On the
contrary, convSPE achieved by far the highest accuracy
with R = 4. This unexpected result seems contradictory
to the fact that it means noisier attention patterns. Further
investigation is required to explain this phenomenon, but we
conjecture that this additional noise in the attention patterns
leads to increased robustness of the trained model, helping
generalization.

tokens beyond position 2 048 (the training sequence length)
seems to concentrate around 2 048 in earlier layers, rather
than paying global or local attention. Such behavior is not
seen in any of our SPE models. This potentially explains
APE’s poor generalization to long sequences suggested by
the stark increase in validation loss after position 2 048 (see
Figure 3 in the main paper, and Table 6 here).

Next, comparing Figures 9 and 10, it is obvious that gated
SPE gives the model the freedom to switch off PE in some
heads to achieve global attention (see Figure 9), whereas
the attention of ungated sineSPE (Figure 10) largely stays
periodic, which might not be always desirable. The same
can be said for convSPE (Figures 11 and 12). The gated
convSPE is able to look much further back in the middle
layers than its ungated counterpart.

D.2. Attention Visualization: CIFAR10

Figure 13 displays attention maps extracted from models
trained on the LRA CIFAR10 task. Note that these are one-
layer networks, and classiﬁcation is done by prepending
a special CLS token to the sequence of pixel values and
using the output at this ﬁrst position as input to a feed-
forward classiﬁer. Consequently, only the attention map
at this single position (which is the one we display here)
matters. (The model is therefore de facto not using self-
attention, but rather attention with a single query and many
keys. This removes the distinction between relative and
absolute positions, which might explain why trainable APE
performs better than SPE on this task.)

D.3. Evaluation of Desired PE Properties

We employ identical word probing and the associated met-
rics introduced in Wang et al. (2021) to compare the trans-
lation invariance and monotonicity properties of APE and
our SPEs. The other properties mentioned in that work,
namely symmetry and direction balance, are not evaluated
here since the attention is uni-directional in our case. The
models are also trained on pop piano music generation.

The metrics are calculated from attention matrices of each
head in the 1st layer, averaged over all possible identical-
token sequences (i.e., a sequence composed of repeated,
same tokens; there are ∼340 of them for our REMI vo-
cabulary). To eliminate the impact of applying row-wise
softmax with causal masking on the translation invariance
property, we compute the metrics on the unnormalized at-
tention matrices, i.e., A = exp(QK(cid:62)/
D) for APE, and
A = exp( (cid:98)Q (cid:98)K
R) for SPEs. Various combinations of
query positions and query-key offsets are considered to ex-
amine whether the PE properties stay consistent when we
extrapolate to longer sequences, as well as to look into their
behavior in local and long-range attention spans.

√
/

√

(cid:62)

Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Figure 13. CIFAR10 attention maps for 3 variants of Linear Transformer-ReLU: learnable APE (top), sineSPE (middle), and convSPE
(bottom). Each row displays the input image, followed by attention weights of the 8 respective heads for each pixel, with the special CLS
token as the query.

0.0000.0020.0040.0060.0080.0100.0000.0020.0040.0060.0080.0100.0000.0020.0040.0060.0080.0100.0000.0020.0040.0060.0080.0100.0000.0020.0040.0060.0080.0100.0000.0020.0040.0060.0080.0100.0000.0020.0040.0060.0080.0100.0120.0140.0000.0020.0040.0060.0080.0100.0120.0140.0000.0020.0040.0060.0080.0100.0120.014Supplementary Material: Relative Positional Encoding for Transformers with Linear Complexity

Table 7. Evaluation of PEs metrics. T: translation invariance, M: monotonicity (lower is better). ug: models trained without SPE gating.

Query positions
Query-key offset

APE

sineSPE

sineSPE (ug)

convSPE

convSPE (ug)

<128

T: 0.4335
M: 0.0152
T: 0.1660
M: 0.2893
T: 0.0141
M: 0.6295
T: 0.3422
M: 0.1781
T: 0.2828
M: 0.1234

0 < pos ≤ 1 024
<512

<1 024

<128

1 024 < pos ≤ 2 048
<512

T: 0.2063
M: 0.0625
T: 0.3078
M: 0.4406
T: 0.0242
M: 0.1844
T: 0.5637
M: 0.2242
T: 0.0192
M: 0.0249

T: 0.1845
M: 0.0616
T: 0.3527
M: 0.4283
T: 0.0231
M: 0.1582
T: 0.6389
M: 0.2189
T: 0.0107
M: 0.0620

T: 0.9142
M: 0.0193
T: 0.1337
M: 0.2826
T: 0.0135
M: 0.6238
T: 0.3209
M: 0.1735
T: 0.3334
M: 0.1505

T: 0.6953
M: 0.0413
T: 0.2504
M: 0.4063
T: 0.0206
M: 0.1623
T: 0.6239
M: 0.3624
T: 0.0188
M: 0.0253

<1 024

T: 0.6458
M: 0.0713
T: 0.3228
M: 0.4167
T: 0.0190
M: 0.1061
T: 0.7648
M: 0.4192
T: 0.0109
M: 0.1254

2 048 < pos ≤ 2 560 (extrapolation)
<1 024
<512
<128

T: 0.9599
M: 0.3974
T: 0.2167
M: 0.3253
T: 0.0105
M: 0.6189
T: 0.3462
M: 0.1486
T: 0.2207
M: 0.1342

T: 0.8959
M: 0.2429
T: 0.3599
M: 0.4060
T: 0.0196
M: 0.1609
T: 0.6135
M: 0.3247
T: 0.0171
M: 0.0217

T: 0.5886
M: 0.1637
T: 0.4147
M: 0.3913
T: 0.0163
M: 0.0994
T: 0.7025
M: 0.2740
T: 0.0106
M: 0.0989

(a) sineSPE

(b) convSPE

Figure 14. Accuracy of Performer-softmax with SPE on the LRA Text task, with different numbers of realizations R during training/testing.
Each value is the result of a single run. Highlighted in bold are values obtained with Rtest = Rtrain. Higher (brighter) is better.

1248163264128Rtest128644Rtrain59.7961.1762.0154.6755.4461.1055.0462.9957.3056.4857.2462.2859.9259.9362.2161.7651.7954.3262.6556.5255.6359.3053.6652.311248163264128Rtest128644Rtrain57.8657.3455.9156.3456.2258.6154.8257.6657.2055.8557.1058.3255.0457.7359.8658.0161.0360.3162.0858.6858.3157.8059.9859.71