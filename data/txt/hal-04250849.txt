WordNet Is All You Need: A Surprisingly Effective
Unsupervised Method for Graded Lexical Entailment
Joseph Renner, Pascal Denis, Rémi Gilleron

To cite this version:

Joseph Renner, Pascal Denis, Rémi Gilleron. WordNet Is All You Need: A Surprisingly Effective
Unsupervised Method for Graded Lexical Entailment. Findings of the Association for Computational
Linguistics: EMNLP 2023, 2023, Singapore, France. ￿hal-04250849￿

HAL Id: hal-04250849

https://hal.science/hal-04250849

Submitted on 20 Oct 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

WordNetIsAllYouNeed:ASurprisinglyEffectiveUnsupervisedMethodforGradedLexicalEntailmentJosephRennerandPascalDenisandRémiGilleronUniv.Lille,Inria,CNRS,CentraleLille,UMR9189-CRIStAL,F-59000,Lille,France{firstname.lastname}@inria.frAbstractWeproposeasimpleunsupervisedapproachwhichexclusivelyreliesonWordNet(Miller,1995)forpredictinggradedlexicalentailment(GLE)inEnglish.InspiredbytheseminalworkofResnik(1995),ourmethodmodelsGLEasthesumoftwoinformation-theoreticscores:asymmetricsemanticsimilarityscoreandanasymmetricspecicitylossscore,bothexploit-ingthehierarchicalsynsetstructureofWord-Net.Ourapproachalsoincludesasimpledis-ambiguationmechanismtohandlepolysemyinagivenwordpair.Despiteitssimplicity,ourmethodachievesperformanceabovethestateoftheart(Spearmanρ=0.75)onHyperLex(Vulicetal.,2017),thelargestGLEdataset,outperformingallpreviousmethods,includingspecializedwordembeddingsapproachesthatuseWordNetasweaksupervision.1IntroductionAcrucialaspectoflanguageunderstandingistheabilitytodrawinferencesbetweensentences.Inmanycases,theseinferencesaredirectlylicensedbythesemanticsofwords:e.g.,thesentenceaduckisintheroomentailsananimalisintheroomsimplybecausetheconceptofduckentailsthatofanimal.Thesecasesof(taxonomic)LexicalEntailment(LE)holdforwordswhoseextensionaldenotationsformapartialorder:thatis,thesetofducksisincludedinthesetofbirdswhichisitselfincludedinthesetofanimals.ThetaxonomicstructureoflexicalconceptsisadeningaspectofhumansemanticmemoryandhasbeenextensivelystudiedincognitivescienceaswellasinNLPduetoitsmultiplerelatedappli-cations.Initialresearchmilestonesincludethecon-structionoftheWordNetlexicaldatabase(Beck-withetal.,2021;Miller,1995),andtherstdis-tributionalapproachesforautomaticallydetectinghypernym-hyponympairs(Hearst(1992);Snowetal.(2004);Baronietal.(2012);Daganetal.XYLEScoreduckanimal5.92duckbird5.75conictdisagreement5.20competenceability4.64auralight3.69sofachair3.38buttercream2.69nounadjective0.50rhymedinner0.00Table1:Thehumanlexicalentailmentscores(0-6)forasmallsubsetoftheHyperlex(Vulicetal.,2017)dataset.Eachrowshouldbereadas:XentailsYtoadegreeofLEscore.(2013)interalia).Morerecently,animportantstrandofresearchhasledtothedevelopmentofwordrepresentationmodelsthatareabletogeomet-ricallyexpressasymmetricrelationslikeLEintheembeddingspace(RollerandErk,2016;VilnisandMcCallum,2015;NickelandKiela,2017).InspiredbythepioneeringworksofRosch(1975)andKampandPartee(1995),Vulicetal.(2017)havechallengedthetraditionalviewthatLEisabinaryrelation,showingthatitisinsteadagradedrelation,basedonhumanjudgements(i.e.,XentailsYtoacertaindegree).TheconcomitantreleaseofHyperlex,1adatasetofEnglishwordpairsscoredbyhumansfortheLErelation,hasspurrednewresearchintodevelopingmodelsforpredictingGradedLexicalEntailment(GLE).AsmallsubsetofthedatasetispresentedinTable1.Anintriguingresearchquestioniswhetherexist-inghand-craftedlexicalhierarchieslikeWordNetareindeedabletocaptureGLE.Preliminaryex-perimentsbyVulicetal.(2017);VulicandMrksic(2017)reportlargelynegativeresults:theirbestWordNet-onlybasedsystemachievesamere0.234Spearmancorrelationscorewithhumanjudgments1https://github.com/cambridgeltl/hyperlexfromHyperlex.Thesepoorperformanceresultsareblamedonthebinarycodingofthehypernym-hyponymrelationinWordNet.YetLEAR(VulicandMrksic,2017),thebestGLEsystemtodateachievinga0.682Spearmancorrelationscore,usesWordNetasasourceofconstraintsforspecializingstaticwordembeddingmodelstothetask.AsstaticwordembeddingsaloneachievepoorperformanceforGLE,thequestionofthecontributionofWord-NetintheLEARimprovedperformanceremainsopen.Inthispaper,weproposeasimplemethodthatdirectlyandsolelyexploitstheinternalstructureofWordNettopredictGLE.OurapproachreliesonIn-formationContent(IC),acontinuousinformation-theoreticmeasureintroducedinResnik(1995)tomodelsemanticsimilarityinWordNet.Specically,weproposetomodelGLEasatrade-offbetweenasymmetricsemanticsimilarityscoreandanasym-metricspecicitylossscore,bothofwhicharedenedintermsofIC.Ourmethodiscompletedwithadisambiguationmechanismtoaddressthefactthat(G)LEissense,ratherthanwordspecic,andisthereforesensitivetopolysemy,anissuethathasbeenlargelyoverlookedinpreviouswork:e.g.,thenounplantentailsbuildingonlyinitsworkingplantsense,andnotinitsbotanicalsense.Thissim-plemethodachievesa0.744Spearmancorrelationscorewithhumanjudgements,outperformingallprevioussystems,includingspecializedwordem-beddingsmethodsandsupervisedmodels,aswellassystemsbasedoncontextuallanguagemodels.Tosum-up,ourmaincontributionsarethreefold.First,weshowthattheinternalstructureofWord-Net,asrevealedbyinformation-theoreticmeasuresandcompletedbyadisambiguationmechanism,isareliablepredictorofthegradednatureofLE.Sec-ond,oursimpleWordNet-onlybasedapproachpro-videsanewstate-of-the-artforGLE,outperformingpreviousmethodsthatspecializewordembeddingstotheLEtaskusingWordNetasweaksupervision.Third,weprovideadetailedanalysisofourmethodshowingtheroleofthetwoinformation-theoretictermsandtheimportanceofsensedisambiguation.Wealsopresentasimpliedversionofourscor-ingfunctionwithoutanyfrequencyinformationinthecomputationofIC,whichfurtherimprovesthecorrelationscore(0.753),thusemphasizingthesin-gularimportanceofWordnethierarchicalstructureforGLE.2ProposedMethodGivena(n)(ordered)pairofwords(X,Y),instan-tiatingapairoflatent(i.e.,unknown)concepts(sX,sY),weaimtopredictascoregle(X,Y)in-dicatingtowhatdegreesXentailssY.Specically,weproposetocomputethescoregle(X,Y)asthesumoftwoterms:gle(X,Y)=Sim(ˆsX,ˆsY)+SpecLoss(ˆsX,ˆsY)(1)whereˆsXandˆsYareestimationsofthelatentcon-ceptssXandsY.ThersttermSim(ˆsX,ˆsY)standsfora(symmetric)semanticsimilarityfunc-tion,capturingthefactthatLErequiresconceptstobesemanticallyclose.Thesecond(asymmet-ric)termSpecLoss(ˆsX,ˆsY)encodesanotherim-portantaspectofLE,namelythefactthatthereisgenerallyalossofspecicityincurredbyusingˆsX(e.g.,dog)insteadofˆsY(e.g.,animal),asthesetdenotationofˆsXisincludedinthatofˆsY.2WhilethegeneralideaofmodelingGLEasatrade-offbetweenasimilaritytermandaspeci-citytermisalreadypresentinVulicetal.(2017),theoriginalityofourapproachistoexclusivelydenethesetermsusingthehierarchicalstructureofWordNet,alexicalsemanticgraphmadeupofwordsenses(akasynsets)andrelationsbetweenthesesynsets.Thisstructureisaccessedthroughinformation-theoreticmeasuresthatwedenenow.InformationContent(IC)Atrstglance,Word-NetmightappearinadequatetomodelGLEbe-causeitencodesthehypernym-hyponymrelationasabinaryrelation.Butthisclaimisobliviousoftwomainfacts.First,WordNethassomebuilt-ingradednessasitmodelsthehypernym-hyponymrelationasatransitiverelation.Second,thebinarynatureofthetaxonomiclinksinWordNetcanbeeasilybypassedbyresortingtothenotionofIC.Thisinformation-theoreticalnotionprovidesacon-tinuousvalueforsynsetsbyfullyexploitingthetreestructureassociatedwiththehypernym-hyponymrelation.FollowingShannon(1951),Resnik(1995)proposestoquantifytheinformationcontent(akaself-entropy)ofeachlexicalconceptsasthelogofitsinverseprobabilitybyIC(s)=log(1/P(s)).WhileonecansimplyestimateP(s)viathewordfrequenciesassociatedwithsinalargetextcorpus,thecrucialinnovationofResnik(1995)wastouse2Synonymsareanobviousexception,astheytriviallyen-taileachotherwhilehavingthesamedenotationhencespeci-city.thetaxonomictreestructureofWordNetinthisestimation.Specically,P(s)isestimatedasP(s)=h∈Hypo(s)wc(h)kwc(k)(2)wherewc(s)isthewordcountforsynsetsinalargecorpus3(inourcase,Wikipedia),Hypo(s)denotesthesetofallhyponymdescendantsofs(sincluded),andkstandsoverallsynsetsinWord-Net.ByfullyexploitingthehierarchicalstructureofWordNet,thenotionofICintuitivelycapturesthemonotonicrelationbetweenthegenerality(resp.specicity)ofconcepts,asmeasuredbytheirheight(resp.depth)inthetaxonomy,andtheirinforma-tiveness.SimilarityWedeneSimastheIC-basedsimi-laritymeasureintroducedinLin(1998).Thesim-ilaritybetweentwosynsetsˆsXandˆsYisdenedastheratiobetweentheinformationsharedbythetwoconcepts,modeledbytheICvalueoftheirleastcommonsubsumernode(denotedaslcsbe-low),andtheinformationneededtofullydescribethetwoconcepts,modeledasthesumoftheirICs,leadingtoSim(ˆsX,ˆsY)=2IC(lcs(ˆsX,ˆsY))IC(ˆsX)+IC(ˆsY).(3)SpecicityLossTheabovesimilaritymeasureisarguablyapoorpredictorofGLEifusedalone.Thismeasurewillassignhighscorestoco-hyponyms(e.g.,catanddog)andequalscorestothesamehypernym-hyponympairwhatevertheorder.Wethereforeneedtocomplementthismea-surewithanother,asymmetricmeasurethatisabletoquantifythefactthattheentailedconceptistypicallylessinformative.Forthis,wedenethespecitylossbySpecLoss(ˆsX,ˆsY)=1−IC(ˆsY)IC(ˆsX).(4)Thisfunctionreturnsvaluescloserto1.0whentheˆsXismorespecicthanˆsYandlower(possiblynegative)valueswhenˆsYismorespecicthanˆsX.Theexampleofco-hyponymsshowstheimpor-tanceofthetrade-offbetweenthetwoscores.In-deed,whilethesimilarityismaximized,thespeci-citylossisminimizedasbothsynsetshavesimilar3wc(s)istheoccurrencecountofallwordsassociatedwithsinWordNet,whereawordcountisnormalizedbyitstotalnumberofsynsets.ICvalues,resultinginasumthatindicatesrela-tivelylowGLEstrength.Similarly,whenˆsXisahypernymofˆsY,thesimilarityscorewillbehighbutthespecicityscorewillbelow(evennegative)andreducethesumtoamoreappropriatescore.SynsetDisambiguationTurningtotheissueofestimatingthelatentsynsetssXandsY,wepro-posetojointlyselectapairofsynsetswithˆsX,ˆsY=argmaxsX∈S(X),sY∈S(Y)Sim(sX,sY)(5)whereS(X)andS(Y)denotethesetofpossiblesynsetsforXandY,respectively.Thatis,weselectthepairofsynsetswiththemaximumsimi-larityvalue.Forexample,giventhewordsplantandbuilding,thismethodshouldhopefullyselectthesynsetcorrespondingtoplantasaworkingplant,notthesynsetcorrespondingitsbotanicalsense.Wehypothesizethathumansimplicitlyper-formsuchjointsenseselectionwhenaskedtoscoretherelationbetweenplantandbuilding.3ExperimentsThissectionpresentsourexperimentalframeworkandresultsofourapproachagainstvariousbase-linesandcompetingsystems.43.1DatasetandSettingsOurevaluationdatasetistheHyperlexdataset(Vulicetal.,2017),whichcontains2616Englishwordpairs(2163nounpairsand453verbpairs).ExtractedfromWordNet,thepairsfromthedatasetwerescoredona0-6scalebyhumansubjectsbasedontheprompt"TowhatdegreeisXatypeofY?".5ScoresofthedifferentsystemsarecomparedusingSpearman’sρcorrelation(Spearman,1904).Asourmethodisfullyunsupervised,wecanevaluateitandothercompetingunsupervisedmethodsandbaselinesontheentireHyperlexdataset.Forensuringfaircomparisonwithsupervisedcompetitors,wealsoreporttheperformanceofourmethodonspecictestsubsetsofHyperlex.Specif-ically,werelyonthetwotestsubsetsprovidedbytheHyperlexauthors:arandomsubset(25%ofthepairs)andatrain/validation/testsplitwithoutany4Ourcodeanddataarepubliclyavailableat:https://gitlab.inria.fr/magnet/GLE_emnlp.5ItisimportanttonotetheuseofWordNetincreatingHyperlexwasrestrictedtowordpairselection,sonostructuralinformationfromWordNethasinuencedthehumanscores.lexicaloverlap(seeVulicetal.(2017)formoredetails).Finally,notethatweuseatextdumpofWikipediaforcountingwordoccurrencesforICcalculationandfrequencybaselines.3.2UnsupervisedSystemsStaticWordEmbeddingsandWordNetBase-linesOurbaselinesystemsaretakenorinspiredfromVulicetal.(2017).TheseincludeacosinesimilarityfunctionbasedonWord2Vec(Mikolovetal.,2013)andthebestWordNet-onlymethodre-portedinVulicetal.(2017),usingtheWu-Palmersimilarity(WuandPalmer,1994).Finally,Vulicetal.(2017)introduceastrongbaseline(ρscoreof0.279)thatcombinesaspecicityterm,denedintermsofaconceptfrequencyratio(i.e.,1−wc(X)wc(Y)forawordpair(X,Y)),andaWord2Veccosinesimilaritytermactingasathreshold.6Weproposeavariationofthisapproach,byinsteadsummingtheWord2Vecvectorcosinesimilarityandthecon-ceptfrequencyratio.Recallthatstaticembeddingscollapseallwordsenses,thuspreventtheuseofdisambiguationtechniqueinthesemethods.CLM-basedMethodsThesuccessofcontex-tuallanguagemodels(CLM)onmanytasksledustostudytheirusageforGLE.Wetestedsev-eraltechniquesofderivingstaticrepresentationsfromcontextualrepresentationsfollowingApid-ianaki(2023).Wefoundthatthebestperform-ingonewasthemethodintroducedbyMisraetal.(2021)(calledtaxonomicverication)forthere-latedtaskofgradedtypicality;inthiscase,themethodusesaGPT-2-XL(Radfordetal.,2019)pretrainedmodel.7Inthisapproach,taxonomicsentencesoftheform"A(n)Xisa(n)Y"arescoredbythemodel,calculatingP(Y|A(n)Xisa(n)).Noticethatsuchcontextualpromptsallowforsomeimplicitjointdisambiguationofthetwowords.SpecializedStaticEmbeddingsThelastcom-petitoristhecurrentstate-of-the-artLEARsys-tem(VulicandMrksic,2017),whichisbasedonstaticembeddingsspecializedforLEthroughWordNet-derivedconstraints.OthersystemswhichalsouseWordNetconstraintsareHyper-Vec(Nguyenetal.,2017)andPoincaréEmbed-dings(NickelandKiela,2017)buttheirreported6SeeEquation(13)inVulicetal.(2017).7Theotherpretrainedmodelswetestedwerebert-baseandbert-large(Devlinetal.,2018),roberta-large(Liuetal.,2019),deberta-v3-large(Heetal.,2021),andpythia-1b(Bidermanetal.,2023).performanceislowerontheGLEtask.ComparingUnsupervisedMethodsAsshowninTable2,allthreebaselinesystemsfromVulicetal.(2017)achieveaρscorebelow0.3.OurbaselinecombiningtheconceptfrequencyratioandaWord2Veccosinesimilarityachievesa0.314ρscore.OurCLM-basedmethodachievesa0.425ρscorewhichisthebestscoreachievedsofarontheGLEtaskusingCLMs.ThebestcompetitortodateistheLEARsystemwitha0.686ρscore(takenfromVulicandMrksic(2017)).8OurWordNet-basedmethod,denotedbyWordNet-SSD,reachesa0.744ρscore.Toourknowledge,thisisthebestcorrelationscorere-portedsofaronHyperlex.Anditisindeedquiteclosetothehumaninter-annotatoragreementcor-relationscoreof0.854,whichwecantakeasanupperboundonthistask.TheseresultsstronglysuggestthatthehierarchicalstructureofWordNetprovideenoughinformationtoaccuratelymodelgradedLE,andthatpreviousWordNet-basedap-proacheshavesofarfailedatproperlyleveragingthisinformation.3.3SupervisedBaselinesandCompetingSystemsWealsocompareourmethod’sperformancetothatofthesupervisedapproachpresentedinVulicetal.(2017).Thismethodtrainsasupervisedlinearre-gressionmodelonWord2Vecembeddings.Asan-otherbaseline,wealsotrainasupervisedlinearre-gressionmodelusingBERTtokenembeddings,in-steadofWord2Vecembeddings.ResultsonthetwotestsplitsofHyperlexarepresentedinTable3.TheregressionmodelwithstaticembeddingsachievesaSpearman’sρof0.53and0.45fortherandomandlexicaltestsplits,respectively,andof0.420and0.257whenusingBERTembeddings.Onthesamesplits,ourunsupervisedmethodsignicantlyoutperformsthesesupervisedmodels,reachingρscoresof0.605and0.636,respectively.4AnalysisThissectionanalysesthedifferentcomponentsofourapproachviaseveraltargetedablationstudies.8NotethatthesysteminWangetal.(2020)isbasedontheLEARsystem,butevaluatedtheSemEval2020Englishtask2,whichisadifferent(fourth)subsetofHyperlex,achievingaSpearman’sρof0.696.Weevaluatedourmethodonthissubsetaswell,achievingaSpearman’srhoof0.741.