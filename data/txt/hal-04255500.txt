Generating robust counterfactual explanations
Victor Guyomard, Françoise Fessant, Thomas Guyet, Tassadit Bouadi,

Alexandre Termier

To cite this version:

Victor Guyomard, Françoise Fessant, Thomas Guyet, Tassadit Bouadi, Alexandre Termier. Generat-
ing robust counterfactual explanations. ECML/PKDD - European Conference on Machine Learning
and Principles and Practice of Knowledge Discovery in Databases, Sep 2023, Turin (Italie), Italy.
pp.1-16. ￿hal-04255500￿

HAL Id: hal-04255500

https://hal.science/hal-04255500

Submitted on 24 Oct 2023

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Distributed under a Creative Commons Attribution 4.0 International License

Generating robust counterfactual explanations

Victor Guyomard1,2, Fran¸coise Fessant1, Thomas Guyet3, Tassadit Bouadi2,
and Alexandre Termier2

1 Orange Innovation, Lannion, France
victor.guyomard@orange.com
2 Univ Rennes, Inria, CNRS, IRISA, Rennes, France
3 Inria, AIstroSight, France

Abstract. Counterfactual explanations have become a mainstay of the
XAI field. This particularly intuitive statement allows the user to un-
derstand what small but necessary changes would have to be made to
a given situation in order to change a model prediction. The quality of
a counterfactual depends on several criteria: realism, actionability, va-
lidity, robustness, etc. In this paper, we are interested in the notion of
robustness of a counterfactual. More precisely, we focus on robustness
to counterfactual input changes. This form of robustness is particularly
challenging as it involves a trade-off between the robustness of the coun-
terfactual and the proximity with the example to explain. We propose
a new framework, CROCO, that generates robust counterfactuals while
managing effectively this trade-off, and guarantees the user a minimal
robustness. An empirical evaluation on tabular datasets confirms the
relevance and effectiveness of our approach.

Keywords: Counterfactual explanation · Robustness · Algorithmic re-
course

1

Introduction

The ever-increasing use of machine learning models in critical decision-making
contexts, such as health care, hiring processes or credit allocation, makes it essen-
tial to provide explanations for the individual decisions made by these models.
To this end, Wachter et al. proposed counterfactual explanation [22]. A coun-
terfactual is defined as the smallest modification of feature values that changes
the prediction of a model to a given output. The counterfactual can provide
actions (or recourse) for individuals to attain more desirable outcomes. This is
particularly important in areas where decisions made by algorithms can have
significant impacts on people’s lives such as finance, health care or criminal jus-
tice. Many methods have been proposed to generate counterfactuals, focusing
on some specific properties such as realism [14,20,7], actionability [19,16] or
sparsity [3,22,11]. According to Artelt et al. [1], many counterfactual generation
methods are vulnerable to small changes, where even a minor change in the value
of a counterfactual feature can cause the counterfactual to have a different out-
come. Such a situation may arise for example in practical implementation of the

2

Guyomard et al.

counterfactual, due to various factors such as unexpected noise, or adversarial
manipulation. As an illustration, a counterfactual may suggest to an individual
to raise its salary by 200$ to obtain a credit, but in practice, the salary is in-
creased by 199$ or 201$, potentially resulting in a negative decision (a rejected
credit) regarding the decision model. This line of discussions falls into the topic
of robustness [15,4,21,9]. To address robustness in the context of counterfactual
explanation, Pawelcyk et al. [15] introduce the notion of recourse invalidation
rate which represents the probability of obtaining a counterfactual with a dif-
ferent predicted class, when small changes (sampled from a noise distribution)
are applied to it. They presented an estimator of the recourse invalidation rate
in the context of Gaussian distributions, and also a framework (PROBE) that
guarantees the recourse invalidation rate to be no greater than a target speci-
fied by the user. A limitation of their approach is that the satisfaction of the
user condition is dependent of the estimator quality, which means that in prac-
tice, the recourse invalidation rate can be greater than the target fixed by the
user. Moreover, PROBE leads in practice to a poor trade-off management be-
tween proximity and robustness i.e the counterfactual is robust but far from
the example to explain. In this paper, we introduce a framework called CROCO
(Cost-efficient RObust COunterfactuals), which is based on a new minimization
problem inspired by PROBE [15]. Our framework introduces the novel concept
of soft recourse invalidation rate, as well as an estimator of it. It enables us
to derive an upper-bound for the recourse invalidation rate with almost certain
probability. This ensures that the user obtains a solution with a recourse invali-
dation rate lower than the predetermined target. An experimental evaluation on
different tabular datasets confirms these theoretical results, and shows that our
method better optimizes the two criteria of robustness and proximity.

2 Related work

Since Wachter et al. seminal paper [22], a variety of counterfactual explana-
tion technics have been proposed. These methods seek to enhance the quality of
counterfactuals by incorporating additional properties, such as constraining the
counterfactual to support the data distribution in order to produce realistic ex-
amples, freezing immutable features (such as race or gender), producing multiple
counterfactuals at once, or even adding causality constraints. We refer the read-
ers to Guidotti et al. [6] for a detailed review about counterfactual explanation
properties and methods. The property of robustness has been studied recently in
the context of counterfactual explanations, where the validity of a counterfactual
is determined by its ability to maintain the same predicted class in the presence
of changes. Mishra et al. [10] distinguish various types of robustness:

Robustness to model change refers to the evolution of the validity of the
counterfactual explanation when machine learning models are re-trained or
when training parameters settings are slightly modified. Rawal et al. [17] have
demonstrated that state-of-the-art counterfactual generation methods have
the tendency to produce solutions that are not robust to model retraining.

Generating robust counterfactual explanations

3

To address this problem, Ferrario and Loi [5] proposed to use counterfac-
tual data augmentation every time machine learning models are retrained.
Upadhyay et al. [18] for their part developed an adversarial training objec-
tive that produces counterfactuals that are robust regarding changes in the
training data. More specifically, they evaluated the robustness on different
types of training data shift which are data correction shift, temporal shift,
and geospatial shift. However, the counterfactuals that are generated suffer
from a much higher cost of change regarding state-of-the art counterfactual
generation methods [15]. In the context of slightly changed training settings,
Black et al. [2] achieved robust counterfactual explanations with a regular-
ization method based upon a K-Lipschitz constant.

Robustness to input perturbations refers to how counterfactuals explana-
tions are sensitive to slight input changes. According to Dominguez-Olmedo
et al. [4], a counterfactual is said robust if small changes in the example
to explain result in valid counterfactuals. They proposed an optimization
problem that applies to linear models and neural networks to generate ro-
bust counterfactuals in this context. For Artelt et al. [1] robustness means
that two examples that are close, must result in two similar counterfactuals.
To address this issue they propose to solve an optimization problem that
includes a density constraint [1]. They empirically show that having a coun-
terfactual that lies in a dense area has the effect of improving the robustness.
Laugel et al. [8] pointed out that such a type of robustness issue cannot solely
be attributed to the explainer, but also arises from the decision boundary of
the classifier, thus increasing the problem complexity.

Robustness to counterfactual input changes refers to the ability of a coun-
terfactual explanation to remain valid when small feature changes are ap-
plied (two similar counterfactuals should have the same predicted class). In
this context, Pawelcyk et al. [15] presented PROBE a framework to produce
robust counterfactuals that is based on an optimization problem. This frame-
work aims to find a trade-off between two criteria that are the recourse inval-
idation rate and the proximity, i.e. the distance between the counterfactual
and the example to explain. From their side, Maragno et al. [9] introduced
an adversarial robust approach that generates counterfactuals that remain
valid in an uncertainty set, meaning that for a given example to explain,
all the solutions in the set are valid counterfactuals. This approach works
for non-differentiable model unlike PROBE. However there is no trade-off
between the recourse invalidation rate and the proximity as all the counter-
factuals in the uncertainty set are valid. In such a scenario, the robustness
constraint cannot be relaxed, then allowing the generation of counterfac-
tuals that are far from the example to explain. Our approach, CROCO, is
part of this category of methods. It is inspired by the PROBE framework,
and improves its limitations. Indeed, the major criticism that we can make
to PROBE is that the guarantees in terms of robustness that it offers to
the user are completely dependent on the quality of their estimator (i.e. the
guarantee is based on a recourse invalidation rate approximation rather than
the true recourse invalidation rate). Our method introduces a new optimiza-

4

Guyomard et al.

tion problem that is proved to induce an almost-sure upper bound on the
true recourse invalidation rate. This leads to a significant improvement in
the trade-off between the robustness of the counterfactual and the proximity
with the example to explain.

3 Problem statement

In this section, we define some notations related to the generation of counter-
factuals, and we formalize the robustness of counterfactual generation by intro-
ducing the notion of recourse invalidation rate.

3.1 Generation of counterfactuals

We consider the generation of counterfactuals for a binary classifier. Let X ⊆ Rn
represents the n-dimensional feature space. A binary classifier is a function h :
X → Y where Y = {0, 1}. We assume that the classification is obtained from a
probabilistic prediction i.e. a function f : X → [0, 1] that returns ˆp which is the
predicted probability for the class 1. Then, the predicted class is the most likely
class according to ˆp. For a given example x, h(x) = g ◦ f (x) where g : [0, 1] → Y
is a function that returns the predicted class from the probability vector. We
take g(u) = 1>t(u), where t is the decision threshold. 1>t(u) equals 1 if u > t
and 0 otherwise.

In this article, we do post-hoc counterfactual generation, meaning that f (and
thus h) are given. And for a given example to explain x ∈ X , whose decision
is h(x), we want to generate a counterfactual ˘x ∈ X . A counterfactual is a
new example close to the example to explain x, and with a different prediction,
i.e. h(˘x) ̸= h(x). If it is true that h (˘x) ̸= h (x), then ˘x is said to be valid.
A counterfactual ˘x is also seen as a change to apply to x: ˘x = x + δ where
δ ∈ Rn. Thus, a counterfactual is associated to a small change δ that modifies
the decision returned by h. Generating a counterfactual is basically solving the
following optimization problem:

min
δ

ℓ (f (x + δ) , 1 − h(x)) + λ ∥δ∥1

(1)

where ℓ : [0, 1]2 (cid:55)→ R+ quantifies the distance between the predicted probability,
f (˘x), and 1 − h(x) that is the opposite of the predicted class for example x.
For instance, Wachter et al. suggested ℓ as the L2 distance, so as to produce
counterfactuals that are close to the desired decision [22]. The other term in the
optimization problem, constraints the change δ applied to the example x to be
small.

In what follows, we will focus specifically on the generation of counterfactuals
in the case of instances that have received a negative decision (which corresponds
to instances predicted as class 0). This choice has no limitation and is motivated
by the fact that the majority of robustness methods are defined in a recourse
context [15,17,18] where the goal is to provide explanations only for negatively
predicted instances. We will also assume that the classifier f is differentiable.

Generating robust counterfactual explanations

5

3.2 Recourse invalidation rate

In order to quantify the robustness of the counterfactual to an input perturba-
tion, the notion of recourse invalidation rate has been introduced by Pawelczyk
et al. [15].

Definition 1 (Recourse invalidation rate). The recourse invalidation rate
for a counterfactual ˘x, of an example x predicted as class 0 can be expressed as:

Γ (˘x; pε) = Eε∼pϵ [1 − h (˘x + ε)]

where ε ∈ Rn is a random variable that follows a probability distribution pε.
Since h (˘x + ε) ∈ {0, 1}, it ensues Γ (˘x; pε) ∈ [0, 1].

Assuming pε is centered, then pε defines a region around a counterfactual ˘x
for similar counterfactuals ˘x + ε. Intuitively, Γ (˘x; pε) gives the rate of similar
counterfactuals that are not valid, i.e. that belong to class 0. Thus, the lower
Γ (˘x; pε), the more robust is the counterfactual. If Γ (˘x; pε) = 0, the counterfac-
tual is considered perfectly robust, given that all the perturbed counterfactuals
result in positive outcomes (i.e., there are all predicted as class 1). However, if
Γ (˘x; pε) = 1, the counterfactual is not at all considered robust, since no noisy
counterfactuals lead to positive outcomes (i.e., there are all predicted as class 0).
Figure 1 illustrates the intuition of the recourse invalidation rate. Γ (˘x; pε)
can be seen as the surface of the neighborhood that overlaps the region, split by
the decision frontier, on the side of the example. This neighborhood represents
the perturbations on the counterfactuals that we would like to accept without
changing its validity. The Figure also shows that finding a robust counterfactual
requires to make a trade-off between the robustness and the magnitude of the
change.

3.3 The PROBE framework for generating robust counterfactuals

Pawelczyk et al.[15] have developed a framework named PROBE that generates
robust counterfactuals regarding the recourse invalidation rate. It adapts the
minimization problem of equation 1 by adding a new term that enforces the
recourse invalidation rate to be under a target value Γt. This target value is
chosen by the user. More formally, generating a counterfactual relies on solving
the following minimization problem:

min
δ

max [Γ (x + δ; pε) − Γt, 0] + ℓ (f (x + δ) , 1 − h(x)) + λ ∥δ∥1

(2)

There are some difficulties with the additional constraint on recourse invalidation
rate. Indeed, the true value of Γ can not be evaluated in practice. Then, PROBE
proposes a Monte-Carlo estimator of Γ . This means that it is estimated by
computing the mean of a sample of perturbations in pε:

˜Γ (˘x; K, pε) =

1
K

K
(cid:88)

k=1

(1 − h (˘x + εk))

(3)

6

Guyomard et al.

Fig. 1. Illustration of the recourse invalidation rate with a uniform distribution pε
(dashed-red circle). The recourse invalidation rate is figured out by the area of the
region in red. In (1) the counterfactual has a low robustness and is at a low distance
from the example. In (2) the counterfactual has a medium robustness and is at a
medium distance, and in (3) the counterfactual has a perfect robustness but is far
from the example (large distance).

Fig. 2. Illustration of the potential problem with PROBE. The red region illustrates
the true recourse invalidation rate (see Figure 1) while the green region illustrates
the approximated recourse invalidation rate through the approximation of the red
region. In this case, the approximation under-estimates the red region and misleadingly
encourages finding a ˘x that would break the robustness constraint.

However, ˜Γ is non-differentiable, because h(x) = g ◦ f (x) and g(u) = 1>t.
Then, it can not be part of a loss of an optimization problem. To overcome this
limitation, the authors proposed a first-order approximation of the true recourse
invalidation rate Γ in the context of a Gaussian distribution noise pε = N (0, σI),
named ˜ΓPROBE.

Then, the optimization algorithm solves the problem in eq. 2, replacing Γ
by ˜ΓPROBE and stops when the approximation of recourse invalidation rate is
under the target value, i.e. when ˜ΓPROBE( ˘x; pε) ≤ Γt.

Thus, for a given counterfactual ˘x returned by PROBE, the user is guaranteed
that ˜ΓPROBE(˘x; pε) ≤ Γt. However, this means that the guarantee depends on
the quality of the estimator. Indeed, it is possible to generate a counterfactual

Generating robust counterfactual explanations

7

where ˜ΓPROBE(˘x; pε) ≤ Γt ≤ Γ (˘x; pε) which would then violate the user-selected
guarantee. The intuition behind this situation is depicted in Figure 2.

To sum up, PROBE has two limitations: 1) It offers users a guarantee based
on the recourse invalidation rate approximation rather than the true recourse
invalidation rate; 2) the approximation applies only for Gaussian distribution of
counterfactual perturbation. This makes the approach not applicable to dataset
with categorical attributes.

Our contribution overcomes the first limitation by introducing a new estima-
tor that is proved to induce an almost-sure upper bound on the true recourse
invalidation rate. Furthermore, our approach is independent to the noise distri-
bution, thus enabling the use of various noise distributions.

4 Our contribution

In this section, we present our method, named CROCO standing for Cost-efficient
RObust COunterfactuals. It improves the generation of robust counterfactuals
according to the recourse invalidation rate.

This method, inspired from PROBE, introduces a new robustness term to
the optimization problem presented in Equation 1. This term is based on an
upper-bound of the recourse invalidation rate.

4.1 An upper bound of the recourse invalidation rate

As it is not feasible to derive a closed-form expression of Γ without making any
assumption about the noise distribution, and given that ˜Γ is not differentiable,
our idea is to compute an upper-bound of Γ .

Let ˘x be a counterfactual for an example x ∈ X , then we define the soft

recourse invalidation rate, Θ(˘x) by:

Θ(˘x; pε) = Eε∼pε [1 − f (˘x + ε)] .

The proposition 1 states that the soft recourse invalidation rate, Θ, induces an
upper-bound of the recourse invalidation rate, Γ .

Proposition 1. 4 Let t ∈ [0, 1] be a decision threshold and ˘x be a counterfactual
for an example x ∈ X , an upper bound of the true recourse invalidation rate is
given by:

Γ (˘x; pε) ≤

Θ (˘x; pε)
(1 − t)

(4)

Similarly to Γ , Θ can not be evaluated directly. However, we can use the

following Monte-Carlo estimator, where K is the number of random samples:

˜Θ (˘x; K, pε) =

1
K

K
(cid:88)

(1 − f (˘x + εk))

k=1

(5)

4 All proofs are provided in Section A.1 of supplementary material.

8

Guyomard et al.

This quantity can be seen as the mean predicted probability for class 0, com-
puted on perturbed samples that are randomly drawn from the pϵ distribution.
The proposed estimator is close to the recourse invalidation rate estimation out-
lined in equation 3, but it differs in that it is differentiable as a composition of
differentiable functions, thus can be included in an objective function.

Moreover, the proposition 2 shows that our estimator, ˜Θ, defines an almost-
sure upper bound of the true recourse invalidation rate. This means that m+ ˜Θ
1−t
has a high probability to be an upper-bound of Γ .

Proposition 2. Let t ∈ [0, 1] be a decision threshold, pε a noise distribution, ˘x
be a counterfactual for an example x ∈ X , then an almost-sure upper-bound of
the recourse invalidation rate is given by:

(cid:32)

P

Γ (˘x; pε) ≤

m + ˜Θ (˘x; K, pε)
1 − t

(cid:33)

≥ 1 − exp (cid:0)−2m2K(cid:1)

(6)

where m > 0 and K is the number of random samples.

With a high number of random samples and a given value of m, the expo-
nential term of proposition 2 can be arbitrarily small. Then for a given value of
our estimator ˜Θ (˘x; K, pε), we have almost surely that the true recourse invali-

dation rate will be in the worst case equals to

m + ˜Θ (˘x; K, pε)
1 − t
to be lower than a given threshold ¯Γt, then we are
we enforce
almost-sure that the true recourse invalidation rate is lower than ¯Γt, i.e. that
the counterfactual is more robust than the given threshold.

m + ˜Θ (˘x; K, pε)
1 − t

. It ensues that if

Note that m ∈ R>0 is a parameter that defines the tightness of the upper-
bound. The lower m, the better the upper-bound. In return, low m requires a
higher K (i.e. more computational resource) to keep the confidence in the bound.
Section A.2 in supplementary material provides a table to choose the values of
m and K with respect to the desired level of confidence.

For instance, with K = 500 and m = 0.1, and t = 0.5, the inequation of the

proposition 2 gives:

(cid:16)

(cid:17)
Γ (˘x) ≤ 0.2 + 2 ˜Θ (˘x)

P

≥ 0.999

(7)

4.2 Generate robust counterfactuals

We propose a minimization problem for the generation of robust counterfactuals
according to the recourse invalidation rate.

Given a neighborhood distribution pε, a number of samples K, a tightness
value m > 0 and a target upper-bound ¯Γt, a counterfactual ˘x = x + δ is found
by minimizing the following objective function:

(cid:32) ˜Θ (x + δ; K, pε) + m
1 − t

min
δ

− ¯Γt

(cid:124)

(cid:123)(cid:122)
Robustness

(cid:33)2

(cid:125)

+ ℓ (f (x + δ) , 1 − h(x))
(cid:125)
(cid:123)(cid:122)
Validity

(cid:124)

+ λ ∥δ∥1
(cid:124) (cid:123)(cid:122) (cid:125)
Proximity

(8)

Generating robust counterfactual explanations

9

Algorithm 1 CROCO optimization for counterfactual generation

Input: x s.t. f (x) < t, f , λ > 0, α, ¯Γt > 0, K,pε
Output: x + δ
δ ← 0;
Compute ˜Θ (x + δ; K, pε)
while f (x + δ) < t and m+ ˜Θ(x+δ;K,pε)

> ¯Γt do

1−t

δ ← δ − α · ∇δLCROCO(x + δ; Θt, pε, λ)
Update ˜Θ (x + δ; K, pε)

▷ From equation 8

end while
Return: x + δ

The last two terms implement the classical trade-off for counterfactual gen-
eration. Indeed, the second term pushes the counterfactual class toward a class
that differs from the example class (if h(x) = 0 then we want h(˘x) = 1), while the
last term minimizes the distance between the counterfactual and the example to
explain.

The first term encourages our new estimator to be close to a target value
¯Γt, i.e. the target upper-bound of the recourse invalidation rate. This pushes to
choose a counterfactual that has an upper bound close to the objective.

Algorithm 1 describes the optimization process for CROCO. Gradient steps
are performed until the counterfactual predicted class is flipped (f (x + δ) ≥ t),
and the value of the upper-bound m+ ˜Θ(x+δ;K,pε)

1−t
CROCO has several benefits, it allows the user to generate counterfactuals
with almost surely a minimal robustness, and this agnostically to the noise dis-
tribution. Moreover, our optimization problem relies on an almost-sure upper
bound of the true recourse invalidation rate instead of relying on an approxima-
tion as Pawelcyk et al. did with PROBE [15]. Our intuition is that this will in
practice improve the trade-off between proximity and robustness.

is below the target value ¯Γt.

5 Experiments and results

We have divided our experiments into two sections. After experimentally con-
firming that our approach preserves the validity of the counterfactuals, the pur-
pose of the first section is to demonstrate empirically that CROCO provides an
effective management of the trade-off between proximity and robustness in com-
parison to PROBE. In the second section, we demonstrate experimentally that
the counterfactuals returned by CROCO exhibits a lower degree of invalidation
with respect to the user-defined target than PROBE do.

First of all, we describe the datasets that we used for evaluation, along with

the metrics we employed as well as the predictive model details.

5.1 Experimental setting

For a fair comparison, we used the CARLA library [13], which was also used for
evaluating PROBE. It contains three binary classification datasets: Adult, Give

10

Guyomard et al.

Me Some Credit (GSC), and COMPAS. These datasets contain both numerical
and categorical features. Both numerical and categorical variables are used to
train the classifier, but the counterfactuals are generated by modifying only the
numerical variables. The proportion of categorical variables for each dataset are
respectively 3/7, 1/12 and 25/40. Additional details about these datasets are
available in the section A.4 of the supplementary material. For every dataset,
the classification model, f , is the fully connected neural network implemented
in the CARLA library5. It is composed of 50 hidden layers and ReLU activation
functions.

We used for evaluation the following metrics:

Validity A counterfactual ˘x of an example x is valid if the classification model

predicts different classes for x and ˘x [11,12]. Formally:

Validity =

(cid:26) 0, if f (˘x) = f (x)
1, if f (˘x) ̸= f (x)

The validity measure lies in [0, 1]. The higher it is, the better.

Distance The distance is the L1 distance between an example, x and its coun-

terfactual, ˘x [11,22].

Distance = ∥˘x − x∥1 = ∥δ∥1

A low value indicates fewer changes of features to apply to the original ex-
ample to obtain the counterfactual. As the distance decreases, the proximity
increases. In the context of counterfactual generation, we assume that the
lower the distance, the more actionable the counterfactual, the better.
Recourse invalidation rate We used ˜Γ (see equation 3) to evaluate recourse
invalidation rate, i.e. the robustness of the counterfactual. This value indi-
cates the risk to have an invalid counterfactual in case the counterfactual
is slightly changing wrt to the automatically recommended counterfactual.
The lower, the better.
The recourse invalidation rate makes the assumption of a neighborhood rep-
resented by a distribution, pε. CROCO makes no hypothesis on this distribu-
tion but PROBE requires a Gaussian distribution. For the sake of fairness,
we use a centered Gaussian distribution with a parameterized variance σ for
the two methods.

For each dataset, we run PROBE with σ2 ∈ {0.005, 0.01, 0.015, 0.02} and
Γt ∈ {0.05, 0.10, 0.15, 0.2, 0.25, 0.3, 0.35}. Regarding the setting of CROCO, we
choose K = 500, m = 0.1, t = 0.5. λ is found through an iterative procedure
that is described in section A.5.2 of supplementary material. For each dataset, we
run CROCO with the same parameters as PROBE: σ2 ∈ {0.005, 0.01, 0.015, 0.02}
and ¯Γt ∈ {0.05, 0.10, 0.15, 0.2, 0.25, 0.3, 0.35}.

We also include the approach of Wachter et al. [22] (referred to as Wachter )
in our experiment. This counterfactual generation method establishes a baseline
for recourse invalidation rate.

5 Function carla.models.catalog.MLModelCatalog of the CARLA library.

Generating robust counterfactual explanations

11

Fig. 3. Trade-off between recourse invalidation rate and distance with Gaussian dis-
tribution noises. Each column corresponds to a dataset and each line to a value of
σ2 ∈ {0.005, 0.01, 0.015, 0.02}. In each subplot the value of σ2 is fixed. Each point of a
curve corresponds to a mean recourse invalidation rate and a mean distance for a given
target, we have target ∈ {0.05, 0.10, 0.15, 0.2, 0.25, 0.3, 0.35}. The points are connected
by target order.

In our experiments, we generated 500 counterfactuals for each dataset and
each parameterized method. We collected their recourse invalidation rate, dis-
tance and validity, that are discussed in the following.

5.2 Comparisons between PROBE and CROCO

In this section, the quality of the counterfactuals generated using CROCO, PROBE
and Watcher is compared.

First of all, Watcher and CROCO achieves a perfect validity for all datasets.
PROBE achieved a perfect validity on all datasets, except for two counterfactual

AdultCompasGSCs2= 0. 005s2= 0. 01s2= 0. 015s2= 0. 0205101520250.51.01.5010200.00.20.40.60.80.000.250.500.750.000.250.500.751.000.000.250.500.751.00Distance (L1)Recourse invalidation rate (G~)CROCOPROBEWachter12

Guyomard et al.

sets, that corresponds to the COMPAS dataset where σ2 = 0.005 and Γt = 0.3
and also the GSC dataset where σ2 = 0.02 and Γt = 0.05. As a consequence, in
the following, we focus the analysis on the trade-off between the distance and the
recourse invalidation rate. The section A.3.1 of the supplementary material con-
tains details regarding the validity obtained for each dataset, and counterfactual
sets that are generated.

Figure 3 compares Watcher, PROBE and CROCO regarding the distance
and recourse invalidation rate on the three different datasets. Each point of a
given curve corresponds to the mean recourse invalidation rate and the mean
distance that is obtained from CROCO or PROBE by fixing a target value.
Note that Watcher has only one point as it has no recourse invalidation rate
target parameter. The standard-deviation values are provided in section A.3.2
of supplementary material. Note that for a given curve, the points are linked by
order of increasing target value.

For the GSC dataset, CROCO achieves both smaller distances (higher prox-
imities) and lower recourse invalidation rates compared to PROBE, regardless
of the value of σ2. The same conclusion can be drawn for the COMPAS dataset,
except for σ2 = 0.005 where CROCO achieves smaller recourse invalidation rates
but at the cost of higher distances.

Regarding the Adult dataset, we observe that PROBE is unstable, as it can
produce solutions with higher recourse invalidation rate than the target fixed by
the user (where ˜Γ ≥ Γt). On the other hand, CROCO is stable and achieves both
smaller distances (higher proximities) and lower recourse invalidation rates. We
also noticed that on all the datasets, distance values increase when σ2 increased,
thus confirming the presence of a trade-off between the two quantities.

When solutions are closely clustered together in terms of mean distances,
both PROBE and CROCO exhibit similar standard deviation values. However,
when solutions are more widely dispersed, PROBE tends to have higher stan-
dard deviation values compared to CROCO (see section A.3.2 of supplementary
material).

We observed that for all datasets and values of σ2, PROBE and CROCO
outperform Wachter in terms of recourse invalidation rates. The only exception
is the Adult dataset when Γt = 0.35, where PROBE produces higher recourse
invalidation rates due to instability issues.

5.3 Target invalidation study

For each counterfactual that is obtained from PROBE or CROCO, we computed
the recourse invalidation rate and compared it with the targeted recourse inval-
idation rate.6 The results are provided in Figure 4. The graphics figure out the
diagonal representing the exact match between the targeted and the recourse
invalidation rate. All points that are above this diagonal correspond to coun-
terfactuals that do not achieve the robustness requested by the user. We notice
that with PROBE, the recourse invalidation rates frequently exceed the target

6 Watcher is not figured out as it does not set a target for recourse invalidation rate.

Generating robust counterfactual explanations

13

Fig. 4. Comparison between targeted recourse invalidation rate and recourse inval-
idation rate. Each column corresponds to a dataset and each line to a value of
σ2 ∈ {0.005, 0.01, 0.015, 0.02}. In each subplot, the value of σ2 is fixed. Each point
corresponds to a counterfactual, on the x-axis is presented the target recourse invali-
dation rate for the counterfactual, and on the y-axis the recourse invalidation rate that
is computed.

AdultCompasGSCs2= 0. 005s2= 0. 01s2= 0. 015s2= 0. 020.00.20.40.60.00.20.40.60.00.20.40.60.00.20.40.60.00.20.40.60.00.20.40.60.00.20.40.6Targeted recourse invalidation rateRecourse invalidation rate (G~)CROCOPROBE14

Guyomard et al.

fixed by the user. It illustrates that the approximation of Γ made by PROBE is
too loose. In contrast, for CROCO, the recourse invalidation rates are typically
lower, indicating that the user-specified target is less invalidated.

We computed the upper bound value derived in proposition 2 for each coun-

terfactual obtained from CROCO.

Figure 5 of section A.3.3 of the supplementary material illustrates the evo-
lution of the upper bound value ( m+ ˜Θ
1−t ) with regard to the recourse invalidation
rate for different values of σ2. Our analysis show that the theoretical bound is
not violated. This means that even in cases where CROCO failed to find a solu-
tion that matches the user target (i.e., where m+ ˜Θ
1−t > ¯Γt), we can still provide
the user a guarantee on the true recourse invalidation rate. This guarantee is
based on the value of ˜Θ that is obtained at the end of the optimization.

6 Conclusion

In this paper, we introduce CROCO, a novel framework for generating coun-
terfactuals that are robust to input changes. A robust method guarantees that
the slightly perturbed counterfactual is still valid. Our approach leverages a
new estimator that provides a theoretical guarantee on the true recourse inval-
idation rate of the generated counterfactuals. Through experiments comparing
CROCO to the state-of-the-art PROBE method, we demonstrate that our ap-
proach achieves a better trade-off between recourse invalidation rate and prox-
imity, while also leading to less invalidation regarding the user-specified target.
While these initial results are promising, it is necessary to evaluate CROCO on a
larger number of datasets to confirm the robustness of the performance obtained.
Moving forward, we plan to extend the capabilities of CROCO by adapting it
to handle categorical variables. Since our approach is independent to the noise
distribution, it seems reasonably possible to generate robust counterfactuals for
data with both numerical and categorical variables. CROCO is implemented in
the CARLA framework and will be soon available for practical usage.

Generating robust counterfactual explanations

15

References

1. Artelt, A., Vaquet, V., Velioglu, R., Hinder, F., Brinkrolf, J., Schilling, M., Ham-
mer, B.: Evaluating robustness of counterfactual explanations. In: Proceedings of
the Symposium Series on Computational Intelligence (SSCI). pp. 01–09. IEEE
(2021)

2. Black, E., Wang, Z., Fredrikson, M.: Consistent counterfactuals for deep models. In:
Proceedings of the International Conference on Learning Representations (ICLR).
OpenReview.net (2022)

3. Brughmans, D., Leyman, P., Martens, D.: Nice: an algorithm for nearest in-
stance counterfactual explanations. arXiv v2 (2021), https://arxiv.org/abs/
2104.07411

4. Dominguez-Olmedo, R., Karimi, A.H., Sch¨olkopf, B.: On the adversarial robustness
of causal algorithmic recourse. In: Proceedings of the 39th International Conference
on Machine Learning (ICML). vol. 162, pp. 5324–5342 (2022)

5. Ferrario, A., Loi, M.: The robustness of counterfactual explanations over time.

Access 10, 82736–82750 (2022)

6. Guidotti, R.: Counterfactual explanations and how to find them: literature review

and benchmarking. Data Mining and Knowledge Discovery pp. 1–55 (2022)

7. Guyomard, V., Fessant, F., Guyet, T.: VCNet: A self-explaining model for realis-
tic counterfactual generation. In: Proceedings of the European Conference on Ma-
chine Learning and Principles and Practice of Knowledge Discovery in Databases
(ECML/PKDD). pp. 437–453 (2022)

8. Laugel, T., Lesot, M.J., Marsala, C., Detyniecki, M.: Issues with post-hoc counter-
factual explanations: a discussion. arXiv (2019), https://arxiv.org/abs/1906.
04774

9. Maragno, D., Kurtz, J., R¨ober, T.E., Goedhart, R., Birbil, S.I., Hertog, D.d.:
Finding regions of counterfactual explanations via robust optimization (2023),
https://arxiv.org/abs/2301.11113

10. Mishra, S., Dutta, S., Long, J., Magazzeni, D.: A survey on the robustness of
feature importance and counterfactual explanations. arXiv (v2) (2023), https:
//arxiv.org/abs/2111.00358

11. Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classifiers
through diverse counterfactual explanations. In: Proceedings of the conference on
Fairness, Accountability, and Transparency (FAccT). pp. 607–617 (2020)

12. de Oliveira, R.M.B., Martens, D.: A framework and benchmarking study for coun-
terfactual generating methods on tabular data. Applied Sciences 11(16), 7274
(2021)

13. Pawelczyk, M., Bielawski, S., van den Heuvel, J., Richter, T., Kasneci, G.: CARLA:
A python library to benchmark algorithmic recourse and counterfactual expla-
nation algorithms. In: Conference on Neural Information Processing Systems
(NeurIPS) – Track on Datasets and Benchmarks. p. 17 (2021)

14. Pawelczyk, M., Broelemann, K., Kasneci, G.: Learning model-agnostic counter-
factual explanations for tabular data. In: Proceedings of The Web Conference
(WWW’20). pp. 3126–3132 (2020)

15. Pawelczyk, M., Datta, T., van-den Heuvel, J., Kasneci, G., Lakkaraju, H.: Proba-
bilistically robust recourse: Navigating the trade-offs between costs and robustness
in algorithmic recourse. In: Proceedings of the International Conference on Learn-
ing Representations (ICLR). OpenReview.net (2023)

16

Guyomard et al.

16. Poyiadzi, R., Sokol, K., Santos-Rodriguez, R., De Bie, T., Flach, P.: Face: feasible
and actionable counterfactual explanations. In: Proceedings of the AAAI/ACM
Conference on AI, Ethics, and Society. pp. 344–350 (2020)

17. Rawal, K., Kamar, E., Lakkaraju, H.: Algorithmic recourse in the wild: Under-
standing the impact of data and model shifts. arXiv v3 (2020), https://arxiv.
org/abs/2012.11788

18. Upadhyay, S., Joshi, S., Lakkaraju, H.: Towards robust and reliable algorithmic
recourse. Advances in Neural Information Processing Systems 34, 16926–16937
(2021)

19. Ustun, B., Spangher, A., Liu, Y.: Actionable recourse in linear classification.
In: Proceedings of the conference on Fairness, Accountability, and Transparency
(FAccT). pp. 10–19 (2019)

20. Van Looveren, A., Klaise, J.: Interpretable counterfactual explanations guided by
prototypes. In: Proceedings of the European Conference on Machine Learning and
Knowledge Discovery in Databases (ECML/PKDD). pp. 650–665 (2021)

21. Virgolin, M., Fracaros, S.: On the robustness of sparse counterfactual explanations

to adverse perturbations. Artificial Intelligence 316, 103840 (2023)

22. Wachter, S., Mittelstadt, B.D., Russell, C.: Counterfactual explanations without
opening the black box: Automated decisions and the GDPR. Harvard Journal of
Law and Technology 31(2), 841–887 (2018)

