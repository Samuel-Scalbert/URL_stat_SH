Designing Guided User Tasks in VR Embodied
Experiences
Hui-Yin Wu, Florent Alain Sauveur Robert, Théo Fafet, Brice Graulier,

Barthélemy Passin-Cauneau, Lucile Sassatelli, Marco Winckler

To cite this version:

Hui-Yin Wu, Florent Alain Sauveur Robert, Théo Fafet, Brice Graulier, Barthélemy Passin-Cauneau,
et al.. Designing Guided User Tasks in VR Embodied Experiences. Proceedings of the ACM on
Human-Computer Interaction , 2022, 6 (158), pp.1-24. ￿10.1145/3532208￿. ￿hal-03635452v2￿

HAL Id: hal-03635452

https://inria.hal.science/hal-03635452v2

Submitted on 2 May 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Designing Guided User Tasks in VR Embodied Experiences

HUI-YIN WU, Centre Inria d’Université Côte d’Azur, France
FLORENT ROBERT, Université Côte d’Azur, CNRS, I3S, Inria, France
THÉO FAFET, Université Côte d’Azur, Polytech, France
BRICE GRAULIER, Université Côte d’Azur, Polytech, France
BARTHELEMY PASSIN-CAUNEAU, Université Côte d’Azur, Polytech, France
LUCILE SASSATELLI, Université Côte d’Azur, CNRS, I3S, Institut Universitaire de France, France
MARCO WINCKLER, Université Côte d’Azur, CNRS, I3S, Inria, France

Fig. 1. According to Dourish [15], in the creation of interactive embodied experiences, gaps of perception
are introduced in : (1) Ontology: between the scene representation and the interpretation of the user and
designer, (2) Intersubjectivity: between the communication of the goals and task constraints from the
designer to user, and (3) Intentionality: between the user’s intentions and the designer’s interpretations of
the user experience.

Virtual reality (VR) offers extraordinary opportunities in user behavior research to study and observe how
people interact in immersive 3D environments. A major challenge of designing these 3D experiences and user
tasks, however, lies in bridging the inter-relational gaps of perception between the designer, the user, and the
3D scene. Paul Dourish identified three gaps of perception: ontology between the scene representation and
the user and designer interpretation, intersubjectivity of task communication between designer and user,
and intentionality between the user’s intentions and designer’s interpretations.

Authors’ addresses: Hui-Yin Wu, hui-yin.wu@inria.fr, Centre Inria d’Université Côte d’Azur, France; Florent Robert, florent.
robert@inria.fr, Université Côte d’Azur, CNRS, I3S, Inria, France; Théo Fafet, theo.fafet@etu.univ-cotedazur.fr, Université
Côte d’Azur, Polytech, France; Brice Graulier, brice.graulier@etu.univ-cotedazur.fr, Université Côte d’Azur, Polytech, France;
Barthelemy Passin-Cauneau, barthelemy.passin-cauneau@etu.univ-cotedazur.fr, Université Côte d’Azur, Polytech, France;
Lucile Sassatelli, lucile.sassatelli@univ-cotedazur.fr, Université Côte d’Azur, CNRS, I3S, Institut Universitaire de France,
France; Marco Winckler, marco.winckler@univ-cotedazur.fr, Université Côte d’Azur, CNRS, I3S, Inria, France.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
2573-0142/2022/6-ART158 $15.00
https://doi.org/10.1145/3532208

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158

158:2

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

We present the GUsT-3D framework for designing Guided User Tasks in embodied VR experiences, i.e.,
tasks that require the user to carry out a series of interactions guided by the constraints of the 3D scene.
GUsT-3D is implemented as a set of tools that support a 4-step workflow to (1) annotate entities in the scene
with navigation and interaction possibilities, (2) define user tasks with interactive and timing constraints, (3)
manage interactions, task validation, and user logging in real-time, and (4) conduct post-scenario analysis
through spatio-temporal queries using ontology definitions. To illustrate the diverse possibilities enabled by
our framework, we present two case studies with an indoor scene and an outdoor scene, and conducted a
formative evaluation involving six expert interviews to assess the framework and the implemented workflow.
Analysis of the responses show that the GUsT-3D framework fits well into a designer’s creative process,
providing a necessary workflow to create, manage, and understand VR embodied experiences.

CCS Concepts: • Human-centered computing → Systems and tools for interaction design; User stud-
ies; • Computing methodologies → Virtual reality; Ontology engineering.

Additional Key Words and Phrases: Embodied experiences, interactive task modeling, virtual reality, user
experience analysis

ACM Reference Format:
Hui-Yin Wu, Florent Robert, Théo Fafet, Brice Graulier, Barthelemy Passin-Cauneau, Lucile Sassatelli, and Marco
Winckler. 2022. Designing Guided User Tasks in VR Embodied Experiences. Proc. ACM Hum.-Comput. Interact.
6, EICS, Article 158 (June 2022), 24 pages. https://doi.org/10.1145/3532208

1 INTRODUCTION
The growth in graphics rendering capabilities has facilitated the adoption of Virtual Reality (VR)
in professional training and simulation scenarios as diverse as cockpit training [27], engineering
education [1], rehabilitation of disabilities [37, 38], prevention of occupational hazards [30], and
many more. On the one hand, the Designer – the team or individual that creates embodied VR
experiences – can realize strong control over the content being presented to the User of their
system. On the other hand, the equipment – headset, controllers, and external sensors – not only
offer natural and rich interactions, but also register user behavior (e.g., movement of the head and
hands) that characterize the embodied user experience.

The possibility of gaining insight into user behavior, perception, and efficacy in a task is key to
understanding the user experience (UX) and measuring the usability of an interactive system. This
has been addressed in research in user-centric design and task modeling [3]. However, in the design
of complex interactive systems, gaps of perception may appear in any pairwise relation between the
designer, user, and the system. In VR, the freedom to move, explore, and interact with an immersive
360° environment further widens these inter-relational gaps between the designer’s desired UX, the
affordances of the 3D scene in VR, and how the user intends to act and react. Dourish’s theory of
embodiment [15] identifies three major gaps of perception for embodied experiences (as depicted
in Figure 1):

• Ontology: gap between the representation of the scene and the interpretations of the user

and designer,

• Intersubjectivity: gap in understanding between the designer to the user when communicating

the task goals and constraints, and

• Intentionality: gap between the user’s intentions and the designer’s interpretations of the

UX.

Contributions: To address these gaps of perception, we propose GUsT-3D, a framework to define a
consistent set of vocabulary used throughout the design, implementation, and analysis of Guided
User Tasks (GUTasks) in a VR embodied experience. The framework enables a workflow consisting
of 4 steps: (1) the annotation of various entities in the scene, including navigation spaces, interaction

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:3

possibilities, object properties and relations, (2) the definition of GUTasks and their constraints
for completion, (3) running the GUTask scenario with real-time interaction, logging, and task
validation, and (4) analysis of user experience through the construction of spatio-temporal queries
and scene graph visualization. This workflow is implemented as a set of tools in Unity. The export
of the scenario in GUsT-3D representation contains sufficient information to re-create the entire
3D scene, with all the original geometric, interactive, and navigational properties. Conversely, a
set of GUTasks can be exploited across different scenes holding the same annotated properties.
We validate these contributions with two case studies and a formative evaluation through expert
interviews with professional Unity users. The implementations are licenced as an open toolkit 1.
The rest of the paper continues as follows: the existing work is reviewed in Section 2 with our
positioning. We then describe our framework and its implementation in Sections 3 and 4. Section 5
demonstrates the feasibility of our approach illustrated by two case studies covering an indoor
escape game scene and an outdoor road crossing scene. The findings of a formative evaluation with
six expert interviews are then presented in Section 6 to validate our approach. Finally, we discuss
the perspectives for this work in the Conclusion (Section 7).

2 RELATED WORK
We present here the existing work for creating and managing interactive user experiences in 3D
virtual environments related to the concept of Guided User Tasks. We focus on frameworks for the
creation and management of UX in 3D environments, and on ontologies for representing interactive
3D applications.

2.1 Frameworks for UX in 3D
Designing a fully-integrated workflow for the creation of VR experiences is complex, often composed
of multiple tools, as demonstrated by Vergara et al. [35] who propose such a workflow for the design
of applications in engineering education, coupled with suggestions of suitable tools for each step of
the workflow. Górski [19] specifically addressed the issue of open access to knowledge within hard-
coded industrial VR applications, and proposed a knowledge-based engineering approach where 3D
content, interaction possibilities, and logic connections between objects can be configured outside of
the VR application. This framework focuses strongly on the creation of the VR application that allows
flexible modification on the exported knowledge representation of the application, and further, the
runtime-loading of the content – whether visual or audio – from the knowledge representation. The
entity-component-system [29] has been popularly used for managing interactions of 2D interfaces,
as well as gaming applications.

Recently, Speicher et al. [32] developed a system for creating extended reality experiences (both
augmented and virtual reality) from paper prototypes. The workflow consists of hand-crafting the
scene from paper materials, capturing the scene with a 360°camera, and importing it into their
application. A “Wizard” can then add and manage interactive events and animations based on
the user’s screen-based gestures. For virtual reality training, the FIVE framework [8] includes a
relation engine to manage object types and entity relations, and a collaborative interaction engine
to manage object interactions. For augmented reality, Gottschalk et al. [20] proposed a product-
user-environment framework for e-commerce in the interior design and furniture sector, with a
specific focus on modelling product features (e.g. textures and components), user models to interact
with the product, and product configuration in the environment. Another work of interest is that
of Li et al. [23] who generate 3D training scenes for wheel chair navigation in VR. The scenes are

1The software is licenced under CeCILL number IDDN.FR.001.160035.000.S.P.2022.000.31235. More information can be
found on https://project.inria.fr/creattive3d/gust-3d/

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:4

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

defined by distance constraints between objects in the scene and constraints for the wheel chair
trajectory (e.g., distance, rotations, path width). However, in these works, there is no scene-wide
context labeling or scene understanding, and no task definition, validation, or user experience
analysis capabilities are included in the workflows.

2.2 Ontologies for creating and representing interactive 3D environments
The most widely known domain specific language for 3D content is X3D [9], formerly known as
VRML. It uses an XML-based syntax, and focuses on representing the 3D model information (e.g.
vertices, texture, material) including coordinates and animation as temporal trajectories. Flotyński
[17] reviewed similar ontologies for 3D, noting their primary purposes for 3D modeling and
not interactive 3D experiences. A number of extensions to X3D have allowed it to incorporate
contextual, multi-hierarchical, and ontological representations of scenes [28], as well as sensors and
controller input for virtual applications [2]. Buffa and Lafon [10] demonstrated its use for an online
interactive 3D warehouse application. Another extension by Flotyński et. al allow the logging of
user interactions [16] in VRML applications.

Our work is strongly influenced by ontologies for robotics and procedural content generation.
The RSG-DSL [6] was developed under the Eclipse framework in order to represent robot world
model as a scene graph, to which functional blocks can be attached to establish the robot’s workflow.
Beßler et. al proposed an ontology for modeling affordance in robotics tasks [4] coupled with the
capabilities for logical reasoning and question answering. The Scenic language [18] was developed
to generate datasets from a high-level scene description to train machine learning perception
systems such as those for autonomous driving. Plan-It [36] is a system that generates 3D indoor
scenes from a high-level scene graph definition. Liu et al. [24] introduce the idea of scene programs
that allow the generation of 3D images from layout descriptions, and conversely reason about
layouts of objects from images. For game applications, the GIGL language was recently introduced
[13] to procedurally generate game maps such as for dungeons in RPG games.

One of the first formal representations of user interactions for virtual reality was the Interactive
Cooperative Objects description proposed by Navarre et al. [26]. The work decomposes gestures
into a granular graph of states to define selection, rotation, and movement of objects for specific
interactive devices, using a chess game as case study. Vanacken [34] introduces the concept of
tasks by designing the “concept” datatype that can be attached to objects to label their interaction
possibilities. In this work task refers to an interaction technique (such as selecting an object) that is
triggered by an event, and results in a state change. More recently, a number of ontologies have
been proposed for the semantic modeling of virtual environments [14, 31] with the goal to improve
the richness of information about the virtual environment in a database for multi-agent systems.

Our positioning: While many frameworks and ontologies have excellent properties for designing
3D content, they present a number of limitations when considering embodied experiences: (1)
reasoning about the user experience within the scene is mostly limited to the geometric properties
of 3D objects [9, 28] or interaction in a single use case [20], (2) they have a strong focus on the 3D
geometry, and those that include interactions are defined at the level of the device and interface (e.g.
a mouse, controller, gesture) [2, 19, 26, 29, 34] instead of for user tasks (e.g. “take an object”, “move to
location”), and hence do not allow systematic task validation (3) there is no temporal representation
of the world and user state changes, and (4) while some works allow the generation or design
of 3D content and interactive scenes from high level descriptions [8, 13, 23, 24, 32, 36], these are
not semantically annotated environments, which firstly, do not have customizable interactive
possibilities or user task management, and secondly do not allow task and interactivity information

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:5

Table 1. Summary table for comparing related contributions including their focus (ontology or framework),
application scenarios, and other observations. A positioning summarizing limitations in existing work is
indicated for ontology and frameworks respectively. System implementations (with source code) were found
for [4, 9, 13, 20, 29, 32].

Focus

Ontology

Related work
[2, 9, 10, 16, 28]

Application scenario
Web 3D; e-commerce, other var-
ied applications

Comments
X3D and its extensions, as reviewed by [17],
primarily for lower-level model and mesh rep-
resentations
an affordance model for Q&A systems
robot world model for robotic reasoning
Primarily for procedural content generation
Positioning on ontology: existing ontologies do not provide designers with vocabulary to consider a human agent
fully immersed in complex 3D scenarios, to express a high level of interactivity and movement, and define specific tasks
for the user to carry out.

[4]
[6]
[13, 18, 24, 36]

games and learning systems

robotics

Framework

[32]

[20]
[19]
[23]

[8]

[29]

virtual and augmented reality

augmented reality; e-commerce
industrial VR applications
rehabilitation for wheelchair
users in VR
general collaborative and interac-
tive VR environments
2D interfaces; based on entity-
component system

wizard of oz system with medium fidelity pa-
per prototyping for more simple scenes and
limited interactivity

knowledge-based approach
optimization approach to generating con-
strained 3D scenes
offers high level control over entity relations
and interactions

Positioning on framework: existing frameworks are mostly designed with targeted scenarios in mind; intersubjectivity
with task definition and guidance, and post-scenario analysis of UX and intentionality are areas that have not yet been
explored together.

to be exported across other similarly designed task scenes. A full comparison of the relevant
ontologies and frameworks are summarized in Table 1.

In our framework, we set out from a knowledge-based approach to represent and manage UX.
We propose providing a set of consistent vocabulary throughout all phases of a workflow to create
an embodied experience, including the scene design, annotation, task description, logging, real-time
validation of tasks, and post-scenario UX analysis. This facilitates the smooth transition of scenes,
assets, tasks, and interpretations between different steps of a complex workflow for VR experience
creation [35]. In addition, GUTasks can be exported and reused across other scenes with the same
ontology annotations, to carry out the same interactive task in different, and even dynamically
generated, scene layouts. Finally, a GUsT-3D scene can be exported with the geometry of entities
in the scene, allowing the rebuilding of the interactive scene – with its 3D models, interactive, task
creation, and navigational properties – in Unity by re-importing the file.

3 OVERVIEW
In this paper we propose the GUsT-3D framework for creating 3D embodied experiences, which is
comprised of three components (Figure 2):

• GUsT-3D: representing the ontology of the scene, implemented in JSON. It is used to export
the annotated 3D scene, including all of the geometric properties, interactive and navigation
possibilities, and their inter-relations as a spatio-temporal scene graph. It also provides the
vocabulary set to define GUTasks and for user logging.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:6

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

• GUTasks: representing the intersubjectivity between the designer and the user, also im-
plemented in JSON using the same vocabulary defined in the GUsT-3D ontology. Allows the
definition of user tasks to be carried out in the VR scenario, their interactive and completion
constraints, and management and validation in real-time.

• Logs and query language: representing the user intentionality, implemented in LINQ
[25]. Allows the designer to construct spatio-temporal queries on any annotated object or
user in the 3D scene.

Fig. 2. 3-Component framework for designing GUTasks in 3D virtual environments based on embodiment
theory: the GUsT-3D vocabulary which is the full representation of the scene ontoloдy, the definition and
management of GUTasks communicated between the designer and user for intersubjectivity, and the logging
and query of UX to allow Designers to analyze user intentionality.

This framework is implemented as a unified toolkit in Unity. The designer realizes a 4-step
workflow for the design of embodied experiences: (1) the design of the 3D scene and annotation of
interaction and navigation properties, and calculation of the scene graph (i.e., relations between
objects), (2) the definition of GUTasks for the user to carry out, their constraints and goals, (3)
the running of the task scenario with real-time guidance, task validation, and logging, and (4)
post-scenario analysis of the scene evolution and UX through query and visualization tools. This
workflow is depicted in Figure 3.

Fig. 3. 4-Step workflow to create Guided User Tasks in 3D environments involves: design and annotation of
the 3D scene, defining GUTasks, running the task scenario, and post-scenario analysis of the user experience.
Each step of the workflow contains a number of sub-tasks that the designer may need to do.

The following section will introduce the tools of this framework and how it integrates into a

design workflow for embodied experiences.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:7

4 GUTASK WORKFLOW AND SYSTEM IMPLEMENTATION
This section details the implementation of our framework for the creation of 3D embodied ex-
periences, shown in Figure 2. We first introduce each of the three elements of the framework
corresponding to Dourish’s theory of embodiment – the GUsT-3D ontology (Section 4.1) for an-
notation and scene graph visualization of entity relations; the definition and management of
intersubjectivity (Section 4.2) through GUTasks; and the analysis of user behavior for intentionality
(Section 4.3) through query and scene graph analysis. We then summarize how this framework fits
into the workflow depicted in Figure 3.

4.1 Ontology: the GUsT-3D representation
We refer to the elements in the scene as entities, which include all objects, terrain, obstacles, and
the user. The designer defines the ontology of the 3D scene, including: (1) defining the interactive
and navigation properties and assigning them to entities in the scene, (2) defining the constraints
for interactions between entities, and (3) generating a global view of entity relations within the
scene.

4.1.1 Layers for defining and annotating entity properties. We introduce the idea of layers, which
is an object category in the scene that has specific interactive properties. Currently four types of
layers are provided in our framework: navigation, object, interactive, and environment.

Navigation layers. define the spaces in which the user can move and the movement constraints.

The three types of navigation layers are:

• ground: the navigable spaces (i.e. a contiguous space where the user can move around without

constraints), such as a room or a strip of sidewalk

• entryways such as doors and passages that connect navigable spaces, and
• obstacles such as walls that partition navigable spaces.

The designer can define new layers that inherit the ground property to create a named navigable
space. Figure 4 shows the visualization of three navigable spaces: garage, kitchen, and bedroom.

Fig. 4. This example indoor scene created in Unity has three navigable spaces: the Garage (blue), the Kitchen
(yellow), and the Bedroom (pink). A user viewpoint is shown for each navigable space.

Interactive property layers. describe the relation between two or more entities, and indicate how

these entities can interact within the 3D scene. These properties can exist :

• Between two scene objects: support objects can support other objects; placed-on objects
can be placed on another support objects; container objects can contain another object;

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:8

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

• Between an object and user: movable, indicating whether an object can be taken and moved
by the user; actable, indicating whether the object can be activated, such as a light switch
or a remote control.

The default layers (e.g., support, container, moveable) are linked to Unity scripts that manage
the outcome of an interaction. The designer can easily define new interactive layers and attach
scripts to them to enable other types of interaction.

Object layers. inherit the interactive property layers to define objects that have interactive
properties int he scene. Furniture can serve as supports or containers, Props can be moved and
Switches can be activated.

Environment layers. are used for entities that do not change the topology nor structure of the
3D scene, but through environmental parameters, allow users to change the perceptions of the
environment. For example, all scenes have a camera, which can be extended to be a perceptive agent,
such as the user. The camera is often used to represent the user’s visual range (i.e. field of view),
current position, and movement direction. Another layer is light, which is the environmental
lighting of the scene.

The flexibility of our layer approach is that objects can be assigned multiple layers, or inherit other
layers, and thus gaining their annotated properties. For example a shelf layer can be defined as
inheriting both support (e.g. placing a book on it) and container (e.g., holding small objects in
its shelves). The ground layer also inherits support, since it has to support furniture and other
objects.

Interactive constraints. The assignment of an interactive property layer only indicates an
4.1.2
interaction possibility, but certain constraints to the interaction may exist. For example, one may
wish to enforce that during the interaction, the target must be in the visual field and within reaching
distance of the user.

In order to allow such a reasoning, basic properties of object relations must be defined from
lower geometric properties. Take distance as an example. Two points p1 and p2 have an Euclidean
distance of dist(p1, p2) = (cid:112)(x1 − x2)2 + (y1 − y2)2 + (z1 − z2)2. However, for user experience, the
concept of distance can vary based on the object’s size, shape, and function. It can also depend on
what type of controller device is used. In real life, we tend to use relative terms such as near or f ar .
Thus, the definition of vocabulary for UX will need to be relative to pairs of objects and users, and
the designer needs a mechanism to flexibly redefine them.

Within GUsT-3D, the designer can redefine these constraints by directly using the pre-defined

layers. In GUsT-3D an interaction constraint is composed of:
• the (Source,T arдet) pair of the interaction, with layers.
• Criterion on under which the interaction can take place, which is a boolean evaluation on a

property for a certain value. The property can be further of three types:

(1) Individual property: a) a geometric property (i.e., position, rotation), b) the localization of

the entity (within a navigable space), and c) the layer which the entity belongs to,

(2) Relational property: a) the Euclidean distance or designer added definitions such as near
or f ar based on the Euclidean distance, b) the relative position between entities including
above, below, contains, and holdinд (e.g., in the case the user is holding an object), and c)
visibility constraints between the camera and another entity such as visible, occluded, and
f acinд.

Using this vocabulary, we can then define an interaction constraint, such as the following for

take_object:

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:9

1
2
3
4
5
6

near { " type ":" evaluation "," property ":" distance "," eval ": " <=" ," value ":2," unit ":" meter "},
take_object {

" type " : " constraint ",
" source " : " user ",
" target " : " movable ",
" criteria ":[{" near ":[" Source "," Target "]},{" visible ":[ " Source "," Target "]}]}

such that the user needs to be within two meters and have direct line of sight of a movable object
to carry out the take_object action.

These are currently limited to relations between at most two entities, but we can imagine

incorporating lambda functions to have more complex constraints.

3D Scene graph generation. Another advantage of having entities in the 3D scene annotated
4.1.3
with an ontology is to extract and represent the relations between entities as a scene graph. The
scene graph becomes a model that can be used to describe the state of the system from a given
perspective in a moment of time. To do this, we take the annotated scene, layer definitions, and the
defined interactive constraints, and calculate the scene graph in three steps:

(1) calculation of navigable spaces. Ground tiles labelled with the same location are grouped

together to define a contiguous space, as shown in Figure 4.

(2) calculation of the navigational graph based on the positioning of entryways between the

navigable spaces, and

(3) reasoning of interactive constraints between all pairs of entities to extract the interactive

relations

These are then exported as a .dot file which can be visualized as a graph where the nodes are
various entities or entity groups, and edges indicate their relations. A partial scene graph calculated
from the annotated scene can be found in Figure 5.

Fig. 5. A partial scene graph generated from the scene to show the relations between various entities, including
navigable spaces, entryways, walls and windows, movables, supports, and containers. Hard edges indicate
location relations, and dashed edges of different colours indicate interactive and navigation relations.

4.2 Intersubjectivity: defining and managing real-time GUTasks
Setting out from an annotated 3D scene, the designer must now define the target tasks for the user
to carry out, which we term Guided User Tasks (GUTasks).

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:10

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

4.2.1 GUTask definition. Based on the grammar for interactive constraints, defining a GUTask
comprises of specifying a list of constraints for the success of the task. Specifically, each constraint
is composed of a time frame, and the desired state to be achieved. Assuming the task scenario is
run from time t0 to tn, expressed in first order logic, a state can be evaluated at :

• a specific time State(tm, [params])
• at some arbitrary point during the task ∃m → State(tm, [params]), or
• at all time steps during the task ∀m < n → State(tm, [params])
Suppose we would like to define a GUTask that requires the user to place a book on the bedroom
table, assuming the book cannot move on its own and that it is not already at the target location,
we can deduce that the user has to carry out a series of actions including locating the book, taking
it, going to the bedroom, finding the table, then placing the book on the table. The definition of
this GUTask would be:

" move_book ":[

{" evaluation_time ":" endOfScenario "," State ":{" localization ":[" book "," bedroom "]}},
{" evaluation_time ":" endOfScenario "," State ":{" placed_on ":[" book "," table "]}}

]

More constraints could be added, requiring the user to carry out actions to achieve multiple states.
The GUTask definition is independent of the scene geometry: the same GUTask could be applied
to two different scenes containing entities annotated with the same layers. However, if the scene
does not contain entities with layers defined in the GUTask, then the task is not relevant to the
scene, and cannot be completed.

4.2.2 Real-time GUTask management. Our framework integrates the mechanism to record user
behavior, and manage and validate GUTasks when the scenario is run.

Logs on entities are recorded at a granularity set to n per second. Two types of logs are included:
LogO created for objects, and LogU for the user. Each launch instance of the scene creates a separate
log file, and an entry for an entity contains:

• Timestamp of the log entry,
• Name of the entity,
• Transform position, rotation, and scale of the entity in world coordinates,
• Localization of entity in the navigable space,
• Item (User only) in the possession of the user,
• Layer (Object only) assigned to the object,
• Status (Object only) of any interactive property (from Section 4.1.2). Since these can be

numerous, the designer can specify a “watchlist” of those to be logged.

The JSON schema for the user and object logs is as below:

{" title ":" ScenarioLog ",
" definitions ":{

" LogU ":{

" properties ":{

" timestamp ":{" description ":" Current timestamp "," type ": " integer "},
" name ": {" description ": " Entity name ", " type ": " string },
" transform ": {" description ":" position , rotation and scale ", " $ref ":# Vector 3"},
" item ": {" description ":" User - held items ", " type ": " array "} ,
" localization ": {" description ":" Occupied navigable space ", " type ": " string "}

}},
" LogO ": {

" properties ":{

" timestamp ":{" type ": " integer "},
" name ": {" description ": " Entity name ", " type ": " string "},
" layer ": {" description ": " Entity layer name ", " type ": " string "},
" transform ":{" description ":" position , rotation and scale ", " $ref ": "# Vector 3"},
" localization ": {" description ": " Occupied navigable space ", " type ": " string "},

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

1
2
3
4

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Designing Guided User Tasks in VR Embodied Experiences

158:11

" status ": {" description ": " Type of current interaction ", " type ": " enum "}

} }
} } } ,
" properties ": {

" StartTimestamp ": {" type ": " string "},
" UserLogs ":{ " type ": " array ", " items ":{" $ref ": "# LogU "}},
" ItemLogs ":{ " type ": " array ", " items ":{" $ref ": "# LogO "}}

} }

{

}

An example object log entry for a potted aloe vera (plant) object being picked up by the player is:

" timestamp ": 22,
" name ": " Potted Aloe Vera ",
" layer ": " Plant " ,
" transform ": {

" Position ": {"x": 3.308,"y": 0.72,"z": 2.39},
" Rotation ": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0},
" Scale ": { "x": 1.0, "y": 1.0, "z": 1.0}},

" localization ": " Balcony ",
" status ": " picking up "

18
19
20
21
22
23
24
25

1
2
3
4
5
6
7
8
9
10
11

Scene graphs can be calculated for each time step from these logs, which would allow one to
deduce fine-grained information on the experience, such as all the entities visible to the user, or
high-level changes such as an object going from being placed_on another object to being held by
the user. This is key to analyzing UX, presented in the next section.

To validate the GUTask, the desired entity state is watched by a controller script. Time passed
and goal constraints are listed to the user to aid them in targeting the correct task, and the designer
can add guidance in the form of text or path hints, which will be provided when the user takes
more than a defined amount of time to complete the task (Figure 8 right image). When the time
period limitation is exceeded, the task is marked as failed.

4.3 Intentionality: reasoning on and visualizing UX through spatio-temporal queries
The query component of the framework is designed to allow the construction of diverse queries
about the scene using the ontology vocabulary. Queries can be made on any entity or layer name.
The structure for queries under our framework is as follows:
[QueryH ead ]+

[Const r aint s]+

[Ref Point ]
UserCamera

[T ar дet ]+
object

layer

Where is

Is there

How many

Is visible

[T ar дet 2]
object

layer

In room

On top

Contained in

Support

Contained in

The query is composed of keywords from the GUsT-3D ontology, including at least the reference
point (usually the user camera), query head, and target, and potentially two fields that can further
limit the search to entities that fulfill location or interaction constraints. The result of a runtime
query can be seen in Figure 9.

Queries can be constructed on entities in real-time or on the logs post-experience. In real time,
entities matching the query are highlighted in the scene. On the logs, the designer can construct
a query on an entity, and observe its temporal evolution at each timestamp, such as localization
and interactions with other entities. This query component thus allows the designer or observer to
monitor all of the entities of interest in the 3D scene. Object and path highlighting aids the designer
in clearly defining the objects and layers related to the GUTasks, and can be used as indications to
guide the user in completing the task, thus improving intersubjectivity communication.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:12

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

4.4 Workflow and system integration in Unity
We can now combine the framework and the workflow from Figures 2 and 3 into Figure 6 to show
how each element of the framework introduced intervenes with the designer workflow, and the
flows of information between the Designer, the User, and the 3D scene.

Fig. 6. The combined framework and task model to indicate how each element of the framework intervenes
on the workflow of creating a 3D embodied experience. The arrows indicate the flow of information on the
three aspects of embodied experiences: ontology (red), intersubjectivity (blue), and intentionality (green)

This framework is realized as a set of editor interfaces in Unity. It realizes our 4-step workflow
of the design of an embodied user experience depicted in Figure 3. We detail below concretely how
the framework integrates into Unity for each step of this workflow.

4.4.1 Design / Annotate of 3D scene ontology. Scene annotation provides the first step to having a
context-aware 3D environment in which to make meaningful inferences as to where the user is,
what actions they are carrying out, and from there analyze the user perceptions and intentions.
Annotation is facilitated through the definition of layers in a configuration window (Figure 7),
indicating how layers are inherited, as well as their units and constraints. Designers can then select
one or more entities in the scene, and assign layers to the selection through a point-and-click
interface.

The GUsT-3D annotated scene can be exported in JSON format, using Unity’s built-in serialization
functions. In the exported file, each object’s geometric properties (position, rotation, and scale)
and layer information are included. We provide an interpreter that allows the recreation of the
scene from a GUsT-3D JSON file and existing 3D models. The interpreter for the JSON scene
representation is implemented as a parser in C# with direct control in the interface to load, update,
or export the current configurations.

When the scene has been fully annotated, the navigable spaces and scene graph are calculated
with an interpreter for the layer relations and the interactive constraints. The navigable spaces
are calculated based on connectivity between the ground tiles and the designated entryways. All
entity properties and relations are then encoded by color and style respectively, and exported to a
.dot file that provides a scene graph visualization as depicted in Figure 5.

4.4.2 Define GUTasks for intersubjectivity. The next step of the workflow is to establish the in-
tersubjectivity through defining GUTasks for the scenario, based on the GUsT-3D ontology. The

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:13

Fig. 7. Schema of the layers definition interface, which allows the designer to add and edit the units,
constraints, and layers either through point-and-click or importing a configuration file. The Unity interface is
shown on the two sides.

interface component in Unity for specifying GUTasks is shown in Figure 8, allowing the easy
modification of the task goal, constraints, as well as user guidance in the form of text or path
indications that can be communicated to the user at specific time points when they have difficulty
accomplishing the task.

Fig. 8. (Left) This interface allows designers to define GUTasks, add and edit task constraints, view the list
of existing tasks. The tasks can be easily loaded and exported in the GUsT-3D representation for reuse in
various scenarios. (Right) The tasks are communicated to the user in real time, and additional help text and
path guidance can be provided at a fixed time point. The timer indicates seconds past.

The GUTask list is also saved as a JSON file separate from the scene representation. They use the
same layer and annotations from GUsT-3D ontology, which means the designer can easily create
and import existing GUTask lists for an annotated 3D scene, allowing the same tasks to be reused
easily across different scenes.

4.5 Run task scenario
To run the scenario with interaction, each interactive modality (e.g., navigation, interacting with
objects) is bound to an input, such as keyboard or joystick. Collision meshes are added around
obstacles such as walls to prevent users from going through them. Interactive constraints such as
distance to interaction for an object are evaluated.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:14

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

Fig. 9. Here we show the window to construct a simple query (Section 4.3) that counts the number of props
within a near distance (3 meters) of the player camera. The two views show the highlighting in the scene
from the player viewpoint (left) and the Unity scene view from above (right). The query and highlights can
help the designer understand the real-time UX, and provide suitable guidance.

In real time, queries can be composed to search for specific scene entities. These queries to the
interface as depicted in Figure 9, are translated in LINQ queries using our C# scripts, which will
retrieve all the scene objects that fulfill the query constraints. The implementation then highlights
all potential entities of query by modifying the object material with a colored outline, which can be
used as indications to the user or designer. While the scenario is running, changes to the scene
including the user’s camera movement (moving in the scene and head rotation) are recorded,
accomplishing of the GUTasks, as well as the interactions with objects, and saved to a log file in
JSON format. Feedback is provided to the user to either guide them through the tasks, or indicate
the success of the task by adding in-scene text and drawing path line indicators.

Fig. 10. (Left) Window for querying the logs post-scenario, including loading a log file, a temporal slider, and
the descriptive text on the state at a time point. (Right) A generated scene graph of the user’s interaction
state at a specific time point, with, for example, visibility of entities and objects being held, providing insights
into UX and intentionality.

4.5.1 Analyze user experience and intentionality. The analysis of user experience, and gaining
insight into intentionality, is facilitated through the loading of the aforementioned user logs, in
which various information is then visualized. This visualization involves composing queries on
specific entities in the scene. The Query Log window allows one to load a log, and drag the time
slider to enumerate all the state changes for the specific entity. At the same time, the scene graph

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:15

is generated for each time stamp, and can be viewed directly in the log window. These elements
are shown in Figure 10.

5 CASE STUDIES
We present here two case studies – an indoor and outdoor scene – to show the wide range of
possible 3D embodied experiences that can be created with our system and its link to the framework
based on embodiment theory.

5.1 Indoor scene: navigation and object interaction
The first case study uses the indoor scene of Figure 4 with three navigable spaces – Garage, Kitchen,
and Bedroom – separated by walls, and connected by doors. In an escape game, users commonly
have to look around to find hints in the scene, collect objects into their inventory, then use these
objects to interact with other entities in the scene. We design a simple GUTask for this scene that
is similar to an escape game task. The user must (1) locate and retrieve a stack of books from
the Bedroom, and (2) navigate to the Garage and place them on the bookshelf to reveal a hidden
passage.

The annotation of the scene ontology involves the navigable spaces (ground tiles for Bedroom,
Garaдe, and Kitchen), and the assigning of object and interaction property layers to other entities
(movable such as book stacks or support like bookshelves). Multiple elements can be selected and
annotated simultaneously with the desired layers and localisation using the interfaces we designed
in Section 4.4. A previously annotated scene exported in the GUsT-3D representation can also be
loaded, which facilitates the loading of ontologies defined by other designers.

Next, we define the GUTask that facilitates the intersubjectivity communication between the
designer and user on what the user needs to do in the scene. The user starts out in the Bedroom
where the stack of books is. Assuming the task is carried out between time t0 and tn, the constraints
for correctly carrying out this GUTask described in first-order logic include :

(1) User at some point must be in the bedroom, near the stack of books, the stack of books must

be visible to the user, and the user must interact with the stack of books:
∃m → (Localisation(tm, U ser, Bedroom) ∧ N ear (tm, U ser, BookStack) ∧
V isible(tm, U ser, BookStack) ∧ Interact(tm, U ser, takeobject, BookStack))

(2) At the end, the user must be in the Garage with the stack of books on the garage bookshelf:

Localisation(tn, U ser, Garaдe) ∧ onTopO f (tn, BookStack, GaraдeBookshel f )

The navigation paths and interactions for the two constraints of the GUTask are depicted in
Figure 11 in two parts: (1) locating and retrieving the stack of books, and (2) navigating to the
Garage to place the stack of books. In the first constraint, there is also a precondition that the stack
of books must both be near to the user, and in the visual field of the camera. Through user logs, we
can observe which objects the user tries to interact with before finding the correct ones, and how
much time the user takes to navigate the environment. It is an example of how a simple GUTask
can be defined in a complex 3D scene with various interactive constraints.

Finally, we show the example analysis of the user intention using a query to locate an object

related to the GUTask, as depicted in Figure 12.

5.2 Outdoor scene: traversing a traffic crossing
The second case study involves a pedestrian crossing as depicted in Figure 13 with a road lined by
two sidewalk strips, and connected by a pedestrian crossing.

The GUTask defined for this scene is that the user must cross from Sidewalk2 to Sidewalk1
in security, involving two sub-constraints: (1) crossing through the pedestrian crossing and not

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:16

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

Fig. 11. Given the GUTask of placing a stack of books on the bookshelf in the garage, this figure depicts
the interaction and navigation paths of the user: (1) locating and retrieving the stack of books, and (2)
navigating to the garage, and placing the stack of books on the bookshelf. We use the same colour and
shape representation of entities as Figure 5. The interaction with the stack of books in constraint (1) requires
evaluating an attention constraint: the object has to be in the user’s visual field.

Fig. 12. The result of the query to locate a bookshelf in a specific location. We show both the navigation
path as visualized in the 3D scene with red indications as well as object highlighting. Below is the proposed
navigation path represented as a subset of the scene graph, using the same colour and shape representation
of entities as Figure 5.

anywhere else on the road, and (2) verifying that the crossing light is green before and during
crossing.

The first step is the annotation of the scene ontology. This scene only includes a small number
of terrain tiles for different navigable spaces (the two sidewalks, road, and crossing), as well as the
traffic light and the timing of its state changes between red and green. The resulting calculation of
navigable spaces is shown in Figure 13.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:17

Fig. 13. The second case study features a basic road crossing scene in an open space. The scene has four
navigable spaces: a road and a crossing that overlap, and two sidewalks. From left to right, this figure shows
the top view, the navigable spaces, and a viewpoint from the user camera.

The second step in the workflow is to define the GUTask to communicate intersubjectivity from
the designer to the user. The initial position of the camera is on Sidewalk2. Assuming the task is
carried out between time t0 and tn, the constraints for correctly carrying out this GUTask include :

(1) User does not at any point appear on the Road:
∀m < n → ¬Localisation(tm, U ser, Road)

(2) User must at some point look at the light when approaching the crossing from Sidewalk2:
∃m(Localisation(tm, U ser, Sidewalk2) ∧ N ear (U ser, Crossinд) ∧ LookAt(U ser, Liдht))

(3) User can only be on the crossing when the light is green:

∀m(Localisation(tm, U ser, Crossinд) → State(tm, liдht, дreen))

(4) The user should be on Sidewalk1 at the end:

Localisation(tn, U ser, Sidewalk1)

This GUTask also involves the element of user attention as an active constraint, requiring the
user to look at the traffic light while approaching the crossing to ensure a safe crossing, which in
addition to navigation, involves also an “attention path” of the user’s gaze. Thus the validation
of this task would involve a navigational and attention path as depicted in Figure 14. During the
experience, the camera position and orientation is dynamically evaluated and recorded to ensure
that all required constraints for this GUTask are met.

Through generated user logs, further insights into user intention can be drawn, such as whether
the user continued to pay attention to the light during the crossing or the variation of walking
speed on different navigable spaces.

6 EVALUATION
We assessed our framework through a formative evaluation, which was initially used in education
for instructional design [5, 7] and later extended and popularized in human-computer interactions
by Hix and Hartson [21] to describe evaluations in formative stages of design [33]. As emphasized
by Carol et al. [12], formative evaluations provide observations and recommendations that can
be immediately used to improve the design of the product or service, and refine the development
specifications. For that, a typical formative evaluation addresses questions such as: what are the
usability issues in our implementation? Do users understand the workflow? Does the system comply
with recognized usability principles? The results are typically qualitative instead of summative,
focusing on the needs of the design team including developers, designers, project managers, and
other members. Using structured and in-depth interviews, formative evaluations can assess these
aspects of our workflow [11].

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:18

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

Fig. 14. Given the GUTask of crossing the street, the previewed navigation path demands the user to approach
the point between Sidewalk1 and the Crossing. Here however, the user is required to look up and check the
status of the traffic light, thus resulting in a guided “Attention Path” constraint in order to validate the task.

With this evaluation, our primary aim is to identify gaps between the goals of our framework and
our current approach to the system implementation, which is done primarily through feedback from
expert interviews, and also collecting specific points to improve on for the implementation before
continuing to adapting the system to a professional use case. This formative evaluation allows us
to obtain feedback from professional users who would potentially play the role of Desiдner in our
workflow, and to improve our system implementation. For that purpose, we have run a series of
semi-structured interviews. The case study of the indoor scene (navigation and object interaction)
was used as illustrative support.

6.1 Participants
We recruited 6 expert users (3 male, 3 female) from a convenience sample, which is around the
recommended number of participants for this type of formative evaluation [22]. In age, 4 participants
are between 18-29, 1 participant between 30-39 years old, and 1 participant between 40-49. One
of the participants is a Master’s student, 3 participants have a Master degree and are currently
working in the domain of extended (virtual and augmented) reality, and 2 are currently conducting
a PhD in the mixed-reality domain. Overall, the average years of experience in the development
of immersive 3D applications is 3 years (min 1, max 7). Concerning the experience with software
available in the market, all six participants reported to have experience with Unity, with some
having experience in other software including Blender (5), Unreal Engine (4), Maya (1), Voxel (1),
iClone (1), Character Creator (1), 3D Exchange (1), and Tinkercad (1).

6.2 Procedure
The interviews were organized in four main steps: presentation of the study, profiling of participants,
presentation of our workflow and implementation to develop the indoor scenario, and debriefing.
We structured our interviews in a similar way proposed by Burmester et al. [11], by presenting
the usage of our application, allowing participants to freely express their positive and negative
feedback, and collecting qualitative feedback on specific points to gain concrete advice for future
versions. Due to the Covid-19 pandemic, the interviews were performed by videoconference and

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:19

lasted about an hour. For the purposes of the evaluation, data was collected anonymously and
participants have given consent to record the interviews.

The presentation of the steps to develop the indoor scenario was at the core of the interview.
At first, we have provided an image of a scene composed of 3 rooms : a kitchen, a bedroom and a
garage. Participants were then asked to assess and rate the perceived usability for performing the
following steps in the workflow in link with embodiment theory:

• S1 - Scene annotation (Ontology): involves creating new layers, and adding and annotating

an object to an existing scene

• S2 - Creating an interactive scenario (Intersubjectivity): involves defining the GUTasks that
the user should carry out in the scenario (i.e. go to the bedroom, then take the lamp placed
on the desk, go to the kitchen, and then place the lamp on the table)

• S3 - Running the scenario (Intersubjectivity): involves the user carrying out the defined

GUTasks in real-time, with logs of the scene changes and user behavior recorded

• S4 - Analysis of user experience (Intentionality): involves querying and visualizing the log

output of the scenario in order to understand the user experience

It is important to note that S1, S2, and S4 correspond to steps that require designer input in Unity
using GUsT-3D, while S3 is performed by the end-user (the person experiencing the VR scenario
and carrying out tasks in the 3D scene) and is mostly automated for the designer such that they
only make observations about how users, in real-time, perform the interactions created in the
scene.

We have created videos showing the real use of the tools with detailed information so that
participants could identify usability problems in running scenarios. The use of the videos also
allowed to perform the evaluation remotely, which was a necessary condition during the lock down
period imposed by the Covid-19 pandemic. Given the remote context of the interviews, we could
not directly observe the usage of our system implementation. Instead, we played usage videos
of the system for each step of the workflow, and we asked the participants to elaborate on what
they perceived as positive and negative at each step of the workflow. More specifically, they were
prompted to provide detailed comments with respect to: perceived usability, learning curve, efficacy,
and flexibility of the system. For steps S1, S2 and S4, we asked participants to provide a score on
a 1 (poor) to 5 (very good) Likert scale on their perceived usability, learning curve, efficacy, and
flexibility of the workflow. Additionally they could also respond “I don’t know” A video was also
played for S3, but since no designer intervention was required, participants were not asked to score
this step. Hereafter we summarize the qualitative feedback from expert participants on each step of
the workflow.

6.3 Results
In the analysis of results of the interviews, we look at the two parts: scores on various metrics for
each step, and the verbal feedback on pros and cons of our workflow. With the selective study pool,
our analysis focuses on the qualitative understanding of how our framework and workflow would
improve their design process, and the limitations it imposes.

An overview of these scored evaluations received from the expert interviews in steps S1, S2, and
S4 is summarized in Figure 15, showing the responses of each participant on each metric for the
three workflow steps. Overall, the participants found that the workflow implemented in Unity made
their design process much more efficient, and on average the responses to the individual steps were
all positive, though some more than others. Participants with more experience in 3D development
were more critical of the flexibility of the interfaces. The missing responses on flexibility are where
the participants responded “I don’t know”. On further inquiry, they indicated that they felt it

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:20

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

Fig. 15. For the three steps in the workflow, the responses of each participant on the usability, learning
curve, efficacy, and flexibility of the implementation. The step “Running the Scenario” does not have these
metrics, since it is mainly composed of the automatic processes that launch the GUTasks and record logs of
the experience. The missing responses indicate where participants responded “I don’t know”.

depended on the goal and scene they were trying to create. We also observe that the more difficult
step of creating and managing an interactive scenario presented a steeper learning curve, but with
much improved efficacy in the designer’s process.

Table 2. Participants were asked to elaborate the positive and negative points of each step of the workflow
and its implementation. Here we summarize the qualitative feedback from expert participants on each step,
with the IDs of the participants that provided each point in parenthesis.

Step

S1
Scene
annotation
(Ontology)

S2
Creating an
interactive
scenario
(Intersubjectivity)

S3
Run task
scenario
(Intersubjectivity)

S4
Analysis of
user experience
(Intentionality)

Positive feedback
- Time gain (P1,2)
- Convenient for non-developer (P2,3)
- Allows real-time testing (P2)
- Easy to learn (P1,3,4,6)
- Any new object can be added (P2,5)
- Has properties on and paths to objects (P4,5,6)
- Ontology can be used with any 3D scene (P6)
- Easy to learn (P1,2,3,5,6)
- Time gain (P2, 3, 4)
- Convenient for non-developer (P2, 3)
- Easy creation of task list (P4)
- Good management of user guidance (P5)
- Recap of all tasks is easy to understand (P1)
- All information in one place (P1)
- Easy to learn (P1, 2)
- Provides a color-coded timer (P2)
- Provides useful indications to the user (P3,5)

- Scene graph is easy to understand (P4)
- Displays information over time (P3, 5, 6)
- Information useful for various analyses (P5, 6)
- Shows user - scene interactions (P4)
- Easy to learn (P2)
- Scene graph generation is automatic (P1, 2)

Negative feedback
- Unclear path indications (P4,5,6)
- Complex syntax for query language (P2)
- Too many separate windows (P1,2,4,5)
- No way to annotate sound properties (P6)
- Can’t be used with Unreal (P6)

- Limited to tasks available in the tools (P2,5)
- No parallel tasks (P5)
- Names and labels in UI are not clear (P1,4,6)

- Unclear object names (P1,4)
- Text on the screen is hard to read (P3,6)
- Don’t know how to interact with objects (P4)
- Cannot change UI display (P2)
- Current time and time unit not displayed (P1)
- Limited visualization methods (P3,4,6)
- Lack of a global view (P4)
- Hard to read with too many items (P2,5)
- UI lacks auto-complete/autoplay (P1,4)
- Cannot export scene graph as an image (P3)
- Scene graph elements can’t be customized
(P2,5)

We also analyzed detailed qualitative feedback from the interviews, as summarized in Table 2.
Overall, participants provided more positive than negative comments for the general workflow.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:21

They also provided many suggestions for improving the usability and the user interface in all the
workflow steps. A number of excerpts of the interviews are particularly illustrative of the type of
feedback we received for different steps. On S1, suggestions from participants P1, P2, P4 and P5
mention that the interface should “Centralize different features in a single window”. P6 who works
in the domain of music, notably appreciated how the annotations with the GUsT-3D ontology “can
be used with any 3D scene or environment”, but also mentioned their needs for annotating sound in
their applications as well as for usage in other game engines such as Unreal, which are currently
not supported by our implementation. On step S2, participants commented that “The labels of the
UI tools could be more easy to understand”. P3 commented on S3 “Display tasks with color depending
on the guidance type, task goals, and times would help to better see the tasks”. Another interesting
comment on S3 from P4 was “I don’t need to code just click and select, and progress is automatically
saved”, which is a convenient feature of the implementation. As for the step S4, the participant
P5 put himself/herself in the shoes of the end-user and suggested that the designer should “Ask
the user how much guidance they want, and when’ , which definitely would help customize the
guidance to be provided by GUTasks specified in the GUsT-3D representation, and thus improve
the user experience. In addition to the suggestions, many of the negative comments focused on
properties of the user interface design, which would be easy to change. For example, in S4 the
comment “Cannot export scene graph as an image”, while we do not yet have this functionality,
the .dot file used for the scene graph is a well-known format that can be easily exported into an
image file. The main negative comment associated with S4 concerns the analysis of user interaction.
Participants P3, P4, and P6 mentioned that the interface only provided basic visualizations, where
for example, P6 suggested navigating the data not just on a temporal slider, but also spatially to
see the state and movement changes for each navigable space. Participant P4 also mentioned the
system’s “Lack of global view”. These comments suggest the strong interest and current needs for
robust and flexible solutions on analyzing and visualizing user experience metrics – an ongoing
and important challenge. Another positive remark from P1 and P2 was the workflow’s potential to
“Record and generate scene graphs automatically”.

The majority of the negative feedback focusing on the user interface, and not on the framework
itself. The results of such as formative evaluation confirm the needs of Designers for an integrated
workflow for the creation and management of interactive user experiences in VR, demonstrates
the added-value of the framework, and also provides valuable suggestions for improvements. We
can thus confidently seek the deployment of our framework in practice after addressing usability
issues.

The results of the formative evaluation have thus evaluated the anticipated use of our developed
tools and workflow on measures of usability, learnability, efficacy, and flexibility. Whilst the results
are quite positive with respect to how users perceive the system, only by deploying the system to
a real use case can we evaluate the real usability (e.g., performance with specific tasks) and user
experience, which were not addressed in this formative study.

7 CONCLUSION
In this paper we have presented a novel framework based on Dourish’s embodiment theory that
presents multiple capabilities to design, run, and analyze embodied experiences in VR. This involves
notably the development of the GUsT-3D representation to establish scene ontology, the Guided
User Tasks (GUTask) definitions to create and run user tasks, and tools for the query, analysis, and
visualization of scene and user behavior evolution. The framework is implemented and we assessed
its usage through two case studies and a formative evaluation involving six structured expert
interviews. Results show that the GUsT-3D framework and corresponding workflow implementation

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:22

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

integrate well into the Designer’s process to create, manage, and analyze VR embodied experiences
of target users.

We are planning a number of future directions for this work. To improve the system flexibility,

we are looking into:

• the creation and loading of a custom ontology that can change and adapt the user experience

in the 3D scenario,

• the definition of constraints on ranges of values or probabilistic models to represent uncer-

tainty to support the design of more diverse or complex GUTasks, and

• providing flexible query compositions to reason on varied elements of user behavior and
perception in the post-scenario analysis, with adapted visualizations that can better help
understand usability and user experience as a whole [3].

In addition, we believe the scene graph generation using the ontology opens exciting pathways
to support model-based evaluations of embodied VR scenarios. The feedback from the expert
interviews in the formative evaluation have provided valuable insights to challenges towards
designing visualization and analysis tools to better understand the embodied user experience. In the
longer term, we can envision this work in the context of managing interactive tasks for multi-user
scenarios, or in multi-agent systems. Finally, a direct step already in motion is the deployment of
this framework to a practical use case, which will allow us to observe how the workflow performs
with expert designers. This is a current and strong need in areas such as neuroscience and cognitive
science, to create serious games for rehabilitation and evaluate training efficacy.

ACKNOWLEDGMENTS
This work has been partially supported by the French National Research Agency though the ANR
CREATTIVE3D project ANR-21-CE33-00001.

REFERENCES
[1] Abdul-Hadi G Abulrub, Alex N Attridge, and Mark A Williams. 2011. Virtual reality in engineering education: The
future of creative learning. In 2011 IEEE global engineering education conference (EDUCON). IEEE, Amman, Jordan,
751–757.

[2] Johannes Behr, Patrick Dähne, and Marcus Roth. 2004. Utilizing X3D for Immersive Environments. In Proceedings
of the Ninth International Conference on 3D Web Technology (Monterey, California) (Web3D ’04). Association for
Computing Machinery, New York, NY, USA, 71–78. https://doi.org/10.1145/985040.985051

[3] Regina Bernhaupt, Célia Martinie, Philippe Palanque, and Günter Wallner. 2020. A Generic Visualization Ap-
proach Supporting Task-Based Evaluation of Usability and User Experience. In 8th International Conference
on Human-Centered Software Engineering - IFIP WG 13.2 International Working Conference, HCSE 2020
(Lecture Notes in Computer Science book series (LNCS), Vol. 12481), IFIP : International Federation for Informa-
tion Processing (Ed.). Springer, Eindhoven, Netherlands, 24–44. https://doi.org/10.1007/978-3-030-64266-2_2

[4] Daniel Beßler, Robert Porzel, Mihai Pomarlan, Michael Beetz, Rainer Malaka, and John Bateman. 2020. A Formal
Model of Affordances for Flexible Robotic Task Execution. In ECAI 2020. IOS Press, Santiago de Compostela, Spain,
2425–2432.

[5] Paul Black and Dylan Wiliam. 2009. Developing the theory of formative assessment. Educational Assessment,

Evaluation and Accountability (formerly: Journal of Personnel Evaluation in Education) 21, 1 (2009), 5–31.

[6] Sebastian Blumenthal and Herman Bruyninckx. 2014. Towards a Domain Specific Language for a Scene Graph based

Robotic World Model. arXiv:1408.0200 [cs.RO]

[7] Carol Boston. 2002. The concept of formative assessment. Practical Assessment, Research, and Evaluation 8, 1 (2002),

9.

[8] Rozenn Bouville, Valérie Gouranton, Thomas Boggini, Florian Nouviale, and Bruno Arnaldi. 2015. # FIVE: High-level
components for developing collaborative and interactive virtual environments. In 2015 IEEE 8th Workshop on Software
Engineering and Architectures for Realtime Interactive Systems (SEARIS). IEEE, 33–40.

[9] Don Brutzman and Leonard Daly. 2010. X3D: extensible 3D graphics for Web authors. Elsevier, San Francisco, CA,

USA.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

Designing Guided User Tasks in VR Embodied Experiences

158:23

[10] Michel Buffa and J-C Lafon. 2000. 3D virtual warehouse on the WEB. In 2000 IEEE Conference on Information

Visualization. An International Conference on Computer Visualization and Graphics. IEEE, London, UK, 479–484.

[11] Michael Burmester, Marcus Mast, Kilian Jäger, and Hendrik Homans. 2010. Valence method for formative evaluation of
user experience. In Proceedings of the 8th ACM conference on Designing Interactive Systems. ACM, Aarhus, Denmark,
364–367.

[12] John Carroll, MARK SINGLEY, and Mary Beth Rosson. 1992. Integrating theory development with design evalu-
ation. Behaviour & Information Technology - Behaviour & IT 11 (Sept. 1992), 247–255. https://doi.org/10.1080/
01449299208924345

[13] Tiannan Chen and Stephen Guy. 2018. GIGL: A domain specific language for procedural content generation with
grammatical representations. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital
Entertainment, Vol. 14(1). AAAI, Edmonton, Canada, 8 pages.

[14] Pierre Chevaillier, Thanh-Hai Trinh, Mukesh Barange, Pierre De Loor, Frédéric Devillers, Julien Soler, and Ronan
Querrec. 2012. Semantic modeling of virtual environments using mascaret. In 2012 5th Workshop on Software
Engineering and Architectures for Realtime Interactive Systems (SEARIS). IEEE, Costa Mesa, CA, USA, 1–8.

[15] Paul Dourish. 2004. Where the action is: the foundations of embodied interaction. MIT press, USA.
[16] Jakub Flotyński and Paweł Sobociński. 2018. Logging Interactions in explorable immersive VR/AR applications. In

2018 International Conference on 3D Immersion (IC3D). IEEE, Brussels, Belgium, 1–8.

[17] Jakub Flotyński and Krzysztof Walczak. 2017. Ontology-Based Representation and Modelling of Synthetic 3D Content:
A State-of-the-Art Review. In Computer Graphics Forum, Vol. 36(8). Wiley Online Library, Poznań, Poland, 329–353.
[18] Daniel J Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Alberto L Sangiovanni-Vincentelli, and Sanjit A
Seshia. 2019. Scenic: a language for scenario specification and scene generation. In Proceedings of the 40th ACM
SIGPLAN Conference on Programming Language Design and Implementation. Association for Computing Machinery,
New York, NY, USA, 63–78.

[19] Filip Górski. 2017. Building virtual reality applications for engineering with knowledge-based approach. Management

and Production Engineering Review 8(4) (2017), 63–73. https://doi.org/10.1515/mper-2017-0037

[20] Sebastian Gottschalk, Enes Yigitbas, Eugen Schmidt, and Gregor Engels. 2020. Model-based product configuration
in augmented reality applications. In International Conference on Human-Centred Software Engineering. Springer,
Eindhoven, The Netherlands, 84–104.

[21] Deborah Hix and H. Rex Hartson. 1993. Developing user interfaces : ensuring usability through product & process.

New York : J. Wiley. http://archive.org/details/developinguserin00hixd

[22] Harry Hochheiser Jonathan Lazar, Jinjuan Feng. 2010. Research Methods in Human-Computer Interaction - 2nd

Edition. Wiley, United States.

[23] Wanwan Li, Javier Talavera, Amilcar Gomez Samayoa, Jyh-Ming Lien, and Lap-Fai Yu. 2020. Automatic Synthesis of
Virtual Wheelchair Training Scenarios. In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE,
Atlanta, GA, USA, 539–547.

[24] Yunchao Liu, Jiajun Wu, Zheng Wu, Daniel Ritchie, William T. Freeman, and Joshua B. Tenenbaum. 2019. Learning to
Describe Scenes with Programs. In International Conference on Learning Representations. ICLR, New Orleans, USA,
13 pages. https://openreview.net/forum?id=SyNPk2R9K7

[25] Erik Meijer, Brian Beckman, and Gavin Bierman. 2006. Linq: reconciling object, relations and xml in the. net framework.
In Proceedings of the 2006 ACM SIGMOD international conference on Management of data. ACM, NY, New York,
706–706.

[26] David Navarre, Philippe A. Palanque, Rémi Bastide, Amélie Schyn, Marco Winckler, Luciana Porcher Nedel, and
Carla Maria Dal Sasso Freitas. 2005. A Formal Description of Multimodal Interaction Techniques for Immersive
Virtual Reality Applications. In Human-Computer Interaction - INTERACT 2005, IFIP TC13 International Conference
(Lecture Notes in Computer Science, Vol. 3585), Maria Francesca Costabile and Fabio Paternò (Eds.). Springer, Rome,
Italy, 170–183. https://doi.org/10.1007/11555261_17

[27] Matthias Oberhauser and Daniel Dreyer. 2017. A virtual reality flight simulator for human factors engineering.

Cognition, Technology & Work 19, 2 (2017), 263–277.

[28] Fabio Pittarello and Alessandro De Faveri. 2006. Semantic description of 3D environments: a proposal based on web
standards. In Proceedings of the eleventh international conference on 3D web technology. ACM, New York, NY, USA,
85–95.

[29] Thibault Raffaillac and Stéphane Huot. 2019. Polyphony: Programming Interfaces and Interactions with the Entity-

Component-System Model. Proceedings of the ACM on Human-Computer Interaction 3, EICS (2019), 1–22.

[30] Rafael Sacks, Jennifer Whyte, Dana Swissa, Gabriel Raviv, Wei Zhou, and Aviad Shapira. 2015. Safety by design:
dialogues between designers and builders using virtual reality. Construction Management and Economics 33, 1 (2015),
55–72.

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

158:24

H. Wu, F. Robert, T. Fafet, B. Graulier, B. Passin-Caineau, L. Sassatelli, M. Winckler

[31] Julien Saunier, Mukesh Barange, Bernard Blandin, Ronan Querrec, and Joanna Taoum. 2016. Designing adaptable
virtual reality learning environments. In Proceedings of the 2016 Virtual Reality International Conference. ACM, New
York, NY, USA, 1–4.

[32] Maximilian Speicher, Katy Lewis, and Michael Nebeling. 2021. Designers, the Stage Is Yours! Medium-Fidelity
Prototyping of Augmented & Virtual Reality Interfaces with 360theater. Proceedings of ACM Human-Computer
Interactions 5, EICS, Article 205 (May 2021), 25 pages. https://doi.org/10.1145/3461727

[33] Sandra Trösterer, Elke Beck, Fabiano Dalpiaz, Elda Paja, Paolo Giorgini, and Manfred Tscheligi. 2012. Formative
user-centered evaluation of security modeling: Results from a case study. International Journal of Secure Software
Engineering (IJSSE) 3, 1 (2012), 1–19.

[34] Lode Vanacken, Chris Raymaekers, and Karin Coninx. 2007. Introducing semantic information during conceptual
modelling of interaction for virtual environments. In Proceedings of the 2007 workshop on Multimodal interfaces in
semantic interaction. ACM, New York, NY, USA, 17–24.

[35] Diego Vergara, Manuel Pablo Rubio, and Miguel Lorenzo. 2017. On the design of virtual reality learning environments

in engineering. Multimodal technologies and interaction 1, 2 (2017), 11.

[36] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X Chang, and Daniel Ritchie. 2019. Planit: Planning and
instantiating indoor scenes with relation graph and spatial prior networks. ACM Transactions on Graphics (TOG) 38,
4 (2019), 1–15. Publisher: ACM New York, NY, USA.

[37] Haojie Wu, Daniel H Ashmead, Haley Adams, and Bobby Bodenheimer. 2018. Using Virtual Reality to Assess the
Street Crossing Behavior of Pedestrians with Simulated Macular Degeneration at a Roundabout. Frontiers in ICT 5
(2018), 27.

[38] Hui-Yin Wu, Aurélie Calabrèse, and Pierre Kornprobst. 2021. Towards accessible news reading design in virtual reality

for low vision. Multimedia Tools and Applications 80 (2021), 1–20.

Received July 2021; revised October 2022; accepted November 2022

Proc. ACM Hum.-Comput. Interact., Vol. 6, No. EICS, Article 158. Publication date: June 2022.

